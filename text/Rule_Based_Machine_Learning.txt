Rule Based Machine Learning

Contents

1 Decision tree learning

1

1.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

1.3 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.3.1 Gini impurity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.3.2 Information gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.3.3 Variance reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.4 Decision tree advantages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.5 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.6 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.6.1 Decision graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.6.2 Alternative search methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.7 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

1.8 Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

1.9 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

1.10 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

2 ID3 algorithm

9

2.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.1.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.1.2 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.1.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.1.4 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.2 The ID3 metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.2.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.2.2 Information gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.3 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.4 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.5 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.6 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

i

ii

CONTENTS

3 C4.5 algorithm

14

3.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

3.1.1 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

3.2 Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

3.3 Improvements from ID.3 algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

3.4 Improvements in C5.0/See5 algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

3.5 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

3.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

3.7 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

4 CHAID

17

4.1 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

4.2 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

4.3 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

4.4 Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

5 Random forest

19

5.1 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

5.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.2.1 Preliminaries: decision tree learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.2.2 Tree bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

5.2.3 From bagging to random forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.2.4 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.3.1 Variable importance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.3.2 Relationship to nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

5.4 Unsupervised learning with random forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

5.5 Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

5.6 Real life Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

5.7 Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

5.8 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

5.9 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

5.10 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

6 Gradient boosting

25

6.1 Informal introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

6.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

6.3 Gradient tree boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

6.3.1 Size of trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

CONTENTS

iii

6.4 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 6.4.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 6.4.2 Stochastic gradient boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 6.4.3 Number of observations in leaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.4.4 Penalize Complexity of Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6.5 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.6 Names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.7 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.8 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

7 Association rule learning

31

7.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

7.2 Useful Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

7.2.1 Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

7.2.2 Confidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

7.2.3 Lift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

7.2.4 Conviction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

7.3 Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

7.4 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

7.5 Alternative measures of interestingness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

7.6 Statistically sound associations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

7.7 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

7.7.1 Apriori algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

7.7.2 Eclat algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

7.7.3 FP-growth algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

7.7.4 Others . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

7.8 Lore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

7.9 Other types of association mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

7.10 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

7.11 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

7.12 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

7.12.1 Bibliographies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

8 Apriori algorithm

40

8.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

8.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

8.2.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

8.2.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

8.3 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

iv

CONTENTS

8.4 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 8.5 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

9 Sequential pattern mining

43

9.1 String Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

9.2 Itemset Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

9.3 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

9.4 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

9.5 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

9.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

9.7 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

10 Bayesian network

46

10.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

10.2 Inference and learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

10.2.1 Inferring unobserved variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

10.2.2 Parameter learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

10.2.3 Structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

10.3 Statistical introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

10.3.1 Introductory examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

10.3.2 Restrictions on priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

10.4 Definitions and concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

10.4.1 Factorization definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

10.4.2 Local Markov property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

10.4.3 Developing Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

10.4.4 Markov blanket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

10.4.5 Hierarchical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

10.4.6 Causal networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

10.5 Inference complexity and approximation algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

10.6 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

10.6.1 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

10.7 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

10.8 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

10.9 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

10.10References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

10.11Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

10.12External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

11 Naive Bayes classifier

61

CONTENTS

v

11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 11.2 Probabilistic model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
11.2.1 Constructing a classifier from the probability model . . . . . . . . . . . . . . . . . . . . . . . 63 11.3 Parameter estimation and event models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
11.3.1 Gaussian naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 11.3.2 Multinomial naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 11.3.3 Bernoulli naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 11.3.4 Semi-supervised parameter estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 11.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 11.4.1 Relation to logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 11.5 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 11.5.1 Gender classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 11.5.2 Document classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 11.6 See also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 11.7 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 11.7.1 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 11.8 External links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 11.9 Text and image sources, contributors, and licenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 11.9.1 Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 11.9.2 Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 11.9.3 Content license . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

Chapter 1
Decision tree learning
This article is about decision trees in machine learning. For the use of the term in decision analysis, see Decision tree.
Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a finite set of values are called classification trees. In these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data but not decisions; rather the resulting classification tree can be an input for decision making. This page deals with decision trees in data mining.
1.1 General
Decision tree learning is a method commonly used in data mining.[1] The goal is to create a model that predicts the value of a target variable based on several input variables. An example is shown in the diagram at right. Each interior node corresponds to one of the input variables; there are edges to children for each of the possible values of that input variable. Each leaf represents a value of the target variable given the values of the input variables represented by the path from the root to the leaf. A decision tree is a simple representation for classifying examples. For this section, assume that all of the features have finite discrete domains, and there is a single target feature called the classification. Each element of the domain of the classification is called a class. A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with a feature are labeled with each of the possible values of the feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes. A tree can be "learned" by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT) [2] is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data. In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorisation and generalisation of a given set of data. Data comes in records of the form:
(x, Y ) = (x1, x2, x3, ..., xk, Y )
1

2

CHAPTER 1. DECISION TREE LEARNING

A tree showing survival of passengers on the Titanic ("sibsp" is the number of spouses or siblings aboard). The figures under the leaves show the probability of survival and the percentage of observations in the leaf.
The dependent variable, Y, is the target variable that we are trying to understand, classify or generalize. The vector x is composed of the input variables, x1, x2, x3 etc., that are used for that task.
1.2 Types
Decision trees used in data mining are of two main types:
· Classification tree analysis is when the predicted outcome is the class to which the data belongs. · Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house,
or a patient's length of stay in a hospital).
The term Classification And Regression Tree (CART) analysis is an umbrella term used to refer to both of the above procedures, first introduced by Breiman et al.[3] Trees used for regression and trees used for classification have some

1.3. METRICS

3

similarities - but also some differences, such as the procedure used to determine where to split.[3] Some techniques, often called ensemble methods, construct more than one decision tree:
· Bagging decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction.[4]
· A Random Forest classifier uses a number of decision trees, in order to improve the classification rate. · Boosted Trees can be used for regression-type and classification-type problems.[5][6]
· Rotation forest - in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features.[7]
A special case of a decision tree is a Decision list,[8] which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node). While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity, permit non-greedy learning methods[9] and monotonic constraints to be imposed.[10] Decision tree learning is the construction of a decision tree from class-labeled training tuples. A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node. There are many specific decision-tree algorithms. Notable ones include:
· ID3 (Iterative Dichotomiser 3)
· C4.5 (successor of ID3)
· CART (Classification And Regression Tree)
· CHAID (CHi-squared Automatic Interaction Detector). Performs multi-level splits when computing classification trees.[11]
· MARS: extends decision trees to handle numerical data better.
· Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning.[12][13]
ID3 and CART were invented independently at around the same time (between 1970 and 1980), yet follow a similar approach for learning decision tree from training tuples.

1.3 Metrics
Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items.[14] Different algorithms use different metrics for measuring "best". These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split.
1.3.1 Gini impurity
Not to be confused with Gini coefficient.
Used by the CART (classification and regression tree) algorithm, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels

4

CHAPTER 1. DECISION TREE LEARNING

in the subset. Gini impurity can be computed by summing the probability fi of an item with label i being chosen times the probability 1 - fi of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.
To compute Gini impurity for a set of items with J classes, suppose i  {1, 2, ..., J} , and let fi be the fraction of items labeled with class i in the set.

J

J

J

J

J



IG(f ) = fi(1 - fi) = (fi - fi2) = fi - fi2 = 1 - fi2 = fifk

i=1

i=1

i=1

i=1

i=1

i= k

1.3.2 Information gain
Main article: Information gain in decision trees

Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy from information theory.
Entropy is defined as below

J IE(f ) = - fi log2 fi
i=1
Information Gain = Entropy(parent) - Weighted Sum of Entropy(Children) IG(T, a) = H(T ) - H(T |a)

1.3.3 Variance reduction
Introduced in CART,[3] variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable x due to the split at this node:





IV (N )

=

1 |S|2


iS jS

1 2 (xi

- xj )2

-



1 |St|2


iSt jSt

1 2 (xi

- xj )2

+

1 |Sf |2


iSf


jSf

1 2 (xi

- xj )2

where S , St , and Sf are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates,
though, written in a form without directly referring to the mean.

1.4 Decision tree advantages
Amongst other data mining methods, decision trees have various advantages:
· Simple to understand and interpret. People are able to understand decision tree models after a brief explanation. · Requires little data preparation. Other techniques often require data normalisation, dummy variables need to
be created and blank values to be removed.

1.5. LIMITATIONS

5

· Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. (For example, relation rules can be used only with nominal variables while neural networks can be used only with numerical variables.)
· Uses a white box model. If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model, the explanation for the results is typically difficult to understand, for example with an artificial neural network.
· Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
· Robust. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.
· Performs well with large datasets. Large amounts of data can be analysed using standard computing resources in reasonable time.

1.5 Limitations
· The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts.[15][16] Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally-optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally-optimal decision tree. To reduce the greedy effect of local-optimality some methods such as the dual information distance (DID) tree were proposed.[17]
· Decision-tree learners can create over-complex trees that do not generalise well from the training data. (This is known as overfitting.[18]) Mechanisms such as pruning are necessary to avoid this problem (with the exception of some algorithms such as the Conditional Inference approach, that does not require pruning [12][13]).
· There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. In such cases, the decision tree becomes prohibitively large. Approaches to solve the problem involve either changing the representation of the problem domain (known as propositionalisation)[19] or using learning algorithms based on more expressive representations (such as statistical relational learning or inductive logic programming).
· For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels.[20] However, the issue of biased predictor selection is avoided by the Conditional Inference approach.[12]

1.6 Extensions
1.6.1 Decision graphs
In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using Minimum message length (MML).[21] Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.[22] The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring. In general, decision graphs infer models with fewer leaves than decision trees.
1.6.2 Alternative search methods
Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias.[23][24]

6
It is also possible for a tree to be sampled using MCMC.[25] The tree can be searched for in a bottom-up fashion.[26]

CHAPTER 1. DECISION TREE LEARNING

1.7 See also
· Decision tree pruning · Binary decision diagram · CHAID · CART · ID3 algorithm · C4.5 algorithm · Decision stump · Decision list · Incremental decision tree · Alternating decision tree · Structured data analysis (statistics) · Logistic model tree · Hierarchical clustering

1.8 Implementations
Many data mining software packages provide implementations of one or more decision tree algorithms. Several examples include Salford Systems CART (which licensed the proprietary code of the original CART authors[3]), IBM SPSS Modeler, RapidMiner, SAS Enterprise Miner, Matlab, R (an open source software environment for statistical computing which includes several CART implementations such as rpart, party and randomForest packages), Weka (a free and open-source data mining suite, contains many decision tree algorithms), Orange (a free data mining software suite, which includes the tree module orngTree), KNIME, Microsoft SQL Server , and scikit-learn (a free and open-source machine learning library for the Python programming language).

1.9 References
[1] Rokach, Lior; Maimon, O. (2008). Data mining with decision trees: theory and applications. World Scientific Pub Co Inc. ISBN 978-9812771711.
[2] Quinlan, J. R., (1986). Induction of Decision Trees. Machine Learning 1: 81-106, Kluwer Academic Publishers [3] Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984). Classification and regression trees. Monterey, CA: Wadsworth
& Brooks/Cole Advanced Books & Software. ISBN 978-0-412-04841-8. [4] Breiman, L. (1996). Bagging Predictors. "Machine Learning, 24": pp. 123-140. [5] Friedman, J. H. (1999). Stochastic gradient boosting. Stanford University.

1.9. REFERENCES

7

[6] Hastie, T., Tibshirani, R., Friedman, J. H. (2001). The elements of statistical learning : Data mining, inference, and prediction. New York: Springer Verlag.
[7] Rodriguez, J.J. and Kuncheva, L.I. and Alonso, C.J. (2006), Rotation forest: A new classifier ensemble method, IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1619-1630.
[8] Rivest, Ron (Nov 1987). "Learning Decision Lists" (PDF). Machine Learning. 3 (2): 229­246. doi:10.1023/A:1022607331053.
[9] Letham, Ben; Rudin, Cynthia; McCormick, Tyler; Madigan, David (2015). "Interpretable Classifiers Using Rules And Bayesian Analysis: Building A Better Stroke Prediction Model". Annals of Applied Statistics. 9: 1350­1371. arXiv:1511.01644 . doi:10.1214/15-AOAS848.
[10] Wang, Fulton; Rudin, Cynthia (2015). "Falling Rule Lists" (PDF). Journal of Machine Learning Research. 38.
[11] Kass, G. V. (1980). "An exploratory technique for investigating large quantities of categorical data". Applied Statistics. 29 (2): 119­127. doi:10.2307/2986296. JSTOR 2986296.
[12] Hothorn, T.; Hornik, K.; Zeileis, A. (2006). "Unbiased Recursive Partitioning: A Conditional Inference Framework". Journal of Computational and Graphical Statistics. 15 (3): 651­674. doi:10.1198/106186006X133933. JSTOR 27594202.
[13] Strobl, C.; Malley, J.; Tutz, G. (2009). "An Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests". Psychological Methods. 14 (4): 323­348. doi:10.1037/a0016973.
[14] Rokach, L.; Maimon, O. (2005). "Top-down induction of decision trees classifiers-a survey". IEEE Transactions on Systems, Man, and Cybernetics, Part C. 35 (4): 476­487. doi:10.1109/TSMCC.2004.843247.
[15] Hyafil, Laurent; Rivest, RL (1976). "Constructing Optimal Binary Decision Trees is NP-complete". Information Processing Letters. 5 (1): 15­17. doi:10.1016/0020-0190(76)90095-8.
[16] Murthy S. (1998). Automatic construction of decision trees from data: A multidisciplinary survey. Data Mining and Knowledge Discovery
[17] Ben-Gal I. Dana A., Shkolnik N. and Singer (20). "Efficient Construction of Decision Trees by the Dual Information Distance Method" (PDF). Quality Technology & Quantitative Management (QTQM), 11( 1), 133-147. Check date values in: |date= (help)
[18] "Principles of Data Mining". 2007. doi:10.1007/978-1-84628-766-4. ISBN 978-1-84628-765-7.
[19] Horváth, Tamás; Yamamoto, Akihiro, eds. (2003). "Inductive Logic Programming". Lecture Notes in Computer Science. 2835. doi:10.1007/b13700. ISBN 978-3-540-20144-1.
[20] Deng,H.; Runger, G.; Tuv, E. (2011). Bias of importance measures for multi-valued attributes and solutions. Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN). pp. 293­300.
[21] http://citeseer.ist.psu.edu/oliver93decision.html
[22] Tan & Dowe (2003)
[23] Papagelis A., Kalles D.(2001). Breeding Decision Trees Using Evolutionary Techniques, Proceedings of the Eighteenth International Conference on Machine Learning, p.393-400, June 28-July 01, 2001
[24] Barros, Rodrigo C., Basgalupp, M. P., Carvalho, A. C. P. L. F., Freitas, Alex A. (2011). A Survey of Evolutionary Algorithms for Decision-Tree Induction. IEEE Transactions on Systems, Man and Cybernetics, Part C: Applications and Reviews, vol. 42, n. 3, p. 291-312, May 2012.
[25] Chipman, Hugh A., Edward I. George, and Robert E. McCulloch. "Bayesian CART model search." Journal of the American Statistical Association 93.443 (1998): 935-948.
[26] Barros R. C., Cerri R., Jaskowiak P. A., Carvalho, A. C. P. L. F., A bottom-up oblique decision tree induction algorithm. Proceedings of the 11th International Conference on Intelligent Systems Design and Applications (ISDA 2011).

8

CHAPTER 1. DECISION TREE LEARNING

1.10 External links
· Building Decision Trees in Python From O'Reilly. · An Addendum to "Building Decision Trees in Python" From O'Reilly. · Decision Trees Tutorial using Microsoft Excel. · Decision Trees page at aitopics.org, a page with commented links. · Decision tree implementation in Ruby (AI4R) · Evolutionary Learning of Decision Trees in C++ · Java implementation of Decision Trees based on Information Gain · A very explicit explanation of information gain as splitting criterion

Chapter 2
ID3 algorithm
Potential ID3-generated decision tree. Attributes are arranged as nodes by ability to classify examples. Values of attributes are represented by branches. In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan[1] used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.
9

10

CHAPTER 2. ID3 ALGORITHM

2.1 Algorithm
The ID3 algorithm begins with the original set S as the root node. On each iteration of the algorithm, it iterates through every unused attribute of the set S and calculates the entropy H(S) (or information gain IG(S) ) of that attribute. It then selects the attribute which has the smallest entropy (or largest information gain) value. The set S is then split by the selected attribute (e.g. age is less than 50, age is between 50 and 100, age is greater than 100) to produce subsets of the data. The algorithm continues to recurse on each subset, considering only attributes never selected before. Recursion on a subset may stop in one of these cases:
· every element in the subset belongs to the same class (+ or -), then the node is turned into a leaf and labelled with the class of the examples
· there are no more attributes to be selected, but the examples still do not belong to the same class (some are + and some are -), then the node is turned into a leaf and labelled with the most common class of the examples in the subset
· there are no examples in the subset, this happens when no example in the parent set was found to be matching a specific value of the selected attribute, for example if there was no example with age >= 100. Then a leaf is created, and labelled with the most common class of the examples in the parent set.
Throughout the algorithm, the decision tree is constructed with each non-terminal node representing the selected attribute on which the data was split, and terminal nodes representing the class label of the final subset of this branch.

2.1.1 Summary
1. Calculate the entropy of every attribute using the data set S 2. Split the set S into subsets using the attribute for which entropy is minimum (or, equivalently, information gain is
maximum) 3. Make a decision tree node containing that attribute 4. Recurse on subsets using remaining attributes.

2.1.2 Pseudocode
ID3 (Examples, Target_Attribute, Attributes) Create a root node for the tree If all examples are positive, Return the single-node tree Root, with label = +. If all examples are negative, Return the single-node tree Root, with label = -. If number of predicting attributes is empty, then Return the single node tree Root, with label = most common value of the target attribute in the examples. Otherwise Begin A  The Attribute that best classifies examples. Decision Tree attribute for Root = A. For each possible value, vi, of A, Add a new tree branch below Root, corresponding to the test A = vi. Let Examples(vi) be the subset of examples that have the value vi for A If Examples(vi) is empty Then below this new branch add a leaf node with label = most common target value in the examples Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes ­ {A}) End Return Root

2.1.3 Properties
ID3 does not guarantee an optimal solution; it can get stuck in local optima. It uses a greedy approach by selecting the best attribute to split the dataset on each iteration. One improvement that can be made on the algorithm can be to use backtracking during the search for the optimal decision tree.
ID3 can overfit to the training data, to avoid overfitting, smaller decision trees should be preferred over larger ones. This algorithm usually produces small trees, but it does not always produce the smallest possible tree.
ID3 is harder to use on continuous data. If the values of any given attribute is continuous, then there are many more places to split the data on this attribute, and searching for the best value to split by can be time consuming.

2.2. THE ID3 METRICS

11

ID3-generated decision tree used to determine whether a particular nucleotide pair within a pre-mRNA sequence corresponds to an mRNA splice site. This tree has been shown to have a 95% correct prediction rate.[2]
2.1.4 Usage
The ID3 algorithm is used by training on a dataset S to produce a decision tree which is stored in memory. At runtime, this decision tree is used to classify new unseen test cases by working down the decision tree using the values of this test case to arrive at a terminal node that tells you what class this test case belongs to.
2.2 The ID3 metrics
2.2.1 Entropy
Entropy H(S) is a measure of the amount of uncertainty in the (data) set S (i.e. entropy characterizes the (data) set S ).
 H(S) = - p(x) log2 p(x)
xX
Where,
· S - The current (data) set for which entropy is being calculated (changes every iteration of the ID3 algorithm)

12

CHAPTER 2. ID3 ALGORITHM

· X - Set of classes in S · p(x) - The proportion of the number of elements in class x to the number of elements in set S
When H(S) = 0 , the set S is perfectly classified (i.e. all elements in S are of the same class). In ID3, entropy is calculated for each remaining attribute. The attribute with the smallest entropy is used to split the set S on this iteration. The higher the entropy, the higher the potential to improve the classification here.
2.2.2 Information gain
Information gain IG(A) is the measure of the difference in entropy from before to after the set S is split on an attribute A . In other words, how much uncertainty in S was reduced after splitting set S on attribute A .
 IG(A, S) = H(S) - p(t)H(t)
tT
Where,
· H(S) - Entropy of set S 
· T - The subsets created from splitting set S by attribute A such that S = tT t · p(t) - The proportion of the number of elements in t to the number of elements in set S · H(t) - Entropy of subset t
In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the largest information gain is used to split the set S on this iteration.

2.3 See also
· CART · C4.5 algorithm

2.4 References
[1] Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106 [2] Taggart, Allison J; DeSimone, Alec M; Shih, Janice S; Filloux, Madeleine E; Fairbrother, William G (2012-06-17). "Large-scale
mapping of branchpoints in human pre-mRNA transcripts in vivo". Nature structural & molecular biology. 19 (7): 719­721. doi:10.1038/nsmb.2327. ISSN 1545-9993. PMC 3465671 . PMID 22705790.
· Mitchell, Tom M. Machine Learning. McGraw-Hill, 1997. pp. 55­58. · Grzymala-Busse, Jerzy W. "Selected Algorithms of Machine Learning from Examples." Fundamenta Informaticae
18, (1993): 193­207.

2.5 Further reading
· Decision Trees and Political Party Classification

2.6. EXTERNAL LINKS

13

2.6 External links
· Seminars - http://www2.cs.uregina.ca/ · Description and examples - http://www.cise.ufl.edu/ · Description and examples - http://www.cis.temple.edu/ · An implementation of ID3 in Python · An implementation of ID3 in Ruby · An implementation of ID3 in Common Lisp · An implementation of ID3 algorithm in C# · An implementation of ID3 in Perl · An implementation of ID3 in Prolog · An implementation of ID3 in C (This code is commented in Italian) · An implementation of ID3 in R

Chapter 3
C4.5 algorithm
C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan.[1] C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. It became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.[2]
3.1 Algorithm
C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set S = s1, s2, ... of already classified samples. Each sample si consists of a p-dimensional vector (x1,i, x2,i, ..., xp,i) , where the xj represent attribute values or features of the sample, as well as the class in which si falls. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists. This algorithm has a few base cases.
· All the samples in the list belong to the same class. When this happens, it simply creates a leaf node for the decision tree saying to choose that class.
· None of the features provide any information gain. In this case, C4.5 creates a decision node higher up the tree using the expected value of the class.
· Instance of previously-unseen class encountered. Again, C4.5 creates a decision node higher up the tree using the expected value.
3.1.1 Pseudocode
In pseudocode, the general algorithm for building decision trees is:[3]
1. Check for the above base cases. 2. For each attribute a, find the normalized information gain ratio from splitting on a. 3. Let a_best be the attribute with the highest normalized information gain.
14

3.2. IMPLEMENTATIONS

15

4. Create a decision node that splits on a_best. 5. Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node.

3.2 Implementations
J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.

3.3 Improvements from ID.3 algorithm
C4.5 made a number of improvements to ID3. Some of these are:
· Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it.[4]
· Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations.
· Handling attributes with differing costs.
· Pruning trees after creation - C4.5 goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes.

3.4 Improvements in C5.0/See5 algorithm
Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially. C5.0 offers a number of improvements on C4.5. Some of these are:[5][6]
· Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude) · Memory usage - C5.0 is more memory efficient than C4.5 · Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees. · Support for boosting - Boosting improves the trees and gives them more accuracy. · Weighting - C5.0 allows you to weight different cases and misclassification types. · Winnowing - a C5.0 option automatically winnows the attributes to remove those that may be unhelpful.
Source for a single-threaded Linux version of C5.0 is available under the GPL.

3.5 See also
· ID3 algorithm

16

CHAPTER 3. C4.5 ALGORITHM

3.6 References
[1] Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. [2] Umd.edu - Top 10 Algorithms in Data Mining [3] S.B. Kotsiantis, Supervised Machine Learning: A Review of Classification Techniques, Informatica 31(2007) 249-268, 2007 [4] J. R. Quinlan. Improved use of continuous attributes in c4.5. Journal of Artificial Intelligence Research, 4:77-90, 1996. [5] Is See5/C5.0 Better Than C4.5? [6] M. Kuhn and K. Johnson, Applied Predictive Modeling, Springer 2013

3.7 External links
· Original implementation on Ross Quinlan's homepage: http://www.rulequest.com/Personal/ · See5 and C5.0

Chapter 4
CHAID
CHAID is a type of decision tree technique, based upon adjusted significance testing (Bonferroni testing). The technique was developed in South Africa and was published in 1980 by Gordon V. Kass, who had completed a PhD thesis on this topic. CHAID can be used for prediction (in a similar fashion to regression analysis, this version of CHAID being originally known as XAID) as well as classification, and for detection of interaction between variables. CHAID stands for CHi-squared Automatic Interaction Detection, based upon a formal extension of the US AID (Automatic Interaction Detection) and THAID (THeta Automatic Interaction Detection) procedures of the 1960s and 1970s, which in turn were extensions of earlier research, including that performed in the UK in the 1950s. In practice, CHAID is often used in the context of direct marketing to select groups of consumers and predict how their responses to some variables affect other variables, although other early applications were in the field of medical and psychiatric research. Like other decision trees, CHAID's advantages are that its output is highly visual and easy to interpret. Because it uses multiway splits by default, it needs rather large sample sizes to work effectively, since with small sample sizes the respondent groups can quickly become too small for reliable analysis. One important advantage of CHAID over alternatives such as multiple regression is that it is non-parametric.
4.1 See also
· Chi-squared distribution · Latent class model · Structural equation modeling · Market segment · Decision tree learning · Multiple comparisons
4.2 References
4.3 Further reading
· Belson, William A.; Matching and prediction on the principle of biological classification, Applied Statistics, Vol. 8 (1959), pp. 65­75
17

18

CHAPTER 4. CHAID

· Morgan, John A.; & Sonquist, James N.; Problems in the analysis of survey data and a proposal, Journal of the American Statistical Association, Vol. 58 (1963), pp. 415­434
· Press, Laurence I.; Rogers, Miles S.; & Shure, Gerald H.; An interactive technique for the analysis of multivariate data, Behavioral Science, Vol. 14 (1969), pp. 364­370
· Kass, Gordon V.; An Exploratory Technique for Investigating Large Quantities of Categorical Data, Applied Statistics, Vol. 29, No. 2 (1980), pp. 119­127
· Hawkins, Douglas M. ; and Kass, Gordon V.; Automatic Interaction Detection, in Hawkins, Douglas M. (ed), Topics in Applied Multivariate Analysis, Cambridge University Press, Cambridge, 1982, pp. 269­302
· Hooton, Thomas M.; Haley, Robert W.; Culver, David H.; White, John W.; Morgan, W. Meade; & Carroll, Raymond J.; The Joint Associations of Multiple Risk Factors with the Occurrence of Nosocomial Infections, American Journal of Medicine, Vol. 70, (1981), pp. 960­970
· Brink, Susanne; & Van Schalkwyk, Dirk J.; Serum ferritin and mean corpuscular volume as predictors of bone marrow iron stores, South African Medical Journal, Vol. 61, (1982), pp. 432­434
· McKenzie, Dean P.; McGorry, Patrick D.; Wallace, Chris S.; Low, Lee H.; Copolov, David L.; & Singh, Bruce S.; Constructing a Minimal Diagnostic Decision Tree, Methods of Information in Medicine, Vol. 32 (1993), pp. 161­166
· Magidson, Jay; The CHAID approach to segmentation modeling: chi-squared automatic interaction detection, in Bagozzi, Richard P. (ed); Advanced Methods of Marketing Research, Blackwell, Oxford, GB, 1994, pp. 118­159
· Hawkins, Douglas M.; Young, S. S.; & Rosinko, A.; Analysis of a large structure-activity dataset using recursive partitioning, Quantitative Structure-Activity Relationships, Vol. 16, (1997), pp. 296­302
· Evgeny, Antipov; & Elena, Pokryshevskaya; Applying CHAID for logistic regression diagnostics and classification accuracy improvement, Journal of Targeting, Measurement and Analysis for Marketing 18 (2010), 109-117

4.4 Sources
· Luchman, J.N.; CHAID: Stata module to conduct chi-square automated interaction detection, Available for free download, or type within Stata: ssc install chaid.
· Luchman, J.N.; CHAIDFOREST: Stata module to conduct random forest ensemble classification based on chi-square automated interaction detection (CHAID) as base learner, Available for free download, or type within Stata: ssc install chaidforest.

Chapter 5
Random forest
This article is about the machine learning technique. For other kinds of random tree, see Random tree (disambiguation).
Random forests or random decision forests[1][2] are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587­588 The first algorithm for random decision forests was created by Tin Kam Ho [1] using the random subspace method,[2] which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.[4][5][6] An extension of the algorithm was developed by Leo Breiman[7] and Adele Cutler,[8] and "Random Forests" is their trademark.[9] The extension combines Breiman's "bagging" idea and random selection of features, introduced first by Ho[1] and later independently by Amit and Geman[10] in order to construct a collection of decision trees with controlled variance.
5.1 History
The general method of random decision forests was first proposed by Ho in 1995,[1] who established that forests of trees splitting with oblique hyperplanes, if randomly restricted to be sensitive to only selected feature dimensions, can gain accuracy as they grow without suffering from overtraining. A subsequent work along the same lines [2] concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level before accuracy being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.[4][5][6] The early development of Breiman's notion of random forests was influenced by the work of Amit and Geman[10] who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree. The idea of random subspace selection from Ho[2] was also influential in the design of random forests. In this method a forest of trees is grown, and variation among the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree or each node. Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Dietterich.[11] The introduction of random forests proper was first made in a paper by Leo Breiman.[7] This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form
19

20

CHAPTER 5. RANDOM FOREST

the basis of the modern practice of random forests, in particular:
1. Using out-of-bag error as an estimate of the generalization error. 2. Measuring variable importance through permutation.
The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.

5.2 Algorithm
5.2.1 Preliminaries: decision tree learning
Main article: Decision tree learning
Decision trees are a popular method for various machine learning tasks. Tree learning "come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining", say Hastie et al., because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate.[3]:352 In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, because they have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]:587­588 This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.
5.2.2 Tree bagging
Main article: Bootstrap aggregating
The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., x with responses Y = y1, ..., y , bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:
For b = 1, ..., B: 1. Sample, with replacement, n training examples from X, Y; call these X , Y . 2. Train a decision or regression tree f on X , Y .
After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':

f^ =

1 B

 B f^b(x)

b=1

or by taking the majority vote in the case of decision trees.

This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training

5.3. PROPERTIES

21

set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets. The number of samples/trees, B, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees B can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample x, using only the trees that did not have x in their bootstrap sample.[12] The training and test error tend to level off after some number of trees have been fit.
5.2.3 From bagging to random forests
Main article: Random subspace method
The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called "feature bagging". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.[13] Typically, for a classification problem with p features, p (rounded down) features are used in each split.[3]:592 For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default.[3]:592
5.2.4 Extensions
Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. These are trained using bagging and the random subspace method, like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity), for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample)[14]

5.3 Properties
5.3.1 Variable importance
Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper[7] and is implemented in the R package randomForest.[8]
The first step in measuring the variable importance in a data set Dn = {(Xi, Yi)}ni=1 is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).
To measure the importance of the j -th feature after training, the values of the j -th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the j -th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.
Features which produce large values for this score are ranked as more important than features which produce small values.
This method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations[15][16] and growing unbiased trees[17] can be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.[18]

22

CHAPTER 5. RANDOM FOREST

5.3.2 Relationship to nearest neighbors
A relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002.[19] It turns out that both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set {(xi, yi)}ni=1 that make predictions y^ for new points x' by looking at the "neighborhood" of the point, formalized by a weight function W:

n y^ = W (xi, x) yi.
i=1
Here, W (xi, x) is the non-negative weight of the i'th training point relative to the new point x'. For any particular x', the weights must sum to one. Weight functions are given as follows:

·

In k-NN, the weights are W (xi, x) =

1 k

if x is one of the k points closest to x', and zero otherwise.

·

In a tree, W (xi, x) =

1 k

if x is one of the k' points in the same leaf as x', and zero otherwise.

Since a forest averages the predictions of a set of m trees with individual weight functions Wj , its predictions are





y^

=

1 m

 m n Wj (xi, x) yi

=

n  1 m

 m Wj(xi, x)

yi.

j=1 i=1

i=1

j=1

This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual
trees. The neighbors of x' in this interpretation are the points xi which fall in the same leaf as x' in at least one tree of the forest. In this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the
structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.[19]

5.4 Unsupervised learning with random forests
As part of their construction, RF predictors naturally lead to a dissimilarity measure between the observations. One can also define an RF dissimilarity measure between unlabeled data: the idea is to construct an RF predictor that distinguishes the "observed" data from suitably generated synthetic data.[7][20] The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. An RF dissimilarity can be attractive because it handles mixed variable types well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The RF dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the "Addcl 1" RF dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The RF dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.[21]

5.5 Variants
Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.[22][23]

5.6 Real life Example
Please contribute to this section

5.7. LIBRARIES

23

5.7 Libraries
Python--from scikit-learn library for Python R--randomForest package in R Matlab--from Statistics and Machine Learning Toolbox in Matlab Spark--Spark MLib KNIME -- KNIME Tree Ensemble

5.8 See also
· Decision tree learning · Gradient boosting · Randomized algorithm · Ensemble learning · Boosting · Non-parametric statistics · Kernel random forest

5.9 References
[1] Ho, Tin Kam (1995). Random Decision Forests (PDF). Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, QC, 14­16 August 1995. pp. 278­282.
[2] Ho, Tin Kam (1998). "The Random Subspace Method for Constructing Decision Forests" (PDF). IEEE Transactions on Pattern Analysis and Machine Intelligence. 20 (8): 832­844. doi:10.1109/34.709601.
[3] Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2008). The Elements of Statistical Learning (2nd ed.). Springer. ISBN 0-387-95284-5.
[4] Kleinberg, Eugene (1996). "An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition" (PDF). Annals of Statistics. 24 (6): 2319­2349. doi:10.1214/aos/1032181157. MR 1425956.
[5] Kleinberg, Eugene (2000). "On the Algorithmic Implementation of Stochastic Discrimination" (PDF). IEEE Transactions on PAMI. 22 (5).
[6] Kleinberg, Eugine. "Stochastic Discrimination and its Implementation".
[7] Breiman, Leo (2001). "Random Forests". Machine Learning. 45 (1): 5­32. doi:10.1023/A:1010933404324.
[8] Liaw, Andy (16 October 2012). "Documentation for R package randomForest" (PDF). Retrieved 15 March 2013.
[9] U.S. trademark registration number 3185828, registered 2006/12/19.
[10] Amit, Yali; Geman, Donald (1997). "Shape quantization and recognition with randomized trees" (PDF). Neural Computation. 9 (7): 1545­1588. doi:10.1162/neco.1997.9.7.1545.
[11] Dietterich, Thomas (2000). "An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization". Machine Learning: 139­157.
[12] Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani (2013). An Introduction to Statistical Learning. Springer. pp. 316­321.

24

CHAPTER 5. RANDOM FOREST

[13] Ho, Tin Kam (2002). "A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors" (PDF). Pattern Analysis and Applications: 102­112.
[14] Geurts, P.; Ernst, D.; Wehenkel, L. (2006). "Extremely randomized trees" (PDF). Machine Learning. 63: 3­42. doi:10.1007/s10994006-6226-1.
[15] Deng,H.; Runger, G.; Tuv, E. (2011). Bias of importance measures for multi-valued attributes and solutions. Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN). pp. 293­300.
[16] Altmann A, Tolosi L, Sander O, Lengauer T (2010). "Permutation importance:a corrected feature importance measure". Bioinformatics. 26: 1340­1347. doi:10.1093/bioinformatics/btq134.
[17] Strobl, Carolin; Boulesteix, Anne-Laure; Augustin, Thomas (2007). "Unbiased split selection for classification trees based on the Gini index" (PDF). Computational Statistics & Data Analysis: 483­501.
[18] Tolosi L, Lengauer T (2011). "Classification with correlated features: unreliability of feature ranking and solutions.". Bioinformatics. 27: 1986­1994. doi:10.1093/bioinformatics/btr300.
[19] Lin, Yi; Jeon, Yongho (2002). Random forests and adaptive nearest neighbors (Technical report). Technical Report No. 1055. University of Wisconsin.
[20] Shi, T., Horvath, S. (2006). "Unsupervised Learning with Random Forest Predictors". Journal of Computational and Graphical Statistics. 15 (1): 118­138. doi:10.1198/106186006X94072. JSTOR 27594168.
[21] Shi, T., Seligson D., Belldegrun AS., Palotie A, Horvath, S. (2005). "Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma". Modern Pathology. 18 (4): 547­557. doi:10.1038/modpathol.3800322. PMID 15529185.
[22] Prinzie, A., Van den Poel, D. (2008). "Random Forests for multiclass classification: Random MultiNomial Logit". Expert Systems with Applications. 34 (3): 1721­1732. doi:10.1016/j.eswa.2007.01.029.
[23] Prinzie, A., Van den Poel, D. (2007). Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB, Dexa 2007, Lecture Notes in Computer Science, 4653, 349­358.

5.10 External links
· Random Forests classifier description (Site of Leo Breiman)
· Liaw, Andy & Wiener, Matthew "Classification and Regression by randomForest" R News (2002) Vol. 2/3 p. 18 (Discussion of the use of the random forest package for R)
· Prinzie, Anita; Poel, Dirk (2007). "Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB" (PDF). Database and Expert Systems Applications. Lecture Notes in Computer Science. p. 349. doi:10.1007/978-3-540-74469-6_35. ISBN 978-3-540-74467-2.
· C# implementation of random forest algorithm for categorization of text documents supporting reading of documents, making dictionaries, filtering stop words, stemming, counting words, making document-term matrix and its usage for building random forest and further categorization.
· A python implementation of the random forest algorithm working in regression, classification with multi-output support.
· AwesomeRandomForest - A list of random forest resources

Chapter 6
Gradient boosting
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. The idea of gradient boosting originated in the observation by Leo Breiman[1] that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed by Jerome H. Friedman[2][3] simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean.[4][5] The latter two papers introduced the abstract view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.
6.1 Informal introduction
(This section follows the exposition of gradient boosting by Li.[6]) Like other boosting methods, gradient boosting combines weak learners into a single strong learner, in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to learn a model F that predicts values y^ = F (x) , minimizing the mean squared error (y^ - y)2 to the true values y (averaged over some training set). At each stage 1  m  M of gradient boosting, it may be assumed that there is some imperfect model Fm (at the outset, a very weak model that just predicts the mean y in the training set could be used). The gradient boosting algorithm does not change Fm in any way; instead, it improves on it by constructing a new model that adds an estimator h to provide a better model Fm+1(x) = Fm(x) + h(x) . The question is now, how to find h ? The gradient boosting solution starts with the observation that a perfect h would imply
Fm+1(x) = Fm(x) + h(x) = y
or, equivalently,
h(x) = y - Fm(x)
Therefore, gradient boosting will fit h to the residual y - Fm(x) . Like in other boosting variants, each Fm+1 learns to correct its predecessor Fm . A generalization of this idea to other loss functions than squared error (and to classification
25

26

CHAPTER 6. GRADIENT BOOSTING

and ranking problems) follows from the observation that residuals y -F (x) are the negative gradients of the squared error

loss

function

1 2

(y

-

F (x))2

.

So,

gradient

boosting

is

a

gradient

descent

algorithm;

and

generalizing

it

entails

"plugging

in" a different loss and its gradient.

6.2 Algorithm
In many supervised learning problems one has an output variable y and a vector of input variables x connected together via a joint probability distribution P (x, y) . Using a training set {(x1, y1), . . . , (xn, yn)} of known values of x and corresponding values of y, the goal is to find an approximation F^(x) to a function F (x) that minimizes the expected value of some specified loss function L(y, F (x)) :

F^ = arg min Ex,y[L(y, F (x))]
F
Gradient boosting method assumes a real-valued y and seeks an approximation F^(x) in the form of a weighted sum of functions hi(x) from some class , called base (or weak) learners:

 M F (x) = ihi(x) + const
i=1
In accordance with the empirical risk minimization principle, the method tries to find an approximation F^(x) that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function F0(x) , and incrementally expanding it in a greedy fashion:

n F0(x) = arg min L(yi, )
 i=1
n Fm(x) = Fm-1(x) + arg min L(yi, Fm-1(xi) + f (xi))
f H i=1
where f is restricted to be a function from the class  of base learner functions.
However, the problem of choosing at each step the best f for an arbitrary loss function L is a hard optimization problem in general, and so we'll "cheat" by solving a much easier problem instead.
The idea is to apply a steepest descent step to this minimization problem. If we only cared about predictions at the points of the training set, and f were unrestricted, we'd update the model per the following equation, where we view L(y, f ) not as a functional of f, but as a function of a vector of values f (x1), . . . , f (xn) :

n Fm(x) = Fm-1(x) - m f L(yi, Fm-1(xi)),

i=1

m

=

arg min


n
i=1

L

( yi,

Fm-1(xi)

-



) L(yi, Fm-1(xi))
f (xi)

.

But as f must come from a restricted class of functions (that's what allows us to generalize), we'll just choose the one that

most closely approximates the gradient of L. Having chosen f, the multiplier  is then selected using line search just as

shown in the second equation above.

In pseudocode, the generic gradient boosting method is:[2][7]

6.3. GRADIENT TREE BOOSTING

27

Input: training set {(xi, yi)}ni=1, a differentiable loss function L(y, F (x)), number of iterations M. Algorithm:

1. Initialize model with a constant value: n
F0(x) = arg min L(yi, ).
 i=1
2. For m = 1 to M:

(a) Compute so-called pseudo-residuals:

[

]

rim = -

L(yi, F (xi)) F (xi)

F (x)=Fm-1(x)

for i = 1, . . . , n.

(b) Fit a base learner hm(x) to pseudo-residuals, i.e. train it using the training set {(xi, rim)}in=1 . (c) Compute multiplier m by solving the following one-dimensional optimization problem:

n m = arg min L (yi, Fm-1(xi) + hm(xi)) .
 i=1

(d) Update the model:

Fm(x) = Fm-1(x) + mhm(x). 3. Output FM (x).

6.3 Gradient tree boosting
Gradient boosting is typically used with decision trees (especially CART trees) of a fixed size as base learners. For this special case Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner.
Generic gradient boosting at the m-th step would fit a decision tree hm(x) to pseudo-residuals. Let J be the number of its leaves. The tree partitions the input space intoJ disjoint regionsR1m, . . . , RJm and predicts a constant value in each region. Using the indicator notation, the output ofhm(x) for input x can be written as the sum:

J hm(x) = bjmI(x  Rjm),
j=1
wherebjm is the value predicted in the regionRjm .[8]
Then the coefficientsbjm are multiplied by some valuem , chosen using line search so as to minimize the loss function, and the model is updated as follows:

n Fm(x) = Fm-1(x) + mhm(x), m = arg min L(yi, Fm-1(xi) + hm(xi)).
 i=1
Friedman proposes to modify this algorithm so that it chooses a separate optimal valuejm for each of the tree's regions, instead of a single m for the whole tree. He calls the modified algorithm "TreeBoost". The coefficients bjm from the tree-fitting procedure can be then simply discarded and the model update rule becomes:

J Fm(x) = Fm-1(x) + jmhm(x)I(x  Rjm),
j=1



jm = arg min

L(yi, Fm-1(xi) + hm(xi)).



xi Rj m

28

CHAPTER 6. GRADIENT BOOSTING

6.3.1 Size of trees
J , the number of terminal nodes in trees, is the method's parameter which can be adjusted for a data set at hand. It controls the maximum allowed level of interaction between variables in the model. With J = 2 (decision stumps), no interaction between variables is allowed. WithJ = 3 the model may include effects of the interaction between up to two variables, and so on. Hastie et al.[7] comment that typically 4  J  8 work well for boosting and results are fairly insensitive to the choice of J in this range, J = 2 is insufficient for many applications, and J > 10 is unlikely to be required.

6.4 Regularization
Fitting the training set too closely can lead to degradation of the model's generalization ability. Several so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure. One natural regularization parameter is the number of gradient boosting iterations M (i.e. the number of trees in the model when the base learner is a decision tree). Increasing M reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of M is often selected by monitoring prediction error on a separate validation data set. Besides controlling M, several other regularization techniques are used.
6.4.1 Shrinkage
An important part of gradient boosting method is regularization by shrinkage which consists in modifying the update rule as follows:

Fm(x) = Fm-1(x) +  · mhm(x), 0 <   1,
where parameter  is called the "learning rate".
Empirically it has been found that using small learning rates (such as  < 0.1 ) yields dramatic improvements in model's generalization ability over gradient boosting without shrinking (  = 1 ).[7] However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.
6.4.2 Stochastic gradient boosting
Soon after the introduction of gradient boosting Friedman proposed a minor modification to the algorithm, motivated by Breiman's bagging method.[3] Specifically, he proposed that at each iteration of the algorithm, a base learner should be fit on a subsample of the training set drawn at random without replacement.[9] Friedman observed a substantial improvement in gradient boosting's accuracy with this modification.
Subsample size is some constant fraction f of the size of the training set. When f = 1, the algorithm is deterministic and identical to the one described above. Smaller values of f introduce randomness into the algorithm and help prevent overfitting, acting as a kind of regularization. The algorithm also becomes faster, because regression trees have to be fit to smaller datasets at each iteration. Friedman[3] obtained that0.5  f  0.8 leads to good results for small and moderate sized training sets. Therefore, f is typically set to 0.5, meaning that one half of the training set is used to build each base learner.
Also, like in bagging, subsampling allows one to define an out-of-bag error of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-ofbag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations.[10]

6.5. USAGE

29

6.4.3 Number of observations in leaves
Gradient tree boosting implementations often also use regularization by limiting the minimum number of observations in trees' terminal nodes (this parameter is called n.minobsinnode in the R gbm package[10]). It is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances. Imposing this limit helps to reduce variance in predictions at leaves.
6.4.4 Penalize Complexity of Tree
Another useful regularization techniques for gradient boosted trees is to penalize model complexity of the learned model. [11] The model complexity can be defined proportional number of leaves in the learned trees. The joint optimization of loss and model complexity corresponds to a post-pruning algorithm to remove branches that fail to reduce the loss by a threshold. Other kinds of regularization such as l2 penalty on the leave values can also be added to avoid overfitting.

6.5 Usage
Recently, gradient boosting has gained some popularity in the field of learning to rank. The commercial web search engines Yahoo[12] and Yandex[13] use variants of gradient boosting in their machine-learned ranking engines.

6.6 Names
The method goes by a variety of names. Friedman introduced his regression technique as a "Gradient Boosting Machine" (GBM).[2] Mason, Baxter et. el. described the generalized abstract class of algorithms as "functional gradient boosting".[4][5] A popular open-source implementation[10] for R calls it "Generalized Boosting Model". Commercial implementations from Salford Systems use the names "Multiple Additive Regression Trees" (MART) and TreeNet, both trademarked.

6.7 See also
· AdaBoost · Random forest · xgboost

6.8 References
[1] Breiman, L. "Arcing The Edge" (June 1997) [2] Friedman, J. H. "Greedy Function Approximation: A Gradient Boosting Machine." (February 1999) [3] Friedman, J. H. "Stochastic Gradient Boosting." (March 1999) [4] Mason, L.; Baxter, J.; Bartlett, P. L.; Frean, Marcus (1999). "Boosting Algorithms as Gradient Descent" (PDF). In S.A. Solla
and T.K. Leen and K. Müller. Advances in Neural Information Processing Systems 12. MIT Press. pp. 512­518. [5] Mason, L.; Baxter, J.; Bartlett, P. L.; Frean, Marcus (May 1999). Boosting Algorithms as Gradient Descent in Function Space
(PDF). [6] Cheng Li. "A Gentle Introduction to Gradient Boosting" (PDF).

30

CHAPTER 6. GRADIENT BOOSTING

[7] Hastie, T.; Tibshirani, R.; Friedman, J. H. (2009). "10. Boosting and Additive Trees". The Elements of Statistical Learning (2nd ed.). New York: Springer. pp. 337­384. ISBN 0-387-84857-6.
[8] Note: in case of usual CART trees, the trees are fitted using least-squares loss, and so the coefficient bjm for the region Rjm is equal to just the value of output variable, averaged over all training instances in Rjm .
[9] Note that this is different from bagging, which samples with replacement because it uses samples of the same size as the training set.
[10] Ridgeway, Greg (2007). Generalized Boosted Models: A guide to the gbm package.
[11] Tianqi Chen. Introduction to Boosted Trees
[12] Cossock, David and Zhang, Tong (2008). Statistical Analysis of Bayes Optimal Subset Ranking, page 14.
[13] Yandex corporate blog entry about new ranking model "Snezhinsk" (in Russian)

Chapter 7
Association rule learning
"OneR" redirects here. For filmmaking technique, see Long take.
Association rule learning is a method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] Based on the concept of strong rules, Rakesh Agrawal et al.[2] introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule {onions, potatoes}  {burger} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements. In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, Continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
7.1 Definition
Following the original definition by Agrawal et al.[2] the problem of association rule mining is defined as: Let I = {i1, i2, . . . , in} be a set of n binary attributes called items. Let D = {t1, t2, . . . , tm} be a set of transactions called the database. Each transaction in D has a unique transaction ID and contains a subset of the items in I . A rule is defined as an implication of the form: X Y Where X, Y  I and X  Y =  . Every rule is composed by two different sets of items, also known as itemsets, X and Y , where X is called antecedent or left-hand-side (LHS) and Y consequent or right-hand-side (RHS). To illustrate the concepts, we use a small example from the supermarket domain. The set of items is I = {milk, bread, butter, beer, diapers} and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represent the absence of an item in a that transaction. An example rule for the supermarket could be {butter, bread}  {milk} meaning that if butter and bread are bought, customers also buy milk. Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and data-sets often contain thousands or millions of transactions.
31

32

CHAPTER 7. ASSOCIATION RULE LEARNING

7.2 Useful Concepts

In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.
Let X be an item-set, X  Y an association rule and T a set of transactions of a given database.

7.2.1 Support
Support is an indication of how frequently the item-set appears in the database.
The support value of X with respect to T is defined as the proportion of transactions in the database which contains the item-set X . In formula: supp(X)
In the example database, the item-set {beer, diapers} has a support of 1/5 = 0.2 since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of supp() is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).[3]

7.2.2 Confidence
Confidence is an indication of how often the rule has been found to be true.
The confidence value of a rule, X  Y , with respect to a set of transactions T , is the proportion of the transactions that contains X which also contains Y .
Confidence is defined as:
conf(X  Y ) = supp(X  Y )/supp(X) .
For example, the rule {butter, bread}  {milk} has a confidence of 0.2/0.2 = 1.0 in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).
Note that supp(X  Y ) means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of events and not sets of items. We can rewrite supp(X  Y ) as the joint probability P (EX  EY ) , where EX and EY are the events that a transaction contains itemset X or Y , respectively.[4] Thus confidence can be interpreted as an estimate of the conditional probability P (EY |EX ) , the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.[3][5]

7.2.3 Lift

The lift of a rule is defined as:

lift(X



Y)

=

supp(XY ) supp(X)×supp(Y )

or the ratio of the observed support to that expected if X and Y were independent.

For

example,

the

rule

{milk, bread}



{butter}

has

a

lift

of

0.2 0.4×0.4

=

1.25

.

If some rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent

are independent of each other. When two events are independent of each other, no rule can be drawn involving those two

events.

If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.

The value of lift is that it considers both the confidence of the rule and the overall data set.[3]

7.3. PROCESS

33

7.2.4 Conviction

The

conviction

of

a

rule

is

defined

as

conv(X



Y)

=

1-supp(Y ) 1-conf(XY )

.

For

example,

the

rule

{milk, bread}



{butter}

has

a

conviction

of

1-0.4 1-0.5

=

1.2

,

and

can

be

interpreted

as

the

ratio

of

the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction)

if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction

value of 1.2 shows that the rule {milk, bread}  {butter} would be incorrect 20% more often (1.2 times as often) if the

association between X and Y was purely random chance.

7.3 Process

Frequent itemset lattice, where the color of the box indicates how many transactions contain the combination of items. Note that lower levels of the lattice can contain at most the minimum number of their parents' items; e.g. {ac} can have only at most min(a, c) items. This is called the downward-closure property.[2]
Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:
1. A minimum support threshold is applied to find all frequent item-sets in a database. 2. A minimum confidence constraint is applied to these frequent item-sets in order to form rules.
While the second step is straightforward, the first step needs more attention.

34

CHAPTER 7. ASSOCIATION RULE LEARNING

Finding all frequent item-sets in a database is difficult since it involves searching all possible item-sets (item combinations). The set of possible item-sets is the power set over I and has size 2n - 1 (excluding the empty set which is not a valid item-set). Although the size of the power-set grows exponentially in the number of items n in I , efficient search is possible using the downward-closure property of support[2][6] (also called anti-monotonicity[7]) which guarantees that for
a frequent itemset, all its subsets are also frequent and thus for an infrequent item-set, all its super-sets must also be infrequent. Exploiting this property, efficient algorithms (e.g., Apriori[8] and Eclat[9]) can find all frequent item-sets.

7.4 History
The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,[2] which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, it is possible that what is now called "association rules" is similar to what appears in the 1966 paper[10] on GUHA, a general data mining method developed by Petr Hájek et al.[11]
An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with supp(X) and conf(X  Y ) greater than user defined constraints.[12]

7.5 Alternative measures of interestingness
In addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:
· All-confidence[13] · Collective strength[14] · Conviction[15] · Leverage[16] · Lift (originally called interest)[17]
Several more measures are presented and compared by Tan et al.[18] and by Hahsler.[4] Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."

7.6 Statistically sound associations
One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery[19][20] controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance levels.

7.7 Algorithms
Many algorithms for generating association rules were presented over time.

7.7. ALGORITHMS

35

Some well known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.

7.7.1 Apriori algorithm
Main article: Apriori algorithm
Apriori[8] uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.

7.7.2 Eclat algorithm
Eclat[9] (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm using set intersection. It is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality enhancing properties. It was first introduced by Zaki, Parthasarathy, Li and Ogihara in a series of papers written in 1997.
Mohammed Javeed Zaki, Srinivasan Parthasarathy, M. Ogihara, Wei Li: New Algorithms for Fast Discovery of Association Rules. KDD 1997.
Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, Wei Li: Parallel Algorithms for Discovery of Association Rules. Data Min. Knowl. Discov. 1(4): 343-373 (1997)

7.7.3 FP-growth algorithm
FP stands for frequent pattern.[21]
In the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances. Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly. Items in each instance that do not meet minimum coverage threshold are discarded. If many instances share most frequent items, FP-tree provides high compression close to tree root.
Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database. Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition. New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts. Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.
Once the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.[22]

7.7.4 Others
AprioriDP
AprioriDP[23] utilizes Dynamic Programming in Frequent itemset mining. The working principle is to eliminate the candidate generation like FP-tree, but it stores support count in specialized data structure instead of tree.

36

CHAPTER 7. ASSOCIATION RULE LEARNING

Context Based Association Rule Mining Algorithm
Main article: Context Based Association Rules
CBPNARM is the newly developed algorithm which is developed in 2013 to mine association rules on the basis of context. It uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set.
Node-set-based algorithms
FIN,[24] PrePost [25] and PPV [26] are three algorithms based on node sets. They use nodes in a coding FP-tree to represent itemsets, and employ a depth-first search strategy to discovery frequent itemsets using "intersection" of node sets.
GUHA procedure ASSOC
GUHA is a general method for exploratory data analysis that has theoretical foundations in observational calculi.[27] The ASSOC procedure[28] is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.
OPUS search
OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.[29] Initially used to find rules for a fixed consequent[29][30] it has subsequently been extended to find rules with any item as a consequent.[31] OPUS search is the core technology in the popular Magnum Opus association discovery system.

7.8 Lore
A famous story about association rule mining is the "beer and diaper" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.[32] Daniel Powers says:[32]
In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.

7.9 Other types of association mining
Multi-Relation Association Rules: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid:

7.10. SEE ALSO

37

"Those who live in a place which is near by a city with humid climate type and also are younger than 20 -> their health condition is good". Such association rules are extractable from RDBMS data or semantic web data.[33]
Context Based Association Rules is a form of association rule. Context Based Association Rules claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables. For example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month.This might be because of abnormal context i.e. salary is drawn at the start of the month [34]
Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.[35][36]
Weighted class learning is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.
High-order pattern discovery facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data. [37]
K-optimal pattern discovery provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.
Approximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.[38]
Generalized Association Rules hierarchical taxonomy (concept hierarchy)
Quantitative Association Rules categorical and quantitative data [39]
Interval Data Association Rules e.g. partition the age into 5-year-increment ranged
Maximal Association Rules
Sequential pattern mining discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.[40]
Sequential Rules discovering relationships between items while considering the time ordering. It is generally applied on a sequence database. For example, a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and CD-Roms, later bought a webcam, with a given confidence and support.
Warmr is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.[41]

7.10 See also
· Sequence mining · Production system (computer science)

7.11 References
[1] Piatetsky-Shapiro, Gregory (1991), Discovery, analysis, and presentation of strong rules, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA.
[2] Agrawal, R.; Imieliski, T.; Swami, A. (1993). "Mining association rules between sets of items in large databases". Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD '93. p. 207. doi:10.1145/170035.170072. ISBN 0897915925.
[3] Hahsler, Michael (2005). "Introduction to arules ­ A computational environment for mining association rules and frequent item sets" (PDF). Journal of Statistical Software.

38

CHAPTER 7. ASSOCIATION RULE LEARNING

[4] Michael Hahsler (2015). A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules. http:// michael.hahsler.net/research/association_rules/measures.html
[5] Hipp, J.; Güntzer, U.; Nakhaeizadeh, G. (2000). "Algorithms for association rule mining --- a general survey and comparison". ACM SIGKDD Explorations Newsletter. 2: 58. doi:10.1145/360402.360421.
[6] Tan, Pang-Ning; Michael, Steinbach; Kumar, Vipin (2005). "Chapter 6. Association Analysis: Basic Concepts and Algorithms" (PDF). Introduction to Data Mining. Addison-Wesley. ISBN 0-321-32136-7.
[7] Pei, Jian; Han, Jiawei; and Lakshmanan, Laks V. S.; Mining frequent itemsets with convertible constraints, in Proceedings of the 17th International Conference on Data Engineering, April 2­6, 2001, Heidelberg, Germany, 2001, pages 433-442
[8] Agrawal, Rakesh; and Srikant, Ramakrishnan; Fast algorithms for mining association rules in large databases, in Bocca, Jorge B.; Jarke, Matthias; and Zaniolo, Carlo; editors, Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994, pages 487-499
[9] Zaki, M. J. (2000). "Scalable algorithms for association mining". IEEE Transactions on Knowledge and Data Engineering. 12 (3): 372­390. doi:10.1109/69.846291.
[10] Hájek, Petr; Havel, Ivan; Chytil, Metodj; The GUHA method of automatic hypotheses determination, Computing 1 (1966) 293-308
[11] Hájek, Petr; Feglar, Tomas; Rauch, Jan; and Coufal, David; The GUHA method, data preprocessing and mining, Database Support for Data Mining Applications, Springer, 2004, ISBN 978-3-540-22479-2
[12] Webb, Geoffrey (1989). "A Machine Learning Approach to Student Modelling". Proceedings of the Third Australian Joint Conference on Artificial Intelligence (AI 89): 195­205.
[13] Omiecinski, Edward R.; Alternative interest measures for mining associations in databases, IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003
[14] Aggarwal, Charu C.; and Yu, Philip S.; A new framework for itemset generation, in PODS 98, Symposium on Principles of Database Systems, Seattle, WA, USA, 1998, pages 18-24
[15] Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; Dynamic itemset counting and implication rules for market basket data, in SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997, pp. 255-264
[16] Piatetsky-Shapiro, Gregory; Discovery, analysis, and presentation of strong rules, Knowledge Discovery in Databases, 1991, pp. 229-248
[17] Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; Dynamic itemset counting and implication rules for market basket data, in SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997, pp. 265-276
[18] Tan, Pang-Ning; Kumar, Vipin; and Srivastava, Jaideep; Selecting the right objective measure for association analysis, Information Systems, 29(4):293-313, 2004
[19] Webb, Geoffrey I. (2007); Discovering Significant Patterns, Machine Learning 68(1), Netherlands: Springer, pp. 1-33 online access
[20] Gionis, Aristides; Mannila, Heikki; Mielikäinen, Taneli; and Tsaparas, Panayiotis; Assessing Data Mining Results via Swap Randomization, ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 1, Issue 3 (December 2007), Article No. 14
[21] Han (2000). "Mining Frequent Patterns Without Candidate Generation". Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. SIGMOD '00: 1­12. doi:10.1145/342009.335372.
[22] Witten, Frank, Hall: Data mining practical machine learning tools and techniques, 3rd edition
[23] D. Bhalodiya, K. M. Patel and C. Patel. An Efficient way to Find Frequent Pattern with Dynamic Programming Approach . NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING, NUiCONE-2013, 28-30 NOVEMBER, 2013.

7.12. EXTERNAL LINKS

39

[24] Z. H. Deng and S. L. Lv. Fast mining frequent itemsets using Nodesets.. Expert Systems with Applications, 41(10): 4505­4512, 2014.
[25] Z. H. Deng, Z. Wang,and J. Jiang. A New Algorithm for Fast Mining Frequent Itemsets Using N-Lists . SCIENCE CHINA Information Sciences, 55 (9): 2008 - 2030, 2012.
[26] Z. H. Deng and Z. Wang. A New Fast Vertical Method for Mining Frequent Patterns . International Journal of Computational Intelligence Systems, 3(6): 733 - 744, 2010.
[27] Rauch, Jan; Logical calculi for knowledge discovery in databases, in Proceedings of the First European Symposium on Principles of Data Mining and Knowledge Discovery, Springer, 1997, pp. 47-57
[28] Hájek, Petr; Havránek, Tomás (1978). Mechanizing Hypothesis Formation: Mathematical Foundations for a General Theory. Springer-Verlag. ISBN 3-540-08738-9.
[29] Webb, Geoffrey I. (1995); OPUS: An Efficient Admissible Algorithm for Unordered Search, Journal of Artificial Intelligence Research 3, Menlo Park, CA: AAAI Press, pp. 431-465 online access
[30] Bayardo, Roberto J., Jr.; Agrawal, Rakesh; Gunopulos, Dimitrios (2000). "Constraint-based rule mining in large, dense databases". Data Mining and Knowledge Discovery. 4 (2): 217­240. doi:10.1023/A:1009895914772.
[31] Webb, Geoffrey I. (2000); Efficient Search for Association Rules, in Ramakrishnan, Raghu; and Stolfo, Sal; eds.; Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2000), Boston, MA, New York, NY: The Association for Computing Machinery, pp. 99-107 online access
[32] http://www.dssresources.com/newsletters/66.php
[33] Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; MRAR: Mining Multi-Relation Association Rules, Journal of Computing and Security, 1, no. 2 (2014)
[34] Shaheen, M; Shahbaz, M; and Guergachi, A; Context Based Positive and Negative Spatio Temporal Association Rule Mining, Elsevier Knowledge-Based Systems, Jan 2013, pp. 261-273
[35] GI Webb and S. Butler and D. Newlands (2003). On Detecting Differences Between Groups. KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
[36] Menzies, Tim; and Hu, Ying; Data Mining for Very Busy People, IEEE Computer, October 2003, pp. 18-25
[37] Wong, Andrew K.C.; Wang, Yang (1997). "High-order pattern discovery from discrete-valued data". IEEE Transactions on Knowledge and Data Engineering (TKDE): 877­893.
[38] Jinze Liu, Susan Paulsen, Xing Sun, Wei Wang, Andrew Nobel, J. P. (2006). Mining approximate frequent itemsets in the presence of noise: Algorithm and analysis. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805
[39] Salleb-Aouissi, Ansaf; Vrain, Christel; Nortet, Cyril (2007). "QuantMiner: A Genetic Algorithm for Mining Quantitative Association Rules". International Joint Conference on Artificial Intelligence (IJCAI): 1035­1040.
[40] Zaki, Mohammed J. (2001); SPADE: An Efficient Algorithm for Mining Frequent Sequences, Machine Learning Journal, 42, pp. 31­60
[41] "Warmr: a data mining tool for chemical data.". J Comput Aided Mol Des. 15 (2): 173­81. Feb 2001. PMID 11272703.

7.12 External links
7.12.1 Bibliographies
· Extensive Bibliography on Association Rules by J.M. Luna · Annotated Bibliography on Association Rules by M. Hahsler · Statsoft Electronic Statistics Textbook: Association Rules by Dell Software

Chapter 8
Apriori algorithm
Apriori[1] is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.
8.1 Overview
The Apriori algorithm was proposed by Agrawal and Srikant in 1994. Apriori is designed to operate on databases containing transactions (for example, collections of items bought by customers, or details of a website frequentation). Other algorithms are designed for finding association rules in data having no transactions (Winepi and Minepi), or having no timestamps (DNA sequencing). Each transaction is seen as a set of items (an itemset). Given a threshold C , the Apriori algorithm identifies the item sets which are subsets of at least C transactions in the database. Apriori uses a "bottom up" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length k from item sets of length k - 1 . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent k -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates. The pseudo code for the algorithm is given below for a transaction database T , and a support threshold of  . Usual set theoretic notation is employed, though note that T is a multiset. Ck is the candidate set for level k . At each step, the algorithm is assumed to generate the candidate sets from the large item sets of the preceding level, heeding the downward closure lemma. count[c] accesses a field of the data structure that represents candidate set c , which is initially assumed to be zero. Many details are omitted below, usually the most important part of the implementation is the data structure used for storing the candidate sets, and counting their frequencies.
40

8.2. EXAMPLES

41

Apriori(T, ) L1  {large 1 - itemsets} k2 while Lk-1 =  Ck  {a  {b} | a  Lk-1  b  a} - {c | {s | s  c  |s| = k - 1}  Lk-1} for transactions t  T Ct  {c | c  Ck  c  t} for candidates c  Ct count[c]  count[c] + 1 Lk  {c | c  Ck  count[c]  } k  k+1  return Lk
k
8.2 Examples
8.2.1 Example 1
Consider the following database, where each row is a transaction and each cell is an individual item of the transaction: The association rules that can be determined from this database are the following:
1. 100% of sets with alpha also contain beta 2. 50% of sets with alpha, beta also have epsilon 3. 50% of sets with alpha, beta also have theta
we can also illustrate this through a variety of examples

8.2.2 Example 2
Assume that a large supermarket tracks sales data by stock-keeping unit (SKU) for each item: each item, such as "butter" or "bread", is identified by a numerical SKU. The supermarket has a database of transactions where each transaction is a set of SKUs that were bought together.
Let the database of transactions consist of following itemsets:
We will use Apriori to determine the frequent item sets of this database. To do so, we will say that an item set is frequent if it appears in at least 3 transactions of the database: the value 3 is the support threshold.
The first step of Apriori is to count up the number of occurrences, called the support, of each member item separately, by scanning the database a first time. We obtain the following result
All the itemsets of size 1 have a support of at least 3, so they are all frequent.
The next step is to generate a list of all pairs of the frequent items.
For example, regarding the pair {1,2}: the first table of Example 2 shows items 1 and 2 appearing together in three of the itemsets; therefore, we say item {1,2} has support of three.
The pairs {1,2}, {2,3}, {2,4}, and {3,4} all meet or exceed the minimum support of 3, so they are frequent. The pairs {1,3} and {1,4} are not. Now, because {1,3} and {1,4} are not frequent, any larger set which contains {1,3} or {1,4} cannot be frequent. In this way, we can prune sets: we will now look for frequent triples in the database, but we can already exclude all the triples that contain one of these two pairs:

42

CHAPTER 8. APRIORI ALGORITHM

in the example, there are no frequent triplets -- {2,3,4} is below the minimal threshold, and the other triplets were excluded because they were super sets of pairs that were already below the threshold.
We have thus determined the frequent sets of items in the database, and illustrated how some items were not counted because one of their subsets was already known to be below the threshold.

8.3 Limitations
Apriori, while historically significant, suffers from a number of inefficiencies or trade-offs, which have spawned other algorithms. Candidate generation generates large numbers of subsets (the algorithm attempts to load up the candidate set with as many as possible before each scan). Bottom-up subset exploration (essentially a breadth-first traversal of the subset lattice) finds any maximal subset S only after all 2|S| - 1 of its proper subsets. Later algorithms such as Max-Miner[2] try to identify the maximal frequent item sets without enumerating their subsets, and perform "jumps" in the search space rather than a purely bottom-up approach.

8.4 References
[1] Rakesh Agrawal and Ramakrishnan Srikant Fast algorithms for mining association rules in large databases. Proceedings of the 20th International Conference on Very Large Data Bases, VLDB, pages 487-499, Santiago, Chile, September 1994.
[2] Bayardo Jr, Roberto J. (1998). "Efficiently mining long patterns from databases" (PDF). ACM SIGMOD Record. 27 (2).

8.5 External links
· ARtool, GPL Java association rule mining application with GUI, offering implementations of multiple algorithms for discovery of frequent patterns and extraction of association rules (includes Apriori)
· ELKI includes Java implementations of Apriori, Eclat and FPGrowth.
· SPMF offers Java open-source implementations of Apriori and several variations such as AprioriClose, UApriori, AprioriInverse, AprioriRare, MSApriori, AprioriTID, and other more efficient algorithms such as FPGrowth and LCM.
· Christian Borgelt provides C implementations for Apriori and many other frequent pattern mining algorithms (Eclat, FPGrowth, etc.). The code is distributed as free software under the MIT license.
· The R package arules contains Apriori and Eclat and infrastructure for representing, manipulating and analyzing transaction data and patterns.
· Orange, an open-source data mining suite, contains widgets for enumerating itemsets and association rules based on Apriori algorithm.

Chapter 9
Sequential pattern mining
Sequential Pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence.[1] It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity. Sequential pattern mining is a special case of structured data mining. There are several key traditional computational problems addressed within this field. These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members. In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms and itemset mining which is typically based on association rule learning.
9.1 String Mining
String mining typically deals with a limited alphabet for items that appear in a sequence, but the sequence itself may be typically very long. Examples of an alphabet can be those in the ASCII character set used in natural language text, nucleotide bases 'A', 'G', 'C' and 'T' in DNA sequences, or amino acids for protein sequences. In biology applications analysis of the arrangement of the alphabet in strings can be used to examine gene and protein sequences to determine their properties. Knowing the sequence of letters of a DNA or a protein is not an ultimate goal in itself. Rather, the major task is to understand the sequence, in terms of its structure and biological function. This is typically achieved first by identifying individual regions or structural units within each sequence and then assigning a function to each structural unit. In many cases this requires comparing a given sequence with previously studied ones. The comparison between the strings becomes complicated when insertions, deletions and mutations occur in a string. A survey and taxonomy of the key algorithms for sequence comparison for bioinformatics is presented by Abouelhoda & Ghanem (2010), which include:[2]
· Repeat-related problems: that deal with operations on single sequences and can be based on exact string matching or approximate string matching methods for finding dispersed fixed length and maximal length repeats, finding tandem repeats, and finding unique subsequences and missing (un-spelled) subsequences.
· Alignment problems: that deal with comparison between strings by first aligning one or more sequences; examples of popular methods include BLAST for comparing a single sequence with multiple sequences in a database, and ClustalW for multiple alignments. Alignment algorithms can be based on either exact or approximate methods, and can also be classified as global alignments, semi-global alignments and local alignment. See sequence alignment.
43

44

CHAPTER 9. SEQUENTIAL PATTERN MINING

9.2 Itemset Mining
Some problems in sequence mining lend themselves discovering frequent itemsets and the order they appear, for example, one is seeking rules of the form "if a {customer buys a car}, he or she is likely to {buy insurance} within 1 week", or in the context of stock prices, "if {Nokia up and Ericsson up}, it is likely that {Motorola up and Samsung up} within 2 days". Traditionally, itemset mining is used in marketing applications for discovering regularities between frequently co-occurring items in large transactions. For example, by analysing transactions of customer shopping baskets in a supermarket, one can produce a rule which reads "if a customer buys onions and potatoes together, he or she is likely to also buy hamburger meat in the same transaction".
A survey and taxonomy of the key algorithms for item set mining is presented by Han et al. (2007).[3]
The two common techniques that are applied to sequence databases for frequent itemset mining are the influential apriori algorithm and the more-recent FP-Growth technique.

9.3 Application
With a great variation of products and user buying behaviors, shelf on which products are being displayed is one of the most important resources in retail environment. Retailers can not only increase their profit but, also decrease cost by proper management of shelf space allocation and products display. To solve this problem, George and Binu (2013) have proposed an approach to mine user buying patterns using PrefixSpan algorithm and place the products on shelves based on the order of mined purchasing patterns.[4]

9.4 Algorithms
Commonly used algorithms include:
· GSP Algorithm · Sequential Pttern Discovery using Equivalence classes (SPADE) · FreeSpan · PrefixSpan · MAPres[5]

9.5 See also
· Association rule learning · Data Mining · Process mining · Sequence analysis (Bioinformatics) · Sequence clustering · Sequence labeling · string (computer science) · Sequence alignment · Time series

9.6. REFERENCES

45

9.6 References
[1] Mabroukeh, N. R.; Ezeife, C. I. (2010). "A taxonomy of sequential pattern mining algorithms". ACM Computing Surveys 43: 1­41. doi:10.1145/1824795.1824798.
[2] Abouelhoda, M.; Ghanem, M. (2010). "String Mining in Bioinformatics". In Gaber, M. M. Scientific Data Mining and Knowledge Discovery. Springer. doi:10.1007/978-3-642-02788-8_9. ISBN 978-3-642-02787-1.
[3] Han, J.; Cheng, H.; Xin, D.; Yan, X. (2007). "Frequent pattern mining: current status and future directions". Data Mining and Knowledge Discovery 15 (1): 55­86. doi:10.1007/s10618-006-0059-1.
[4] George, A.; Binu, D. (2013). "An Approach to Products Placement in Supermarkets Using PrefixSpan Algorithm". Journal of King Saud University-Computer and Information Sciences 25 (1): 77­87. doi:10.1016/j.jksuci.2012.07.001.
[5] Ahmad, Ishtiaq; Qazi, Wajahat M.; Khurshid, Ahmed; Ahmad, Munir; Hoessli, Daniel C.; Khawaja, Iffat; Choudhary, M. Iqbal; Shakoori, Abdul R.; Nasir-ud-Din, (1 May 2008). "MAPRes: Mining association patterns among preferred amino acid residues in the vicinity of amino acids targeted for post-translational modifications". Proteomics 8 (10): 1954­1958. doi:10.1002/pmic.200700657. PMID 18491291.

9.7 External links
· SPMF includes open-source implementations of GSP, PrefixSpan, SPADE, SPAM an many others.

Chapter 10
Bayesian network
Sprinkler

Rain



 

Grass wet
A simple Bayesian network. Rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet.
A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if m parent nodes represent m Boolean variables then the probability function could be represented by a table of 2m entries, one entry for each of the 2m possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks.
46

10.1. EXAMPLE

47

Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.

10.1 Example

SPRINKLER

RAIN T

F

F 0.4 0.6 T 0.01 0.99

SPRINKLER

RAIN

RAIN

T

F

0.2 0.8

GRASS WET

GRASS WET

SPRINKLER RAIN T

F

F

F 0.0 1.0

F

T 0.8 0.2

T

F 0.9 0.1

T

T 0.99 0.01

A simple Bayesian network with conditional probability tables
Suppose that there are two events which could cause grass to be wet: either the sprinkler is on or it's raining. Also, suppose that the rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler is usually not turned on). Then the situation can be modeled with a Bayesian network (shown to the right). All three variables have two possible values, T (for true) and F (for false).
The joint probability function is:

P(G, S, R) = P(G | S, R)P(S | R)P(R)
where the names of the variables have been abbreviated to G = Grass wet (yes/no), S = Sprinkler turned on (yes/no), and R = Raining (yes/no). The model can answer questions like "What is the probability that it is raining, given the grass is wet?" by using the conditional probability formula and summing over all nuisance variables:

P(R

=

T

|

G

=

T)

=

P(G = T, R = P(G = T )

T)

=

 S{T,F }
S,R{T

P(G = T, S, ,F } P(G = T

R=T , S, R)

)

Using the expansion for the joint probability function P(G, S, R) and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,

48

CHAPTER 10. BAYESIAN NETWORK

P(G = T, S = T, R = T ) = P(G = T | S = T, R = T )P(S = T | R = T )P(R = T ) = 0.99 × 0.01 × 0.2 = 0.00198.
Then the numerical results (subscripted by the associated variable values) are

P(R = T | G = T ) =

0.00198T T T + 0.1584T F T

0.00198T T T + 0.288T T F + 0.1584T F T + 0.0T F F

=

891 2491



35.77%.

If, on the other hand, we wish to answer an interventional question: "What is the probability that it would rain, given that we wet the grass?" the answer would be governed by the post-intervention joint distribution function P(S, R | do(G = T )) = P (S | R)P (R) obtained by removing the factor P(G | S, R) from the pre-intervention distribution. As expected, the probability of rain is unaffected by the action: P(R | do(G = T )) = P (R) .
If, moreover, we wish to predict the impact of turning the sprinkler on, we have

P (R, G | do(S = T )) = P (R)P (G | R, S = T )
with the term P (S = T | R) removed, showing that the action has an effect on the grass but not on the rain.
These predictions may not be feasible when some of the variables are unobserved, as in most policy evaluation problems. The effect of the action do(x) can still be predicted, however, whenever a criterion called "back-door" is satisfied.[1][2] It states that, if a set Z of nodes can be observed that d-separates[3] (or blocks) all back-door paths from X to Y then P (Y, Z | do(x)) = P (Y, Z, X = x)/P (X = x | Z) . A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called "sufficient" or "admissible." For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separate the (only) back-door path S  R  G. However, if S is not observed, there is no other set that d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. We then say that P(G | do(S = T)) is not "identified." This reflects the fact that, lacking interventional data, we cannot determine if the observed dependence between S and G is due to a causal connection or is spurious (apparent dependence arising from a common cause, R). (see Simpson's paradox)
To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of "do-calculus"[1][4] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[5]
Using a Bayesian network can save considerable amounts of memory, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 210 = 1024 values. If the local distributions of no variable depends on more than three parent variables, the Bayesian network representation only needs to store at most 10 · 23 = 80 values.
One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.

10.2 Inference and learning
There are three main inference tasks for Bayesian networks.

10.2. INFERENCE AND LEARNING

49

10.2.1 Inferring unobserved variables
Because a Bayesian network is a complete model for the variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to find out updated knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when one wants to choose values for the variable subset which minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.
The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space-time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation, and variational methods.

10.2.2 Parameter learning
In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on a distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, one commonly specifies the conditional distribution for the hidden state's temporal evolution to maximize the entropy rate of the implied stochastic process.)
Often these conditional distributions include parameters which are unknown and must be estimated from data, sometimes using the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex when there are unobserved variables. A classical approach to this problem is the expectation-maximization algorithm which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions this process converges on maximum likelihood (or maximum posterior) values for parameters.
A more fully Bayesian approach to parameters is to treat parameters as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, so in practice classical parameter-setting approaches are more common.

10.2.3 Structure learning
In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications the task of defining the network is too complex for humans. In this case the network structure and the parameters of the local distributions must be learned from data. Automatically learning the graph structure of a Bayesian network is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl (1987)[6] and rests on the distinction between the three possible types of adjacent triplets allowed in a directed acyclic graph (DAG):
1. X  Y  Z
2. X  Y  Z
3. X  Y  Z

50

CHAPTER 10. BAYESIAN NETWORK

Type 1 and type 2 represent the same dependencies ( X and Z are independent given Y ) and are, therefore, indistinguishable. Type 3, however, can be uniquely identified, since X and Z are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when X and Z have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independencies observed.[1][7][8][9]
An alternative method of structural learning uses optimization based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al.[10][11] discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.
Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[12]
A Bayesian network can be augmented with nodes and edges using rule-based machine learning techniques. Inductive logic programming can be used to mine rules and create new nodes.[13] Statistical relational learning (SRL) approaches use a scoring function based on the Bayes network structure to guide the structural search and augment the network.[14] A common SRL scoring function is the area under the ROC curve.

10.3 Statistical introduction
Given data x and parameter  , a simple Bayesian analysis starts with a prior probability (prior) p() and likelihood p(x | ) to compute a posterior probability p( | x)  p(x | )p() . Often the prior on  depends in turn on other parameters  that are not mentioned in the likelihood. So, the prior p() must be replaced by a likelihood p( | ) , and a prior p() on the newly introduced parameters  is required, resulting in a posterior probability

p(, |x)  p(x|)p(|)p(). This is the simplest example of a hierarchical Bayes model. The process may be repeated; for example, the parameters  may depend in turn on additional parameters  , which will require their own prior. Eventually the process must terminate, with priors that do not depend on any other unmentioned parameters.
10.3.1 Introductory examples
Suppose we have measured the quantities x1, . . . , xn each with normally distributed errors of known standard deviation ,
xi  N (i, 2) Suppose we are interested in estimating the i . An approach would be to estimate the i using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply

i = xi

10.4. DEFINITIONS AND CONCEPTS

51

However, if the quantities are related, so that for example we may think that the individual i have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model,
e.g.,

xi  N (i, 2),
i  N (,  2)
with improper priors   flat,   flat  (0, ) . When n  3 , this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual i will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.
10.3.2 Restrictions on priors
Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable  in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will be improper (not normalizable), and estimates made by minimizing the expected loss will be inadmissible.

10.4 Definitions and concepts
See also: Glossary of graph theory § Directed acyclic graphs
There are several equivalent definitions of a Bayesian network. For all the following, let G = (V,E) be a directed acyclic graph (or DAG), and let X = (Xv)v  V be a set of random variables indexed by V.

10.4.1 Factorization definition

X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can

be written as a product of the individual density functions, conditional on their parent variables:[15]

(

)

p(x) = vV p xv xpa(v)

where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).

For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional

probabilities using the chain rule (given a topological ordering of X) as follows:[15]

P(X1

=

x1,

.

..

,

Xn

=

xn)

=

n
v=1

P

(Xv

=

xv

|

Xv+1

=

xv+1,

..

.

, Xn

=

xn)

Compare this with the definition above, which can be written as:

P(X1

=

x1,

.

.

.

,

Xn

=

xn)

=

n
v=1

P(Xv

= xv

| Xj

= xj for each Xj

which is a parent of Xv )

The difference between the two expressions is the conditional independence of the variables from any of their non-

descendants, given the values of their parent variables.

10.4.2 Local Markov property
X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[16]

52

CHAPTER 10. BAYESIAN NETWORK

Xv  XV \de(v) | Xpa(v) all forv  V where de(v) is the set of descendants and V \ de(v) is the set of non-descendants of v. This can also be expressed in terms similar to the first definition, as
P(Xv = xv | Xi = xi for each Xi which is not a descendant of Xv ) = P (Xv = xv | Xj = xj for each Xj which is a parent of Xv )
Note that the set of parents is a subset of the set of non-descendants because the graph is acyclic.
10.4.3 Developing Bayesian networks
To develop a Bayesian network, we often first develop a DAG G such that we believe X satisfies the local Markov property with respect to G. Sometimes this is done by creating a causal DAG. We then ascertain the conditional probability distributions of each variable given its parents in G. In many cases, in particular in the case where the variables are discrete, if we define the joint distribution of X to be the product of these conditional distributions, then X is a Bayesian network with respect to G.[17]
10.4.4 Markov blanket
The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. This set renders it independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[16]
d-separation
This definition can be made more general by defining the "d"-separation of two nodes, where d stands for directional.[18][19] Let P be a trail from node u to v. A trail is a loop-free, undirected path between two nodes (i.e. the direction of edges is ignored for constructing the path), in which edges may have any direction. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:
1. P contains a directed chain, u . . .  m  . . . v or u . . .  m  . . . v , such that the middle node m is in Z, 2. P contains a fork, u . . .  m  . . . v , such that the middle node m is in Z, or 3. P contains an inverted fork (or collider), u . . .  m  . . . v , such that the middle node m is not in Z and no
descendant of m is in Z.
Thus u and v are said to be d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are called d-connected. X is a Bayesian network with respect to G if, for any two nodes u, v:
Xu  Xv | XZ where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)

10.5. INFERENCE COMPLEXITY AND APPROXIMATION ALGORITHMS

53

10.4.5 Hierarchical models
The term hierarchical model is sometimes considered a particular type of Bayesian network, but has no formal definition. Sometimes the term is reserved for models with three or more levels of random variables; other times, it is reserved for models with latent variables. In general, however, any moderately complex Bayesian network is usually termed "hierarchical".
10.4.6 Causal networks
Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv is causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:

a  b  c and a  b  c
are equivalent: that is they impose exactly the same conditional independence requirements.
A causal network is a Bayesian network with an explicit requirement that the relationships be causal. The additional semantics of the causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to the one of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[1] Using these semantics, one can predict the impact of external interventions from data obtained prior to intervention.

10.5 Inference complexity and approximation algorithms
In 1990 while working at Stanford University on large bioinformatic applications, Greg Cooper proved that exact inference in Bayesian networks is NP-hard.[20] This result prompted a surge in research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[21] First, they proved that there is no tractable deterministic algorithm that can approximate probabilistic inference to within an absolute error < 1/2. Second, they proved that there is no tractable randomized algorithm that can approximate probabilistic inference to within an absolute error  < 1/2 with confidence probability greater than 1/2.
At about the same time, Dan Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a CNF formula) and that approximate inference, even for Bayesian networks with restricted architecture, is NP-hard.[22][23]
In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[24] was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 1/p(n) where p(n) was any polynomial on the number of nodes in the network n.

10.6 Applications
Bayesian networks are used for modelling beliefs in computational biology and bioinformatics (gene regulatory networks, protein structure, gene expression analysis,[25] learning epistasis from GWAS data sets[26]) medicine,[27] biomonitoring,[28] document classification, information retrieval,[29] semantic search,[30] image processing, data fusion, decision support systems,[31] engineering, sports betting,[32][33] gaming, law,[34][35][36] study design[37] and risk analysis.[38] There are texts applying Bayesian networks to bioinformatics[39][40] and financial and marketing informatics.[41][42]

54

CHAPTER 10. BAYESIAN NETWORK

10.6.1 Software
· WinBUGS One of the first computational implementations of MCMC samplers. No longer maintained and not recommended for active use.
· OpenBUGS (website), further (open source) development of WinBUGS.
· Just another Gibbs sampler (JAGS) (website) Another open source alternative to WinBUGS. Uses Gibbs sampling.
· Stan (software) (website) -- Stan is an open-source package for obtaining Bayesian inference using the No-UTurn sampler, a variant of Hamiltonian Monte Carlo. It's somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors. RStan is the R interface to Stan. It is maintained by Andrew Gelman and colleagues.
· Direct Graphical Models (DGM) is an open source C++ library, implementing various tasks in probabilistic graphical models with pairwise dependencies.
· OpenMarkov, open source software and API implemented in Java
· Graphical Models Toolkit (GMTK) -- GMTK is an open source, publicly available toolkit for rapidly prototyping statistical models using dynamic graphical models (DGMs) and dynamic Bayesian networks (DBNs). GMTK can be used for applications and research in speech and language processing, bioinformatics, activity recognition, and any time series application.
· PyMC -- PyMC is a python module that implements Bayesian statistical models and fitting algorithms, including Markov chain Monte Carlo. Its flexibility and extensibility make it applicable to a large suite of problems. Along with core sampling functionality, PyMC includes methods for summarizing output, plotting, goodness-of-fit and convergence diagnostics.
· GeNIe&Smile (website) -- SMILE is a C++ library for BN and ID, and GeNIe is a GUI for it
· SamIam (website), a Java-based system with GUI and Java API
· Bayes Server - User Interface and API for Bayesian networks, includes support for time series and sequences
· Belief and Decision Networks on AIspace
· BayesiaLab by Bayesia
· Hugin
· AgenaRisk
· Netica by Norsys
· dVelox by Apara Software
· System Modeler by Inatas AB
· UnBBayes by GIA-UnB (Intelligence Artificial Group - University of Brasilia)
· using the Facial Dysmorphology Novel Analysis FDNA technology
· Uninet -- Continuous Bayesian networks modelling continuous variables, with a wide range of parametric and non-parametric marginal distributions, and dependence with copula. Hybrid discrete continuous models are also supported. Free for non-commercial use. Developed by LightTwist Software.
· Tetrad, an open-source project written in Java and developed by the Department of Philosophy at Carnegie Mellon University, that deals with causal models and statistical data.
· Dezide

10.7. HISTORY

55

10.7 History
The term "Bayesian networks" was coined by Judea Pearl in 1985 to emphasize three aspects:[43]
1. The often subjective nature of the input information.
2. The reliance on Bayes' conditioning as the basis for updating information.
3. The distinction between causal and evidential modes of reasoning, which underscores Thomas Bayes' posthumously published paper of 1763.[44]
In the late 1980s Judea Pearl's text Probabilistic Reasoning in Intelligent Systems[45] and Richard E. Neapolitan's text Probabilistic Reasoning in Expert Systems[46] summarized the properties of Bayesian networks and established Bayesian networks as a field of study. Informal variants of such networks were first used by legal scholar John Henry Wigmore, in the form of Wigmore charts, to analyse trial evidence in 1913.[35]:66­76 Another variant, called path diagrams, was developed by the geneticist Sewall Wright[47] and used in social and behavioral sciences (mostly with linear parametric models).

10.8 See also
· Artificial intelligence · Bayes' theorem · Bayesian inference · Bayesian probability · Bayesian programming · Belief propagation · Causal loop diagram · Chow­Liu tree · Computational intelligence · Computational phylogenetics · Deep belief network · Dempster­Shafer theory ­ a Generalization of Bayes' theorem · Dynamic Bayesian network · Expectation­maximization algorithm · Factor graph · Graphical model · Hierarchical temporal memory · Influence diagram · Judea Pearl · Kalman filter

56

CHAPTER 10. BAYESIAN NETWORK

· Machine learning · Memory-prediction framework · Mixture distribution · Mixture model · Naive Bayes classifier · Path analysis · Polytree · Sensor fusion · Sequence alignment · Speech recognition · Structural equation modeling · Subjective logic · Variable-order Bayesian network · Wigmore chart · World view

10.9 Notes
[1] Pearl, Judea (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press. ISBN 0-521-77362-8. OCLC 42291253.
[2] "The Back-Door Criterion" (PDF). Retrieved 2014-09-18.
[3] "d-Separation without Tears" (PDF). Retrieved 2014-09-18.
[4] J., Pearl (1994). "A Probabilistic Calculus of Actions". In Lopez de Mantaras, R.; Poole, D. UAI'94 Proceedings of the Tenth international conference on Uncertainty in artificial intelligence. San Mateo CA: Morgan Kaufman. pp. 454­462. ISBN 155860-332-8.
[5] I. Shpitser, J. Pearl, "Identification of Conditional Interventional Distributions" In R. Dechter and T.S. Richardson (Eds.), Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, 437­444, Corvallis, OR: AUAI Press, 2006.
[6] Rebane, G. and Pearl, J., "The Recovery of Causal Poly-trees from Statistical Data," Proceedings, 3rd Workshop on Uncertainty in AI, (Seattle, WA) pages 222­228, 1987
[7] Spirtes, P.; Glymour, C. (1991). "An algorithm for fast recovery of sparse causal graphs" (PDF). Social Science Computer Review. 9 (1): 62­72. doi:10.1177/089443939100900106.
[8] Spirtes, Peter; Glymour, Clark N.; Scheines, Richard (1993). Causation, Prediction, and Search (1st ed.). Springer-Verlag. ISBN 978-0-387-97979-3.
[9] Verma, Thomas; Pearl, Judea (1991). "Equivalence and synthesis of causal models". In Bonissone, P.; Henrion, M.; Kanal, L.N.; Lemmer, J.F. UAI '90 Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence. Elsevier. pp. 255­270. ISBN 0-444-89264-8.
[10] Friedman, Nir; Geiger, Dan; Goldszmidt, Moises (November 1997). "Bayesian Network Classifiers". Machine Learning. 29 (2-3): 131­163. doi:10.1023/A:1007465528199. Retrieved 24 February 2015.

10.9. NOTES

57

[11] Friedman, Nir; Linial, Michal; Nachman, Iftach; Pe'er, Dana (August 2000). "Using Bayesian Networks to Analyze Expression Data". Journal of Computational Biology. 7 (3-4): 601­620. doi:10.1089/106652700750050961. PMID 11108481. Retrieved 24 February 2015.
[12] Petitjean, F.; Webb, G.I.; Nicholson, A.E. (2013). Scaling log-linear analysis to high-dimensional data (PDF). International Conference on Data Mining. Dallas, TX, USA: IEEE.
[13] Nassif, Houssam; Wu, Yirong; Page, David; Burnside, Elizabeth (2012). "Logical Differential Prediction Bayes Net, Improving Breast Cancer Diagnosis for Older Women" (PDF). American Medical Informatics Association Symposium (AMIA'12). Chicago: 1330­1339. Retrieved 18 July 2014.
[14] Nassif, Houssam; Kuusisto, Finn; Burnside, Elizabeth S; Page, David; Shavlik, Jude; Santos Costa, Vitor (2013). "Score As You Lift (SAYL): A Statistical Relational Learning Approach to Uplift Modeling" (PDF). European Conference on Machine Learning (ECML'13). Prague: 595­611.
[15] Russell & Norvig 2003, p. 496.
[16] Russell & Norvig 2003, p. 499.
[17] Neapolitan, Richard E. (2004). Learning Bayesian networks. Prentice Hall. ISBN 978-0-13-012534-7.
[18] Geiger, Dan; Verma, Thomas; Pearl, Judea (1990). "Identifying independence in Bayesian Networks" (PDF). Networks. 20: 507­534. doi:10.1177/089443939100900106.
[19] Richard Scheines, D-separation
[20] Gregory F. Cooper (1990). "The Computational Complexity of Probabilistic Inference Using Bayesian Belief Networks" (PDF). Artificial Intelligence. 42: 393­405. doi:10.1016/0004-3702(90)90060-d.
[21] Paul Dagum; Michael Luby (1993). "Approximating probabilistic inference in Bayesian belief networks is NP-hard". Artificial Intelligence. 60 (1): 141­153. doi:10.1016/0004-3702(93)90036-b.
[22] D. Roth, On the hardness of approximate reasoning, IJCAI (1993)
[23] D. Roth, On the hardness of approximate reasoning, Artificial Intelligence (1996)
[24] [] Paul Dagum; Michael Luby (1997). "An optimal approximation algorithm for Bayesian inference". Artificial Intelligence. 93 (1-2): 1­27. doi:10.1016/s0004-3702(97)00013-1.
[25] Friedman, N.; Linial, M.; Nachman, I.; Pe'er, D. (2000). "Using Bayesian Networks to Analyze Expression Data". Journal of Computational Biology. 7 (3­4): 601­620. doi:10.1089/106652700750050961. PMID 11108481.
[26] Jiang, X.; Neapolitan, R.E.; Barmada, M.M.; Visweswaran, S. (2011). "Learning Genetic Epistasis using Bayesian Network Scoring Criteria". BMC Bioinformatics. 12: 89. doi:10.1186/1471-2105-12-89. PMC 3080825 . PMID 21453508.
[27] J. Uebersax (2004). Genetic Counseling and Cancer Risk Modeling: An Application of Bayes Nets. Marbella, Spain: Ravenpack International.
[28] Jiang X, Cooper GF (July­August 2010). "A Bayesian spatio-temporal method for disease outbreak detection". J Am Med Inform Assoc. 17 (4): 462­71. doi:10.1136/jamia.2009.000356. PMC 2995651 . PMID 20595315.
[29] Luis M. de Campos; Juan M. Fernández-Luna; Juan F. Huete (2004). "Bayesian networks and information retrieval: an introduction to the special issue". Information Processing & Management. Elsevier. 40 (5): 727­733. doi:10.1016/j.ipm.2004.03.001. ISBN 0-471-14182-8.
[30] Christos L. Koumenides and Nigel R. Shadbolt. 2012. Combining link and content-based information in a Bayesian inference model for entity search. In Proceedings of the 1st Joint International Workshop on Entity-Oriented and Semantic Search (JIWES '12). ACM, New York, NY, USA, , Article 3 , 6 pages. doi:10.1145/2379307.2379310
[31] F.J. Díez; J. Mira; E. Iturralde; S. Zubillaga (1997). "DIAVAL, a Bayesian expert system for echocardiography". Artificial Intelligence in Medicine. 10 (1): 59­73. doi:10.1016/s0933-3657(97)00384-9. PMID 9177816.
[32] Constantinou, Anthony; Fenton, N.; Neil, M. (2012). "pi-football: A Bayesian network model for forecasting Association Football match outcomes". Knowledge-Based Systems. 36: 322­339. doi:10.1016/j.knosys.2012.07.008.

58

CHAPTER 10. BAYESIAN NETWORK

[33] Constantinou, Anthony; Fenton, N.; Neil, M. (2013). "Profiting from an inefficient Association Football gambling market: Prediction, Risk and Uncertainty using Bayesian networks.". Knowledge-Based Systems. 50: 60­86. doi:10.1016/j.knosys.2013.05.008.
[34] G. A. Davis (2003). "Bayesian reconstruction of traffic accidents". Law, Probability and Risk. 2 (2): 69­89. doi:10.1093/lpr/2.2.69.
[35] J. B. Kadane & D. A. Schum (1996). A Probabilistic Analysis of the Sacco and Vanzetti Evidence. New York: Wiley. ISBN 0-471-14182-8.
[36] O. Pourret, P. Naim & B. Marcot (2008). Bayesian Networks: A Practical Guide to Applications. Chichester, UK: Wiley. ISBN 978-0-470-06030-8.
[37] Karvanen, Juha (2014). "Study design in causal models". Scandinavian Journal of Statistics. 42: 361­377. doi:10.1111/sjos.12110.
[38] Trucco, P.; Cagno, E.; Ruggeri, F.; Grande, O. (2008). "A Bayesian Belief Network modelling of organisational factors in risk analysis: A case study in maritime transportation". Reliability Engineering & System Safety. 93 (6): 845­856. doi:10.1016/j.ress.2007.03.035.
[39] Neapolitan, Richard (2009). Probabilistic Methods for Bioinformatics. Burlington, MA: Morgan Kaufmann. p. 406. ISBN 9780123704764.
[40] Grau J.; Ben-Gal I.; Posch S.; Grosse I. (2006. "VOMBAT: Prediction of Transcription Factor Binding Sites using Variable Order Bayesian Trees," (PDF). Nucleic Acids Research, vol. 34, issue W529­W533, 2006.
[41] Neapolitan, Richard & Xia Jiang (2007). Probabilistic Methods for Financial and Marketing Informatics. Burlingon, MA: Morgan Kaufmann. p. 432. ISBN 0123704774.
[42] Shmilovici A., Kahiri Y., Ben-Gal I., Hauser S.(2009. "Measuring the Efficiency of the Intraday Forex Market with a Universal Data Compression Algorithm," (PDF). Computational Economics, Vol. 33 (2), 131-154, 2009.
[43] Pearl, J. (1985). Bayesian Networks: A Model of Self-Activated Memory for Evidential Reasoning (UCLA Technical Report CSD-850017). Proceedings of the 7th Conference of the Cognitive Science Society, University of California, Irvine, CA. pp. 329­334. Retrieved 2009-05-01.
[44] Bayes, T.; Price, Mr. (1763). "An Essay towards solving a Problem in the Doctrine of Chances". Philosophical Transactions of the Royal Society. 53: 370­418. doi:10.1098/rstl.1763.0053.
[45] Pearl, J. Probabilistic Reasoning in Intelligent Systems. San Francisco CA: Morgan Kaufmann. p. 1988. ISBN 1558604790.
[46] Neapolitan, Richard E. (1989). Probabilistic reasoning in expert systems: theory and algorithms. Wiley. ISBN 978-0-47161840-9.
[47] Wright, S. (1921). "Correlation and Causation" (PDF). Journal of Agricultural Research. 20 (7): 557­585.

10.10 References
· Ben-Gal, Irad (2007). "Bayesian Networks". In Ruggeri, Fabrizio; Kennett, Ron S.; Faltin, Frederick W. Encyclopedia of Statistics in Quality and Reliability (PDF). Encyclopedia of Statistics in Quality and Reliability. John Wiley & Sons. doi:10.1002/9780470061572.eqr089. ISBN 978-0-470-01861-3.
· Bertsch McGrayne, Sharon. The Theory That Would not Die. Yale.
· Borgelt, Christian; Kruse, Rudolf (March 2002). Graphical Models: Methods for Data Analysis and Mining. Chichester, UK: Wiley. ISBN 0-470-84337-3.
· Borsuk, Mark Edward (2008). "Ecological informatics: Bayesian networks". In Jørgensen , Sven Erik; Fath, Brian. Encyclopedia of Ecology. Elsevier. ISBN 978-0-444-52033-3.
· Castillo, Enrique; Gutiérrez, José Manuel; Hadi, Ali S. (1997). "Learning Bayesian Networks". Expert Systems and Probabilistic Network Models. Monographs in computer science. New York: Springer-Verlag. pp. 481­528. ISBN 0-387-94858-9.
· Comley, Joshua W.; Dowe, David L. (June 2003). "General Bayesian networks and asymmetric languages". Proceedings of the 2nd Hawaii International Conference on Statistics and Related Fields. Hawaii.

10.10. REFERENCES

59

· Comley, Joshua W.; Dowe, David L. (2005). "Minimum Message Length and Generalized Bayesian Nets with Asymmetric Languages". In Grünwald, Peter D.; Myung, In Jae; Pitt, Mark A. Advances in Minimum Description Length: Theory and Applications. Neural information processing series. Cambridge, Massachusetts: Bradford Books (MIT Press) (published April 2005). pp. 265­294. ISBN 0-262-07262-9. (This paper puts decision trees in internal nodes of Bayes networks using Minimum Message Length (MML). Camera-ready final version submitted on 15 October 2003. An earlier version is Comley and Dowe (2003), .pdf.)
· Darwiche, Adnan (2009). Modeling and Reasoning with Bayesian Networks. Cambridge University Press. ISBN 978-0521884389.
· Dowe, David L. (2010). MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness, in Handbook of Philosophy of Science (Volume 7: Handbook of Philosophy of Statistics), Elsevier, ISBN 978-0-444-51862-0, pp 901­982.
· Fenton, Norman; Neil, Martin E. (November 2007). Managing Risk in the Modern World: Applications of Bayesian Networks ­ A Knowledge Transfer Report from the London Mathematical Society and the Knowledge Transfer Network for Industrial Mathematics. London (England): London Mathematical Society.
· Fenton, Norman; Neil, Martin E. (July 23, 2004). "Combining evidence in risk analysis using Bayesian Networks" (PDF). Safety Critical Systems Club Newsletter. 13 (4). Newcastle upon Tyne, England. pp. 8­13.
· Andrew Gelman; John B Carlin; Hal S Stern; Donald B Rubin (2003). "Part II: Fundamentals of Bayesian Data Analysis: Ch.5 Hierarchical models". Bayesian Data Analysis. CRC Press. pp. 120­. ISBN 978-1-58488-388-3.
· Heckerman, David (March 1, 1995). "Tutorial on Learning with Bayesian Networks". In Jordan, Michael Irwin. Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Massachusetts: MIT Press (published 1998). pp. 301­354. ISBN 0-262-60032-3..
Also appears as Heckerman, David (March 1997). "Bayesian Networks for Data Mining". Data Mining and Knowledge Discovery. 1 (1): 79­119. doi:10.1023/A:1009730122752. An earlier version appears as Technical Report MSR-TR-95-06, Microsoft Research, March 1, 1995. The paper is about both parameter and structure learning in Bayesian networks.
· Jensen, Finn V; Nielsen, Thomas D. (June 6, 2007). Bayesian Networks and Decision Graphs. Information Science and Statistics series (2nd ed.). New York: Springer-Verlag. ISBN 978-0-387-68281-5.
· Karimi, Kamran; Hamilton, Howard J. (2000). "Finding temporal relations: Causal bayesian networks vs. C4. 5" (PDF). Twelfth International Symposium on Methodologies for Intelligent Systems.
· Korb, Kevin B.; Nicholson, Ann E. (December 2010). Bayesian Artificial Intelligence. CRC Computer Science & Data Analysis (2nd ed.). Chapman & Hall (CRC Press). doi:10.1007/s10044-004-0214-5. ISBN 1-58488-387-1.
· Lunn, David; Spiegelhalter, David; Thomas, Andrew; Best, Nicky; et al. (November 2009). "The BUGS project: Evolution, critique and future directions". Statistics in Medicine. 28 (25): 3049­3067. doi:10.1002/sim.3680. PMID 19630097.
· Neil, Martin; Fenton, Norman E.; Tailor, Manesh (August 2005). Greenberg, Michael R., ed. "Using Bayesian Networks to Model Expected and Unexpected Operational Losses" (pdf). Risk Analysis. 25 (4): 963­972. doi:10.1111/j.1539-6924.2005.00641.x. PMID 16268944.
· Pearl, Judea (September 1986). "Fusion, propagation, and structuring in belief networks". Artificial Intelligence. 29 (3): 241­288. doi:10.1016/0004-3702(86)90072-X.
· Pearl, Judea (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Representation and Reasoning Series (2nd printing ed.). San Francisco, California: Morgan Kaufmann. ISBN 0-934613-73-7.
· Pearl, Judea; Russell, Stuart (November 2002). "Bayesian Networks". In Arbib, Michael A. Handbook of Brain Theory and Neural Networks. Cambridge, Massachusetts: Bradford Books (MIT Press). pp. 157­160. ISBN 0-262-01197-2.

60

CHAPTER 10. BAYESIAN NETWORK

· Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.
· Zhang, Nevin Lianwen; Poole, David (May 1994). "A simple approach to Bayesian network computations". Proceedings of the Tenth Biennial Canadian Artificial Intelligence Conference (AI-94). Banff, Alberta: 171­178. This paper presents variable elimination for belief networks.

10.11 Further reading
· Computational Intelligence: A Methodological Introduction by Kruse, Borgelt, Klawonn, Moewes, Steinbrecher, Held, 2013, Springer, ISBN 9781447150121
· Graphical Models - Representations for Learning, Reasoning and Data Mining, 2nd Edition, by Borgelt, Steinbrecher, Kruse, 2009, J. Wiley & Sons, ISBN 9780470749562
· Bayesian Netwrks and BayesiaLab - A practical introduction for researchers by Stefan Conrady and Lionel Jouffe

10.12 External links
· A tutorial on learning with Bayesian Networks · An Introduction to Bayesian Networks and their Contemporary Applications · On-line Tutorial on Bayesian nets and probability · Web-App to create Bayesian nets and run it with a Monte Carlo method · Continuous Time Bayesian Networks · Bayesian Networks: Explanation and Analogy · A live tutorial on learning Bayesian networks · A hierarchical Bayes Model for handling sample heterogeneity in classification problems, provides a classification
model taking into consideration the uncertainty associated with measuring replicate samples. · Hierarchical Naive Bayes Model for handling sample uncertainty, shows how to perform classification and learning
with continuous and discrete variables with replicated measurements.

Chapter 11
Naive Bayes classifier
In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950's. It was introduced under a different name into the text retrieval community in the early 1960's,[1]:488 and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines.[2] It also finds application in automatic medical diagnosis.[3] Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,[1]:718 which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. In the statistics and computer science literature, Naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes.[4] All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method;[4] Russell and Norvig note that "[naive Bayes] is sometimes called a Bayesian classifier, a somewhat careless usage that has prompted some Bayesians to call it the idiot Bayes model."[1]:482
11.1 Introduction
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness and diameter features. For some types of probability models, naive Bayes classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods. Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers.[5] Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.[6]
61

62

CHAPTER 11. NAIVE BAYES CLASSIFIER

An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification.

11.2 Probabilistic model
Abstractly, naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector x = (x1, . . . , xn) representing some n features (independent variables), it assigns to this instance probabilities

p(Ck|x1, . . . , xn)
for each of K possible outcomes or classes Ck .[7]
The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. We therefore reformulate the model to make it more tractable. Using Bayes' theorem, the conditional probability can be decomposed as

p(Ck|x)

=

p(Ck) p(x|Ck) p(x)

In plain English, using Bayesian probability terminology, the above equation can be written as

posterior

=

prior × likelihood evidence

In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on C and
the values of the features Fi are given, so that the denominator is effectively constant. The numerator is equivalent to the joint probability model

p(Ck, x1, . . . , xn) which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:
p(Ck, x1, . . . , xn) = p(x1, . . . , xn, Ck) = p(x1|x2, . . . , xn, Ck)p(x2, . . . , xn, Ck) = p(x1|x2, . . . , xn, Ck)p(x2|x3, . . . , xn, Ck)p(x3, . . . , xn, Ck) = ... = p(x1|x2, . . . , xn, Ck)p(x2|x3, . . . , xn, Ck) . . . p(xn-1|xn, Ck)p(xn|Ck)p(Ck)
Now the "naive" conditional independence assumptions come into play: assume that each feature Fi is conditionally independent of every other feature Fj for j = i , given the category C . This means that
p(xi|xi+1, . . . , xn, Ck) = p(xi|Ck) Thus, the joint model can be expressed as
p(Ck|x1, . . . , xn)  p(Ck, x1, . . . , xn)  p(Ck) p(x1|Ck) p(x2|Ck) p(x3|Ck) · · · n  p(Ck) p(xi|Ck) .
i=1

11.3. PARAMETER ESTIMATION AND EVENT MODELS

63

This means that under the above independence assumptions, the conditional distribution over the class variable C is:

1

n

p(Ck|x1, . . . , xn) = Z p(Ck) p(xi|Ck)

i=1

where the evidence Z = p(x) is a scaling factor dependent only on x1, . . . , xn , that is, a constant if the values of the feature variables are known.

11.2.1 Constructing a classifier from the probability model
The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label y^ = Ck for some k as follows:

n

y^ = argmax p(Ck) p(xi|Ck).

k{1,...,K}

i=1

11.3 Parameter estimation and event models

A class' prior may be calculated by assuming equiprobable classes (i.e., priors = 1 / (number of classes)), or by calculating an estimate for the class probability from the training set (i.e., (prior for a given class) = (number of samples in the class) / (total number of samples)). To estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the features from the training set.[8]
The assumptions on distributions of features are called the event model of the Naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused.[9][10]

11.3.1 Gaussian naive Bayes
When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution. For example, suppose the training data contain a continuous attribute, x . We first segment the data by the class, and then compute the mean and variance of x in each class. Let µc be the mean of the values in x associated with class c, and let c2 be the variance of the values in x associated with class c. Suppose we have collected some observation value v . Then, the probability distribution of v given a class c , p(x = v|c) , can be computed by plugging v into the equation for a Normal distribution parameterized by µc and c2 . That is,

p(x = v|c) =  1

e-

(v-µc 2c2

)2

2c2

Another common technique for handling continuous values is to use binning to discretize the feature values, to obtain a
new set of Bernoulli-distributed features; some literature in fact suggests that this is necessary to apply naive Bayes, but it is not, and the discretization may throw away discriminative information.[4]

11.3.2 Multinomial naive Bayes
With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial (p1, . . . , pn) where pi is the probability that event i occurs (or K such multinomials in the

64

CHAPTER 11. NAIVE BAYES CLASSIFIER

multiclass case). A feature vector x = (x1, . . . , xn) is then a histogram, with xi counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram x is given by

p(x|Ck)

=

 (i xi)!
i xi!

 pkixi
i

The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:[2]

(

)

n

log p(Ck|x)  log p(Ck) pkixi

i=1

n = log p(Ck) + xi · log pki

i=1

= b + wkx

where b = log p(Ck) and wki = log pki .

If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.

Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible
ways to alleviate those problems, including the use of tf­idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.[2]

11.3.3 Bernoulli naive Bayes
In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks,[9] where binary term occurrence features are used rather than term frequencies. If xi is a boolean expressing the occurrence or absence of the i'th term from the vocabulary, then the likelihood of a document given a class Ck is given by[9]
n p(x|Ck) = pxkii (1 - pki)(1-xi)
i=1
where pki is the probability of class Ck generating the term wi . This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.

11.3.4 Semi-supervised parameter estimation
Given a way to train a naive Bayes classifier from labeled data, it's possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop:[11]
Given a collection D = L  U of labeled samples L and unlabeled samples U, start by training a naive Bayes classifier on L. Until convergence, do:

11.4. DISCUSSION

65

Predict class probabilities P (C|x) for all examples x in D . Re-train the model based on the probabilities (not the labels) predicted in the previous step.
Convergence is determined based on improvement to the model likelihood P (D|) , where  denotes the parameters of the naive Bayes model.
This training algorithm is an instance of the more general expectation­maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naive Bayes is the M-step. The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem.[11]

11.4 Discussion
Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities,[12] this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model.[3] Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below.

11.4.1 Relation to logistic regression
In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generativediscriminative pair with (multinomial) logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood p(C, x) , while logistic regression fits the same probability model to optimize the conditional p(C|x) .[13]
The link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as "predict class C1 if the odds of p(C1|x) exceed those of p(C2|x) ". Expressing this in log-space gives:

log

p(C1|x) p(C2|x)

=

log p(C1|x) - log p(C2|x)

>

0

The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression. Since naive Bayes is also a linear model for the two "discrete" event models, it can be reparametrised as a linear function b + wx > 0 . Obtaining the probabilities is then a matter of applying the logistic function to b + wx , or in the multiclass case, the softmax function.

Discriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has
shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster.[13]

11.5 Examples
11.5.1 Gender classification
Problem: classify whether a given person is a male or a female based on the measured features. The features include height, weight, and foot size.

66

CHAPTER 11. NAIVE BAYES CLASSIFIER

Training
Example training set below. The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances): Let's say we have equiprobable classes so P(male)= P(female) = 0.5. This prior probability distribution might be based on our knowledge of frequencies in the larger population, or on frequency in the training set.
Testing
Below is a sample to be classified as a male or female. We wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by
(male) posterior = P (male) p(height|male) p(weight|male) p(size foot|male) evidence
For the classification as female the posterior is given by
(female) posterior = P (female) p(height|female) p(weight|female) p(size foot|female) evidence
The evidence (also termed normalizing constant) may be calculated:
evidence = P (male) p(height|male) p(weight|male) p(size foot|male) +P (female) p(height|female) p(weight|female) p(size foot|female)
However, given the sample the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored. We now determine the probability distribution for the sex of the sample.

P (male) = 0.5

p(height|male)

=

1 
22

exp ( -(6 - µ)2 ) 22



1.5789

where µ = 5.855 and 2 = 3.5033 · 10-2 are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here ­ it is a probability density rather than a

probability, because height is a continuous variable.

p(weight|male) = 5.9881 · 10-6
p(size foot|male) = 1.3112 · 10-3 (male) numerator posterior = product their = 6.1984 · 10-9 P (female) = 0.5 p(height|female) = 2.2346 · 10-1 p(weight|female) = 1.6789 · 10-2 p(size foot|female) = 2.8669 · 10-1 (female) numerator posterior = product their = 5.3778 · 10-4 Since posterior numerator is greater in the female case, we predict the sample is female.

11.5. EXAMPLES

67

11.5.2 Document classification
Here is a worked example of naive Bayesian classification to the document classification problem. Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modelled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as

p(wi |C )
(For this treatment, we simplify things further by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.) Then the probability that a given document D contains all of the words wi , given a class C, is
 p(D|C) = p(wi|C)
i
The question that we desire to answer is: "what is the probability that a given document D belongs to a given class C?" In other words, what is p(C|D) ? Now by definition

p(D  C) p(D|C) =
p(C ) and

p(D  C) p(C|D) =
p(D) Bayes' theorem manipulates these into a statement of probability in terms of likelihood.

p(C )

p(C|D) =

p(D|C )

p(D)

Assume for the moment that there are only two mutually exclusive classes, S and ¬S (e.g. spam and not spam), such that every element (email) is in either one or the other;

 p(D|S) = p(wi|S)
i
and

 p(D|¬S) = p(wi|¬S)
i
Using the Bayesian result above, we can write:

p(S)  p(S|D) = p(D) p(wi|S)
i

68

p(¬S) 

p(¬S|D) = p(D)

p(wi|¬S)

i

Dividing one by the other gives:

 p(S|D) = p(S) i p(wi|S) p(¬S|D) p(¬S) i p(wi|¬S) Which can be re-factored as:

CHAPTER 11. NAIVE BAYES CLASSIFIER

p(S|D) = p(S)  p(wi|S) p(¬S|D) p(¬S) i p(wi|¬S)
Thus, the probability ratio p(S | D) / p(¬S | D) can be expressed in terms of a series of likelihood ratios. The actual probability p(S | D) can be easily computed from log (p(S | D) / p(¬S | D)) based on the observation that p(S | D) + p(¬S | D) = 1.
Taking the logarithm of all these ratios, we have:

ln

p(S|D)

= ln

p(S)

 + ln

p(wi|S)

p(¬S|D)

p(¬S) i p(wi|¬S)

(This technique of "log-likelihood ratios" is a common technique in statistics. In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)

Finally,

the

document

can

be

classified

as

follows.

It

is

spam

if

p(S|D)

>

p(¬S|D)

(i.e.,

ln

p(S|D) p(¬S|D)

>

0

),

otherwise

it

is not spam.

11.6 See also
· AODE · Bayesian spam filtering · Bayesian network · Random naive Bayes · Linear classifier · Logistic regression · Perceptron · Take-the-best heuristic

11.7 References
[1] Russell, Stuart; Norvig, Peter (2003) [1995]. Artificial Intelligence: A Modern Approach (2nd ed.). Prentice Hall. ISBN 978-0137903955.
[2] Rennie, J.; Shih, L.; Teevan, J.; Karger, D. (2003). Tackling the poor assumptions of Naive Bayes classifiers (PDF). ICML.

11.8. EXTERNAL LINKS

69

[3] Rish, Irina (2001). An empirical study of the naive Bayes classifier (PDF). IJCAI Workshop on Empirical Methods in AI.
[4] Hand, D. J.; Yu, K. (2001). "Idiot's Bayes -- not so stupid after all?". International Statistical Review. 69 (3): 385­399. doi:10.2307/1403452. ISSN 0306-7734.
[5] Zhang, Harry. The Optimality of Naive Bayes (PDF). FLAIRS2004 conference.
[6] Caruana, R.; Niculescu-Mizil, A. (2006). An empirical comparison of supervised learning algorithms. Proc. 23rd International Conference on Machine Learning. CiteSeerX: 10.1.1.122.5901.
[7] Narasimha Murty, M.; Susheela Devi, V. (2011). Pattern Recognition: An Algorithmic Approach. ISBN 0857294946.
[8] John, George H.; Langley, Pat (1995). Estimating Continuous Distributions in Bayesian Classifiers. Proc. Eleventh Conf. on Uncertainty in Artificial Intelligence. Morgan Kaufmann. pp. 338­345.
[9] McCallum, Andrew; Nigam, Kamal (1998). A comparison of event models for Naive Bayes text classification (PDF). AAAI-98 workshop on learning for text categorization.
[10] Metsis, Vangelis; Androutsopoulos, Ion; Paliouras, Georgios (2006). Spam filtering with Naive Bayes--which Naive Bayes?. Third conference on email and anti-spam (CEAS).
[11] Nigam, Kamal; McCallum, Andrew; Thrun, Sebastian; Mitchell, Tom (2000). "Learning to classify text from labeled and unlabeled documents using EM" (PDF). Machine Learning.
[12] Niculescu-Mizil, Alexandru; Caruana, Rich (2005). Predicting good probabilities with supervised learning (PDF). ICML. doi:10.1145/1102351.1102430.
[13] Ng, Andrew Y.; Jordan, Michael I. (2002). On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. NIPS.
11.7.1 Further reading
· Domingos, Pedro; Pazzani, Michael (1997). "On the optimality of the simple Bayesian classifier under zero-one loss". Machine Learning. 29: 103­137.
· Webb, G. I.; Boughton, J.; Wang, Z. (2005). "Not So Naive Bayes: Aggregating One-Dependence Estimators". Machine Learning. Springer. 58 (1): 5­24. doi:10.1007/s10994-005-4258-6.
· Mozina, M.; Demsar, J.; Kattan, M.; Zupan, B. (2004). Nomograms for Visualization of Naive Bayesian Classifier (PDF). Proc. PKDD-2004. pp. 337­348.
· Maron, M. E. (1961). "Automatic Indexing: An Experimental Inquiry". JACM. 8 (3): 404­417. doi:10.1145/321075.321084.
· Minsky, M. (1961). Steps toward Artificial Intelligence. Proc. IRE. pp. 8­30.

11.8 External links
· Book Chapter: Naive Bayes text classification, Introduction to Information Retrieval · Naive Bayes for Text Classification with Unbalanced Classes · Benchmark results of Naive Bayes implementations · Hierarchical Naive Bayes Classifiers for uncertain data (an extension of the Naive Bayes classifier).
Software
· Naive Bayes classifiers are available in many general-purpose machine learning and NLP packages, including Apache Mahout, Mallet, NLTK, Orange, scikit-learn and Weka.

70

CHAPTER 11. NAIVE BAYES CLASSIFIER

· IMSL Numerical Libraries Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Data mining routines in the IMSL Libraries include a Naive Bayes classifier.
· An interactive Microsoft Excel spreadsheet Naive Bayes implementation using VBA (requires enabled macros) with viewable source code.
· jBNC - Bayesian Network Classifier Toolbox
· Statistical Pattern Recognition Toolbox for Matlab.
· ifile - the first freely available (Naive) Bayesian mail/spam filter
· NClassifier - NClassifier is a .NET library that supports text classification and text summarization. It is a port of Classifier4J.
· Classifier4J - Classifier4J is a Java library designed to do text classification. It comes with an implementation of a Bayesian classifier.

11.9. TEXT AND IMAGE SOURCES, CONTRIBUTORS, AND LICENSES

71

11.9 Text and image sources, contributors, and licenses
11.9.1 Text
· Decision tree learning Source: https://en.wikipedia.org/wiki/Decision_tree_learning?oldid=733878686 Contributors: Nealmcb, Michael Hardy, TheEternalVortex, Greenrd, Maximus Rex, Tschild, Populus, Khalid hassani, Pgan002, Raand, Dan aka jack, Andreas Kaufmann, Discospinster, John Vandenberg, Giraffedata, Mdd, Equinoxe, VeXocide, InBalance, Bushytails, GregorB, Qwertyus, Rjwilmsi, Gmelli, Salix alba, Vonkje, Cedar101, SmackBot, Diegotorquemada, Mcld, Riedl, Zven, Mitar, Krexer, Kuru, Jrouquie, Beetstra, Courcelles, Ceran, Pgr94, Yaris678, Talgalili, Headbomb, A3RO, Nobar, Martinkunev, Destynova, Jessicapierce, Dobi~enwiki, User A1, Jalaska13, A m sheldon, Xs2me, Polyextremophile, Semifinalist, Extabgrad, Dodabe~enwiki, Foxj, Stephen Milborrow, Dank, Pichpich, Addbot, AndrewHZ, MrOllie, Download, Peni, Yobot, TestEditBot, AnomieBOT, Jim1138, Royote, Citation bot, FrescoBot, Hobsonlane, X7q, Kelos omos1, Orchidbox, Citation bot 1, Boxplot, Thinking of England, Janez Demsar, Mwojnars, Gzorg, Onel5969, Wik-dt, Chad.burrus, Bethnim, ZéroBot, Chire, Liorrokach, Pxtreme75, ClueBot NG, Psorakis, Aristitleism, Frietjes, Widr, Helpful Pixie Bot, BG19bot, QualitycontrolUS, BendelacBOT, Mightyteja, Djplaner, CitationCleanerBot, A923812, Golmschenk, Sboosali, BattyBot, Douglas H Fisher, Zhang1989cn, JYBot, Dexbot, Lizhengui, Declaration1776, Jey42, Mgibby5, Pimgd, SiraRaven, Slash1986, Prubbens, Monkbot, 00tau, HossPatrol, DizzyRebel, Mpritham, Fmadd and Anonymous: 96
· ID3 algorithm Source: https://en.wikipedia.org/wiki/ID3_algorithm?oldid=729209083 Contributors: Zeno Gantner, Hike395, Charles Matthews, Dcoetzee, Babbage, Bdachev0710, Enochlau, Giftlite, Matt Crypto, Andreas Kaufmann, Interiot, Alai, GregorB, Zzyzx11, Qwertyus, Evandrojr, Fijal, Bgwhite, YurikBot, AndyBoySouthPas, Varano, Cedar101, Zmoboros, Diegotorquemada, Yingyang0, Dead3y3, Pgr94, Andre.holzner, Don Quixote de la Mancha, Martarius, EoGuy, The 11th plague of Egypt, PixelBot, XLinkBot, Addbot, Download, Yobot, Choij, Xqbot, X7q, TobeBot, Pestrickland, Danielebrunello, AsTeRfr~enwiki, Gostrc, Antiqueight, CitationCleanerBot, Rotacidni, Sajith.janaprasad, Nthns43, JayantGuptaIIIT, Lcmgcd, JJMC89, Polyfill, InTheUniversal, In veritas, Acoggins38 and Anonymous: 55
· C4.5 algorithm Source: https://en.wikipedia.org/wiki/C4.5_algorithm?oldid=727181226 Contributors: BAxelrod, Charles Matthews, Zeiden, Bdachev0710, Thaukko~enwiki, Pgan002, Andreas Kaufmann, Qutezuce, Violetriga, BlueNovember, Alansohn, Wtmitchell, Male1979, Qwertyus, Fijal, YurikBot, Personman, JLaTondre, Deepdraft, SmackBot, Diegotorquemada, Betacommand, OSborn, Stl~enwiki, Bumbulski, Nkarthiks, Whenning, Rayjapan, Acertain, AntiSpamBot, Jamesontai, Jura05, VolkovBot, Anoko moonlight, SheepNotGoats, Weston.pace, Svick, WDavis1911, Howie Goodell, Addbot, Realmahbub, Yobot, AnomieBOT, X7q, Mostafa mahdieh, Roboo.jack, Larry.europe, XAPKOHHEH, ZéroBot, ChuispastonBot, ClueBot NG, Rulequest, Helpful Pixie Bot, BG19bot, Ctsourak, Chafe66, BattyBot, Hmainsbot1, Yonathaniel and Anonymous: 52
· CHAID Source: https://en.wikipedia.org/wiki/CHAID?oldid=716634621 Contributors: Edward, Ronz, Urhixidur, D6, Zzuuzz, SmackBot, Hmains, Thegn, Wybot, Wizardman, Chris53516, Ioannes Pragensis, .anacondabot, Magioladitis, Xerxes minor, Jfroelich, Ericmelse, Melcombe, Sunsetsky, Addbot, AndrewHZ, Lightbot, Colinpendred, X7q, Greenboite, PigFlu Oink, Delmonde, Thetexasranger, Moondoggie999, Frze, UdnyYule, Rainer.park and Anonymous: 13
· Random forest Source: https://en.wikipedia.org/wiki/Random_forest?oldid=734542216 Contributors: Michael Hardy, Willsmith, Zeno Gantner, Ronz, Den fjättrade ankan~enwiki, Hike395, Nstender, Giftlite, Neilc, Pgan002, Sam Hocevar, Urhixidur, Andreas Kaufmann, Rich Farmbrough, Bender235, O18, Rajah, Ferkel, 3mta3, Knowledge Seeker, Rrenaud, Justin Ormont, Qwertyus, Rjwilmsi, Punk5, Nigosh, Mathbot, LuisPedroCoelho, Bgwhite, RussBot, Dsol, Diegotorquemada, Mcld, Bluebot, Eep1mp, Cybercobra, Mitar, Ben Moore, Shorespirit, Ninetyone, Innohead, Bumbulski, Jason Dunsmore, Talgalili, Thijs!bot, Tolstoy the Cat, Headbomb, Utopiah, Baccyak4H, Hue White, David Eppstein, Trusilver, Yogeshkumkar12, Dvdpwiki, Gerifalte~enwiki, MrSampson, Jim.Callahan,Orlando, WereSpielChequers, Melcombe, Headlessplatter, Jashley13, Xiawi, Alexbot, Dboehmer, MystBot, Addbot, AndrewHZ, Bastion Monk, MrOllie, Jperl, Legobot, Yobot, AnomieBOT, Randomexpert, Jim1138, Citation bot, Twri, V35b, Nippashish, Sgtf, X7q, Dront, Yurislator, Delmonde, Sulhan, John of Reading, Dcirovic, ZéroBot, Chire, Jwollbold, Pokbot, V.cheplygina, Joel B. Lewis, EmmanuelleGouillart, Helpful Pixie Bot, BG19bot, QualitycontrolUS, Spaligo, Stevetihi, Solomon7968, Schreckse, A923812, Fylbecatulous, JoshuSasori, JimmyJimmereeno, ChrisGualtieri, Dexbot, Kosio.the.truthseeker, IOverThoughtThis, Bvlb, Svershin, FooCow, Austrartsua, Kyungeui, Monkbot, Prashanth india, Loraof, HossPatrol, Dongkyu Kim, Puxiao129, StudentDH, CodieLarkin, Nepaxt, Djenks243 and Anonymous: 99
· Gradient boosting Source: https://en.wikipedia.org/wiki/Gradient_boosting?oldid=735155349 Contributors: Ronz, Topbanana, Benwing, Bender235, Violetriga, Justin Ormont, Qwertyus, Cedar101, Thrasibule, Hongooi, Dgianotti, Medovina, Andre.holzner, Jazzcat81, RichardLWhite, Semifinalist, JeffDonner, MrOllie, Davedev15, Yobot, Jtamad, LilHelpa, Sophus Bie, X7q, Yihucha166, Trappist the monk, CristiCbz, Alephnot, Chire, HHinman, Mark viking, Pprettenhofer, DerHessi, Romain.bossart, Paul2520, Crowwork, P.thesling, Engheta, XQQ14, Gary2015 and Anonymous: 31
· Association rule learning Source: https://en.wikipedia.org/wiki/Association_rule_learning?oldid=730165995 Contributors: SimonP, Michael Hardy, Angela, Azazello, Witbrock, Dfrankow, Neilc, Raand, Urhixidur, Adambro, Stesmo, WilliamKF, Qwertyus, Rjwilmsi, GünniX, Pseudomonas, Grafen, Gareth Jones, Crasshopper, Chughgaurav~enwiki, NHSavage, SmackBot, Reedy, Amux, Chris the speller, Mitar, Lambiam, Dicklyon, Beefyt, CmdrObot, ShelfSkewed, Liquider, Harrigan, UberScienceNerd, Qwertyplus, Magioladitis, Jeffreydiehl, A3nm, David Eppstein, Pythonner, Jnnnnn, Samtheboy, Dvdpwiki, Cobi, Hamman Samuel, Themacolyte, MichaelSchoenitzer, TXiKiBoT, Coastside, Kotsiantis, Jlpinar83, Autofei, Niceguyedc, Auntof6, Xodarap00, Stephengmatthews, Alokito, Rahul234, Life of Riley, Sunsetsky, Addbot, MichaelMampaey, Mhahsler, Aelkris, MrOllie, Greg4cr, Favonian, Yobot, Wim Leers, KamikazeBot, AnomieBOT, Nyubis, Broncobus, Materialscientist, Citation bot, LilHelpa, Andrewmc123, FrescoBot, Citation bot 1, RedBot, Geoffrey I Webb, Trappist the monk, Cincoutprabu, Ali hadian, RjwilmsiBot, Mango bush, 2aprilboy, Frostyandy2k, Jbr jbr, MartinThoma, Donner60, Chiu.chienpei, ChuispastonBot, Phoglenix, Pokbot, Kounoupis, ClueBot NG, Helpful Pixie Bot, HMSSolent, BG19bot, Uksas, Himanshujain123, Jdubin, AnsafSalleb, Ftrxx, Dexbot, Rahulkj, TwoTwoHello, 7804j, Behroozomidvar, Dataesp, Dexterous1802, Rmasba, Kr4gfo87, Dsousacosta, Eracle.adeluca, Denny73, Monkbot, 4costlygrace, Nicoabie, D Bhalodia, Dr.shaheen.khan, Joselunaariza, Ramezanics, Gingerlime, SnazzyFiend, ACB Smith, Bradley.sigma, Dndm97, Lana2235, Tehkidevo and Anonymous: 134
· Apriori algorithm Source: https://en.wikipedia.org/wiki/Apriori_algorithm?oldid=734148868 Contributors: Timo Honkasalo, Michael Hardy, Kku, Maximus Rex, Altenmann, Enochlau, Pgan002, Listener~enwiki, Exa~enwiki, Sonett72, Phreed, Bender235, Kbh3rd, Davidgothberg,

72

CHAPTER 11. NAIVE BAYES CLASSIFIER

Pearle, BlueNovember, Ddddan, Kdau, Male1979, Qwertyus, Mohawkjohn, DVdm, Martin Hinks, DanMS, Alex.Szatmary, Cedar101, Bask, SmackBot, Windrago, Brothers, SB Johnny, Memming, Viebel, SQGibbon, Dicklyon, Beefyt, Liquider, Simeon, Thijs!bot, Headbomb, Applemeister, Magioladitis, A3nm, Piojo, Ncusa367, Mange01, Redskin9, BagpipingScotsman, Cobi, VolkovBot, AgamemnonZ, SueHay, EverGreg, Calliopejen1, Desouky, DeXXus, Kotsiantis, EPadmirateur, Sfan00 IMG, Akiry, Xodarap00, Chaosdruid, VSEPR, Wmwallace, XLinkBot, Mdruiter, Addbot, Aelkris, Yobot, Sumail, Jim1138, Anreto, A.amitkumar, Thepbac, LucienBOT, HJ Mitchell, Gigiiity goo, Horcrux92, Shafaet, EmausBot, Tommy2010, ZéroBot, TheLunarSage, Omargamil, ClueBot NG, Frietjes, Kagundu, Loriendrew, Dexbot, Adrek14, John mikol, Iwilsonp, 21t cog, Stefpac and Anonymous: 111
· Sequential pattern mining Source: https://en.wikipedia.org/wiki/Sequential_pattern_mining?oldid=728312772 Contributors: Michael Hardy, Rp, Kku, ZimZalaBim, Witbrock, Andreas Kaufmann, Bender235, Natarajanganesan, Joerg Kurt Wegner, Qwertyus, Rjwilmsi, SmackBot, Khazar, Alaibot, Cs california, Darklilac, John b cassel, A3nm, Infrangible, Alexbateman, ImZenith, Ikhono, Addbot, Yobot, Niksab, Erik9bot, HRoestBot, Jonesey95, Bolufe, Trappist the monk, Jesse V., EmausBot, Jj98, Dexbot, Fakharian.m, Selvi muthukumar, Wmqazi, Segedyjr, Stamptrader, Monkbot, Fredrik.erl and Anonymous: 25
· Bayesian network Source: https://en.wikipedia.org/wiki/Bayesian_network?oldid=733993517 Contributors: Ap, Fnielsen, SimonP, Fccoelho, ChangChienFu, Axon, Edward, Michael Hardy, Modster, Lexor, Ctwardy, Kku, Delirium, Eric119, Angela, Jordi Burguet Castell, Hike395, Kgajos, Hyacinth, Roachmeister, Benwing, Gidonb, Wile E. Heresiarch, Cutler, David Gerard, Giftlite, BenFrantzDale, Bfinn, Everyking, Neilc, Estel~enwiki, MarkSweep, Gene s, Urhixidur, Robin klein, Naku~enwiki, Rich Farmbrough, Guanabot, Mattlaabs~enwiki, Bender235, Neko-chan, MisterSheik, Shadow demon, Skinkie~enwiki, 3mta3, Haham hanuka, Terrycojones, Samohyl Jan, Ceyockey, Oleg Alexandrov, Linas, Henrik, Mindmatrix, Bluegrass, BlaiseFEgan, Marudubshinki, BD2412, Qwertyus, Kbdank71, Rjwilmsi, Salix alba, Oblivious, Mathbot, CarolGray, Fresheneesz, Chobot, Adoniscik, YurikBot, Wavelength, KamuiShirou, Pi Delport, Ogai, Buster79, Jpbowen, Balizarde, Kyle Cronan, DaveWF, Bruyninc~enwiki, SmackBot, Zanetu, UmassThrower, David Poole, Commander Keane bot, RDBrown, MalafayaBot, Jdthood, RichardHudson, Trucmuche~enwiki, Ohconfucius, Lambiam, Freewol, Arialblack, IronGargoyle, Dfass, Codesimian, Hu12, Kurtan~enwiki, Joostvandeputte~enwiki, CmdrObot, PuerExMachina, Phobius, Vizier, AndrewHowse, Skittleys, The.snake, Paddles, Lfrittelli, Letranova, Headbomb, EdJohnston, AnAj, Smartse, Tomixdf, Mack2, AndreasWittenstein, Ph.eyes, Magioladitis, Jarekt, Destynova, Johnbibby, Trioculite, A3nm, David Eppstein, Gregheth, Rajashar, Mpanahi, Iccaldwell, MoA)gnome, Andre.holzner, Absurdburger, AgarwalSumeet, Causenet, LiveLearn, MarcoLittel, Daniel5Ko, Normanfenton, DavidCBryant, The enemies of god, VolkovBot, Jim.Callahan,Orlando, Camrn86, JohnBlackburne, AlnoktaBOT, Maghnus, Siddharth.mv, Mtanti, Neversay.misher, Silya, Daniel347x, Andrewaskew, Nathanpowell, Kbkorb, Chilti, SieBot, IradBG, Garde, Ddxc, OKBot, CharlesGillingham, Melcombe, Kvihill, Sonarpulse, Leisink, Wsun, Tomas e, Mild Bill Hiccup, Angelferrer, Dandinpantelimon, ClickStudent, Thomas Kist, DragonBot, Tsourakakis, Brews ohare, Practical321, Qwfp, MairAW, Kolyma, Erreip, J Hazard, Addbot, DOI bot, Zariski83, AndrewHZ, Esolu, Tassedethe, Lightbot, Gail, Zorrobot, Wireless friend, Luckasbot, Yobot, Twexcom, Abhikshah, FoxLemmor, Ausaen, AnomieBOT, Rubinbot, Wrongfilter, Puhfyn, Citation bot, Zhlsh1113, Xqbot, J04n, Omnipaedista, RibotBOT, SassoBot, Mattg82, FrescoBot, Olexa Riznyk, Lebd, Citation bot 1, Jonesey95, Escribiente, Meborsuk, Trappist the monk, Grsmvg, PapaJupe, RjwilmsiBot, Ripchip Bot, EmausBot, John of Reading, WikitanvirBot, Kercker, Hous21, Converge on truth, Dcirovic, HiW-Bot, Cskudzu, Shaider iba, Erget2005, Akseli.palen, JGS2010, ClueBot NG, Probinf, Tatome, Arrandale, Frietjes, Habil zare, Rxnt, BG19bot, Danielkorzekwa, Papadim.G, Chafe66, Jwatson89, Giliev, Raspabill, Enterprisey, Dexbot, Peripattikos, Foerstj, Mogism, Mark viking, Sieste, Fpetitjean~enwiki, Jodosma, Mikayé, Djsyclik, Paul2520, Anrnusna, Stamptrader, Paheld, Monkbot, Priyankp87, Abacenis, Ibsen 13, Mathewk1300, Laeciosantosbsb, Widianpear, Qqq06, Ereator, Pknad, Gliall, Sulabh.avatar, Illinoisnois, Ckling and Anonymous: 209
· Naive Bayes classifier Source: https://en.wikipedia.org/wiki/Naive_Bayes_classifier?oldid=734442649 Contributors: The Anome, Awaterl, Olivier, Michael Hardy, Bewildebeast, Zeno Gantner, Karada, Cyp, Den fjättrade ankan~enwiki, Hike395, Njoshi3, WhisperToMe, Toreau, Phil Boswell, RedWolf, Bkell, Wile E. Heresiarch, Giftlite, Akella, JimD, Bovlb, Macrakis, Neilc, Pgan002, MarkSweep, Gene s, Cagri, Anirvan, Trevor MacInnis, Thorwald, Splatty, Rich Farmbrough, Violetriga, Peterjoel, Smalljim, John Vandenberg, BlueNovember, Jason Davies, Caesura, Oleg Alexandrov, KKramer~enwiki, Btyner, Mandarax, Qwertyus, Rjwilmsi, Hgkamath, Johnnyw, Mathbot, Intgr, Sderose, Kri, YurikBot, Wavelength, PiAndWhippedCream, Cancan101, Bovineone, Arichnad, Karipuf, BOT-Superzerocool, Evryman, Johndburger, Cedar101, Mebden, XAVeRY, SmackBot, InverseHypercube, CommodiCast, Stimpy, Yamaguchi , ToddDeLuca, Gilliam, NickGarvey, Chris the speller, Dster, OrangeDog, PerVognsen, Can't sleep, clown will eat me, Memming, Mitar, Neshatian, Jklin, Ringger, WMod-NS, Tobym, Shorespirit, Mat1971, Dstanfor, Arauzo, Dantiston, Sytelus, Vera Rita~enwiki, Dkemper, Prolog, Ninjakannon, Jrennie, MSBOT, Coffee2theorems, Tremilux, Saurabh911, Robotman1974, David Eppstein, User A1, HebrewHammerTime, AllenDowney, Troos, AntiSpamBot, Newtman, STBotD, Mike V, RJASE1, VolkovBot, Maghnus, Anna Lincoln, Mbusux, Anders gorm, EverGreg, Fcady2007, Jojalozzo, Ddxc, Dchwalisz, AlanUS, Melcombe, Headlessplatter, Kotsiantis, Justin W Smith, Motmahp, Calimo, Dianegarey, Doobliebop, Alousybum, Sunsetsky, XLinkBot, Herlocker, Addbot, RPHv, Tsunanet, MrOllie, LaaknorBot, Yobot, TaBOT-zerem, Twexcom, AnomieBOT, Rubinbot, Smk65536, The Almighty Bob, Cantons-de-l'Est, , FrescoBot, X7q, Proffviktor, Svourdroculed, Rickyphyllis, Jonesey95, Serols, Geoffrey I Webb, Classifier1234, Mwojnars, Wingiii, Larry.europe, Helwr, EmausBot, Orphan Wiki, Tommy2010, Theonefoster, GarouDan, Joseagonzalez, ClueBot NG, Hofmic, NilsHaldenwang, Luoli2000, BG19bot, MusikAnimal, Chafe66, Kavishwar.wagholikar, Geduowenyang, Hipponix, Fcbarbi, Librawill, ChrisGualtieri, XMU zhangy, Dexbot, Alialamifard, CorvetteC6RVip, Jamesmcmahon0, OhGodItsSoAmazing, Tonytonov, Jmagasin, ScienceRandomness, Qingyuanxingsi, Micpalmia, Sofia Koutsouveli, Yuchsiao, Mvdyck, Don neufeld, YoniSmolin, Rapanshi, Ananth.sankar.1963, Hmerzic, Tamas.kenez, Mihaeos, DatGuy and Anonymous: 201
11.9.2 Images
· File:Animation2.gif Source: https://upload.wikimedia.org/wikipedia/commons/c/c0/Animation2.gif License: CC-BY-SA-3.0 Contributors: Own work Original artist: MG (talk · contribs)
· File:CART_tree_titanic_survivors.png Source: https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png License: CC BY-SA 3.0 Contributors: Own work Original artist: Stephen Milborrow
· File:Fisher_iris_versicolor_sepalwidth.svg Source: https://upload.wikimedia.org/wikipedia/commons/4/40/Fisher_iris_versicolor_sepalwidth. svg License: CC BY-SA 3.0 Contributors: en:Image:Fisher iris versicolor sepalwidth.png Original artist: en:User:Qwfp (original); Pbroks13 (talk) (redraw)

11.9. TEXT AND IMAGE SOURCES, CONTRIBUTORS, AND LICENSES

73

· File:Free-to-read_lock_75.svg Source: https://upload.wikimedia.org/wikipedia/commons/8/80/Free-to-read_lock_75.svg License: CC0 Contributors: Adapted from <a href='//en.wikipedia.org/wiki/File:Open_Access_logo_PLoS_white_green.svg' class='image' title='Open_Access_logo_PLoS_white_green.svg'><im alt='Open_Access_logo_PLoS_white_green.svg' src='//upload.wikimedia.org/wikipedia/commons/thumb/9/90/Open_Access_logo_PLoS_white_ green.svg/9px-Open_Access_logo_PLoS_white_green.svg.png' width='9' height='14' srcset='//upload.wikimedia.org/wikipedia/commons/thumb/ 9/90/Open_Access_logo_PLoS_white_green.svg/14px-Open_Access_logo_PLoS_white_green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/ commons/thumb/9/90/Open_Access_logo_PLoS_white_green.svg/18px-Open_Access_logo_PLoS_white_green.svg.png 2x' data-file-width='640' data-file-height='1000' /></a> Original artist: This version:Trappist_the_monk (talk) (Uploads)
· File:FrequentItems.png Source: https://upload.wikimedia.org/wikipedia/commons/0/0c/FrequentItems.png License: CC BY-SA 3.0 Contributors: Own work (Original text: I (Xodarap00 (talk)) created this work entirely by myself.) Original artist: Xodarap00 (talk)
· File:ID3_algorithm_decision_tree.png Source: https://upload.wikimedia.org/wikipedia/commons/4/46/ID3_algorithm_decision_tree.png License: CC BY-SA 4.0 Contributors: Own work Original artist: Acoggins38
· File:ID3_decision_tree-_splicing.png Source: https://upload.wikimedia.org/wikipedia/commons/0/09/ID3_decision_tree-_splicing.png License: CC BY-SA 4.0 Contributors: Own work Original artist: Acoggins38
· File:Mergefrom.svg Source: https://upload.wikimedia.org/wikipedia/commons/0/0f/Mergefrom.svg License: Public domain Contributors: ? Original artist: ?
· File:Portal-puzzle.svg Source: https://upload.wikimedia.org/wikipedia/en/f/fd/Portal-puzzle.svg License: Public domain Contributors: ? Original artist: ?
· File:Question_book-new.svg Source: https://upload.wikimedia.org/wikipedia/en/9/99/Question_book-new.svg License: Cc-by-sa-3.0 Contributors: Created from scratch in Adobe Illustrator. Based on Image:Question book.png created by User:Equazcion Original artist: Tkgd2007
· File:SimpleBayesNet.svg Source: https://upload.wikimedia.org/wikipedia/commons/0/0e/SimpleBayesNet.svg License: Public domain Contributors: Own work (Original text: self-made) Original artist: AnAj
· File:SimpleBayesNetNodes.svg Source: https://upload.wikimedia.org/wikipedia/commons/f/fd/SimpleBayesNetNodes.svg License: Public domain Contributors: http://en.wikipedia.org/wiki/File:SimpleBayesNet.svg Original artist: AnAj
· File:Text_document_with_red_question_mark.svg Source: https://upload.wikimedia.org/wikipedia/commons/a/a4/Text_document_with_ red_question_mark.svg License: Public domain Contributors: Created by bdesham with Inkscape; based upon Text-x-generic.svg from the Tango project. Original artist: Benjamin D. Esham (bdesham)
· File:Unbalanced_scales.svg Source: https://upload.wikimedia.org/wikipedia/commons/f/fe/Unbalanced_scales.svg License: Public domain Contributors: ? Original artist: ?
· File:Wiki_letter_w_cropped.svg Source: https://upload.wikimedia.org/wikipedia/commons/1/1c/Wiki_letter_w_cropped.svg License: CCBY-SA-3.0 Contributors: This file was derived from Wiki letter w.svg: <a href='//commons.wikimedia.org/wiki/File:Wiki_letter_w.svg' class='image'><img alt='Wiki letter w.svg' src='https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Wiki_letter_w.svg/50px-Wiki_ letter_w.svg.png' width='50' height='50' srcset='https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Wiki_letter_w.svg/75px-Wiki_ letter_w.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Wiki_letter_w.svg/100px-Wiki_letter_w.svg.png 2x' datafile-width='44' data-file-height='44' /></a> Original artist: Derivative work by Thumperward
11.9.3 Content license
· Creative Commons Attribution-Share Alike 3.0

