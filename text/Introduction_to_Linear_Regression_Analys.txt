INTRODUCTION TO LINEAR REGRESSION ANALYSIS

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice, Harvey Goldstein, Iain M. Johnstone, Geert Molenberghs, David W. Scott, Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg Editors Emeriti: Vic Barnett, J. Stuart Hunter, Joseph B. Kadane, Jozef L. Teugels
A complete list of the titles in this series appears at the end of this volume.

INTRODUCTION TO LINEAR REGRESSION ANALYSIS
Fifth Edition
DOUGLAS C. MONTGOMERY Arizona State University School of Computing, Informatics, and Decision Systems Engineering Tempe, AZ ELIZABETH A. PECK The Coca-Cola Company (retired) Atlanta, GA G. GEOFFREY VINING Virginia Tech Department of Statistics Blacksburg, VA
A JOHN WILEY & SONS, INC., PUBLICATION

Copyright © 2012 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our Customer Care Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic formats. For more information about Wiley products, visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Montgomery, Douglas C. Introduction to linear regression analysis / Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey
Vining. ­ 5th ed. p. cm. ­ (Wiley series in probability and statistics ; 821)
Includes bibliographical references and index. ISBN 978-0-470-54281-1 (hardback) 1. Regression analysis. I. Peck, Elizabeth A., 1953­ II. Vining, G. Geoffrey, 1954­ III. Title. QA278.2.M65 2012 519.5'36­dc23
2012003859
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1

CONTENTS

PREFACE

xiii

1. INTRODUCTION

1

1.1 Regression and Model Building / 1 1.2 Data Collection / 5 1.3 Uses of Regression / 9 1.4 Role of the Computer / 10

2. SIMPLE LINEAR REGRESSION

12

2.1 Simple Linear Regression Model / 12 2.2 Least-Squares Estimation of the Parameters / 13
2.2.1 Estimation of 0 and 1 / 13 2.2.2 Properties of the Least-Squares Estimators
and the Fitted Regression Model / 18 2.2.3 Estimation of 2 / 20 2.2.4 Alternate Form of the Model / 22 2.3 Hypothesis Testing on the Slope and Intercept / 22 2.3.1 Use of t Tests / 22 2.3.2 Testing Significance of Regression / 24 2.3.3 Analysis of Variance / 25 2.4 Interval Estimation in Simple Linear Regression / 29 2.4.1 Confidence Intervals on 0, 1 and 2 / 29 2.4.2 Interval Estimation of the Mean Response / 30 2.5 Prediction of New Observations / 33 2.6 Coefficient of Determination / 35

v

vi CONTENTS

2.7 A Service Industry Application of Regression / 37 2.8 Using SAS® and R for Simple Linear Regression / 39 2.9 Some Considerations in the Use of Regression / 42 2.10 Regression Through the Origin / 45 2.11 Estimation by Maximum Likelihood / 51 2.12 Case Where the Regressor x is Random / 52
2.12.1 x and y Jointly Distributed / 53 2.12.2 x and y Jointly Normally Distributed:
Correlation Model / 53 Problems / 58

3. MULTIPLE LINEAR REGRESSION

67

3.1 Multiple Regression Models / 67 3.2 Estimation of the Model Parameters / 70
3.2.1 Least-Squares Estimation of the Regression Coefficients / 71
3.2.2 Geometrical Interpretation of Least Squares / 77 3.2.3 Properties of the Least-Squares Estimators / 79 3.2.4 Estimation of 2 / 80 3.2.5 Inadequacy of Scatter Diagrams
in Multiple Regression / 82 3.2.6 Maximum-Likelihood Estimation / 83 3.3 Hypothesis Testing in Multiple Linear Regression / 84 3.3.1 Test for Significance of Regression / 84 3.3.2 Tests on Individual Regression Coefficients
and Subsets of Coefficients / 88 3.3.3 Special Case of Orthogonal Columns in X / 93 3.3.4 Testing the General Linear Hypothesis / 95 3.4 Confidence Intervals in Multiple Regression / 97 3.4.1 Confidence Intervals on the Regression Coefficients / 98 3.4.2 CI Estimation of the Mean Response / 99 3.4.3 Simultaneous Confidence Intervals on Regression
Coefficients / 100 3.5 Prediction of New Observations / 104 3.6 A Multiple Regression Model for the Patient
Satisfaction Data / 104 3.7 Using SAS and R for Basic Multiple Linear Regression / 106 3.8 Hidden Extrapolation in Multiple Regression / 107 3.9 Standardized Regression Coefficients / 111 3.10 Multicollinearity / 117 3.11 Why Do Regression Coefficients Have the Wrong Sign? / 119 Problems / 121

CONTENTS vii

4. MODEL ADEQUACY CHECKING

129

4.1 Introduction / 129 4.2 Residual Analysis / 130
4.2.1 Definition of Residuals / 130 4.2.2 Methods for Scaling Residuals / 130 4.2.3 Residual Plots / 136 4.2.4 Partial Regression and Partial Residual Plots / 143 4.2.5 Using Minitab®, SAS, and R for Residual Analysis / 146 4.2.6 Other Residual Plotting and Analysis Methods / 149 4.3 PRESS Statistic / 151 4.4 Detection and Treatment of Outliers / 152 4.5 Lack of Fit of the Regression Model / 156 4.5.1 Formal Test for Lack of Fit / 156 4.5.2 Estimation of Pure Error from Near Neighbors / 160 Problems / 165

5. TRANSFORMATIONS AND WEIGHTING

TO CORRECT MODEL INADEQUACIES

171

5.1 Introduction / 171 5.2 Variance-Stabilizing Transformations / 172 5.3 Transformations to Linearize the Model / 176 5.4 Analytical Methods for Selecting a Transformation / 182
5.4.1 Transformations on y: The Box­Cox Method / 182 5.4.2 Transformations on the Regressor Variables / 184 5.5 Generalized and Weighted Least Squares / 188 5.5.1 Generalized Least Squares / 188 5.5.2 Weighted Least Squares / 190 5.5.3 Some Practical Issues / 191 5.6 Regression Models with Random Effect / 194 5.6.1 Subsampling / 194 5.6.2 The General Situation for a Regression Model
with a Single Random Effect / 198 5.6.3 The Importance of the Mixed Model in Regression / 202 Problems / 202

6. DIAGNOSTICS FOR LEVERAGE AND INFLUENCE

211

6.1 Importance of Detecting Influential Observations / 211 6.2 Leverage / 212 6.3 Measures of Influence: Cook's D / 215 6.4 Measures of Influence: DFFITS and DFBETAS / 217 6.5 A Measure of Model Performance / 219

viii CONTENTS

6.6 Detecting Groups of Influential Observations / 220 6.7 Treatment of Influential Observations / 220 Problems / 221

7. POLYNOMIAL REGRESSION MODELS

223

7.1 Introduction / 223 7.2 Polynomial Models in One Variable / 223
7.2.1 Basic Principles / 223 7.2.2 Piecewise Polynomial Fitting (Splines) / 229 7.2.3 Polynomial and Trigonometric Terms / 235 7.3 Nonparametric Regression / 236 7.3.1 Kernel Regression / 237 7.3.2 Locally Weighted Regression (Loess) / 237 7.3.3 Final Cautions / 241 7.4 Polynomial Models in Two or More Variables / 242 7.5 Orthogonal Polynomials / 248 Problems / 254

8. INDICATOR VARIABLES

260

8.1 General Concept of Indicator Variables / 260 8.2 Comments on the Use of Indicator Variables / 273
8.2.1 Indicator Variables versus Regression on Allocated Codes / 273
8.2.2 Indicator Variables as a Substitute for a Quantitative Regressor / 274
8.3 Regression Approach to Analysis of Variance / 275 Problems / 280

9. MULTICOLLINEARITY

285

9.1 Introduction / 285 9.2 Sources of Multicollinearity / 286 9.3 Effects of Multicollinearity / 288 9.4 Multicollinearity Diagnostics / 292
9.4.1 Examination of the Correlation Matrix / 292 9.4.2 Variance Inflation Factors / 296 9.4.3 Eigensystem Analysis of X'X / 297 9.4.4 Other Diagnostics / 302 9.4.5 SAS and R Code for Generating Multicollinearity
Diagnostics / 303 9.5 Methods for Dealing with Multicollinearity / 303
9.5.1 Collecting Additional Data / 303 9.5.2 Model Respecification / 304 9.5.3 Ridge Regression / 304

CONTENTS ix

9.5.4 Principal-Component Regression / 313 9.5.5 Comparison and Evaluation of Biased Estimators / 319 9.6 Using SAS to Perform Ridge and Principal-Component Regression / 321 Problems / 323

10. VARIABLE SELECTION AND MODEL BUILDING

327

10.1 Introduction / 327 10.1.1 Model-Building Problem / 327 10.1.2 Consequences of Model Misspecification / 329 10.1.3 Criteria for Evaluating Subset Regression Models / 332
10.2 Computational Techniques for Variable Selection / 338 10.2.1 All Possible Regressions / 338 10.2.2 Stepwise Regression Methods / 344
10.3 Strategy for Variable Selection and Model Building / 351 10.4 Case Study: Gorman and Toman Asphalt Data Using SAS / 354 Problems / 367

11. VALIDATION OF REGRESSION MODELS

372

11.1 Introduction / 372 11.2 Validation Techniques / 373
11.2.1 Analysis of Model Coefficients and Predicted Values / 373 11.2.2 Collecting Fresh Data--Confirmation Runs / 375 11.2.3 Data Splitting / 377 11.3 Data from Planned Experiments / 385 Problems / 386

12. INTRODUCTION TO NONLINEAR REGRESSION

389

12.1 Linear and Nonlinear Regression Models / 389 12.1.1 Linear Regression Models / 389 12.2.2 Nonlinear Regression Models / 390
12.2 Origins of Nonlinear Models / 391 12.3 Nonlinear Least Squares / 395 12.4 Transformation to a Linear Model / 397 12.5 Parameter Estimation in a Nonlinear System / 400
12.5.1 Linearization / 400 12.5.2 Other Parameter Estimation Methods / 407 12.5.3 Starting Values / 408 12.6 Statistical Inference in Nonlinear Regression / 409 12.7 Examples of Nonlinear Regression Models / 411 12.8 Using SAS and R / 412 Problems / 416

x

CONTENTS

13. GENERALIZED LINEAR MODELS

421

13.1 Introduction / 421 13.2 Logistic Regression Models / 422
13.2.1 Models with a Binary Response Variable / 422 13.2.2 Estimating the Parameters in a Logistic
Regression Model / 423 13.2.3 Interpretation of the Parameters in
a Logistic Regression Model / 428 13.2.4 Statistical Inference on Model
Parameters / 430 13.2.5 Diagnostic Checking in Logistic
Regression / 440 13.2.6 Other Models for Binary
Response Data / 442 13.2.7 More Than Two Categorical Outcomes / 442 13.3 Poisson Regression / 444 13.4 The Generalized Linear Model / 450 13.4.1 Link Functions and Linear Predictors / 451 13.4.2 Parameter Estimation and Inference
in the GLM / 452 13.4.3 Prediction and Estimation with the GLM / 454 13.4.4 Residual Analysis in the GLM / 456 13.4.5 Using R to Perform GLM Analysis / 458 13.4.6 Overdispersion / 461 Problems / 462

14. REGRESSION ANALYSIS OF TIME SERIES DATA

474

14.1 Introduction to Regression Models for Time Series Data / 474
14.2 Detecting Autocorrelation: The Durbin-Watson Test / 475
14.3 Estimating the Parameters in Time Series Regression Models / 480
Problems / 496

15. OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

500

15.1 Robust Regression / 500 15.1.1 Need for Robust Regression / 500 15.1.2 M-Estimators / 503 15.1.3 Properties of Robust Estimators / 510

CONTENTS xi
15.2 Effect of Measurement Errors in the Regressors / 511 15.2.1 Simple Linear Regression / 511 15.2.2 The Berkson Model / 513
15.3 Inverse Estimation--The Calibration Problem / 513 15.4 Bootstrapping in Regression / 517
15.4.1 Bootstrap Sampling in Regression / 518 15.4.2 Bootstrap Confidence Intervals / 519 15.5 Classification and Regression Trees (CART) / 524 15.6 Neural Networks / 526 15.7 Designed Experiments for Regression / 529 Problems / 537

APPENDIX A. STATISTICAL TABLES

541

APPENDIX B. DATA SETS FOR EXERCISES

553

APPENDIX C. SUPPLEMENTAL TECHNICAL MATERIAL

574

C.1 Background on Basic Test Statistics / 574 C.2 Background from the Theory of Linear Models / 577 C.3 Important Results on SSR and SSRes / 581 C.4 Gauss-Markov Theorem, Var() = 2I / 587 C.5 Computational Aspects of Multiple Regression / 589 C.6 Result on the Inverse of a Matrix / 590 C.7 Development of the PRESS Statistic / 591 C.8 Development of S(2i) / 593 C.9 Outlier Test Based on R-Student / 594 C.10 Independence of Residuals and Fitted Values / 596 C.11 Gauss­Markov Theorem, Var() = V / 597 C.12 Bias in MSRes When the Model Is Underspecified / 599 C.13 Computation of Influence Diagnostics / 600 C.14 Generalized Linear Models / 601

APPENDIX D. INTRODUCTION TO SAS

613

D.1 Basic Data Entry / 614 D.2 Creating Permanent SAS Data Sets / 618 D.3 Importing Data from an EXCEL File / 619 D.4 Output Command / 620 D.5 Log File / 620 D.6 Adding Variables to an Existing SAS Data Set / 622

xii CONTENTS
APPENDIX E. INTRODUCTION TO R TO PERFORM LINEAR REGRESSION ANALYSIS
E.1 Basic Background on R / 623 E.2 Basic Data Entry / 624 E.3 Brief Comments on Other Functionality in R / 626 E.4 R Commander / 627
REFERENCES INDEX

623
628 642

PREFACE
Regression analysis is one of the most widely used techniques for analyzing multifactor data. Its broad appeal and usefulness result from the conceptually logical process of using an equation to express the relationship between a variable of interest (the response) and a set of related predictor variables. Regression analysis is also interesting theoretically because of elegant underlying mathematics and a welldeveloped statistical theory. Successful use of regression requires an appreciation of both the theory and the practical problems that typically arise when the technique is employed with real-world data.
This book is intended as a text for a basic course in regression analysis. It contains the standard topics for such courses and many of the newer ones as well. It blends both theory and application so that the reader will gain an understanding of the basic principles necessary to apply regression model-building techniques in a wide variety of application environments. The book began as an outgrowth of notes for a course in regression analysis taken by seniors and first-year graduate students in various fields of engineering, the chemical and physical sciences, statistics, mathematics, and management. We have also used the material in many seminars and industrial short courses for professional audiences. We assume that the reader has taken a first course in statistics and has familiarity with hypothesis tests and confidence intervals and the normal, t, 2, and F distributions. Some knowledge of matrix algebra is also necessary.
The computer plays a significant role in the modern application of regression. Today even spreadsheet software has the capability to fit regression equations by least squares. Consequently, we have integrated many aspects of computer usage into the text, including displays of both tabular and graphical output, and general discussions of capabilities of some software packages. We use Minitab®, JMP®, SAS®, and R for various problems and examples in the text. We selected these packages because they are widely used both in practice and in teaching regression and they have good regression. Many of the homework problems require software
xiii

xiv PREFACE
for their solution. All data sets in the book are available in electronic form from the publisher. The ftp site ftp://ftp.wiley.com/public/sci_tech_med/introduction_linear_ regression hosts the data, problem solutions, PowerPoint files, and other material related to the book.
CHANGES IN THE FIFTH EDITION
We have made extensive changes in this edition of the book. This includes the reorganization of text material, new examples, new exercises, a new chapter on time series regression, and new material on designed experiments for regression models. Our objective was to make the book more useful as both a text and a reference and to update our treatment of certain topics.
Chapter 1 is a general introduction to regression modeling and describes some typical applications of regression. Chapters 2 and 3 provide the standard results for least-squares model fitting in simple and multiple regression, along with basic inference procedures (tests of hypotheses, confidence and prediction intervals). Chapter 4 discusses some introductory aspects of model adequacy checking, including residual analysis and a strong emphasis on residual plots, detection and treatment of outliers, the PRESS statistic, and testing for lack of fit. Chapter 5 discusses how transformations and weighted least squares can be used to resolve problems of model inadequacy or to deal with violations of the basic regression assumptions. Both the Box­Cox and Box­Tidwell techniques for analytically specifying the form of a transformation are introduced. Influence diagnostics are presented in Chapter 6, along with an introductory discussion of how to deal with influential observations. Polynomial regression models and their variations are discussed in Chapter 7. Topics include the basic procedures for fitting and inference for polynomials and discussion of centering in polynomials, hierarchy, piecewise polynomials, models with both polynomial and trigonometric terms, orthogonal polynomials, an overview of response surfaces, and an introduction to nonparametric and smoothing regression techniques. Chapter 8 introduces indicator variables and also makes the connection between regression and analysis-of-variance models. Chapter 9 focuses on the multicollinearity problem. Included are discussions of the sources of multicollinearity, its harmful effects, diagnostics, and various remedial measures. We introduce biased estimation, including ridge regression and some of its variations and principalcomponent regression.Variable selection and model-building techniques are developed in Chapter 10, including stepwise procedures and all-possible-regressions. We also discuss and illustrate several criteria for the evaluation of subset regression models. Chapter 11 presents a collection of techniques useful for regression model validation.
The first 11 chapters are the nucleus of the book. Many of the concepts and examples flow across these chapters. The remaining four chapters cover a variety of topics that are important to the practitioner of regression, and they can be read independently. Chapter 12 in introduces nonlinear regression, and Chapter 13 is a basic treatment of generalized linear models. While these are perhaps not standard topics for a linear regression textbook, they are so important to students and professionals in engineering and the sciences that we would have been seriously remiss without giving an introduction to them. Chapter 14 covers regression

PREFACE xv
models for time series data. Chapter 15 includes a survey of several important topics, including robust regression, the effect of measurement errors in the regressors, the inverse estimation or calibration problem, bootstrapping regression estimates, classification and regression trees, neural networks, and designed experiments for regression.
In addition to the text material, Appendix C contains brief presentations of some additional topics of a more technical or theoretical nature. Some of these topics will be of interest to specialists in regression or to instructors teaching a more advanced course from the book. Computing plays an important role in many regression courses. Mintab, JMP, SAS, and R are widely used in regression courses. Outputs from all of these packages are provided in the text. Appendix D is an introduction to using SAS for regression problems. Appendix E is an introduction to R.
USING THE BOOK AS A TEXT
Because of the broad scope of topics, this book has great flexibility as a text. For a first course in regression, we would recommend covering Chapters 1 through 10 in detail and then selecting topics that are of specific interest to the audience. For example, one of the authors (D.C.M.) regularly teaches a course in regression to an engineering audience.Topics for that audience include nonlinear regression (because mechanistic models that are almost always nonlinear occur often in engineering), a discussion of neural networks, and regression model validation. Other topics that we would recommend for consideration are multicollinearity (because the problem occurs so often) and an introduction to generalized linear models focusing mostly on logistic regression. G.G.V. has taught a regression course for graduate students in statistics that makes extensive use of the Appendix C material.
We believe the computer should be directly integrated into the course. In recent years, we have taken a notebook computer and computer projector to most classes and illustrated the techniques as they are introduced in the lecture. We have found that this greatly facilitates student understanding and appreciation of the techniques. We also require that the students use regression software for solving the homework problems. In most cases, the problems use real data or are based on real-world settings that represent typical applications of regression.
There is an instructor's manual that contains solutions to all exercises, electronic versions of all data sets, and questions/problems that might be suitable for use on examinations.
ACKNOWLEDGMENTS
We would like to thank all the individuals who provided helpful feedback and assistance in the preparation of this book. Dr. Scott M. Kowalski, Dr. Ronald G. Askin, Dr. Mary Sue Younger, Dr. Russell G. Heikes, Dr. John A. Cornell, Dr. André I. Khuri, Dr. George C. Runger, Dr. Marie Gaudard, Dr. James W. Wisnowski, Dr. Ray Hill, and Dr. James R. Simpson made many suggestions that greatly improved both earlier editions and this fifth edition of the book. We particularly appreciate the many graduate students and professional practitioners who provided feedback,

xvi PREFACE
often in the form of penetrating questions, that led to rewriting or expansion of material in the book. We are also indebted to John Wiley & Sons, the American Statistical Association, and the Biometrika Trustees for permission to use copyrighted material.
Douglas C. Montgomery Elizabeth A. Peck G. Geoffrey Vining

CHAPTER 1

INTRODUCTION

1.1 REGRESSION AND MODEL BUILDING

Regression analysis is a statistical technique for investigating and modeling the relationship between variables. Applications of regression are numerous and occur in almost every field, including engineering, the physical and chemical sciences, economics, management, life and biological sciences, and the social sciences. In fact, regression analysis may be the most widely used statistical technique.
As an example of a problem in which regression analysis may be helpful, suppose that an industrial engineer employed by a soft drink beverage bottler is analyzing the product delivery and service operations for vending machines. He suspects that the time required by a route deliveryman to load and service a machine is related to the number of cases of product delivered. The engineer visits 25 randomly chosen retail outlets having vending machines, and the in-outlet delivery time (in minutes) and the volume of product delivered (in cases) are observed for each. The 25 observations are plotted in Figure 1.1a. This graph is called a scatter diagram. This display clearly suggests a relationship between delivery time and delivery volume; in fact, the impression is that the data points generally, but not exactly, fall along a straight line. Figure 1.1b illustrates this straight-line relationship.
If we let y represent delivery time and x represent delivery volume, then the equation of a straight line relating these two variables is

y = 0 + 1x

(1.1)

where 0 is the intercept and 1 is the slope. Now the data points do not fall exactly on a straight line, so Eq. (1.1) should be modified to account for this. Let

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
1

2

INTRODUCTION

80

80

70

70

60

60

Delivery time, y Delivery time, y

50

50

40

40

30

30

20

20

10

10

0

0

10

20

30

Delivery volume, x

(a)

0

0

10

20

30

Delivery volume, x

(b)

Figure 1.1 (a) Scatter diagram for delivery volume. (b) Straight-line relationship between delivery time and delivery volume.

the difference between the observed value of y and the straight line (0 + 1x) be an error . It is convenient to think of  as a statistical error; that is, it is a random variable that accounts for the failure of the model to fit the data exactly. The error may be made up of the effects of other variables on delivery time, measurement errors, and so forth. Thus, a more plausible model for the delivery time data is

y = 0 + 1x + 

(1.2)

Equation (1.2) is called a linear regression model. Customarily x is called the independent variable and y is called the dependent variable. However, this often causes confusion with the concept of statistical independence, so we refer to x as the predictor or regressor variable and y as the response variable. Because Eq. (1.2) involves only one regressor variable, it is called a simple linear regression model.
To gain some additional insight into the linear regression model, suppose that we can fix the value of the regressor variable x and observe the corresponding value of the response y. Now if x is fixed, the random component  on the right-hand side of Eq. (1.2) determines the properties of y. Suppose that the mean and variance of  are 0 and 2, respectively. Then the mean response at any value of the regressor variable is

E ( y | x) = y|x = E ( 0 + 1x +  ) = 0 + 1x

Notice that this is the same relationship that we initially wrote down following inspection of the scatter diagram in Figure 1.1a. The variance of y given any value of x is

Var ( y |

x)

=



2 y|x

=

Var ( 0

+

1x

+

)

=

2

Thus, the true regression model y|x = 0 + 1x is a line of mean values, that is, the height of the regression line at any value of x is just the expected value of y for that

REGRESSION AND MODEL BUILDING

3

y

Observed values of y for a given x are sampled from these distributions
N (0, 2 = 2)

Observed value of y
y |x

N (0, 2 = 2) Observed value of y

10

20

30

x

Figure 1.2 How observations are generated in linear regression.

Straight­line approximation y True relationship
x
Figure 1.3 Linear regression approximation of a complex relationship.

x. The slope, 1 can be interpreted as the change in the mean of y for a unit change in x. Furthermore, the variability of y at a particular value of x is determined by the variance of the error component of the model, 2. This implies that there is a distribution of y values at each x and that the variance of this distribution is the same at each x.
For example, suppose that the true regression model relating delivery time to delivery volume is y|x = 3.5 + 2x, and suppose that the variance is 2 = 2. Figure 1.2 illustrates this situation. Notice that we have used a normal distribution to describe the random variation in . Since y is the sum of a constant 0 + 1x (the mean) and a normally distributed random variable, y is a normally distributed random variable. For example, if x = 10 cases, then delivery time y has a normal distribution with mean 3.5 + 2(10) = 23.5 minutes and variance 2. The variance 2 determines the amount of variability or noise in the observations y on delivery time. When 2 is small, the observed values of delivery time will fall close to the line, and when 2 is large, the observed values of delivery time may deviate considerably from the line.
In almost all applications of regression, the regression equation is only an approximation to the true functional relationship between the variables of interest. These functional relationships are often based on physical, chemical, or other engineering or scientific theory, that is, knowledge of the underlying mechanism. Consequently, these types of models are often called mechanistic models. Regression models, on the other hand, are thought of as empirical models. Figure 1.3 illustrates a situation where the true relationship between y and x is relatively complex, yet it may be approximated quite well by a linear regression equation. Sometimes the underlying mechanism is more complex, resulting in the need for a more complex approximating function, as in Figure 1.4, where a "piecewise linear" regression function is used to approximate the true relationship between y and x.
Generally regression equations are valid only over the region of the regressor variables contained in the observed data. For example, consider Figure 1.5. Suppose that data on y and x were collected in the interval x1  x  x2. Over this interval the

4

INTRODUCTION

True relationship

Piecewise linear

y

y

regression approximation

x
Figure 1.4 Piecewise linear approximation of a complex relationship.

x1

x2

x3

x

Figure 1.5 The danger of extrapolation in regression.

linear regression equation shown in Figure 1.5 is a good approximation of the true relationship. However, suppose this equation were used to predict values of y for values of the regressor variable in the region x2  x  x3. Clearly the linear regression model is not going to perform well over this range of x because of model error or equation error.
In general, the response variable y may be related to k regressors, x1, x2, . . . , xk, so that

y = 0 + 1x1 + 2 x2 + + k xk + 

(1.3)

This is called a multiple linear regression model because more than one regressor is involved. The adjective linear is employed to indicate that the model is linear in the parameters 0, 1, . . . , k, not because y is a linear function of the x's. We shall see subsequently that many models in which y is related to the x's in a nonlinear fashion can still be treated as linear regression models as long as the equation is linear in the 's.
An important objective of regression analysis is to estimate the unknown parameters in the regression model. This process is also called fitting the model to the data. We study several parameter estimation techniques in this book. One of these techmques is the method of least squares (introduced in Chapter 2). For example, the least-squares fit to the delivery time data is

y^ = 3.321 + 2.1762x

where y^ is the fitted or estimated value of delivery time corresponding to a delivery volume of x cases. This fitted equation is plotted in Figure 1.1b.
The next phase of a regression analysis is called model adequacy checking, in which the appropriateness of the model is studied and the quality of the fit ascertained. Through such analyses the usefulness of the regression model may be determined. The outcome of adequacy checking may indicate either that the model is reasonable or that the original fit must be modified. Thus, regression analysis is an iterative procedure, in which data lead to a model and a fit of the model to the data is produced. The quality of the fit is then investigated, leading either to modification

DATA COLLECTION

5

of the model or the fit or to adoption of the model. This process is illustrated several times in subsequent chapters.
A regression model does not imply a cause-and-effect relationship between the variables. Even though a strong empirical relationship may exist between two or more variables, this cannot be considered evidence that the regressor variables and the response are related in a cause-and-effect manner. To establish causality, the relationship between the regressors and the response must have a basis outside the sample data--for example, the relationship may be suggested by theoretical considerations. Regression analysis can aid in confirming a cause-and-effect relationship, but it cannot be the sole basis of such a claim.
Finally it is important to remember that regression analysis is part of a broader data-analytic approach to problem solving. That is, the regression equation itself may not be the primary objective of the study. It is usually more important to gain insight and understanding concerning the system generating the data.

1.2 DATA COLLECTION
An essential aspect of regression analysis is data collection. Any regression analysis is only as good as the data on which it is based. Three basic methods for collecting data are as follows:
· A retrospective study based on historical data · An observational study · A designed experiment
A good data collection scheme can ensure a simplified and a generally more applicable model. A poor data collection scheme can result in serious problems for the analysis and its interpretation.The following example illustrates these three methods.

Example 1.1

Consider the acetone­butyl alcohol distillation column shown in Figure 1.6. The operating personnel are interested in the concentration of acetone in the distillate (product) stream. Factors that may influence this are the reboil temperature, the condensate temperature, and the reflux rate. For this column, operating personnel maintain and archive the following records:
· The concentration of acetone in a test sample taken every hour from the product stream
· The reboil temperature controller log, which is a plot of the reboil temperature
· The condenser temperature controller log · The nominal reflux rate each hour

The nominal reflux rate is supposed to be constant for this process. Only infre-

quently does production change this rate. We now discuss how the three different

data collection strategies listed above could be applied to this process.



6

INTRODUCTION

Condenser

Coolant

Trays

50
45 Reflux
40
35

Distillate

30

25

20

Sidedraw

Feed

15 10
Trays 5 0

Reboiler

Steam

Bottoms Figure 1.6 Acetone­butyl alcohol distillation column.

Retrospective Study We could pursue a retrospective study that would use either all or a sample of the historical process data over some period of time to determine the relationships among the two temperatures and the reflux rate on the acetone concentration in the product stream. In so doing, we take advantage of previously collected data and minimize the cost of the study. However, these are several problems:
1. We really cannot see the effect of reflux on the concentration since we must assume that it did not vary much over the historical period.
2. The data relating the two temperatures to the acetone concentration do not correspond directly. Constructing an approximate correspondence usually requires a great deal of effort.
3. Production controls temperatures as tightly as possible to specific target values through the use of automatic controllers. Since the two temperatures vary so little over time, we will have a great deal of difficulty seeing their real impact on the concentration.
4. Within the narrow ranges that they do vary, the condensate temperature tends to increase with the reboil temperature. As a result, we will have a great deal

DATA COLLECTION

7

of difficulty separating out the individual effects of the two temperatures. This leads to the problem of collinearity or multicollinearity, which we discuss in Chapter 9.
Retrospective studies often offer limited amounts of useful information. In general, their primary disadvantages are as follows:

· Some of the relevant data often are missing. · The reliability and quality of the data are often highly questionable. · The nature of the data often may not allow us to address the problem at hand. · The analyst often tries to use the data in ways they were never intended to be
used. · Logs, notebooks, and memories may not explain interesting phenomena identi-
fied by the data analysis.

Using historical data always involves the risk that, for whatever reason, some of the data were not recorded or were lost. Typically, historical data consist of information considered critical and of information that is convenient to collect. The convenient information is often collected with great care and accuracy. The essential information often is not. Consequently, historical data often suffer from transcription errors and other problems with data quality. These errors make historical data prone to outliers, or observations that are very different from the bulk of the data. A regression analysis is only as reliable as the data on which it is based.
Just because data are convenient to collect does not mean that these data are particularly useful. Often, data not considered essential for routine process monitoring and not convenient to collect do have a significant impact on the process. Historical data cannot provide this information since they were never collected. For example, the ambient temperature may impact the heat losses from our distillation column. On cold days, the column loses more heat to the environment than during very warm days. The production logs for this acetone­butyl alcohol column do not record the ambient temperature. As a result, historical data do not allow the analyst to include this factor in the analysis even though it may have some importance.
In some cases, we try to use data that were collected as surrogates for what we really needed to collect. The resulting analysis is informative only to the extent that these surrogates really reflect what they represent. For example, the nature of the inlet mixture of acetone and butyl alcohol can significantly affect the column's performance. The column was designed for the feed to be a saturated liquid (at the mixture's boiling point). The production logs record the feed temperature but do not record the specific concentrations of acetone and butyl alcohol in the feed stream. Those concentrations are too hard to obtain on a regular basis. In this case, inlet temperature is a surrogate for the nature of the inlet mixture. It is perfectly possible for the feed to be at the correct specific temperature and the inlet feed to be either a subcooled liquid or a mixture of liquid and vapor.
In some cases, the data collected most casually, and thus with the lowest quality, the least accuracy, and the least reliability, turn out to be very influential for explaining our response. This influence may be real, or it may be an artifact related to the inaccuracies in the data. Too many analyses reach invalid conclusions because they

8

INTRODUCTION

lend too much credence to data that were never meant to be used for the strict purposes of analysis.
Finally, the primary purpose of many analyses is to isolate the root causes underlying interesting phenomena. With historical data, these interesting phenomena may have occurred months or years before. Logs and notebooks often provide no significant insights into these root causes, and memories clearly begin to fade over time. Too often, analyses based on historical data identify interesting phenomena that go unexplained.

Observational Study We could use an observational study to collect data for this problem. As the name implies, an observational study simply observes the process or population. We interact or disturb the process only as much as is required to obtain relevant data. With proper planning, these studies can ensure accurate, complete, and reliable data. On the other hand, these studies often provide very limited information about specific relationships among the data.
In this example, we would set up a data collection form that would allow the production personnel to record the two temperatures and the actual reflux rate at specified times corresponding to the observed concentration of acetone in the product stream. The data collection form should provide the ability to add comments in order to record any interesting phenomena that may occur. Such a procedure would ensure accurate and reliable data collection and would take care of problems 1 and 2 above. This approach also minimizes the chances of observing an outlier related to some error in the data. Unfortunately, an observational study cannot address problems 3 and 4. As a result, observational studies can lend themselves to problems with collinearity.

Designed Experiment The best data collection strategy for this problem uses a designed experiment where we would manipulate the two temperatures and the reflux ratio, which we would call the factors, according to a well-defined strategy, called the experimental design. This strategy must ensure that we can separate out the effects on the acetone concentration related to each factor. In the process, we eliminate any collinearity problems. The specified values of the factors used in the experiment are called the levels. Typically, we use a small number of levels for each factor, such as two or three. For the distillation column example, suppose we use a "high" or +1 and a "low" or -1 level for each of the factors. We thus would use two levels for each of the three factors. A treatment combination is a specific combination of the levels of each factor. Each time we carry out a treatment combination is an experimental run or setting. The experimental design or plan consists of a series of runs.
For the distillation example, a very reasonable experimental strategy uses every possible treatment combination to form a basic experiment with eight different settings for the process. Table 1.1 presents these combinations of high and low levels.
Figure 1.7 illustrates that this design forms a cube in terms of these high and low levels. With each setting of the process conditions, we allow the column to reach equilibrium, take a sample of the product stream, and determine the acetone concentration. We then can draw specific inferences about the effect of these factors. Such an approach allows us to proactively study a population or process.

USES OF REGRESSION

9

TABLE 1.1 Designed Experiment for the Distillation Column

Reboil Temperature
-1 +1 -1 +1 -1 +1 -1 +1

Condensate Temperature
-1 -1 +1 +1 -1 -1 +1 +1

Reflux Rate
-1 -1 -1 -1 +1 +1 +1 +1

Reflux Rate

+1 -1

Co-n1dTeenmsapt.e+1

-1 Reboil +1 Temp.

Figure 1.7 The designed experiment for the distillation column.

1.3 USES OF REGRESSION
Regression models are used for several purposes, including the following:
1. Data description 2. Parameter estimation 3. Prediction and estimation 4. Control
Engineers and scientists frequently use equations to summarize or describe a set of data. Regression analysis is helpful in developing such equations. For example, we may collect a considerable amount of delivery time and delivery volume data, and a regression model would probably be a much more convenient and useful summary of those data than a table or even a graph.
Sometimes parameter estimation problems can be solved by regression methods. For example, chemical engineers use the Michaelis­Menten equation y = 1x/ (x + 2) +  to describe the relationship between the velocity of reaction y and concentration x. Now in this model, 1 is the asymptotic velocity of the reaction, that is, the maximum velocity as the concentration gets large. If a sample of observed values of velocity at different concentrations is available, then the engineer can use regression analysis to fit this model to the data, producing an estimate of the maximum velocity. We show how to fit regression models of this type in Chapter 12.
Many applications of regression involve prediction of the response variable. For example, we may wish to predict delivery time for a specified number of cases of soft drinks to be delivered. These predictions may be helpful in planning delivery activities such as routing and scheduling or in evaluating the productivity of delivery operations. The dangers of extrapolation when using a regression model for prediction because of model or equation error have been discussed previously (see Figure 1.5). However, even when the model form is correct, poor estimates of the model parameters may still cause poor prediction performance.
Regression models may be used for control purposes. For example, a chemical engineer could use regression analysis to develop a model relating the tensile strength of paper to the hardwood concentration in the pulp. This equation could

10 INTRODUCTION
then be used to control the strength to suitable values by varying the level of hardwood concentration. When a regression equation is used for control purposes, it is important that the variables be related in a causal manner. Note that a cause-andeffect relationship may not be necessary if the equation is to be used only for prediction. In this case it is only necessary that the relationships that existed in the original data used to build the regression equation are still valid. For example, the daily electricity consumption during August in Atlanta, Georgia, may be a good predictor for the maximum daily temperature in August. However, any attempt to reduce the maximum temperature by curtailing electricity consumption is clearly doomed to failure.
1.4 ROLE OF THE COMPUTER
Building a regression model is an iterative process. The model-building process is illustrated in Figure 1.8. It begins by using any theoretical knowledge of the process that is being studied and available data to specify an initial regression model. Graphical data displays are often very useful in specifying the initial model. Then the parameters of the model are estimated, typically by either least squares or maximum likelihood. These procedures are discussed extensively in the text. Then model adequacy must be evaluated. This consists of looking for potential misspecification of the model form, failure to include important variables, including unnecessary variables, or unusual/inappropriate data. If the model is inadequate, then must be made and the parameters estimated again. This process may be repeated several times until an adequate model is obtained. Finally, model validation should be carried out to ensure that the model will produce results that are acceptable in the final application.
A good regression computer program is a necessary tool in the model-building process. However, the routine application of standard regression compnter programs often does not lead to successful results. The computer is not a substitute for creative thinking about the problem. Regression analysis requires the intelligent and artful use of the computer. We must learn how to interpret what the computer is telling us and how to incorporate that information in subsequent models. Generally, regression computer programs are part of more general statistics software packages, such as Minitab, SAS, JMP, and R. We discuss and illustrate the use of

Data
Model specification

No

No

Parameter

Yes

Model adequacy

Yes

estimation

checking

Model Yes Model

validation

use

Theory

Figure 1.8 Regression model-building process.

ROLE OF THE COMPUTER

11

these packages throughout the book. Appendix D contains details of the SAS procedures typically used in regression modeling along with basic instructions for their use. Appendix E provides a brief introduction to the R statistical software package. We present R code for doing analyses throughout the text. Without these skills, it is virtually impossible to successfully build a regression model.

CHAPTER 2

SIMPLE LINEAR REGRESSION

2.1 SIMPLE LINEAR REGRESSION MODEL

This chapter considers the simple linear regression model, that is, a model with a single regressor x that has a relationship with a response y that is a straight line. This simple linear regression model is

y = 0 + 1x + 

(2.1)

where the intercept 0 and the slope 1 are unknown constants and  is a random error component. The errors are assumed to have mean zero and unknown variance 2. Additionally we usually assume that the errors are uncorrelated. This means that the value of one error does not depend on the value of any other error.
It is convenient to view the regressor x as controlled by the data analyst and measured with negligible error, while the response y is a random variable. That is, there is a probability distribution for y at each possible value for x. The mean of this distribution is

and the variance is

E (y x) = 0 + 1x

(2.2a)

Var (y x) = Var (0 + 1x + ) =  2

(2.2b)

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
12

LEAST-SQUARES ESTIMATION OF THE PARAMETERS

13

Thus, the mean of y is a linear function of x although the variance of y does not
depend on the value of x. Furthermore, because the errors are uncorrelated, the
responses are also uncorrelated. The parameters 0 and 1 are usually called regression coefficients. These coef-
ficients have a simple and often useful interpretation. The slope 1 is the change in the mean of the distribution of y produced by a unit change in x. If the range of data on x includes x = 0, then the intercept 0 is the mean of the distribution of the response y when x = 0. If the range of x does not include zero, then 0 has no practical interpretation.

2.2 LEAST-SQUARES ESTIMATION OF THE PARAMETERS
The parameters 0 and 1 are unknown and must be estimated using sample data. Suppose that we have n pairs of data, say (y1, x1), (y2, x2), . . . , (yn, xn). As noted in Chapter 1, these data may result either from a controlled experiment designed specifically to collect the data, from an observational study, or from existing historical records (a retrospective study).

2.2.1 Estimation of 0 and 1
The method of least squares is used to estimate 0 and 1. That is, we estimate 0 and 1 so that the sum of the squares of the differences between the observations yi and the straight line is a minimum. From Eq. (2.1) we may write

yi = 0 + 1xi + i, i = 1, 2, ... , n

(2.3)

Equation (2.1) maybe viewed as a population regression model while Eq.
(2.3) is a sample regression model, written in terms of the n pairs of data (yi, xi) (i = 1, 2, . . . , n). Thus, the least-squares criterion is

n

( ) S (0, 1 ) =

2
yi - 0 - 1xi

i=1

The least-squares estimators of 0 and 1, say ^0 and ^1, must satisfy

( ) S

n
= -2

0 ^0,^1

i=1

yi - ^0 - ^1xi

=0

and

( ) S

n
= -2

1 ^0,^1

i=1

yi - ^0 - ^1xi

xi = 0

(2.4)

Simplifying these two equations yields

14

SIMPLE LINEAR REGRESSION

n

n

  n^0 + ^1 xi = yi

i=1

i=1

n

n

n

   ^0 xi + ^1 xi2 = yi xi

i=1

i=1

i=1

(2.5)

Equations (2.5) are called the least-squares normal equations. The solution to the normal equations is

^0 = y - ^1x

(2.6)

and

 n  n 

   ^1 =

n i=1

yi xi

-



i=1

yi   n

i=1

xi 

 n 2

 n i=1

xi2

-



i=1 xi  n

(2.7)

where

  y

=

1 n

n i=1

yi

and

x

=

1 n

n i=1

xi

are the averages of yi and xi, respectively. Therefore, ^0 and ^1 in Eqs. (2.6) and (2.7) are the least-squares estimators of the intercept and slope, respectively. The fitted simple linear regression model is then

y^ = ^0 + ^1x

(2.8)

Equation (2.8) gives a point estimate of the mean of y for a particular x. Since the denominator of Eq. (2.7) is the corrected sum of squares of the xi and
the numerator is the corrected sum of cross products of xi and yi, we may write these quantities in a more compact notation as

 n 2

   Sxx

=

n i=1

xi2 - 

i=1 xi  n

n
= (xi - x )2
i=1

and

(2.9)

 n  n 

    Sxy

=

n i=1

 yi xi -

i=1

yi   n

i=1

xi 

=

n i=1

yi (xi - x )

(2.10)

LEAST-SQUARES ESTIMATION OF THE PARAMETERS

15

Thus, a convenient way to write Eq. (2.7) is

^ 1

=

Sxy Sxx

(2.11)

The difference between the observed value yi and the corresponding fitted value y^i is a residual. Mathematically the ith residual is

( ) ei = yi - y^i = yi - ^0 + ^1xi , i = 1, 2, ... , n

(2.12)

Residuals play an important role in investigating model adequacy and in detecting departures from the underlying assumptions. This topic is discussed in subsequent chapters.

Example 2.1 The Rocket Propellant Data

A rocket motor is manufactured by bonding an igniter propellant and a sustainer propellant together inside a metal housing. The shear strength of the bond between the two types of propellant is an important quality characteristic. It is suspected that shear strength is related to the age in weeks of the batch of sustainer propellant. Twenty observations on shear strength and the age of the corresponding batch of propellant have been collected and are shown in Table 2.1. The scatter diagram, shown in Figure 2.1, suggests that there is a strong statistical relationship between shear strength and propellant age, and the tentative assumption of the straight-line model y = 0 + 1x +  appears to be reasonable.

TABLE 2.1 Data for Example 2.1

Observation, i

Shear Strength, yi (psi)

1

2158.70

2

1678.15

3

2316.00

4

2061.30

5

2207.50

6

1708.30

7

1784.70

8

2575.00

9

2357.90

10

2256.70

11

2165.20

12

2399.55

13

1779.80

14

2336.75

15

1765.30

16

2053.50

17

2414.40

18

2200.50

19

2654.20

20

1753.70

Age of Propellant, xi (weeks)
15.50 23.75 8.00 17.00 5.50 19.00 24.00
2.50 7.50 11.00 13.00 3.75 25.00 9.75 22.00 18.00 6.00 12.50 2.00 21.50

16

SIMPLE LINEAR REGRESSION

Shear strength

2700 2600 2500 2400 2300 2200 2100 2000 1900 1800 1700 1600
0 2 4 6 8 10 12 14 16 18 20 22 24 26 Age of propellant
Figure 2.1 Scatter diagram of shear strength versus propellant age, Example 2.1.

To estimate the model parameters, first calculate

 n 2

  Sxx

=

n i=1

xi2 - 

i=1 xi  n

= 4677.69 - 71, 422.56 = 1106.56 20

and

n

n

   Sxy

=

n i=1

xi yi

-

i=1

xi
i=1
n

yi

= 528, 492.64 - (267.25)(42, 627.15)
20

= -41,112.65

Therefore, from Eqs. (2.11) and (2.6), we find that

^ 1

=

Sxy Sxx

=

-41, 112.65 1106.56

=

-37.15

and
^0 = y - ^1x = 2131.3575 - (-37.15)13.3625 = 2627.82

LEAST-SQUARES ESTIMATION OF THE PARAMETERS

17

TABLE 2.2 Data, Fitted Values, and Residuals for Example 2.1

Observed Value, yi
2158.70 1678.15 2316.00 2061.30 2207.50 1708.30 1784.70 2575.00 2357.90 2256.70 2165.20 2399.55 1799.80 2336.75 1765.30 2053.50 2414.40 2200.50 2654.20 1753.70

Fitted Value, y^i
2051.94 1745.42 2330.59 1996.21 2423.48 1921.90 1736.14 2534.94 2349.17 2219.13 2144.83 2488.50 1698.98 2265.58 1810.44 1959.06 2404.90 2163.40 2553.52 1829.02

Residual, ei
106.76 -67.27 -14.59
65.09 -215.98 -213.60
48.56 40.06 8.73 37.57 20.37 -88.95 80.82 71.17 -45.14 94.44 9.50 37.10 100.68 -75.32

 yi = 42,627.15

 y^i = 42,627.15

 ei = 0.00

The least-squares fit is

y^ = 2627.82 - 37.15x

We may interpret the slope -37.15 as the average weekly decrease in propellant

shear strength due to the age of the propellant. Since the lower limit of the x's is

near the origin, the intercept 2627.82 represents the shear strength in a batch of

propellant immediately following manufacture. Table 2.2 displays the observed

values yi, the fitted values y^i, and the residuals.



After obtaining the least-squares fit, a number of interesting questions come to mind:
1. How well does this equation fit the data? 2. Is the model likely to be useful as a predictor? 3. Are any of the basic assumptions (such as constant variance and uncorrelated
errors) violated, and if so, how serious is this?
All of these issues must be investigated before the model is finally adopted for use. As noted previously, the residuals play a key role in evaluating model adequacy. Residuals can be viewed as realizations of the model errors i. Thus, to check the constant variance and uncorrelated errors assumption, we must ask ourselves if the residuals look like a random sample from a distribution with these properties. We

18

SIMPLE LINEAR REGRESSION

TABLE 2.3 Minitab Regression Output for Example 2.1

Regression Analysis

The regression equation is Strength = 2628- 37.2 Age

Predictor Constant Age

Coef 2627.82
-37.154

StDev 44.18
2.889

T 59.47 -12.86

S = 96.11 R-Sq = 90.2% R-Sq(adj) = 89.6%

Analysis of Variance

Source

DF

Regression

1

Error

18

Total

19

SS 1527483
166255 1693738

MS 1527483
9236

P 0.000 0.000

F

P

165.38 0.000

return to these questions in Chapter 4, where the use of residuals in model adequacy checking is explored.
Computer Output Computer software packages are used extensively in fitting regression models. Regression routines are found in both network and PC-based statistical software, as well as in many popular spreadsheet packages. Table 2.3 presents the output from Minitab, a widely used PC-based statistics package, for the rocket propellant data in Example 2.1. The upper portion of the table contains the fitted regression model. Notice that before rounding the regression coefficients agree with those we calculated manually. Table 2.3 also contains other information about the regression model. We return to this output and explain these quantities in subsequent sections.

2.2.2 Properties of the Least-Squares Estimators and the Fitted Regression Model
The least-squares estimators ^0 and ^1 have several important properties. First, note from Eqs. (2.6) and (2.7) that ^0 and ^1 are linear combinations of the observations yi. For example,

 ^1

=

Sxy Sxx

=

n
ci yi
i=1

where ci = (xi - x ) Sxx for i = 1, 2, . . . , n.
The least-squares estimators ^0 and ^1 are unbiased estimators of the model parameters 0 and 1. To show this for ^1, consider

( )   E ^1

 = E 

n



i=1 ci yi 

=

n
ciE (yi )
i=1

n

n

n

   = ci (0 + 1xi ) = 0 ci + 1 ci xi

i=1

i=1

i=1

LEAST-SQUARES ESTIMATION OF THE PARAMETERS

19

since

E(i) = 0

by

assumption.

Now

we

can

show

directly

that



n i=

1

ci

=

0

and

in=1 ci xi = 1, so

( ) E ^1 = 1

That is, if we assume that the model is correct [E(yi) = 0 + 1xi], then ^1 is an unbiased estimator of 1. Similarly we may show that ^0 is an unbiased estimator of 0, or
( ) E ^0 = 0
The variance of ^1 is found as

( )   Var ^1

 = Var 

n



i=1 ci yi 

=

n
ci2Var (yi )
i=1

(2.13)

because the observations yi are uncorrelated, and so the variance of the sum is just
the sum of the variances. The variance of each term in the sum is ci2Var (yi ), and we
have assumed that Var(yi) = 2; consequently,

n

( )   Var ^1

= 2

n i=1

ci2

=

2

(xi
i=1
Sx2x

- x )2

=

2 Sxx

(2.14)

The variance of ^0 is
( ) ( ) Var ^0 = Var y - ^1x ( ) ( ) = Var (y) + x2Var ^1 - 2xCov y, ^1

Now the variance of y is just Var (y) =  2 n, and the covariance between y and ^1
can be shown to be zero (see Problem 2.25). Thus,

( ) ( ) Var ^0

= Var (y) + x 2Var ^1

=



2

 

1 n

+

x2 Sxx

 

(2.15)

Another important result concerning the quality of the least-squares estimators ^0 and ^1 is the Gauss-Markov theorem, which states that for the regression model (2.1) with the assumptions E() = 0, Var() = 2, and uncorrelated errors, the leastsquares estimators are unbiased and have minimum variance when compared with all other unbiased estimators that are linear combinations of the yi. We often say that the least-squares estimators are best linear unbiased estimators, where "best" implies minimum variance. Appendix C.4 proves the Gauss-Markov theorem for the more general multiple linear regression situation, of which simple linear regression is a special case.
There are several other useful properties of the least-squares fit:

20

SIMPLE LINEAR REGRESSION

1. The sum of the residuals in any regression model that contains an intercept 0 is always zero, that is,

n

n

  (yi - y^i ) = ei = 0

i=1

i=1

This property follows directly from the first normal equation in Eqs. (2.5) and is demonstrated in Table 2.2 for the residuals from Example 2.1. Rounding errors may affect the sum.
2. The sum of the observed values yi equals the sum of the fitted values y^i, or

n

n

  yi = y^i

i=1

i=1

Table 2.2 demonstrates this result for Example 2.1.
3. The least-squares regression line always passes through the centroid [the point (y, x)] of the data.
4. The sum of the residuals weighted by the corresponding value of the regressor variable always equals zero, that is,

n
 xiei = 0
i=1
5. The sum of the residuals weighted by the corresponding fitted value always equals zero, that is,

n
 y^iei = 0
i=1

2.2.3 Estimation of 2
In addition to estimating 0 and 1, an estimate of 2 is required to test hypotheses and construct interval estimates pertinent to the regression model. Ideally we would like this estimate not to depend on the adequacy of the fitted model. This is only possible when there are several observations on y for at least one value of x (see Section 4.5) or when prior information concerning 2 is available. When this approach cannot be used, the estimate of 2 is obtained from the residual or error sum of squares,

n

n

  SSRes = ei2 = (yi - y^i )2

i=1

i=1

(2.16)

A convenient computing formula for SSRes may be found by substituting y^i = ^0 + ^1xi into Eq. (2.16) and simplifying, yielding

LEAST-SQUARES ESTIMATION OF THE PARAMETERS

21

n
 SSRes = yi2 - ny2 - ^1Sxy i=1
But

(2.17)

n

n

  yi2 - ny2 = (yi - y)2  SST

i=1

i=1

is just the corrected sum of squares of the response observations, so SSRes = SST - ^1Sxy

(2.18)

The residual sum of squares has n - 2 degrees of freedom, because two degrees of freedom are associated with the estimates ^0 and ^1 involved in obtaining y^i.
Section C.3 shows that the expected value of SSRes is E(SSRes) = (n - 2)2, so an unbiased estimator of 2 is

^ 2

=

SSRes n-2

= MSRes

(2.19)

The quantity MSRes is called the residual mean square. The square root of ^ 2 is sometimes called the standard error of regression, and it has the same units as the
response variable y. Because ^ 2 depends on the residual sum of squares, any violation of the assump-
tions on the model errors or any misspecification of the model form may seriously damage the usefulness of ^ 2 as an estimate of 2. Because ^ 2 is computed from the regression model residuals, we say that it is a model-dependent estimate of 2.

Example 2.2 The Rocket Propellant Data

To estimate 2 for the rocket propellant data in Example 2.1, first find

 n 2

   SST

=

n i=1

yi2

- ny2

=

n i=1

yi2

- 

i=1 yi  n

= 92, 547, 433.45 - (42, 627.15)2 = 1, 693, 737.60
20

From Eq. (2.18) the residual sum of squares is

SSRes = SST - ^1Sxy
= 1, 693, 737.60 - (-37.15)(-41,112.65) = 166, 402.65

Therefore, the estimate of 2 is computed from Eq. (2.19) as

^ 2 = SSRes = 166, 402.65 = 9244.59

n-2

18

22

SIMPLE LINEAR REGRESSION

Remember that this estimate of 2 is model dependent. Note that this differs slightly
 from the value given in the Minitab output (Table 2.3) because of rounding.

2.2.4 Alternate Form of the Model

There is an alternate form of the simple linear regression model that is occasionally
useful. Suppose that we redefine the regressor variable xi as the deviation from its own average, say xi - x. The regression model then becomes

yi = 0 + 1 (xi - x ) + 1x + i = (0 + 1x ) + 1 (xi - x ) + i = 0 + 1 (xi - x ) + i

(2.20)

Note that redefining the regressor variable in Eq. (2.20) has shifted the origin of the x's from zero to x. In order to keep the fitted values the same in both the original and transformed models, it is necessary to modify the original intercept. The relationship between the original and transformed intercept is

0 = 0 + 1x

(2.21)

It is easy to show that the least-squares estimator of the transformed intercept is ^0 = y. The estimator of the slope is unaffected by the transformation. This alternate form of the model has some advantages. First, the least-squares estimators
( ) ^0 = y and ^1 = Sxy Sxx are uncorrelated, that is, Cov ^0, ^1 = 0. This will make some
applications of the model easier, such as finding confidence intervals on the mean
of y (see Section 2.4.2). Finally, the fitted model is

y^ = y + ^1 (x - x )

(2.22)

Although Eqs. (2.22) and (2.8) are equivalent (they both produce the same value of y^ for the same value of x), Eq. (2.22) directly reminds the analyst that the regression model is only valid over the range of x in the original data. This region is centered at x .

2.3 HYPOTHESIS TESTING ON THE SLOPE AND INTERCEPT
We are often interested in testing hypotheses and constructing confidence intervals about the model parameters. Hypothesis testing is discussed in this section, and Section 2.4 deals with confidence intervals. These procedures require that we make the additional assumption that the model errors i are normally distributed. Thus, the complete assumptions are that the errors are normally and independently distributed with mean 0 and variance 2, abbreviated NID(0, 2). In Chapter 4 we discuss how these assumptions can be checked through residual analysis.
2.3.1 Use of t Tests
Suppose that we wish to test the hypothesis that the slope equals a constant, say 10. The appropriate hypotheses are

HYPOTHESIS TESTING ON THE SLOPE AND INTERCEPT

23

H0: 1 = 10, H1: 1  10

(2.23)

where we have specified a two-sided alternative. Since the errors i are NID(0, 2), the observations yi are NID(0 + 1xi, 2). Now ^1 is a linear combination of the observations, so ^1 is normally distributed with mean 1 and variance 2/Sxx using the mean and variance of ^1 found in Section 2.2.2. Therefore, the statistic
Z0 = ^1 - 10  2 Sxx

is distributed N(0, 1) if the null hypothesis H0: 1 = 10 is true. If 2 were known, we

could use Z0 to test the hypotheses (2.23). Typically, 2 is unknown. We have already

seen that MSRes is an unbiased estimator of 2. Appendix C.3 establishes that

(n

-

2)MSRes/2

follows

a



2 n-

2

distribution

and

that

MSRes

and

^ 1

are

independent.

By the definition of a t statistic given in Section C.1,

t0 = ^1 - 10 MSRes Sxx

(2.24)

follows a tn-2 distribution if the null hypothesis H0: 1 = 10 is true. The degrees of freedom associated with t0 are the number of degrees of freedom associated with MSRes. Thus, the ratio t0 is the test statistic used to test H0: 1 = 10. The test procedure computes t0 and compares the observed value of t0 from Eq. (2.24) with the upper /2 percentage point of the tn-2 distribution (t/2,n-2). This procedure rejects the null hypothesis if

t0 > t 2,n-2

(2.25)

Alternatively, a P-value approach could also be used for decision making. The denominator of the test statistic, t0, in Eq. (2.24) is often called the estimated
standard error, or more simply, the standard error of the slope. That is,

( ) se ^1 =

MSRes Sxx

(2.26)

Therefore, we often see t0 written as

( ) t0

=

^1 - 10 se ^1

(2.27)

A similar procedure can be used to test hypotheses about the intercept. To test

H0: 0 = 00, H1: 0  00

(2.28)

we would use the test statistic

( ) ( ) t0 =

^0 - 00 MSRes 1 n + x 2 Sxx

=

^0 - 00 se ^0

(2.29)

24

SIMPLE LINEAR REGRESSION

( ) where se ^0 = ( MSRes 1 n + x2 Sx ) is the standard error of the intercept. We reject
the null hypothesis H0: 0 = 00 if |t0| > t/2,n-2.

2.3.2 Testing Significance of Regression A very important special case of the hypotheses in Eq. (2.23) is

H0: 1 = 0, H1: 1  0

(2.30)

These hypotheses relate to the significance of regression. Failing to reject H0: 1 = 0 implies that there is no linear relationship between x and y. This situation is illus-
trated in Figure 2.2. Note that this may imply either that x is of little value in explaining the variation in y and that the best estimator of y for any x is y^ = y (Figure 2.2a) or that the true relationship between x and y is not linear (Figure 2.2b). Therefore, failing to reject H0: 1 = 0 is equivalent to saying that there is no linear relationship between y and x.
Alternatively, if H0: 1 = 0 is rejected, this implies that x is of value in explaining the variability in y. This is illustrated in Figure 2.3. However, rejecting H0: 1 = 0 could mean either that the straight-line model is adequate (Figure 2.3a) or that even

y

y

x

x

(a)

(b)

Figure 2.2 Situations where the hypothesis H0: 1 = 0 is not rejected.

y

y

x

x

(a)

(b)

Figure 2.3 Situations where the hypothesis H0: 1 = 0 is rejected.

HYPOTHESIS TESTING ON THE SLOPE AND INTERCEPT

25

though there is a linear effect of x, better results could be obtained with the addition
of higher order polynomial terms in x (Figure 2.3b). The test procedure for H0: 1 = 0 may be developed from two approaches. The
first approach simply makes use of the t statistic in Eq. (2.27) with 10 = 0, or

( ) t0

=

^ 1 se ^1

The null hypothesis of significance of regression would be rejected if |t0| > t/2,n-2.

Example 2.3 The Rocket Propellant Data

We test for significance of regression in the rocket propellant regression model of Example 2.1. The estimate of the slope is ^1 = -37.15, and in Example 2.2, we computed the estimate of 2 to be MSRes = ^ 2 = 9244.59. The standard error of the
slope is

( ) se ^1 =

MSRes = Sxx

9244.59 = 2.89 1106.56

Therefore, the test statistic is

( ) t0

=

^ 1 se ^1

= -37.15 = -12.85 2.89

If we choose  = 0.05, the critical value of t is t0.025,18 = 2.101. Thus, we would reject

H0: 1 = 0 and conclude that there is a linear relationship between shear strength

and the age of the propellant.



Minitab Output The Minitab output in Table 2.3 gives the standard errors of the slope and intercept (called "StDev" in the table) along with the t statistic for testing H0: 1 = 0 and H0: 0 = 0. Notice that the results shown in this table for the slope essentially agree with the manual calculations in Example 2.3. Like most computer software, Minitab uses the P-value approach to hypothesis testing. The P value for the test for significance of regression is reported as P = 0.000 (this is a rounded value; the actual P value is 1.64 × 10-10). Clearly there is strong evidence that strength is linearly related to the age of the propellant. The test statistic for H0: 0 = 0 is reported as t0 = 59.47 with P = 0.000. One would feel very confident in claiming that the intercept is not zero in this model.

2.3.3 Analysis of Variance
We may also use an analysis-of-variance approach to test significance of regression. The analysis of variance is based on a partitioning of total variability in the response variable y. To obtain this partitioning, begin with the identity

yi - y = (y^i - y) + (yi - y^i )

(2.31)

26

SIMPLE LINEAR REGRESSION

Squaring both sides of Eq. (2.31) and summing over all n observations produces

n

n

n

n

    (yi - y)2 = (y^i - y)2 + (yi - y^i )2 + 2 (y^i - y)(yi - y^i )

i=1

i=1

i=1

i=1

Note that the third term on the right-hand side of this expression can be rewritten as

n

n

n

   2 (y^i - y)(yi - y^i ) = 2 y^i (yi - y^i ) - 2y (yi - y^i )

i=1

i=1

i=1

n

n

  = 2 y^iei - 2y ei = 0

i=1

i=1

since the sum of the residuals is always zero (property 1, Section 2.2.2) and the sum of the residuals weighted by the corresponding fitted value y^i is also zero (property 5, Section 2.2.2). Therefore,

n

n

n

   (yi - y)2 = (y^i - y)2 + (yi - y^i )2

i=1

i=1

i=1

(2.32)

The left-hand side of Eq. (2.32) is the corrected sum of squares of the observa-

tions, SST, which measures the total variability in the observations. The two compo-

nents of SST measure, respectively, the amount of variability in the observations yi

accounted for by the regression line and the residual variation left unexplained by

the regression line. We recognize SSRes = ( in=1 yi - y^i )2 as the residual or error sum

of

squares

from

Eq.

(2.16).

It

is

customary

to

call



n i=1

(

y^ i

- y)2

the

regression

or

model sum of squares.

Equation (2.32) is the fundamental analysis-of-variance identity for a regression

model. Symbolically, we usually write

SST = SSR + SSRes

(2.33)

Comparing Eq. (2.33) with Eq. (2.18) we see that the regression sum of squares may be computed as

SSR = ^1Sxy

(2.34)

The degree-of-freedom breakdown is determined as follows. The total sum of
squares, SST, has dfT = n - 1 degrees of freedom because one degree of freedom is
lost as a result of the constraint ( in=1 yi - y) on the deviations yi - y. The model or
regression sum of squares, SSR, has dfR = 1 degree of freedom because SSR is completely determined by one parameter, namely, ^1 [see Eq. (2.34)]. Finally, we noted previously that SSR has dfRes = n - 2 degrees of freedom because two

HYPOTHESIS TESTING ON THE SLOPE AND INTERCEPT

27

constraints are imposed on the deviations yi - y^i as a result of estimating ^0 and ^1. Note that the degrees of freedom have an additive property:

dfT = dfR + dfRes
n - 1 = 1+ (n - 2)

(2.35)

We can use the usual analysis-of-variance F test to test the hypothesis H0: 1 = 0.

Appendix

C.3

shows

that

(1)

SSRes

=

(n

-

2)MSRes/2

follows

a



2 n-

2

distribution;

(2) if the null hypothesis H0: 1 = 0 is true, then SSR/2 follows a 12 distribution; and

(3) SSRes and SSR are independent. By the definition of an F statistic given in Appendix C.1,

F0

=

SSR SSRes

dfR dfRes

=

SSR 1
SSRes (n -

2)

=

MSR MSRes

(2.36)

follows the F1,n-2 distribution. Appendix C.3 also shows that the expected values of these mean squares are

E (MSRes ) =  2, E (MSR ) =  2 + 12Sxx

These expected mean squares indicate that if the observed value of F0 is large, then it is likely that the slope 1  0. Appendix C.3 also shows that if 1  0, then F0 follows a noncentral F distribution with 1 and n - 2 degrees of freedom and a noncentrality parameter of
 = 12Sxx 2
This noncentrality parameter also indicates that the observed value of F0 should be large if 1  0. Therefore, to test the hypothesis H0: 1 = 0, compute the test statistic F0 and reject H0 if
F0 > F,1,n-2
The test procedure is summarized in Table 2.4.

TABLE 2.4 Analysis of Variance for Testing Significance of Regression

Source of Variation
Regression Residual Total

Sum of Squares
SSR = ^1Sxy SSRes = SST - ^1Sxy SST

Degrees of Freedom
1 n-2 n-1

Mean Square
MSR MSRes

F0 MSR/MSRes

28

SIMPLE LINEAR REGRESSION

Example 2.4 The Rocket Propellant Data

We will test for significance of regression in the model developed in Example 2.1 for the rocket propellant data. The fitted model is y^ = 2627.82 - 37.15x, SST = 1,693,737.60, and Sxy = -41,112.65. The regression sum of squares is computed from Eq. (2.34) as

SSR = ^1Sxy = (-37.15)(-41,112.65) = 1, 527, 334.95

The analysis of variance is summarized in Table 2.5. The computed value of F0 is

165.21, and from Table A.4, F0.01,1,18 = 8.29. The P value for this test is 1.66 × 10-10.

Consequently, we reject H0: 1 = 0.



Minitab Output The Minitab output in Table 2.3 also presents the analysis-ofvariance test significance of regression. Comparing Tables 2.3 and 2.5, we note that there are some slight differences between the manual calculations and those performed by computer for the sums of squares. This is due to rounding the manual calculations to two decimal places. The computed values of the test statistics essentially agree.

More About the t Test We noted in Section 2.3.2 that the t statistic

( ) t0

=

^ 1 se ^1

=

^ 1 MSRes Sxx

(2.37)

could be used for testing for significance of regression. However, note that on squaring both sides of Eq. (2.37), we obtain

t02

=

^ 12 Sxx MSRes

=

^ 1Sxy MSRes

=

MSR MSRes

(2.38)

Thus, t02 in Eq. (2.38) is identical to F0 of the analysis-of-variance approach in Eq. (2.36). For example; in the rocket propellant example t0 = -12.5, so
t02 = (-12.5)2 = 165.12 F0 = 165.21. In general, the square of a t random variable
with f degrees of freedom is an F random variable with one and f degrees of freedom in the numerator and denominator, respectively. Although the t test for H0: 1 = 0 is equivalent to the F test in simple linear regression, the t test is somewhat more adaptable, as it could be used for one-sided alternative hypotheses (either H1: 1 < 0 or H1: 1 > 0), while the F test considers only the two-sided alternative. Regression

TABLE 2.5 Analysis-of-Variance Table for the Rocket Propellant Regression Model

Source of Variation
Regression Residual Total

Sum of Squares
1,527,334.95 166,402.65
1,693,737.60

Degrees of Freedom
1 18 19

Mean Square
1,527,334.95 9,244.59

F0 165.21

P value 1.66 × 10-10

INTERVAL ESTIMATION IN SIMPLE LINEAR REGRESSION

29

computer programs routinely produce both the analysis of variance in Table 2.4 and the t statistic. Refer to the Minitab output in Table 2.3.
The real usefulness of the analysis of variance is in multiple regression models. We discuss multiple regression in the next chapter.
Finally, remember that deciding that 1 = 0 is a very important conclusion that is only aided by the t or F test. The inability to show that the slope is not statistically different from zero may not necessarily mean that y and x are unrelated. It may mean that our ability to detect this relationship has been obscured by the variance of the measurement process or that the range of values of x is inappropriate. A great deal of nonstatistical evidence and knowledge of the subject matter in the field is required to conclude that 1 = 0.

2.4 INTERVAL ESTIMATION IN SIMPLE LINEAR REGRESSION
In this section we consider confidence interval estimation of the regression model parameters. We also discuss interval estimation of the mean response E(y) for given values of x. The normality assumptions introduced in Section 2.3 continue to apply.

2.4.1 Confidence Intervals on 0, 1, and 2
In addition to point estimates of 0, 1, and 2, we may also obtain confidence interval estimates of these parameters. The width of these confidence intervals is a measure of the overall quality of the regression line. If the errors are normally and
( ) ( ) independently distributed, then the sampling distribution of both ^1 - 1 se ^1 ( ) ( ) and ^0 - 0 se ^0 is t with n - 2 degrees of freedom. Therefore, a 100(1 - )
percent confidence interval (CI) on the slope 1 is given by

( ) ( ) ^1 - t 2,n-2se ^1  ^1  ^1 + t 2,n-2se ^1

(2.39)

and a 100(1 - ) percent CI on the intercept 0 is
( ) ( ) ^0 - t 2,n-2se ^0  0  ^0 + t 2,n-2se ^0

(2.40)

These CIs have the usual frequentist interpretation. That is, if we were to take repeated samples of the same size at the same x levels and construct, for example, 95% CIs on the slope for each sample, then 95% of those intervals will contain the true value of 1.
If the errors are normally and independently distributed, Appendix C.3 shows that the sampling distribution of (n - 2)MSRes/2 is chi square with n - 2 degrees of freedom. Thus,

{ } ( ) P

  2 1- 2,n-2

n - 2 MSRes 2



2

2,n-2

= 1-

and consequently a 100(1 - ) percent CI on 2 is

30

SIMPLE LINEAR REGRESSION

(n - 2) MSRes   2  (n - 2) MSRes

2  2,n-2

2 1- 2,n-2

(2.41)

Example 2.5 The Rocket Propellant Data

( ) We construct 95% CIs on 1 and 2 using the rocket propellant data from Example
2.1. The standard error of ^1 is se ^1 = 2.89 and t0.025,18 = 2.101. Therefore, from Eq. (2.35), the 95% CI on the slope is
( ) ( ) ^1 - t0.025,18se ^1  1  ^1 + t0.025,18se ^1
-37.15 - (2.101)(2.89)  1  -37.15 + (2.101)(2.89)

or

-43.22  1  -31.08

In other words, 95% of such intervals will include the true value of the slope. If we had chosen a different value for , the width of the resulting CI would have
been different. For example, the 90% CI on 1 is -42.16  1  -32.14, which is narrower than the 95% CI. The 99% CI is -45.49  1  28.81, which is wider than the 95% CI. In general, the larger the confidence coefficient (1 - ) is, the wider the CI.
The 95% CI on 2 is found from Eq. (2.41) as follows:

(n - 2) MSRes   2  (n - 2) MSRes

2 0.025, n - 2

2 0.975, n - 2

18 (9244.59)   2  18 (9244.59)

2 0.025,18

2 0.975,18

From Table

A.2,

2 0.025,18

=

31.5

and

2 0.975,18

=

8.23. Therefore, the

desired

CI

becomes

18 (9244.59)   2  18 (9244.59)

31.5

8.23

or

5282.62   2  20, 219.03



2.4.2 Interval Estimation of the Mean Response
A major use of a regression model is to estimate the mean response E(y) for a particular value of the regressor variable x. For example, we might wish to estimate the mean shear strength of the propellant bond in a rocket motor made from a batch of sustainer propellant that is 10 weeks old. Let x0 be the level of the regressor variable for which we wish to estimate the mean response, say E(y|x0). We assume that

INTERVAL ESTIMATION IN SIMPLE LINEAR REGRESSION

31

x0 is any value of the regressor variable within the range of the original data on x used to fit the model. An unbiased point estimator of E(y|x0) is found from the fitted model as

E (y x0 ) = ^ y x0 = ^0 + ^1x0

(2.42)

To obtain a 100(1 - ) percent CI on E(y|x0), first note that ^ y x0 is a normally distributed random variable because it is a linear combination of the observations yi. The variance of ^ y x0 is

( ) Var (^ y x0 ) = Var ^0 + ^1x0 = Var y + ^1 (x0 - x )

=

2 n

+  2 (x0 - x )2
Sxx

=



2

 

1 n

+

(x0 - x
Sxx

)2

 

( ) since (as noted in Section 2.2.4) Cov y, ^1 = 0. Thus, the sampling distribution of

^ y x0 - E (y x0 )
( ) MSRes 1 n + (x0 - x )2 Sxx

is t with n - 2 degrees of freedom. Consequently, a 100(1 - ) percent CI on the mean response at the point x = x0 is

^ y x0 - t 2,n-2

MSRes

 

1 n

+

(x0 - x )2
Sxx

 

 E (y x0 )  ^ y x0 + t 2,n-2

MSRes

 

1 n

+

(x0 - x )2
Sxx

 

(2.43)

Note that the width of the CI for E(y|x0) is a function of x0. The interval width is a minimum for x0 = x and widens as x0 - x increases. Intuitively this is reasonable, as we would expect our best estimates of y to be made at x values near the center of the data and the precision of estimation to deteriorate as we move to the boundary of the x space.

Example 2.6 The Rocket Propellant Data

Consider finding a 95% CI on E(y|x0) for the rocket propellant data in Example 2.1. The CI is found from Eq. (2.43) as

^ y x0 - t 2,n-2

MSRes

 

1 n

+

(x0 - x )2
Sxx

 

 E (y x0 )  ^ y x0 + t 2,n-2

MSRes

 

1 n

+

(x0 - x )2
Sxx

 

32

SIMPLE LINEAR REGRESSION

^ y x0 - (2.101)

9244.59

 

1 20

+

(x0

- 13.3625)2
1106.56

 

 E (y x0 )  ^ y x0 + (2.101)

9244.59

 

1 20

+

(x0

- 13.3625)2
1106.56

 

If we substitute values of x0 and the fitted value y^0 = ^ y x0 at the value of x0 into this last equation, we will obtain the 95% CI on the mean response at x = x0. For example, if x0 = x = 13.3625, then ^ y x0 = 2131.40, and the CI becomes
2086.230  E (y 13.3625)  2176.571

Table 2.6 contains the 95% confidence limits on E(y|x0) for several other values of

x0. These confidence limits are illustrated graphically in Figure 2.4. Note that the

width of the CI increases as x0 - x increases.



TABLE 2.6 Confidence Limits on E(y|x0) for Several Values of x0

Lower

Upper

Confidence Limit

x0

Confidence Limit

2438.919 2341.360 2241.104 2136.098 2086.230 2024.318 1905.890 1782.928 1657.395

3 6 9 12 x = 13.3625 15 18 21 24

2593.821 2468.481 2345.836 2227.942 2176.571 2116.822 2012.351 1912.412 1815.045

3000

2500

Shear Strength, y

2000

1500

1000

5

10 15 20 25

Age of Propellant, x

Figure 2.4 The upper and lower 95% confidence limits for the propellant data.

PREDICTION OF NEW OBSERVATIONS

33

Many regression textbooks state that one should never use a regression model to extrapolate beyond the range of the original data. By extrapolation, we mean using the prediction equation beyond the boundary of the x space. Figure 1.5 illustrates clearly the dangers inherent in extrapolation; model or equation error can severely damage the prediction.
Equation (2.43) points out that the issue of extrapolation is much more subtle; the further the x value is from the center of the data, the more variable our estimate of E(y|x0). Please note, however, that nothing "magical" occurs at the boundary of the x space. It is not reasonable to think that the prediction is wonderful at the observed data value most remote from the center of the data and completely awful just beyond it. Clearly, Eq. (2.43) points out that we should be concerned about prediction quality as we approach the boundary and that as we move beyond this boundary, the prediction may deteriorate rapidly. Furthermore, the farther we move away from the original region of x space, the more likely it is that equation or model error will play a role in the process.
This is not the same thing as saying "never extrapolate." Engineers and economists routinely use prediction equations to forecast a variable of interest one or more time periods in the future. Strictly speaking, this forecast is an extrapolation. Equation (2.43) supports such use of the prediction equation. However, Eq. (2.43) does not support using the regression model to forecast many periods in the future. Generally, the greater the extrapolation, the higher is the chance of equation error or model error impacting the results.
The probability statement associated with the CI (2.43) holds only when a single CI on the mean response is to be constructed. A procedure for constructing several CIs that, considered jointly, have a specified confidence level is a simultaneous statistical inference problem. These problems are discussed in Chapter 3.

2.5 PREDICTION OF NEW OBSERVATIONS
An important application of the regression model is prediction of new observations y corresponding to a specified level of the regressor variable x. If x0 is the value of the regressor variable of interest, then

y^0 = ^0 + ^1x0

(2.44)

is the point estimate of the new value of the response y0. Now consider obtaining an interval estimate of this future observation y0. The CI
on the mean response at x = x0 [Eq. (2.43)] is inappropriate for this problem because it is an interval estimate on the mean of y (a parameter), not a probability statement about future observations from that distribution. We now develop a prediction interval for the future observation y0.
Note that the random variable
 = y0 - y^0
is normally distributed with mean zero and variance

34

SIMPLE LINEAR REGRESSION

Var

(

)

=

Var

( y0

-

y^ 0

)

=



2

 1

+

1 n

+

(x0 - x
Sxx

)2

 

because the future observation y0 is independent of y^0. If we use y^0 to predict y0, then the standard error of  = y0 - y^0 is the appropriate statistic on which to base a prediction interval. Thus, the 100(1 - ) percent prediction interval on a future
observation at x0 is

y^0 - t 2,n-2

MSRes

 

1

+

1 n

+

(x0 - x
Sxx

)2

 

 y0  y^0 + t 2,n-2

MSRes

  1

+

1 n

+

(x0 - x )2
Sxx

 

(2.45)

The prediction interval (2.45) is of minimum width at x0 = x and widens as x0 - x increases. By comparing (2.45) with (2.43), we observe that the prediction interval at x0 is always wider than the CI at x0 because the prediction interval depends on both the error from the fitted model and the error associated with future observations.

Example 2.7 The Rocket Propellant Data

We find a 95% prediction interval on a future value of propellant shear strength in a motor made from a batch of sustainer propellant that is 10 weeks old. Using (2.45), we find that the prediction interval is

y^0 - t 2,n-2

MSRes

 

1

+

1 n

+

(x0 - x
Sxx

)2

 

 y0  y^0 + t 2,n-2

MSRes

  1

+

1 n

+

(x0 - x )2
Sxx

 

2256.32 - (2.101)

 9244.59 

1

+

1 20

+

(10

- 13.3625)2
1106.56

 

 y0  2256.32 + (2.101)

9244.59

 

1

+

1 20

+

(10

- 13.3625)2
1106.56

 

which simplifies to
2048.32  y0  2464.32
Therefore, a new motor made from a batch of 10-week-old sustainer propellant could reasonably be expected to have a propellant shear strength between 2048.32 and 2464.32 psi.

3000

COEFFICIENT OF DETERMINATION

35

2500

Shear Strength, y

2000

Upper 95% prediction limit

1500

Lower 95% prediction limit

1000

5

10

15

20

25

Age of Propellant, x

Figure 2.5 The 95% confidence and prediction intervals for the propellant data.

Figure 2.5 shows the 95% prediction interval calculated from (2.45) for the rocket

propellant regression model. Also shown on this graph is the 95% CI on the mean

[that is, E(y|x) from Eq. (2.43). This graph nicely illustrates the point that the predic-

tion interval is wider than the corresponding CI.



We may generalize (2.45) somewhat to find a 100(1 - ) percent prediction
interval on the mean of m future observations on the response at x = x0. Let y0 be the mean of m future observations at x = x0. A point estimator of y0 is y^0 = ^0 + ^1x0. The 100(1 - )% prediction interval on y0 is

y^0 - t 2,n-2

MSRes

 

1 m

+

1 n

+

(x0 - x
Sxx

)2

 

 y0  y^0 + t 2,n-2

MSRes

 

1 m

+

1 n

+

(x0 - x
Sxx

)2

 

(2.46)

2.6 COEFFICIENT OF DETERMINATION

The quantity

R2 = SSR = 1 - SSRes

SST

SST

(2.47)

is called the coefficient of determination. Since SST is a measure of the variability in y without considering the effect of the regressor variable x and SSRes is a measure

36

SIMPLE LINEAR REGRESSION

of the variability in y remaining after x has been considered, R2 is often called the proportion of variation explained by the regressor x. Because 0  SSRes  SST, it follows that 0  R2  1. Values of R2 that are close to 1 imply that most of the variability in y is explained by the regression model. For the regression model for the
rocket propellant data in Example 2.1, we have

R2 = SSR = 1, 527, 334.95 = 0.9018 SST 1, 693, 737.60

that is, 90.18% of the variability in strength is accounted for by the regression model. The statistic R2 should be used with caution, since it is always possible to make
R2 large by adding enough terms to the model. For example, if there are no repeat points (more than one y value at the same x value), a polynomial of degree n - 1 will give a "perfect" fit (R2 = 1) to n data points. When there are repeat points, R2 can never be exactly equal to 1 because the model cannot explain the variability
related to "pure" error. Although R2 cannot decrease if we add a regressor variable to the model, this
does not necessarily mean the new model is superior to the old one. Unless the error
sum of squares in the new model is reduced by an amount equal to the original
error mean square, the new model will have a larger error mean square than the
old one because of the loss of one degree of freedom for error. Thus, the new model
will actually be worse than the old one. The magnitude of R2 also depends on the range of variability in the regressor
variable. Generally R2 will increase as the spread of the x's increases and decrease
as the spread of the x's decreases provided the assumed model form is correct. By the delta method (also see Hahn 1973), one can show that the expected value of R2
from a straight-line regression is approximately

( ) E R2



12 Sxx 12 Sxx

n-1 +2

n-1

Clearly the expected value of R2 will increase (decrease) as Sxx (a measure of the spread of the x's) increases (decreases). Thus, a large value of R2 may result simply
because x has been varied over an unrealistically large range. On the other hand, R2 may be small because the range of x was too small to allow its relationship with
y to be detected. There are several other misconceptions about R2. In general, R2 does not measure
the magnitude of the slope of the regression line. A large value of R2 does not imply a steep slope. Furthermore, R2 does not measure the appropriateness of the linear model, for R2 will often be large even though y and x are nonlinearly related. For example, R2 for the regression equation in Figure 2.3b will be relatively large even though the linear approximation is poor. Remember that although R2 is large,
this does not necessarily imply that the regression model will be an accurate
predictor.

A SERVICE INDUSTRY APPLICATION OF REGRESSION

37

2.7 A SERVICE INDUSTRY APPLICATION OF REGRESSION

A hospital is implementing a program to improve service quality and productivity. As part of this program the hospital management is attempting to measure and evaluate patient satisfaction. Table B.17 contains some of the data that have been collected on a random sample of 25 recently discharged patients. The response variable is satisfaction, a subjective response measure on an increasing scale. The potential regressor variables are patient age, severity (an index measuring the severity of the patient's illness), an indicator of whether the patient is a surgical or medical patient (0 = surgical, 1 = medical), and an index measuring the patient's anxiety level. We start by building a simple linear regression model relating the response variable satisfaction to severity.
Figure 2.6 is a scatter diagram of satisfaction versus severity. There is a relatively mild indication of a potential linear relationship between these two variables. The output from JMP for fitting a simple linear regression model to these data is shown in Figure 2.7. JMP is an SAS product that is a menu-based PC statistics package with an extensive array of regression modeling and analysis capabilities.

Satisfaction

110

100

90

80

70

60

50

40

30

20

20

30

40

50

60

70

Severity

Figure 2.6 Scatter diagram of satisfaction versus severity.

38

SIMPLE LINEAR REGRESSION

Satisfaction

Response Satisfaction Whole Model Regression Plot
110 100
90 80 70 60 50 40 30 20
20 30 40 50 60 70 Severity
Actual by Predicted Plot
110 100
90 80 70 60 50 40 30 20
20 30 40 50 60 70 80 90 100 110 Satisfaction Predicted
P=0.0004 RSq=0.43 RMSE=16.432

Satisfaction Actual

Summary of Fit

RSquare RSquare Adj Root Mean Square Error Mean of Response Observations (or Sum Wgts)

0.426596 0.401666 16.43242
66.72 25

Analysis of Variance

Source

DF

Model

1

Error

23

C. Total

24

Sum of Squares 4620.482 6210.558
10831.040

Mean Square 4620.48 270.02

F Ratio 17.1114 Prob > F 0.0004*

Parameter Estimates

Term

Estimate

Intercept

115.6239

Std Error 12.27059

t Ratio 9.42

Prob>|t| <.0001*

Figure 2.7 JMP output for the simple linear regression model for the patient satisfaction data.

USING SAS® AND R FOR SIMPLE LINEAR REGRESSION

39

At the top of the JMP output is the scatter plot of the satisfaction and severity data, along with the fitted regression line. The straight line fit looks reasonable although there is considerable variability in the observations around the regression line. The second plot is a graph of the actual satisfaction response versus the predicted response. If the model were a perfect fit to the data all of the points in this plot would lie exactly along the 45-degree line. Clearly, this model does not provide a perfect fit. Also, notice that while the regressor variable is significant (the ANOVA F statistic is 17.1114 with a P value that is less than 0.0004), the coefficient of determination R2 = 0.43. That is, the model only accounts for about 43% of the variability in the data. It can be shown by the methods discussed in Chapter 4 that there are no fundamental problems with the underlying assumptions or measures of model adequacy, other than the rather low value of R2.
Low values for R2 occur occasionally in practice. The model is significant, there are no obvious problems with assumptions or other indications of model inadequacy, but the proportion of variability explained by the model is low. Now this is not an entirely disastrous situation. There are many situations where explaining 30 to 40% of the variability in y with a single predictor provides information of considerable value to the analyst. Sometimes, a low value of R2 results from having a lot of variability in the measurements of the response due to perhaps the type of measuring instrument being used, or the skill of the person making the measurements. Here the variability in the response probably arises because the response is an expression of opinion, which can be very subjective. Also, the measurements are taken on human patients, and there can be considerably variability both within people and between people. Sometimes, a low value of R2 is a result of a poorly specified model. In these cases the model can often be improved by the addition of one or more predictor or regressor variables. We see in Chapter 3 that the addition of another regressor results in considerable improvement of this model.

2.8 USING SAS® AND R FOR SIMPLE LINEAR REGRESSION
The purpose of this section is to introduce readers to SAS and to R. Appendix D gives more details about using SAS, including how to import data from both text and EXCEL files. Appendix E introduces the R statistical software package. R is becoming increasingly popular since it is free over the Internet.
Table 2.7 gives the SAS source code to analyze the rocket propellant data that we have been analyzing throughout this chapter. Appendix D provides detail explaining how to enter the data into SAS. The statement PROC REG tells the software that we wish to perform an ordinary least-squares linear regression analysis. The "model" statement specifies the specific model and tells the software which analyses to perform. The variable name to the left of the equal sign is the response. The variables to the right of the equal sign but before the solidus are the regressors. The information after the solidus specifies additional analyses. By default, SAS prints the analysis-of-variance table and the tests on the individual coefficients. In this case, we have specified three options: "p" asks SAS to print the predicted values, "clm" (which stands for confidence limit, mean) asks SAS to print the confidence band, and "cli" (which stands for confidence limit, individual observations) asks SAS to print the prediction band.

40

SIMPLE LINEAR REGRESSION

TABLE 2.7 SAS Code for Rocket Propellant Data
data rocket; input shear age; cards; 2158.70 15.50 1678.15 23.75 2316.00 8.00 2061.30 17.00 2207.50 5.50 1708.30 19.00 1784.70 24.00 2575.00 2.50 2357.90 7.50 2256.70 11.00 2165.20 13.00 2399.55 3.75 1779.80 25.99 2336.75 9.75 1765.30 22.00 2053.50 18.00 2414.40 6.00 2200.50 12.50 2654.20 2.00 1753.70 21.50 proc reg; model shear=age/p clm cli; run;

Table 2.8 gives the SAS output for this analysis. PROC REG always produces the analysis-of-variance table and the information on the parameter estimates. The "p clm cli" options on the model statement produced the remainder of the output file.
SAS also produces a log file that provides a brief summary of the SAS session. The log file is almost essential for debugging SAS code. Appendix D provides more details about this file.
R is a popular statistical software package, primarily because it is freely available at www.r-project.org. An easier-to-use version of R is R Commander. R itself is a high-level programming language. Most of its commands are prewritten functions. It does have the ability to run loops and call other routines, for example, in C. Since it is primarily a programming language, it often presents challenges to novice users. The purpose of this section is to introduce the reader as to how to use R to analyze simple linear regression data sets.
The first step is to create the data set. The easiest way is to input the data into a text file using spaces for delimiters. Each row of the data file is a record. The top row should give the names for each variable. All other rows are the actual data records. For example, consider the rocket propellant data from Example 2.1 given in Table 2.1. Let propellant.txt be the name of the data file. The first row of the text file gives the variable names:

TABLE 2.8 SAS Output for Analysis of Rocket Propellant Data.

SAS system 1

The REG Procedure Model: MODEL1
Dependent Variable: shear

Number of Observations Read

20

Number of Observations Used

20

Source

Analysis of Variance

Sum of

DF

Squares

Mean Square

F Value Pr > F

Model Error Corrected Total

1

1527483 1527483

165.38 <.0001

18

166255 9236.38100

19

1693738

Root MSE Dependent Mean Coeff Var

96.10609 2131.35750
4.50915

R- square Adj R- Sq

0.9018 0.8964

Variable

Parameter Estimates

Parameter

DF

Estimate

Standard Error

t value Pr > |t|

Intercept age

1

2627.82236 44.18391

59.47 <.0001

1

-37.15359 2.88911

-12.86 <.0001

The SAS System 2

The REG Procedure Model: MODEL1
Dependent Variable: shear

Output Statistics

Dependent Predicted Std Error

Obs Variable Value

Mean Predict 95% CL Mean 95% CL Predict Residual

1

2159

2

1678

3

2316

4

2061

5

2208

6

1708

7

1785

8

2575

9

2358

10

2257

11

2165

12

2400

13

1780

14

2337

15

1765

16

2054

17

2414

18

2201

19

2654

20

1754

2052 1745 2331 1996 2423 1922 1736 2535 2349 2219 2145 2488 1699 2266 1810 1959 2405 2163 2554 1829

22.3597 36.9114 26.4924 23.9220 31.2701 26.9647 37.5010 38.0356 27.3623 22.5479 21.5155 35.1152 39.9031 23.8903 32.9362 25.3245 30.2370 21.6340 39.2360 31.8519

2005 1668 2275 1946 2358 1865 1657 2455 2292 2172 2100 2415 1615 2215 1741 1906 2341 2118 2471 1762

2099 1823 2386 2046 2489 1979 1815 2615 2407 2267 2190 2562 1783 2316 1880 2012 2468 2209 2636 1896

1845 1529 2121 1788 2211 1712 1519 2318 2139 2012 1938 2274 1480 2058 1597 1750 2193 1956 2335 1616

2259 1962 2540 2204 2636 2132 1953 2752 2559 2427 2352 2703 1918 2474 2024 2168 2617 2370 2772 2042

106.7583 -67.2746 -14.5936
65.0887 -215.9776 -213.6041
48.5638 40.0616
8.7296 37.5671 20.3743 -88.9464 80.8174 71.1752 -45.1434 94.4423
9.4992 37.0975 100.6848 -75.3202

Sum of Residuals Sum of squared Residuals Predicted Residual SS (PRESS)

0 166255 205944

42

SIMPLE LINEAR REGRESSION

strength age

The next row is the first data record, with spaces delimiting each data item:

2158.70 15.50 The R code to read the data into the package is: prop <- read.table("propellant.txt",header=TRUE, sep="")

The object prop is the R data set, and "propellant.txt" is the original data file. The phrase, header=TRUE tells R that the first row is the variable names. The phrase sep="" tells R that the data are space delimited.
The commands
prop.model <- lm(strengthage, data=prop) summary(prop.model)
tell R

· to estimate the model, and
· to print the analysis of variance, the estimated coefficients, and their tests.
R Commander is an add-on package to R. It also is freely available. It provides an easy-to-use user interface, much like Minitab and JMP, to the parent R product. R Commander makes it much more convenient to use R; however, it does not provide much flexibility in its analysis. R Commander is a good way for users to get familiar with R. Ultimately, however, we recommend the use of the parent R product.

2.9 SOME CONSIDERATIONS IN THE USE OF REGRESSION
Regression analysis is widely used and, unfortunately, frequently misused. There are several common abuses of regression that should be mentioned:
1. Regression models are intended as interpolation equations over the range of the regressor variable(s) used to fit the model.As observed previously, we must be careful if we extrapolate outside of this range. Refer to Figure 1.5.
2. The disposition of the x values plays an important role in the least-squares fit. While all points have equal weight in determining the height of the line, the slope is more strongly influenced by the remote values of x. For example, consider the data in Figure 2.8. The slope in the least-squares fit depends heavily on either or both of the points A and B. Furthermore, the remaining data would give a very different estimate of the slope if A and B were deleted. Situations

SOME CONSIDERATIONS IN THE USE OF REGRESSION

43

A

y

y

B
x Figure 2.8 Two influential observations.

x
Figure 2.9 A point remote in x space.

such as this often require corrective action, such as further analysis and possible deletion of the unusual points, estimation of the model parameters with some technique that is less seriously influenced by these points than least squares, or restructuring the model, possibly by introducing further regressors.
A somewhat different situation is illustrated in Figure 2.9, wher one of the 12 observations is very remote in x space. In this example the slope is largely determined by the extreme point. If this point is deleted, the slope estimate is probably zero. Because of the gap between the two clusters of points, we really have only two distinct information units with which to fit the model. Thus, there are effectively far fewer than the apparent 10 degrees of freedom for error.
Situations such as these seem to occur fairly often in practice. In general we should be aware that in some data sets one point (or a small cluster of points) may control key model properties.
3. Outliers are observations that differ considerably from the rest of the data. They can seriously disturb the least-squares fit. For example, consider the data in Figure 2.10. Observation A seems to be an outlier because it falls far from the line implied by the rest of the data. If this point is really an outlier, then the estimate of the intercept may be incorrect and the residual mean square may be an inflated estimate of 2. The outlier may be a "bad value" that has resulted from a data recording or some other error. On the other hand, the data point may not be a bad value and may be a highly useful piece of evidence concerning the process under investigation. Methods for detecting and dealing with outliers are discussed more completely in Chapter 4.
4. As mentioned in Chapter 1, just because a regression analysis has indicated a strong relationship between two variables, this does not imply that the variables are related in any causal sense. Causality implies necessary correlation. Regression analysis can only address the issues on correlation. It cannot address the issue of necessity. Thus, our expectations of discovering cause-andeffect relationships from regression should be modest.

44

SIMPLE LINEAR REGRESSION

A

y

x Figure 2.10 An outlier.

TABLE 2.9 Data Illustrating Nonsense Relationships between Variables

Year
1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937

Number of Certified Mental Defectives per 10,000 of Estimated
Population in the U.K ( y)
8 8 9 10 11 11 12 16 18 19 20 21 22 23

Number of Radio Receiver Licenses Issued (Millions) in the U.K (x1)
1.350 1.960 2.270 2.483 2.730 3.091 3.647 4.620 5.497 6.260 7.012 7.618 8.131 8.593

Source: Kendall and Yule [1950] and Tufte [1974].

First Name of President of the U.S. (x2)
Calvin Calvin Calvin Calvin Calvin Calvin Herbert Herbert Herbert Herbert Franklin Franklin Franklin Franklin

As an example of a "nonsense" relationship between two variables, consider the data in Table 2.9. This table presents the number of certified mental defectives in the United Kingdom per 10,000 of estimated population (y), the number of radio receiver licenses issued (x1), and the first name of the President of the United States (x2) for the years 1924­1937. We can show that the regression equation relating y to x1 is
y^ = 4.582 + 2.204x1
The t statistic for testing H0: 1 = 0 for this model is t0 = 27.312 (the P value is 3.58 × 10-12), and the coefficient of determination is R2 = 0.9842. That is, 98.42% of the variability in the data is explained by the number of radio

REGRESSION THROUGH THE ORIGIN

45

receiver licenses issued. Clearly this is a nonsense relationship, as it is highly unlikely that the number of mental defectives in the population is functionally related to the number of radio receiver licenses issued. The reason for this strong statistical relationship is that y and x1 are monotonically related (two sequences of numbers are monotonically related if as one sequence increases, the other always either increases or decreases). In this example y is increasing because diagnostic procedures for mental disorders are becoming more refined over the years represented in the study and x1 is increasing because of the emergence and low-cost availability of radio technology over the years.
Any two sequences of numbers that are monotonically related will exhibit similar properties. To illustrate this further, suppose we regress y on the number of letters in the first name of the U.S. president in the corresponding year. The model is

y^ = -26.442 + 5.900x2
with t0 = 8.996 (the P value is 1.11 × 10-6) and R2 = 0.8709. Clearly this is a nonsense relationship as well.
5. In some applications of regression the value of the regressor variable x required to predict y is unknown. For example, consider predicting maximum daily load on an electric power generation system from a regression model relating the load to the maximum daily temperature. To predict tomorrow's maximum load, we must first predict tomorrow's maximum temperature. Consequently, the prediction of maximum load is conditional on the temperature forecast. The accuracy of the maximum load forecast depends on the accuracy of the temperature forecast. This must be considered when evaluating model performance.

Other abuses of regression are discussed in subsequent chapters. For further reading on this subject, see the article by Box [1966].

2.10 REGRESSION THROUGH THE ORIGIN

Some regression situations seem to imply that a straight line passing through the origin should be fit to the data. A no-intercept regression model often seems appropriate in analyzing data from chemical and other manufacturing processes. For example, the yield of a chemical process is zero when the process operating temperature is zero.
The no-intercept model is

y = 1x +  Given n observations (yi, xi), i = 1, 2, . . . , n, the least-squares function is

(2.48)

n
 S (1 ) = (yi - 1xi )2 i=1

46

SIMPLE LINEAR REGRESSION

The only normal equation is

n

n

  ^1 xi2 = yi xi

i=1

i=1

and the least-squares estimator of the slope is

(2.49)

n

 yi xi

^1 =

i=1 n

 xi2

i=1

The estimator of ^1 is unbiased for 1, and the fitted regression model is y^ = ^1x

The estimator of 2 is

(2.50) (2.51)

n

n

n

   (yi - y^i )2

yi2 - ^1 yi xi

^ 2 = MSRes = i=1 n - 1

= i=1

i=1
n-1

(2.52)

with n - 1 degrees of freedom. Making the normality assumption on the errors, we may test hypotheses and
construct confidence and prediction intervals for the no-intercept model. The 100(1 - ) percent CI on 1 is

^1 - t 2,n-1

MSRes
n

 1  ^1 + t 2,n-1

 xi2

i=1

MSRes
n
 xi2
i=1

(2.53)

A 100(1 - ) percent CI on E(y|x0), the mean response at x = x0, is

^ y x0 - t 2,n-1

x02 MSRes
n

 E (y x0 )  ^ y x0 + t 2,n-1

 xi2

i=1

x02 MSRes
n
 xi2
i=1

(2.54)

The 100(1 - ) percent prediction interval on a future observation at x = x0, say y0, is

REGRESSION THROUGH THE ORIGIN

47

y^0 - t 2,n-1





 

MSRes

 

1

+

x02
n



 



y0



y^ 0

+ t

2,n-1



i=1 xi2 





 

MSRes

 

1

+

x02
n

  



i=1 xi2 

(2.55)

Both the CI (2.54) and the prediction interval (2.55) widen as x0 increases. Furthermore, the length of the CI (2.54) at x = 0 is zero because the model assumes that the mean y at x = 0 is known with certainty to be zero. This behavior is considerably different than observed in the intercept model. The prediction interval (2.55) has nonzero length at x0 = 0 because the random error in the future observation must be taken into account.
It is relatively easy to misuse the no-intercept model, particularly in situations where the data lie in a region of x space remote from the origin. For example, consider the no-intercept fit in the scatter diagram of chemical process yield (y) and operating temperature (x) in Figure 2.11a. Although over the range of the regressor variable 100°F  x  200°F, yield and temperature seem to be linearly related, forcing the model to go through the origin provides a visibly poor fit. A model containing an intercept, such as illustrated in Figure 2.11b, provides a much better fit in the region of x space where the data were collected.
Frequently the relationship between y and x is quite different near the origin than it is in the region of x space containing the data. This is illustrated in Figure 2.12 for the chemical process data. Here it would seem that either a quadratic or a more complex nonlinear regression model would be required to adequately express the relationship between y and x over the entire range of x. Such a model should only be entertained if the range of x in the data is sufficiently close to the origin.
The scatter diagram sometimes provides guidance in deciding whether or not to fit the no-intercept model.Alternatively we may fit both models and choose between them based on the quality of the fit. If the hypothesis 0 = 0 cannot be rejected in the intercept model, this is an indication that the fit may be improved by using the

Yield, y Yield, y

50

100

150

200

Temperature, x (°F)

(a)

50

100

150

200

Temperature, x (°F)

(b)

Figure 2.11 Scatter diagrams and regression lines for chemical process yield and operating temperature: (a) no-intercept model; (b) intercept model.

48

SIMPLE LINEAR REGRESSION

Yield, y

0

50

100

150

200

Temperature, x (°F)

Figure 2.12 True relationship between yield and temperature.

no-intercept model. The residual mean square is a useful way to compare the quality of fit. The model having the smaller residual mean square is the best fit in the sense that it minimizes the estimate of the variance of y about the regression line.
Generally R2 is not a good comparative statistic for the two models. For the intercept model we have

n

 (y^i - y)2

 R2

=

i=1 n
(yi - y)2

=

variation in y explained by regression total observed variation in y

i=1

Note that R2 indicates the proportion of variability around y explained by regression. In the no-intercept case the fundamental analysis-of-variance identity (2.32) becomes

n

n

n

   yi2 = y^i2 + (yi - y^i )2

i=1

i=1

i=1

so that the no-intercept model analogue for R2 would be

n
 y^i2

R02 =

i=1 n

 yi2

i=1

The statistic R02 indicates the proportion of variability around the origin (zero) accounted for by regression. We occasionally find that R02 is larger than R2 even though the residual mean square (which is a reasonable measure of the overall
quality of the fit) for the intercept model is smaller than the residual mean square

REGRESSION THROUGH THE ORIGIN

49

for the no-intercept model. This arises because R02 is computed using uncorrected sums of squares.
There are alternative ways to define R2 for the no-intercept model. One
possibility is

n
 (yi - y^i )2

R02

= 1-

i=1 n

 (yi - y)2

i=1

However,

in

cases

where

( 

n i=1

yi

-

y^ i

)2

is

large,

R02 

can

be

negative.

We

prefer

to

use

MSRes as a basis of comparison between intercept and no-intercept regression

models. A nice article on regression models with no intercept term is Hahn [1979].

Example 2.8 The Shelf-Stocking Data

The time required for a merchandiser to stock a grocery store shelf with a soft drink product as well as the number of cases of product stocked is shown in Table 2.10. The scatter diagram shown in Figure 2.13 suggests that a straight line passing through the origin could be used to express the relationship between time and the number of cases stocked. Furthermore, since if the number of cases x = 0, then shelf stocking time y = 0, this model seems intuitively reasonable. Note also that the range of x is close to the origin.
The slope in the no-intercept model is computed from Eq. (2.50) as

n

yi xi

 ^1 =

i=1 n

xi2

= 1841.98 = 0.4026 4575.00

i=1

Therefore, the fitted equation is

y^ = 0.4026x

This regression line is shown in Figure 2.14. The residual mean square for this
model is MSRes = 0.0893 and R02 = 0.9883. Furthermore, the t statistic for testing H0:
 1 = 0 is t0 = 91.13, for which the P value is 8.02 × 10-21. These summary statistics do
not reveal any startling inadequacy in the no-intercept model.

We may also fit the intercept model to the data for comparative purposes. This results in

y^ = -0.0938 + 0.4071x

The t statistic for testing H0: 0 = 0 is t0 = -0.65, which is not significant, implying that the no-intercept model may provide a superior fit. The residual mean square

50

SIMPLE LINEAR REGRESSION

TABLE 2.10 Shelf-Stocking Data for Example 2.8

Times, y (minutes)

Cases Stocked, x

10.15

25

2.96

6

3.00

8

6.88

17

0.28

2

5.06

13

9.14

23

11.86

30

11.69

28

6.04

14

7.57

19

1.74

4

9.38

24

0.16

1

1.84

5

15

Upper 95% prediction limits

15

Upper 95% confidence limits

10

Lower 95% confidence limits

Lower 95% prediction limits

10

5 5

Time, y Time, y

0 0 4 8 12 16 20 24 28 32 Cases stocked, x
Figure 2.13 Scatter diagram of shelf-stocking data.

0 0 5 10 15 20 25 30 Cases stocked, x
Figure 2.14 The confidence and prediction bands for the shelf-stocing data.

for the intercept model is MSRes = 0.0931 and R2 = 0.9997. Since MSRes for the nointercept model is smaller than MSRes for the intercept model, we conclude that the no-intercept model is superior. As noted previously, the R2 statistics are not directly comparable.
Figure 2.14 also shows the 95% confidence interval or E(y|x0) computed from Eq. (2.54) and the 95% prediction interval on a single future observation y0 at x = x0 computed from Eq. (2.55). Notice that the length of the confidence interval at x0 = 0 is zero.
SAS handles the no-intercept case. For this situation, the model statement follows:
model time = cases/noint

ESTIMATION BY MAXIMUM LIKELIHOOD

51

2.11 ESTIMATION BY MAXIMUM LIKELIHOOD

The method of least squares can be used to estimate the parameters in a linear regression model regardless of the form of the distribution of the errors . Least squares produces best linear unbiased estimators of 0 and 1. Other statistical procedures, such as hypothesis testing and CI construction, assume that the errors
are normally distributed. If the form of the distribution of the errors is known, an
alternative method of parameter estimation, the method of maximum likelihood,
can be used. Consider the data (yi, xi), i = 1, 2, . . . , n. If we assume that the errors in the regres-
sion model are NID(0, 2), then the observations yi in this sample are normally and independently distributed random variables with mean 0 + 1xi and variance 2. The likelihood function is found from the joint distribution of the observations. If
we consider this joint distribution with the observations given and the parameters 0, 1, and 2 unknown constants, we have the likelihood function. For the simple linear regression model with normal errors, the likelihood function is

( ) ( ) L yi, xi, 0, i,  2

n
=
i=1

2 2

-1

2

exp

-

1 2

2

( yi

-

0

-

1 xi

)2

 

( )  =

2 2

-1

2

exp

 - 

1 2

2

n

( yi

-

0

-

1 xi

)2

 

i=1



(2.56)

The maximum-likelihood estimators are the parameter values, say 0, 1, and  2, that maximize L, or equivalently, ln L. Thus,

ln

L(

yi,

xi,

0,

i,



2

)

=

-



n 2



ln

2

-



n 2



ln



2

 =

-



1 2

2



n i=1

( yi

- 0

- 1xi )2

and the maximum-likelihood estimators 0, 1, and  2 must satisfy

( )  ln L

=1

   0 0,1, 2

2

n i=1

yi - 0 - 1xi

=0

( )  ln L

=1

   1 0,1, 2

2

n i=1

yi - 0 - 1xi

xi = 0

(2.57)
(2.58a) (2.58b)

and

( )  ln L

=- n + 1

 2 0,1, 2

2 2 2 4

n i=1

yi - 0 - 1xi 2 = 0

(2.58c)

The solution to Eq. (2.58) gives the maximum-likelihood estimators:

0 = y - 1x

(2.59a)

52

SIMPLE LINEAR REGRESSION

n
 yi (xi - x)

=

i=1 n

 (xi - x)2

i=1

n
 ( ) yi - 0 - 1xi 2
 2 = i=1 n

(2.59b) (2.59c)

Notice that the maximum-likelihood estimators of the intercept and slope, 0 and 1, are identical to the least-squares estimators of these parameters. Also,  2 is a biased estimator of 2. The biased estimator is related to the unbiased estimator ^ 2
[Eq. (2.19)] by  2 = [(n - 1) n]^ 2. The bias is small if n is moderately large. Generally
the unbiased estimator ^ 2 is used. In general, maximum-likelihood estimators have better statistical properties than
least-squares estimators. The maximum-likelihood estimators are unbiased (including  2, which is asymptotically unbiased, or unbiased as n becomes large) and have minimum variance when compared to all other unbiased estimators. They are also consistent estimators (consistency is a large-sample property indicating that the estimators differ from the true parameter value by a very small amount as n becomes large), and they are a set of sufficient statistics (this implies that the estimators contain all of the "information" in the original sample of size n). On the other hand, maximum-likelihood estimation requires more stringent statistical assumptions than the least-squares estimators. The least-squares estimators require only second-moment assumptions (assumptions about the expected value, the variances, and the covariances among the random errors). The maximum-likelihood estimators require a full distributional assumption, in this case that the random errors follow a normal distribution with the same second moments as required for the least-squares estimates. For more information on maximum-likelihood estimation in regression models, see Graybill [1961, 1976], Myers [1990], Searle [1971], and Seber [1977].

2.12 CASE WHERE THE REGRESSOR x IS RANDOM
The linear regression model that we have presented in this chapter assumes that the values of the regressor variable x are known constants. This assumption makes the confidence coefficients and type I (or type II) errors refer to repeated sampling on y at the same x levels. There are many situations in which assuming that the x's are fixed constants is inappropriate. For example, consider the soft drink delivery time data from Chapter 1 (Figure 1.1). Since the outlets visited by the delivery person are selected at random, it is unrealistic to believe that we can control the delivery volume x. It is more reasonable to assume that both y and x are random variables.
Fortunately, under certain circumstances, all of our earlier results on parameter estimation, testing, and prediction are valid. We now discuss these situations.

CASE WHERE THE REGRESSOR X IS RANDOM

53

2.12.1 x and y Jointly Distributed
Suppose that x and y are jointly distributed random variables but the form of this joint distribution is unknown. It can be shown that all of our previous regression results hold if the following conditions are satisfied:

1. The conditional distribution of y given x is normal with conditional mean 0 + 1x and conditional variance 2.
2. The x's are independent random variables whose probability distribution does not involve 0, 1, and 2.

While all of the regression procedures are unchanged when these conditions hold, the confidence coefficients and statistical errors have a different interpretation. When the regressor is a random variable, these quantities apply to repeated sampling of (xi, yi) values and not to repeated sampling of yi at fixed levels of xi.

2.12.2 x and y Jointly Normally Distributed: Correlation Model
Now suppose that y and x are jointly distributed according to the bivariate normal distribution. That is,

f (y, x) =

1

21 2 1 - 2

exp

 - 

2(1

1 -

2

)

 

y - 1 1

 

2

+

 

x

- 2 2

 

2

-

2

 

y - 1 1

 

 

x

- 2 2

 

 

(2.60)

where

1

and



2 1

the

mean

and

variance

of

y,

2

and



2 2

the

mean

and

variance

of x, and

 = E (y - 1 )(x - 2 ) = 12

1 2

1 2

is the correlation coefficient between y and x. The term 12 is the covariance of y and x.
The conditional distribntion of y for a given value of x is

f (y x) =

1 2  1.2

exp

 - 

1 2

 

y

-

0 -  1.2

1

x

 

2

  

(2.61)

where

0

=

1

-

2



 

1 2

1

=

1 2



(2.62a) (2.62b)

54

SIMPLE LINEAR REGRESSION

and

( ) 

2 1.2

=



2 1

1- 2

(2.62c)

That is, the conditional distribution of y given x is normal with conditional mean

E (y x) = 0 + 1x

(2.63)

and

conditional

variance



2 1.2

.

Note

that

the

mean

of

the

conditional

distribution

of

y given x is a straight-line regression model. Furthermore, there is a relationship

between the correlation coefficient  and the slope 1. From Eq. (2.62b) we see that

if  = 0, then 1= 0, which implies that there is no linear regression of y on x. That

is, knowledge of x does not assist us in predicting y.

The method of maximum likelihood may be used to estimate the parameters 0

and 1. It may be shown that the maximum-likelihood estimators of these param-

eters are

^0 = y - ^1x

(2.64a)

and

n

 ^1 =

i=1 n

yi (xi - x ) (xi - x )2

=

Sxy Sxx

i=1

(2.64b)

The estimators of the intercept and slope in Eq. (2.64) are identical to those given

by the method of least squares in the case where x was assumed to be a controllable

variable. In general, the regression model with y and x jointly normally distributed

may be analyzed by the methods presented previously for the model with x a con-

trollable variable. This follows because the random variable y given x is indepen-

dently

and

normally

distributed

with

mean

0

+

1x

and

constant

variance



2 1.2

.

As

noted in Section 2.12.1, these results will also hold for any joint distribution of y

and x such that the conditional distribution of y given x is normal.

It is possible to draw inferences about the correlation coefficient  in this model.

The estimator of  is the sample correlation coefficient

n

 r =

  

n

yi (xi - x )

i=1

(xi - x )2

n

( yi

-

y )2

1 

2

=

Sxy
[Sxx SST

]1

2

 i=1

i=1



(2.65)

Note that

^ 1

=

 

SST Sxx

1 

2

r

(2.66)

CASE WHERE THE REGRESSOR X IS RANDOM

55

so that the slope ^ is just the sample correlation coefficient r multiplied by a scale factor that is the square root of the spread of the y's divided by the spread of the x's. Thus, ^1 and r are closely related, although they provide somewhat different information. The sample correlation coefficient r is a measure of the linear association between y and x, while ^1 measures the change in the mean of y for a unit
change in x. In the case of a controllable variable x, r has no meaning because the magnitude of r depends on the choice of spacing for x. We may also write, from Eq. (2.66),

r2

= ^12

Sxx SST

=

^ 1Sxy SST

=

SSR SST

= R2

which we recognize from Eq. (2.47) as the coefficient of determination. That is, the coefficient of determination R2 is just the square of the correlation coefficient between y and x.
While regression and correlation are closely related, regression is a more powerful tool in many situations. Correlation is only a measure of association and is of little use in prediction. However, regression methods are useful in developing quantitative relationships between variables, which can be used in prediction.
It is often useful to test the hypothesis that the correlation coefficient equals zero, that is,

H0:  = 0, H1:   0 The appropriate test statistic for this hypothesis is

(2.67)

t0

=

r

n-2 1- r2

(2.68)

which follows the t distribution with n - 2 degrees of freedom if H0:  = 0 is true. Therefore, we would reject the null hypothesis if |t0| > t/2, n-2. This test is equivalent to the t test for H0: 1 = 0 given in Section 2.3. This equivalence follows directly from Eq. (2.66).
The test procedure for the hypotheses

H0:  = 0, H1:   0

(2.69)

where 0  0 is somewhat more complicated. For moderately large samples (e.g., n  25) the statistic

Z = arctanh r = 1 ln 1 + r 2 1-r

(2.70)

is approximately normally distributed with mean

Z

=

arctanh 

=

1 ln 1 +  2 1-

56

SIMPLE LINEAR REGRESSION

and variance



2 Z

= (n - 3)-1

Therefore, to test the hypothesis H0:  = 0, we may compute the statistic

Z0 = (arctanh r - arctanh 0 )(n - 3)1 2

(2.71)

and reject H0:  = 0 if |Z0| > Z/2. It is also possible to construct a 100(1 - ) percent CI for  using the transforma-
tion (2.70). The 100(1 - ) percent CI is

tanh

 

arctanh

r

-

Z 2  n - 3 







tanh

 

arctanh

r

+

Z 2  n - 3 

(2.72)

where tanh u = (eu - e-u)/(eu + e-u).

Example 2.9 The Delivery Time Data

Consider the soft drink delivery time data introduced in Chapter 1. The 25 observations on delivery time y and delivery volume x are listed in Table 2.11. The scatter diagram shown in Figure 1.1 indicates a strong linear relationship between delivery time and delivery volume. The Minitab output for the simple linear regression model is in Table 2.12.
The sample correlation coefficient between delivery time y and delivery volume x is

r

=

Sxy
[Sxx SST

]1

2

=

2473.3440
[(1136.5600 ) (5784.5426 )]1

2

= 0.9646

TABLE 2.11 Data Example 2.9

Observation

Delivery Time, y

Number of Cases, x

1

16.68

7

2

11.50

3

3

12.03

3

4

14.88

4

5

13.75

6

6

18.11

7

7

8.00

2

8

17.83

7

9

79.24

30

10

21.50

5

11

40.33

16

12

21.00

10

13

13.50

4

Observation
14 15 16 17 18 19 20 21 22 23 24 25

Delivery Time, y
19.75 24.00 29.00 15.35 19.00 9.50 35.10 17.90 52.32 18.75 19.83 10.75

Number of Cases, x
6 9 10 6 7 3 17 10 26 9 8 4

CASE WHERE THE REGRESSOR X IS RANDOM

57

TABLE 2.12 MlNITAB Output for Soft Drink Delivery Time Data

Regression Analysis: Time versus Cases

The regression equation is Time = 3.32 + 2.18 Cases

Predictor Constant Cases
S = 4.18140

Coef 3.321 2.1762
R- Sq= 93.0%

SE Coef 1.371 0.1240

T 2.42
17.55

R- Sq(adj) = 92.7%

Analysis of Variance

Source

DF

Regression

1

Residual Error

23

Total

24

SS 5382.4
402.1 5784.5

MS 5382.4
17.5

P 0.024 0.000
F 307.85

P 0.000

If we assume that delivery time and delivery volume are jointly normally distributed, we may test the hypotheses
H0:  = 0, H1:   0

using the test statistic

t0

=

r

n-2 1- r2

=

0.9646 23 1 - 0.9305

= 17.55

Since t0.025,23 = 2.069, we reject H0 and conclude that the correlation coefficient   0. Note from the Minitab output in Table 2.12 that this is identical to the t-test statistic for H0: 1 = 0. Finally, we may construct an approximate 95% CI on  from (2.72). Since arctanh r = arctanh 0.9646 = 2.0082, Eq. (2.72) becomes

tanh



2.0082

-

1.96 22









tanh



2.0082

+

1.96 22



which reduces to

0.9202    0.9845



Although we know that delivery time and delivery volume are highly correlated, this information is of little use in predicting, for example, delivery time as a function of the number of cases of product delivered. This would require a regression model. The straight-line fit (shown graphically in Figure 1.1b) relating delivery time to delivery volume is

y^ = 3.321 + 2.1762x

58

SIMPLE LINEAR REGRESSION

Further analysis would be required to determine if this equation is an adequate fit to the data and if it is likely to be a successful predictor.

PROBLEMS
2.1 Table B.1 gives data concerning the performance of the 26 National Football League teams in 1976. It is suspected that the number of yards gained rushing by opponents (x8) has an effect on the number of games won by a team (y). a. Fit a simple linear regression model relating games won y to yards gained rushing by opponents x8. b. Construct the analysis-of-variance table and test for significance of regression. c. Find a 95% CI on the slope. d. What percent of the total variability in y is explained by this model? e. Find a 95% CI on the mean number of games won if opponents' yards rushing is limited to 2000 yards.
2.2 Suppose we would like to use the model developed in Problem 2.1 to predict the number of games a team will win if it can limit opponents' yards rushing to 1800 yards. Find a point estimate of the number of games won when x8 = 1800. Find a 90% prediction interval on the number of games won.
2.3 Table B.2 presents data collected during a solar energy project at Georgia Tech. a. Fit a simple linear regression model relating total heat flux y (kilowatts) to the radial deflection of the deflected rays x4 (milliradians). b. Construct the analysis-of-variance table and test for significance of regression. c. Find a 99% CI on the slope. d. Calculate R2. e. Find a 95% CI on the mean heat flux when the radial deflection is 16.5 milliradians.
2.4 Table B.3 presents data on the gasoline mileage performance of 32 different automobiles. a. Fit a simple linear regression model relating gasoline mileage y (miles per gallon) to engine displacement xl (cubic inches). b. Construct the analysis-of-variance table and test for significance of regression. c. What percent of the total variability in gasoline mileage is accounted for by the linear relationship with engine displacement? d. Find a 95% CI on the mean gasoline mileage if the engine displacement is 275 in.3 e. Suppose that we wish to predict the gasoline mileage obtained from a car with a 275-in.3 engine. Give a point estimate of mileage. Find a 95% prediction interval on the mileage. f. Compare the two intervals obtained in parts d and e. Explain the difference between them. Which one is wider, and why?

PROBLEMS 59
2.5 Consider the gasoline mileage data in Table B.3. Repeat Problem 2.4 (parts a, b, and c) using vehicle weight x10 as the regressor variable. Based on a comparison of the two models, can you conclude that x1 is a better choice of regressor than x10?
2.6 Table B.4 presents data for 27 houses sold in Erie, Pennsylvania. a. Fit a simple linear regression model relating selling price of the house to the current taxes (x1). b. Test for significance of regression. c. What percent of the total variability in selling price is explained by this model? d. Find a 95% CI on 1. e. Find a 95% CI on the mean selling price of a house for which the current taxes are $750.
2.7 The purity of oxygen produced by a fractional distillation process is thought to be related to the percentage of hydrocarbons in the main condensor of the processing unit. Twenty samples are shown below.

Purily (%)
86.91
89.85 90.28 86.34 92.58 87.33 86.29 91.86 95.61 89.86

Hydrocarbon (%)
1.02
1.11 1.43 1.11 1.01 0.95 1.11 0.87 1.43 1.02

Purily (%)
96.73
99.42 98.66 96.07 93.65 87.31 95.00 96.85 85.20 90.56

Hydrocarbon (%)
1.46
1.55 1.55 1.55 1.40 1.15 1.01 0.99 0.95 0.98

a. Fit a simple linear regression model to the data. b. Test the hypothesis H0: 1 = 0. c. Calculate R2. d. Find a 95% CI on the slope. e. Find a 95% CI on the mean purity when the hydrocarbon percentage is
1.00.
2.8 Consider the oxygen plant data in Problem 2.7 and assume that purity and hydrocarbon percentage are jointly normally distributed random variables. a. What is the correlation between oxygen purity and hydrocarbon percentage? b. Test the hypothesis that  = 0. c. Construct a 95% CI for .
2.9 Consider the soft drink delivery time data in Table 2.9. After examining the original regression model (Example 2.9), one analyst claimed that the model

60

SIMPLE LINEAR REGRESSION

was invalid because the intercept was not zero. He argued that if zero cases were delivered, the time to stock and service the machine would be zero, and the straight-line model should go through the origin. What would you say in response to his comments? Fit a no-intercept model to these data and determine which model is superior.
2.10 The weight and systolic blood pressure of 26 randomly selected males in the age group 25­30 are shown below. Assume that weight and blood pressure (BP) are jointly normally distributed. a. Find a regression line relating systolic blood pressure to weight. b. Estimate the correlatiou coefficient. c. Test the hypothesis that  = 0. d. Test the hypothesis that  = 0.6. e. Find a 95% CI for .

Subject
1 2 3 4 5 6 7 8 9 10 11 12 13

Weight
165 167 180 155 212 175 190 210 200 149 158 169 170

Symbolic BP
130 133 150 128 151 146 150 140 148 125 133 135 150

Subject
14 15 16 17 18 19 20 21 22 23 24 25 26

Weight
172 159 168 174 183 215 195 180 143 240 235 192 187

Systolic BP
153 128 132 149 158 150 163 156 124 170 165 160 159

2.11 Consider the weight and blood pressure data in Problem 2.10. Fit a nointercept model to the data and compare it to the model obtained in Problem 2.10. Which model would you conclude is superior?
2.12 The number of pounds of steam used per month at a plant is thought to be related to the average monthly ambient temperature. The past year's usages and temperatures follow.

Month
Jan. Feb. Mar. Apr. May Jun.

Temperature
21 24 32 47 50 59

Usage/l000
185.79 214.47 288.03 424.84 454.68 539.03

Month
Jul. Aug. Sep. Oct. Nov. Dec.

Temperature
68 74 62 50 41 30

Usage/l000
621.55 675.06 562.03 452.93 369.95 273.98

PROBLEMS 61
a. Fit a simple linear regression model to the data. b. Test for significance of regression. c. Plant management believes that an increase in average ambient tempera-
ture of 1 degree will increase average monthly steam consumption by 10,000 lb. Do the data support this statement? d. Construct a 99% prediction interval on steam usage in a month with average ambient temperature of 58°.
2.13 Davidson ("Update on Ozone Trends in California's South Coast Air Basin," Air and Waste, 43, 226, 1993) studied the ozone levels in the South Coast Air Basin of California for the years 1976­1991. He believes that the number of days the ozone levels exceeded 0.20 ppm (the response) depends on the seasonal meteorological index, which is the seasonal average 850-millibar temperature (the regressor). The following table gives the data.

Year
1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991

Days
91 105 106 108 88 91 58 82 81 65 61 48 61 43 33 36

Index
16.7 17.1 18.2 18.1 17.2 18.2 16.0 17.2 18.0 17.2 16.9 17.1 18.2 17.3 17.5 16.6

a. Make a scatterplot of the data. b. Estimate the prediction equation. c. Test for significance of regression. d. Calculate and plot the 95% confidence and prediction bands.
2.14 Hsuie, Ma, and Tsai ("Separation and Characterizations of Thermotropic Copolyesters of p-Hydroxybenzoic Acid, Sebacic Acid, and Hydroquinone," Journal of Applied Polymer Science, 56, 471­476, 1995) study the effect of the molar ratio of sebacic acid (the regressor) on the intrinsic viscosity of copolyesters (the response). The following table gives the data.

62

SIMPLE LINEAR REGRESSION

Ratio
1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3

Viscosity
0.45 0.20 0.34 0.58 0.70 0.57 0.55 0.44

a. Make a scatterplot of the data. b. Estimate the prediction equation. c. Perform a complete, appropriate analysis (statistical tests, calculation of
R2, and so forth). d. Calculate and plot the 95% confidence and prediction bands.
2.15 Byers and Williams ("Viscosities of Binary and Ternary Mixtures of Polynomatic Hydrocarbons," Journal of Chemical and Engineering Data, 32, 349­354, 1987) studied the impact of temperature on the viscosity of toluene­ tetralin blends. The following table gives the data for blends with a 0.4 molar fraction of toluene.

Temperature (°C)
24.9 35.0 44.9 55.1 65.2 75.2 85.2 95.2

Viscosity (mPa · s)
1.1330 0.9772 0.8532 0.7550 0.6723 0.6021 0.5420 0.5074

a. Estimate the prediction equation. b. Perform a complete analysis of the model. c. Calculate and plot the 95% confidence and prediction bands.
2.16 Carroll and Spiegelman ("The Effects of Ignoring Small Measurement Errors in Precision Instrument Calibration," Journal of Quality Technology, 18, 170­ 173, 1986) look at the relationship between the pressure in a tank and the volume of liquid. The following table gives the data. Use an appropriate statistical software package to perform an analysis of these data. Comment on the output produced by the software routine.

Volume
2084 2084 2273 2273 2273 2463 2463 2651 2652 2652 2842

Pressure
4599 4600 5044 5043 5044 5488 5487 5931 5932 5932 6380

Volume
2842 3030 3031 3031 3221 3221 3409 3410 3600 3600 3788

Pressure
6380 6818 6817 6818 7266 7268 7709 7710 8156 8158 8597

PROBLEMS 63

Volume
3789 3789 3979 3979 4167 4168 4168 4358 4358 4546 4547

Pressure
8599 8600 9048 9048 9484 9487 9487 9936 9938 10377 10379

2.17 Atkinson (Plots, Transformations, and Regression, Clarendon Press, Oxford, 1985) presents the following data on the boiling point of water (°F) and barometric pressure (inches of mercury). Construct a scatterplot of the data and propose a model that relates a model that relates boiling point to barometric pressure. Fit the model to the data and perform a complete analysis of the model using the techniques we have discussed in this chapter.

Boiling Point
199.5 199.3 197.9 198.4 199.4 199.9 200.9 201.1

Barometric Pressure
20.79 20.79 22.40 22.67 23.15 23.35 23.89 23.99

Boiling Point
201.9 201.3 203.6 204.6 209.5 208.6 210.7 211.9 212.2

Barometric Pressure
24.02 24.01 25.14 26.57 28.49 27.76 29.64 29.88 30.06

2.18 On March 1, 1984, the Wall Street Journal published a survey of television advertisements conducted by Video Board Tests, Inc., a New York ad-testing company that interviewed 4000 adults. These people were regular product users who were asked to cite a commercial they had seen for that product category in the past week. In this case, the response is the number of millions of retained impressions per week. The regressor is the amount of money spent by the firm on advertising. The data follow.

Firm
Miller Lite Pepsi Stroh's

Amount Spent (millions)
50.1 74.1 19.3

Returned Impressions per week (millions)
32.1 99.6 11.7
(Continued)

64

SIMPLE LINEAR REGRESSION

Firm
Federal Express Burger King Coca-Cola McDonald's MCI Diet Cola Ford Levi's Bud Lite ATT Bell Calvin Klein Wendy's Polaroid Shasta Meow Mix Oscar Meyer Crest Kibbles N Bits

Amount Spent (millions)
22.9 82.4 40.1 185.9 26.9 20.4 166.2 27 45.6 154.9
5 49.7 26.9
5.7 7.6 9.2 32.4 6.1

Returned Impressions per week (millions)
21.9 60.8 78.6 92.4 50.7 21.4 40.1 40.8 10.4 88.9 12 29.2 38 10 12.3 23.4 71.1
4.4

a. Fit the simple linear regression model to these data. b. Is there a significant relationship between the amount a company spends
on advertising and retained impressions? Justify your answer statistically. c. Construct the 95% confidence and prediction bands for these data. d. Give the 95% confidence and prediction intervals for the number of
retained impressions for MCI.
2.19 Table B.17 Contains the Patient Satisfaction data used in Section 2.7. a. Fit a simple linear regression model relating satisfaction to age. b. Compare this model to the fit in Section 2.7 relating patient satisfaction to severity.
2.20 Consider the fuel consumption data given in Table B.18. The automotive engineer believes that the initial boiling point of the fuel controls the fuel consumption. Perform a thorough analysis of these data. Do the data support the engineer's belief?
2.21 Consider the wine quality of young red wines data in Table B.19. The winemakers believe that the sulfur content has a negative impact on the taste (thus, the overall quality) of the wine. Perform a thorough analysis of these data. Do the data support the winemakers' belief?
2.22 Consider the methanol oxidation data in Table B.20. The chemist believes that ratio of inlet oxygen to the inlet methanol controls the conversion process. Perform a through analysis of these data. Do the data support the chemist's belief?
2.23 Consider the simple linear regression model y = 50 + 10x +  where  is NID (0, 16). Suppose that n = 20 pairs of observations are used to fit this model.

PROBLEMS 65
Generate 500 samples of 20 observations, drawing one observation for each level of x = 1, 1.5, 2, . . . , 10 for each sample. a. For each sample compute the least-squares estimates of the slope and
intercept. Construct histograms of the sample values of ^0 and ^1. Discuss the shape of these histograms. b. For each sample, compute an estimate of E(y|x = 5). Construct a histogram of the estimates you obtained. Discuss the shape of the histogram. c. For each sample, compute a 95% CI on the slope. How many of these intervals contain the true value 1 = 10? Is this what you would expect? d. For each estimate of E(y|x = 5) in part b, compute the 95% CI. How many of these intervals contain the true value of E(y|x = 5) = 100? Is this what you would expect?
2.24 Repeat Problem 2.20 using only 10 observations in each samle, drawing one observation from each level x = 1, 2, 3, . . . , 10. What impact does using n = 10 have on the questions asked in Problem 2.17? Compare the lengths of the CIs and the appearance of the histograms.
2.25 Consider the simple linear regression model y = 0 + 1x + , with E() = 0, Var() = 2, and  uncorrelated.
( ) a. Show that Cov ^0, ^1 = -x 2 Sxx. ( ) b. Show that Cov y, 1 = 0.
2.26 Consider the simple linear regression model y = 0 + 1x + , with E() = 0, Var() = 2, and  uncorrelated.
a. Show that E (MSR ) =  2 + 12Sxx .
b. Show that E(MSRes) = 2. 2.27 Suppose that we have fit the straight-line regression model y^ = ^0 + ^1x1 but
the response is affected by a second variable x2 such that the true regression function is
E (y) = 0 + 1x1 + 2 x2
a. Is the least-squares estimator of the slope in the original simple linear regression model unbiased?
b. Show the bias in ^1.
2.28 Consider the maximum-likelihood estimator  2 of 2 in the simple linear regression model. We know that  2 is a biased estimator for 2. a. Show the amount of bias in  2. b. What happens to the bias as the sample size n becomes large?
2.29 Suppose that we are fitting a straight line and wish to make the standard error of the slope as small as possible. Suppose that the "region of interest" for x is -1  x  1. Where should the observations x1, x2, . . . , xn be taken? Discuss the practical aspects of this data collection plan.

66

SIMPLE LINEAR REGRESSION

2.30 Consider the data in Problem 2.12 and assume that steam usage and average temperature are jointly normally distributed. a. Find the correlation between steam usage and monthly average ambient temperature. b. Test the hypothesis that  = 0. c. Test the hypothesis that  = 0.5. d. Find a 99% CI for .
2.31 Prove that the maximum value of R2 is less than 1 if the data contain repeated (different) observations on y at the same value of x.
2.32 Consider the simple linear regression model

y = 0 + 1x + 
where the intercept 0 is known. a. Find the least-squares estimator of 1 for this model. Does this answer
seem reasonable?
( ) b. What is the variance of the slope ^1 for the least-squares estimator found in part a? c. Find a 100(1 - ) percent CI for 1. Is this interval narrower than the
estimator for the case where both slope and intercept are unknown?
2.33 Consider the least-squares residuals ei = yi - y^i, i = 1, 2, . . . , n, from the simple linear regression model. Find the variance of the residuals Var(ei). Is the variance of the residuals a constant? Discuss.

CHAPTER 3

MULTIPLE LINEAR REGRESSION

A regression model that involves more than one regressor variable is called a multiple regression model. Fitting and analyzing these models is discussed in this chapter. The results are extensions of those in Chapter 2 for simple linear regression.

3.1 MULTIPLE REGRESSION MODELS

Suppose that the yield in pounds of conversion in a chemical process depends on temperature and the catalyst concentration. A multiple regression model that might describe this relationship is

y = 0 + 1x1 + 2x2 + 

(3.1)

where y denotes the yield, x1 denotes the temperature, and x2 denotes the catalyst concentration. This is a multiple linear regression model with two regressor variables. The term linear is used because Eq. (3.1) is a linear function of the unknown parameters 0, 1 and 2.
The regression model in Eq. (3.1) describes a plane in the three-dimensional space of y, x1 and x2. Figure 3.1a shows this regression plane for the model

E (y) = 50 + 10x1 + 7x2

where we have assumed that the expected value of the error term  in Eq. (3.1) is zero. The parameter 0 is the intercept of the regression plane. If the range of the data includes x1 = x2 = 0, then 0 is the mean of y when x1 = x2 = 0. Otherwise 0 has

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
67

68

MULTIPLE LINEAR REGRESSION

240 200 160 E(y) 120
80 40
00

24 x1

68 (a)

x2 10

8

6

4

10 8

2

6

10 0 2 4 x2

0 0

220

203

186

169

67 84 101 118 135 152

2

4

6

8

10 x1

(b)

Figure 3.1 (a) The regression plane for the model E(y) = 50 + 10x1 + 7x2. (b) The contour plot.

no physical interpretation. The parameter 1 indicates the expected change in response (y) per unit change in x1 when x2 is held constant. Similarly 2 measures the expected change in y per unit change in x1 when x2 is held constant. Figure 3.1b shows a contour plot of the regression model, that is, lines of constant expected response E(y) as a function of x1 and x2. Notice that the contour lines in this plot are parallel straight lines.
In general, the response y may be related to k regressor or predictor variables. The model

y = 0 + 1x1 + 2x2 +... + k xk + 

(3.2)

is called a mnltiple linear regression model with k regressors. The parameters j, j = 0, 1, . . . , k, are called the regression coefficients. This model describes a hyperplane in the k-dimensional space of the regressor variables xj. The parameter j represents the expected change in the response y per unit change in xj when all of the remaining regressor variables xi(i  j) are held constant. For this reason the parameters j, j = 1, 2, . . . , k, are often called partial regression coefficients.
Multiple linear regression models are often used as empirical models or approximating functions. That is, the true functional relationship between y and x1, x2, . . . , xk is unknown, but over certain ranges of the regressor variables the linear regression model is an adequate approximation to the true unknown function.
Models that are more complex in structure than Eq. (3.2) may often still be analyzed by multiple linear regression techniques. For example, consider the cubic polynomial model

y = 0 + 1x + 2x2 + 3x3 + 

(3.3)

If we let x1 = x, x2 = x2, and x3 = x3, then Eq. (3.3) can be written as

y = 0 + 1x1 + 2 x2 + 3x3 + 

(3.4)

which is a multiple linear regression model with three regressor variables. Polynomial models are discussed in more detail in Chapter 7.

MULTIPLE REGRESSION MODELS

69

x2

10

720

653

800

8

586

600 E(y) 400

6

519

452

4

385

200

10

318

8

00

24 x1

6

6

8

10 0 2 4 x2

2 0
0

2

117

4

6

8

251 184 10 x1

(a)

(b)

Figure 3.2 (a) Three-dimensional plot of regression model E(y) = 50 + 10x1 + 7x2 + 5x1x2. (b) The contour plot.

Models that include interaction effects may also be analyzed by multiple linear regression methods. For example, suppose that the model is

y = 0 + 1x1 + 2 x2 + 12 x1x2 +  If we let x3 = x1x2 and 3 = 12, then Eq. (3.5) can be written as
y = 0 + 1x1 + 2 x2 + 3x3 + 

(3.5) (3.6)

which is a linear regression model. Figure 3.2a shows the three-dimensional plot of the regression model

y = 50 + 10x1 + 7x2 + 5x1x2

and Figure 3.2b the corresponding two-dimensional contour plot. Notice that, although this model is a linear regression model, the shape of the surface that is generated by the model is not linear. In general, any regression model that is linear in the parameters (the 's) is a linear regression model, regardless of the shape of the surface that it generates.
Figure 3.2 provides a nice graphical interpretation of an interaction. Generally, interaction implies that the effect produced by changing one variable (x1, say) depends on the level of the other variable (x2). For example, Figure 3.2 shows that changing x1 from 2 to 8 produces a much smaller change in E(y) when x2 = 2 than when x2 = 10. Interaction effects occur frequently in the study and analysis of realworld systems, and regression methods are one of the techniques that we can use to describe them.
As a final example, consider the second-order model with interaction

y = 0 + 1x1 + 2 x2 + 11x12 + 22 x22 + 12 x1x2 + 

(3.7)

If we let x3 = x12, x4 = x22, x5 = x1x2, 3 = 11, 4 = 22, and 5 = 12, then Eq. (3.7) can be written as a multiple linear regression model as follows:

y = 0 + 1x1 + 2 x2 + 3x3 + 4x4 + 5x5 + 

70

MULTIPLE LINEAR REGRESSION

1000 800 600
E(y) 400 200 00

24 x1

68 (a)

x2 10

8

6

4

10

8 6

2

2 4 x2 10 0

0 0

25 100

250 175 325 800 750 700 625 550 475400

2

4

6

8

10 x1

(b)

Figure 3.3 (a) Three-dimensional plot of the regression model E (y) = 800 + 10x1 + 7x2 -
8.5x12 - 5x22 + 4x1x2, (b) The contour plot.

Figure 3.3 shows the three-dimensional plot and the corresponding contour plot for
E ( y) = 800 + 10x1 + 7x2 - 8.5x12 - 5x22 + 4x1x2
These plots indicate that the expected change in y when x1 is changed by one unit (say) is a function of both x1 and x2. The quadratic and interaction terms in this model produce a mound-shaped function. Depending on the values of the regression coefficients, the second-order model with interaction is capable of assuming a wide variety of shapes; thus, it is a very flexible regression model.
In most real-world problems, the values of the parameters (the regression coefficients i) and the error variance 2 are not known, and they must be estimated from sample data. The fitted regression equation or model is typically used in prediction of future observations of the response variable y or for estimating the mean response at particular levels of the y's.

3.2 ESTIMATION OF THE MODEL PARAMETERS
3.2.1 Least-Squares Estimation of the Regression Coefficients
The method of least squares can be used to estimate the regression coefficients in Eq. (3.2). Suppose that n > k observations are available, and let yi denote the ith observed response and xij denote the ith observation or level of regressor xj. The data will appear as in Table 3.1. We assume that the error term  in the model has E() = 0, Var() = 2, and that the errors are uncorrelated.

TABLE 3.1 Data for Multiple Linear Regression

Regressors

Observation, i Response, y

x1

x2

...

xk

1

y1

x11

x12

...

x1k

2

y2

x21

x22

...

x2k

n

yn

xn1

xn2

...

xnk

ESTIMATION OF THE MODEL PARAMETERS

71

Throughout this chapter we assume that the regressor variables x1, x2, . . . , xk are fixed (i.e., mathematical or nonrandom) variables, measured without error. However, just as was discussed in Section 2.12 for the simple linear regression model, all of our results are still valid for the case where the regressors are random variables. This is certainly important, because when regression data arise from an observational study, some or most of the regressors will be random variables. When the data result from a designed experiment, it is more likely that the x's will be fixed variables. When the x's are random variables, it is only necessary that the observations on each regressor be independent and that the distribution not depend on the regression coefficients (the 's) or on 2. When testing hypotheses or constructing CIs, we will have to assume that the conditional distribution of y given x1, x2, . . . , xk be normal with mean 0 + 1x1 + 2x2 + . . . + kxk and variance 2.
We may write the sample regression model corresponding to Eq. (3.2) as

yi = 0 + 1xi1 + 2 xi2 + + k xik + i
k
 = 0 + j xij + i, i = 1, 2,... , n j =1

(3.8)

The least-squares function is

   n

n

k

2

S (0, 1,... , k ) =



2 i

i=1

=

i=1  yi - 0 -

j=1 j xij 

(3.9)

The function S must be minimized with respect to 0, 1, . . . , k. The least-squares estimators of 0, 1, . . . , k must satisfy

  S
0

^0,^1,...,^k

= -2

n i=1

  yi

- ^0

-

k j =1

^

j

xij

 

=0

(3.10a)

and

  S
 j

^0,^1,...,^k

n
= -2
i=1

  yi

- ^0

-

k j =1

^ j

xij

 

xij

= 0,

j = 1, 2,... , k

Simplifying Eq. (3.10), we obtain the least-squares normal equations

(3.10b)

n

n

n

n

    n^0 + ^1 xi1 + ^2 xi2 + + ^k xik = yi

i=1

i=1

i=1

i=1

n

n

n

n

n

     ^0 xi1 + ^1 xi21 + ^2 xi1xi2 + + ^k xi1xik = xi1yi

i=1

i=1

i=1

i=1

i=1

n

n

n

   ^0 xik + ^1 xik xi1 + ^2 xik xi2 +

i=1

i=1

i=1

n

n

  + ^k xi2k = xik yi

i=1

i=1

(3.11)

72

MULTIPLE LINEAR REGRESSION

Note that there are p = k + 1 normal equations, one for each of the unknown regression coefficients. The solution to the normal equations will be the least-squares estimators ^0, ^1,... , ^k.
It is more convenient to deal with multiple regression models if they are expressed in matrix notation. This allows a very compact display of the model, data, and results. In matrix notation, the model given by Eq. (3.8) is
y = Xb + e

where

 y1 

1 x11 x12

x1k 

y

=

 

y2

 

,

X = 1

x21

x22

x2

k

 







yn 

1 xn1 xn2

xnk 

0 

b

=

 

1

 ,



k 

1 

e

=



2

 



n 

In general, y is an n × 1 vector of the observations, X is an n × p matrix of the levels of the regressor variables,  is a p × 1 vector of the regression coefficients, and  is an n × 1 vector of random errors.
We wish to find the vector of least-squares estimators, b^ , that minimizes

n

 S(b ) =



2 i

=

e e

=

(y

-

Xb )(y

-

Xb )

i=1

Note that S() may be expressed as

S (b ) = yy - b Xy - yXb + b XXb
= yy - 2b Xy + b XXb

since  Xy is a 1 × 1 matrix, or a scalar, and its transpose( Xy) = yX is the same scalar. The least-squares estimators must satisfy

which simplifies to

S = -2Xy + 2XXb^ = 0 b b^
XXb^ = Xy

(3.12)

Equations (3.12) are the least-squares normal equations. They are the matrix analogue of the scalar presentation in (3.11).

ESTIMATION OF THE MODEL PARAMETERS

73

To solve the normal equations, multiply both sides of (3.12) by the inverse of XX. Thus, the least-squares estimator of  is

b^ = (XX)-1 Xy

(3.13)

provided that the inverse matrix (XX)-1 exists. The (XX)-1 matrix will always exist if the regressors are linearly independent, that is, if no column of the X matrix is a linear combination of the other columns.
It is easy to see that the matrix form of the normal equations (3.12) is identical to the scalar form (3.11). Writing out (3.12) in detail, we obtain

 n



n


 

i=1

xi1



n




i=1

xik

n
 xi1
i=1 n
 xi21
i=1
n
 xik xi1
i=1

n
 xi2
i=1 n
 xi1xi2
i=1
n
 xik xi2
i=1

  n

xik

 

 

^ 0

 

 

n



yi 

i=1

    i=1 

  n
i=1

xi1

xik

  

  

^1

  

 

=

   

n i=1



xi1

yi

 



  



n

   n



  xi2k
i=1

   ^k 

 

i=1

xik

yi

 

If the indicated matrix multiplication is performed, the scalar form of the normal equations (3.11) is obtained. In this display we see that XX is a p × p symmetric matrix and Xy is a p × 1 column vector. Note the special structure of the XX matrix. The diagonal elements of XX are the sums of squares of the elements in the columns of X, and the off-diagonal elements are the sums of cross products of the elements in the columns of X. Furthermore, note that the elements of Xy are the sums of cross products of the columns of X and the observations yi.
The fitted regression model corresponding to the levels of the regressor variables x = [1, x1, x2, . . . , xk] is

k
 y^ = xb^ = ^0 + ^ j xj j =1

The vector of fitted values y^i corresponding to the observed values yi is

y^ = Xb^ = X (XX)-1 Xy = Hy

(3.14)

The n × n matrix H = X(XX)-1X is usually called the hat matrix. It maps the vector of observed values into a vector of fitted values. The hat matrix and its properties play a central role in regression analysis.
The difference between the observed value yi and the corresponding fitted value y^i is the residual ei = yi - y^i. The n residuals may be conveniently written in matrix notation as

e = y - y^

(3.15a)

74

MULTIPLE LINEAR REGRESSION

There are several other ways to express the vector of residuals e that will prove useful, including

e = y - Xb^ = y - Hy = (I - H)y

(3.15b)

Example 3.1 The Delivery Time Data

A soft drink bottler is analyzing the vending machine service routes in his distribution system. He is interested in predicting the amount of time required by the route driver to service the vending machines in an outlet. This service activity includes stocking the machine with beverage products and minor maintenance or housekeeping. The industrial engineer responsible for the study has suggested that the two most important variables affecting the delivery time (y) are the number of cases of product stocked (x1) and the distance walked by the route driver (x2). The engineer has collected 25 observations on delivery time, which are shown in Table 3.2. (Note that this is an expansion of the data set used in Example 2.9.) We will fit the multiple linear regression model
y = 0 + 1x1 + 2x2 + 
to the delivery time data in Table 3.2.

TABLE 3.2 Delivery Time Data for Example 3.1

Observation Number

Delivery Time, y (min)

Number of Cases, x1

1

16.68

7

2

11.50

3

3

12.03

3

4

14.88

4

5

13.75

6

6

18.11

7

7

8.00

2

8

17.83

7

9

79.24

30

10

21.50

5

11

40.33

16

12

21.00

10

13

13.50

4

14

19.75

6

15

24.00

9

16

29.00

10

17

15.35

6

18

19.00

7

19

9.50

3

20

35.10

17

21

17.90

10

22

52.32

26

23

18.75

9

24

19.83

8

25

10.75

4

Distance, x2 (ft)
560 220 340 80 150 330 110 210 1460 605 688 215 255 462 448 776 200 132 36 770 140 810 450 635 150

ESTIMATION OF THE MODEL PARAMETERS

75

Graphics can be very useful in fitting multiple regression models. Figure 3.4 is a scatterplot matrix of the delivery time data. This is just a two-dimensional array of two-dimensional plots, where (except for the diagonal) each frame contains a scatter diagram. Thus, each plot is an attempt to shed light on the relationship between a pair of variables. This is often a better summary of the relationships than a numerical summary (such as displaying the correlation coefficients between each pair of variables) because it gives a sense of linearity or nonlinearity of the relationship and some awareness of how the individual data points are arranged over the region.
When there are only two regressors, sometimes a three-dimensional scatter diagram is useful in visualizing the relationship between the response and the regressors. Figure 3.5 presents this plot for the delivery time data. By spinning these plots, some software packages permit different views of the point cloud. This view provides an indication that a multiple linear regression model may provide a reasonable fit to the data.
To fit the multiple regression model we first form the X matrix and y vector:

1 7 560 

16.68

1 3

220

 

11.50

1 3 340 

12.03

1 4

80

 

14.88

1 6 150 

13.75





1 7 330 

 18.11

1 2

110

 

 

8.00

1 7 210 

17.83

1 30 1460

79.24

1 5 605 

21.50

1 16

688

 

40.33

1 10 215 

21.00

X

=

 1

4

 255 ,

y

=

 13.50

1 6

462

 

19.75

1 9 448 

24.00

1 10

776

 

29.00

1 6 200 

15.35

1 7

132

 

19.00

1 3

36

 

 

9.50

1 17 770 

35.10

1 10

140

 

17.90

1 26 810 

52.32

1 9

450

 

18.75

1 8 635 

19.83





1 4 150 

 10.75

20 40 60 80

76

MULTIPLE LINEAR REGRESSION

5 15 25

Time

5 15 25

Cases

0 400 1000

Distance

20 40 60 80

0 400 1000

Figure 3.4 Scatterplot matrix for the delivery time data from Example 3.1.

Time 79.24

55.49

31.75

8.00 30.00

20.67

11.33

Cases

1460
985
511 Distance 2.00 36

Figure 3.5 Three-dimensional scatterplot of the delivery time data from Example 3.1.

The XX matrix is

1 1

XX

=

 

7

3

560 220

and the Xy vector is

1 4

1 1 

7 3

560

220 

=

  

25 219

150 1 4 150 10, 232

219 3, 055 133, 899

10, 232 

133,

899

 

6, 725,688 

ESTIMATION OF THE MODEL PARAMETERS

77

1 1

Xy

=

 

7

3

560 220

1 4 150

16.68 11.50  10.75

=



 

7,

337,

559.60 375.44 072.00

The least-squares estimator of  is
b^ = (XX)-1 Xy

or

^^01 ^ 2

    

=

   10,

25 219 232

219 3, 055 133, 899

10, 232-1  559.60

133, 899

 

7, 375.44

6, 725,688 337,072.00

 0.11321518 -0.00444859 -0.00008367  559.60

= -0.00444859

0.00274378

-0.00004786

 

7, 375.44

-0.00008367 -0.00004786 0.00000123 337,072.00

2.34123115 = 1.61590712
0.01438483

The least-squares fit (with the regression coefficients reported to five decimals) is

y^ = 2.34123 + 1.61591x1 + 0.01438x2

Table 3.3 shows the observations yi along with the corresponding fitted values y^i

and the residuals ei from this model.



Computer Output Table 3.4 presents a portion of the Minitab output for the soft drink delivery time data in Example 3.1. While the output format differs from one computer program to another, this display contains the information typically generated. Most of the output in Table 3.4 is a straightforward extension to the multiple regression case of the computer output for simple linear regression. In the next few sections we will provide explanations of this output information.

3.2.2 A Geometrical Interpretation of Least Squares
An intuitive geometrical interpretation of least squares is sometimes helpful. We may think of the vector of observations y = [y1, y2, . . . , yn] as defining a vector from the origin to the point A in Figure 3.6. Note that y1, y2, . . . , yn form the coordinates of an n-dimensional sample space. The sample space in Figure 3.6 is three-dimensional.
The X matrix consists of p (n × 1) column vectors, for example, 1 (a column vector of 1's), x1, x2, . . . , xk. Each of these columns defines a vector from the origin in the

78

MULTIPLE LINEAR REGRESSION

TABLE 3.3 Observations, Fitted Values, and Residuals for Example 3.1

Observation Number
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

yi
16.68 11.50 12.03 14.88 13.75 18.11 8.00 17.83 79.24 21.50 40.33 21.00 13.50 19.75 24.00 29.00 15.35 19.00 9.50 35.10 17.90 52.32 18.75 19.83 10.75

y^ i
21.7081 10.3536 12.0798 9.9556 14.1944 18.3996 7.1554 16.6734 71.8203 19.1236 38.0925 21.5930 12.4730 18.6825 23.3288 29.6629 14.9136 15.5514 7.7068 40.8880 20.5142 56.0065 23.3576 24.4028 10.9626

ei = yi - yi
-5.0281 1.1464 -0.0498 4.9244 -0.4444 -0.2896 0.8446 1.1566 7.4197 2.3764 2.2375
-0.5930 1.0270 1.0675 0.6712 -0.6629 0.4364 3.4486 1.7932 -5.7880 -2.6142 -3.6865 -4.6076 -4.5728 -0.2126

TABLE 3.4 Minitab Output for Soft Drink Time Data

Regression Analysis: Time versus Cases, Distance

The regression equation is Time = 2.34 + 1.62 cases + 0.0144 Distance

Predictor Constant Cases Distance
S = 3.25947

Coef 2.341 1.6159 0.014385
R - Sq = 96.0%

SE Coef 1.097 0.1707 0.003613

T 2.13 9.46 3.98

P 0.044 0.000 0.001

R - Sq (adj) = 95.6%

Analysis of Variance

Source

DF

Regression

2

Residual Error 22

Total

24

SS 5550.8
233.7 5784.5

MS 2775.4
10.6

F 261.24

Source

DF

Cases

1

Distance

1

Seq SS 5382.4
168.4

P 0.000

ESTIMATION OF THE MODEL PARAMETERS

79

1

> >
>

A

Y -Y

Y

0

Y = X

C

X

B

3 2 Figure 3.6 A geometrical interpretation of least squares.
sample space. These p vectors form a p-dimensional subspace called the estimation space. The estimation space for p = 2 is shown in Figure 3.6. We may represent any point in this subspace by a linear combination of the vectors 1, x1, . . . , xk. Thus, any point in the estimation space is of the form X. Let the vector X determine the point B in Figure 3.6. The squared distance from B to A is just
S (b ) = (y - Xb )(y - Xb )
Therefore, minimizing the squared distance of point A defined by the observation vector y to the estimation space requires finding the point in the estimation space that is closest to A. The squared distance is a minimum when the point in the estimation space is the foot of the line from A normal (or perpendicular) to the estimation space. This is point C in Figure 3.6. This point is defined by the vector y^ = Xb^ . Therefore, since y - y^ = y - Xb^ is perpendicular to the estimation space, we may write
( ) X y - Xb^ = 0 or XXb^ = Xy
which we recognize as the least-squares normal equations.
3.2.3 Properties of the Least-Squares Estimators The statistical properties of the least-squares estimator b^ may be easily demonstrated. Consider first bias, assuming that the model is correct:
( ) E b^ = E (XX)-1 Xy = E (XX)-1 X(Xb + e )
= E (XX)-1 XXb + (XX)-1 Xe  = b
since E() = 0 and (XX)-1XX = I. Thus, b^ is an unbiased estimator of  if the model is correct.

80

MULTIPLE LINEAR REGRESSION

The variance property of b^ is expressed by the covariance matrix

( ) ( ) ( ) Cov

b^

= E b^ - E

b^

 b^ - E

b^



  

which is a p × p symmetric matrix whose jth diagonal element is the variance of ^ j and whose (ij)th off-diagonal element is the covariance between ^i and ^ j. The
covariance matrix of b^ is found by applying a variance operator to b^ :

( ) ( ) Cov b^ = Var b^ = Var (XX)-1 Xy

Now (XX)-1X is a matrix of constants, and the variance of y is 2I, so
( ) Var b^ = Var (XX)-1 Xy = (XX)-1 XVar (y) (XX)-1 X
=  2 (XX)-1 XX (XX)-1 =  2 (XX)-1

Therefore, if we let C = (XX)-1, the variance of ^ j is 2Cjj and the covariance between ^i and ^ j is 2Cij.
Appendix C.4 establishes that the least-squares estimator b^ is the best linear
unbiased estimator of  (the Gauss-Markov theorem). If we further assume that the errors i are normally distributed, then as we see in Section 3.2.6, b^ is also the maximum-likelihood estimator of . The maximum-likelihood estimator is the
minimum variance unbiased estimator of .

3.2.4 Estimation of 2

As in simple linear regression, we may develop an estimator of 2 from the residual sum of squares

n

n

  SSRes = (yi - y^i )2 = ei2 = ee

i=1

i=1

Substituting e = y - Xb^ , we have

( ) ( ) SSRes = y - Xb^  y - Xb^
= yy - b^ Xy - yXb^ + b^ XXb^ = yy - 2b^ Xy + b^ XXb^

Since XXb^ = Xy, this last equation becomes

SSRes = yy - b^ Xy

(3.16)

Appendix C.3 shows that the residual sum of squares has n - p degrees of freedom associated with it since p parameters are estimated in the regression model. The residual mean square is

ESTIMATION OF THE MODEL PARAMETERS

81

MSRes

=

SSRes n- p

(3.17)

Appendix C.3 also shows that the expected value of MSRes is 2, so an unbiased estimator of 2 is given by

^ 2 = MSRes

(3.18)

As noted in the simple linear regression case, this estimator of 2 is model dependent.

Example 3.2 The Delivery Time Data
We now estimate the error variance 2 for the multiple regression model fit to the soft drink delivery time data in Example 3.1. Since

25
 yy = yi2 = 18,310.6290 i=1

and

b^ Xy = [2.34123115 1.61590721
= 18,076.90304

 559.60
0.01438483] 7,375.44
337, 072.00 

the residual sum of squares is
SSRes = yy - b^ Xy = 18,310.6290 - 18,076.9030 = 233.7260

Therefore, the estimate of 2 is the residual mean square

^ 2 = SSRes = 233.7260 = 10.6239 n - p 25 - 3

The Minitab output in Table 3.4 reports the residual mean square as 10.6



The model-dependent nature of this estimate 2 may be easily demonstrated. Table 2.12 displays the computer output from a least-squares fit to the delivery time data using only one regressor, cases (xl). The residual mean square for this model is 17.5, which is considerably larger than the result obtained above for the tworegressor model. Which estimate is "correct"? Both estimates are in a sense correct, but they depend heavily on the choice of model. Perhaps a better question is which model is correct? Since 2 is the variance of the errors (the unexplained noise about

82

MULTIPLE LINEAR REGRESSION

the regression line), we would usually prefer a model with a small residual mean square to a model with a large one.

3.2.5 Inadequacy of Scatter Diagrams in Multiple Regression
We saw in Chapter 2 that the scatter diagram is an important tool in analyzing the relationship between y and x in simple linear regression. We also saw in Example 3.1 that a matrix of scatterplots was useful in visualizing the relationship between y and two regressors. It is tempting to conclude that this is a general concept; that is, examinjng scatter diagrams of y versus xl, y versus x2, . . . , y versus xk is always useful in assessing the relationships between y and each of the regressors xl, x2, . . . , xk. Unfortunately, this is not true in general.
Following Daniel and Wood [1980], we illustrate the inadequacy of scatter diagrams for a problem with two regressors. Consider the data shown in Figure 3.7. These data were generated from the equation
y = 8 - 5x1 + 12x2
The matrix of scatterplots is shown in Figure 3.7. The y-versus-x1, plot does not exhibit any apparent relationship between the two variables. The y-versus-x2 plot indicates that a linear relationship exists, with a slope of approximately 8. Note that both scatter diagrams convey erroneous information. Since in this data set there are two pairs of points that have the same x2 values (x2 = 2 and x2 = 4), we could measure
the x1 effect at fixed x2 from both pairs. This gives, ^1 = (17 - 27) (3 - 1) = -5 for x2 = 2 and ^1 = (26 - 16) (6 - 8) = -5 for x2 = 4 the correct results. Knowing ^1, we could

10 30 50

y

x1

x2

10

2

1

17

3

2

48

4

5

27

1

2

55

5

6

26

6

4

9

7

3

16

8

4

2468 y
x1

2468

123456

x2

10 30 50

123456

Figure 3.7 A matrix of scatterplots.

ESTIMATION OF THE MODEL PARAMETERS

83

now estimate the x2 effect. This procedure is not generally useful, however, because many data sets do not have duplicate points.
This example illustrates that constructing scatter diagrams of y versus xj (j = 1, 2, . . . , k) can be misleading, even in the case of only two regressors operating in a perfectly additive fashion with no noise. A more realistic regression situation with several regressors and error in the y's would confuse the situation even further. If there is only one (or a few) dominant regressor, or if the regressors operate nearly independently, the matrix of scatterplots is most useful. However, when several important regressors are themselves interrelated, then these scatter diagrams can be very misleading. Analytical methods for sorting out the relationships between several regressors and a response are discussed in Chapter 10.

3.2.6 Maximum-Likelihood Estimation
Just as in the simple linear regression case, we can show that the maximum-likelihood estimators for the model parameters in multiple linear regression when the model errors are normally and independently distributed are also least-squares estimators. The model is

y = Xb + e

and the errors are normally and independently distributed with constant variance 2, or  is distributed as N(0, 2I). The normal density function for the errors is

f (i ) = 

1 2

exp



-

1 2

2



2 i



The likelihood function is the joint density of 1, 2, . . . , n or in=1 f (i ). Therefore,
the likelihood function is

 L(e, b,  2 ) =

n i=1

f

(i

)

=

(2

1
)n

2



n

exp 

-

1 2

2

e e



Now since we can write  = y - X, the likelihood function becomes

L(y,

X,

b,



2

)

=

(2

1
)n

2



n

exp 

-

1 2

2

(y

-

Xb

)

(y

-

Xb

)

(3.19)

As in the simple linear regression case, it is convenient to work with the log of the likelihood,

ln

L

(y,

X,

b,



2

)

=

-

n 2

ln

(2

)

-

n

ln

(

)

-

1 2

2

(y

-

Xb

)



(y

-

Xb

)

It is clear that for a fixed value of  the log-likelihood is maximized when the term

(y - Xb )(y - Xb )

84

MULTIPLE LINEAR REGRESSION

is minimized. Therefore, the maximum-likelihood estimator of  under normal
errors is equivalent to the least-squares estimator b^ = (XX)-1 Xy. The maximum-
likelihood estimator of 2 is

(y - Xb^ ) (y - Xb^ )
2 = n

(3.20)

These are multiple linear regression generalizations of the results given for simple linear regression in Section 2.11. The statistical properties of the maximum-likelihood estimators are summarized in Section 2.11.

3.3 HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION
Once we have estimated the parameters in the model, we face two immediate questions:
1. What is the overall adequacy of the model? 2. Which specific regressors seem important?
Several hypothesis testing procedures prove useful for addressing these questions. The formal tests require that our random errors be independent and follow a normal distribution with mean E(i) = 0 and variance Var(i) = 2.

3.3.1 Test for Significance of Regression
The test for significance of regression is a test to determine if there is a linear relationship between the response y and any of the regressor variables x1, x2, . . . , xk. This procedure is often thought of as an overall or global test of model adequacy. The appropriate hypotheses are

H0: 1 = 1 = = k = 0 H1: j  0 for at least one j

Rejection of this null hypothesis implies that at least one of the regressors x1, x2, . . . , xk contributes significantly to the model.
The test procedure is a generalization of the analysis of variance used in simple linear regression. The total sum of squares SST is partitioned into a sum of squares due to regression, SSR, and a residual sum of squares, SSRes. Thus,
SST = SSR + SSRes

Appendix C.3 shows that if the null hypothesis is true, then SSR/2 follows a k2

distribution, which has the same number of degrees of freedom as number of regres-

sor

variables

in

the

model.

Appendix

C.3

also

shows

that

SSRes

2

~

2 n-k-1

and

that SSRes and SSR are independent. By the definition of an F statistic given in

Appendix C.1,

HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION

85

F0

=

SSR k
SSRes (n - k

- 1)

=

MSR MSRes

follows the Fk, n-k-1 distribution. Appendix C.3 shows that

E (MSRes ) =  2

E

(MSR

)

=



2

+

b * Xc X c b * k 2

where * = (1, 2, . . . , k) and Xc is the "centered" model matrix given by

 x11 - x1

 

x21

-

x1



Xc

=

 

xi1

-

x1



 

xn1

-

x1

x12 - x2 x22 - x2
xi2 - x2
xn2 - x2

x1k - xk 

x2k

-

xk

 



xik

-

xk

 



xnk

-

xk

 

These expected mean squares indicate that if the observed value of F0 is large, then it is likely that at least one j  0. Appendix C.3 also shows that if at least one j  0, then F0 follows a noncentral F distribution with k and n - k - 1 degrees of freedom and a noncentrality parameter of

 = b*XcXcb* 2

This noncentrality parameter also indicates that the observed value of F0 should be large if at least one j  0. Therefore, to test the hypothesis H0: 1 = 2 = . . . = k = 0, compute the test statistic F0 and reject H0 if

F0 > F ,k,n-k-1

The test procedure is usually summarized in an analysis-or-variance table such as Table 3.5.
A computational formula for SSR is found by starting with

SSRes = yy - b^ Xy

(3.21)

TABLE 3.5 Analysis of Variance for Significance of Regression in Multiple Regression

Source of Variation Sum of Squares Degrees of Freedom Mean Square

F0

Regression Residual Total

SSR SSRes SST

k n-k-1 n-1

MSR MSRes

MSR/MSRes

86

MULTIPLE LINEAR REGRESSION

and since

 n 2

 n 2

   SST

=

n i=1

yi2 - 

i=1 yi  n

= yy -  i=1 yi  n

we may rewrite the above equation as

   n  2 

 n 2 

SSRes

=

yy -



i=1 yi  n

-

 b^ Xy 

-



i=1 yi  n

   

or





(3.22)

SSRes = SST - SSR

Therefore, the regression sum of squares is

 n  2

SSR

=

b^ Xy

-



i=1 yi  n

the residual sum of squares is

SSRes = yy - b^ Xy

and the total sum of squares is

 n  2

SST

=

yy

-



i=1 yi  n

(3.23) (3.24) (3.25) (3.26)

Example 3.3 The Delivery Time Data

We now test for significance of regression using the delivery time data from Example 3.1. Some of the numerical quantities required are calculated in Example 3.2. Note that

 n  2

SST

=

yy

-



i=1 yi  n

= 18,310.6290 - (559.60)2 = 5784.5426
25

HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION

87

 n  2

SSR

=

b^ Xy

-



i=1 yi  n

= 18,076.9030 - (559.60)2 = 5550.8166
25

and

SSRes = SST - SSR = yy - b^ Xy = 233.7260

The analysis of variance is shown in Table 3.6. To test H0: 1 = 2 = 0, we calculate the statistic

F0

=

MSR MSRes

=

2775.4083 10.6239

=

261.24

Since the P value is very small, we conclude that delivery time is related to delivery

volume and/or distance. However, this does not necessarily imply that the relation-

ship found is an appropriate one for predicting delivery time as a function of volume

and distance. Further tests of model adequacy are required.



Minitab Output The MlNITAB output in Table 3.4 also presents the analysis of variance for testing significance of regression. Apart from rounding, the results are in agreement with those reported in Table 3.6.

R2 and Adjusted R2 Two other ways to assess the overall adequacy of the model are R2 and the adjusted R2, denoted RA2 dj. The MlNITAB output in Table 3.4 reports the R2 for the multiple regression model for the delivery time data as R2 = 0.96, or
96.0%. In Example 2.9, where only the single regressor x1 (cases) was used, the value of R2 was smaller, namely R2 = 0.93, or 93.0% (see Table 2.12). In general, R2 never
decreases when a regressor is added to the model, regardless of the value of the
contribution of that variable. Therefore, it is difficult to judge whether an increase in R2 is really telling us anything important.
Some regression model builders prefer to use an adjusted R2 statistic, defined as

RA2 dj

=

1

-

SSRes SST

(n - p) (n - 1)

(3.27)

TABLE 3.6 Test for Significance of Regression for Example 3.3

Source Variation

Sum of Squares

Degrees of Freedom

Mean Square

Regression Residual Total

5550.8166 233.7260 5784.5426

2

2775.4083

22

10.6239

24

F0 261.24

P Value 4.7 × 10-16

88

MULTIPLE LINEAR REGRESSION

Since SSRes/(n - p) is the residual mean square and SST/(n - 1) is constant regardless of how many variables are in the model, RA2 dj will only increase on adding a variable to the model if the addition of the variable reduces the residual mean square. Minitab (Table 3.4) reports RA2 dj = 0.956 (95.6%) for the two-variable model, while for the simple linear regression model with only x1 (cases), RA2 dj = 0.927, or 92.7% (see Table 2.12). Therefore, we would conclude that adding x2 (distance) to the model did result in a meaningful reduction of total variability.
In subsequent chapters, when we discuss model building and variable selection,
it is frequently helpful to have a procedure that can guard against overfitting the model, that is, adding terms that are unnecessary. The adjusted R2 penalizes us for
adding terms that are not helpful, so it is very useful in evaluating and comparing
candidate regression models.

3.3.2 Tests on Individual Regression Coefficients and Subsets of Coefficients

Once we have determined that at least one of the regressors is important, a logical question becomes which one(s). Adding a variable to a regression model always causes the sum of squares for regression to increase and the residual sum of squares to decrease. We must decide whether the increase in the regression sum of squares is sufficient to warrant using the additional regressor in the model. The addition of a regressor also increases the variance of the fitted value y^ , so we must be careful to include only regressors that are of real value in explaining the response. Furthermore, adding an unimportant regressor may increase the residual mean square, which may decrease the usefulness of the model.
The hypotheses for testing the significance of any individual regression coefficient, such as j, are

H0: j = 0, H1: j  0

(3.28)

If H0: j = 0 is not rejected, then this indicates that the regressor xj can be deleted from the model. The test statistic for this hypothesis is

( ) t0 =

^ j ^ 2Cjj

=

^ j se ^ j

(3.29)

where Cjj is the diagonal element of (XX)-1 corresponding to ^ j. The null hypothesis H0: j = 0 is rejected if |t0| > t/2,n-k-1. Note that this is really a partial or marginal test because the regression coefficient ^ j depends on all of the other regressor variables xi(i  j) that are in the model. Thus, this is a test of the contribution of xj given the
other regressors in the model.

Example 3.4 The Delivery Time Data

To illustrate the procedure, consider the delivery time data in Example 3.1. Suppose we wish to assess the value of the regressor variable x2 (distance) given that the regressor x1 (cases) is in the model. The hypotheses are
H0: 2 = 0, H1: 2  0

HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION

89

The main diagonal element of (XX)-1 corresponding to 2 is C22 = 0.00000123, so the t statistic (3.29) becomes

t0 =

^2 = ^ 2C22

0.01438
(10.6239)(0.00000123)

=

3.98

Since t0.025,22 = 2.074, we reject H0: 2 = 0 and conclude that the regressor x2 (distance)

contributes significantly to the model given that x1 (cases) is also in the model. This

t test is also provided in the Minitab output (Table 3.4), and the P value reported

is 0.001.



We may also directly determine the contribution to the regression sum of squares of a regressor, for example, xj, given that other regressors x1(i  j) are included in the model by using the extra-sum-of-squares method. This procedure can also be used to investigate the contribution of a subset of the regressor variables to the model.
Consider the regression model with k regressors

y = Xb + e

where y is n × 1, X is n × p,  is p × 1,  is n × 1, and p = k + 1. We would like to determine if some subset of r < k regressors contributes significantly to the regression model. Let the vector of regression coefficients be partitioned as follows:

b

=

 b1  b2

 

where 1 is (p - r) × 1 and 2 is r × 1. We wish to test the hypotheses H0: b2 = 0, H1: b2  0

(3.30)

The model may be written as

y = Xb + e = X1b1 + X2b2 + e

(3.31)

where the n × (p - r) matrix Xl represents the columns of X associated with 1 and the n × r matrix X2 represents the columns of X associated with 2. This is called the full model.
For the full model, we know that b^ = (XX)-1 Xy. The regression sum of squares
for this model is

SSR (b ) = b^ Xy ( p degrees of freedom)

and

MSRes

=

yy - b^ Xy n- p

90

MULTIPLE LINEAR REGRESSION

To find the contribution of the terms in 2 to the regression, fit the model assuming that the null hypothesis H0: 2 = 0 is true. This reduced model is

y = X1b1 + e

(3.32)

The least-squares estimator of 1 in the reduced model is b^1 = (X1X1 )-1 X1y . The
regression sum of squares is

SSR (b1 ) = b^1X1y ( p - r degrees of freedom)

(3.33)

The regression sum of squares due to 2 given that 1 is already in the model is

SSR (b2 b1 ) = SSR (b ) - SSR (b1 )

(3.34)

with p - (p - r) = r degrees of freedom. This sum of squares is called the extra sum of squares due to 2 because it measures the increase in the regression sum of squares that results from adding the regressors xk-r+1, xk-r+2, . . . , xk to a model that already contains x1, x2, . . . , xk-r. Now SSR(2|1) is independent of MSRes, and the null hypothesis 2 = 0 may be tested by the statistic

F0

=

SSR (b2 b1 )
MSRes

r

(3.35)

If 2  0, then F0 follows a noncentral F distribution with a noncentrality parameter of



=

1 2

b2X2

I

-

X1(X1X1 )-1

X1  X2b2

This result is quite important. If there is multicollinearity in the data, there are situations where 2 is markedly nonzero, but this test actually has almost no power (ability to indicate this difference) because of a near-collinear relationship between X1 and X2. In this situation,  is nearly zero even though 2 is truly important. This relationship also points out that the maximal power for this test occurs when X1 and X2, are orthogonal to one another. By orthogonal we mean that X2X1 = 0.
If F0 > F,r,n-p, we reject H0, concluding that at least one of the parameters in 2 is not zero, and consequently at least one of the regressors xk-r+1, xk-r+2, . . . , xk in X2 contribute significantly to the regression model. Some authors call the test in (3.35) a partial F test because it measures the contribution of the regressors in X2 given that the other regressors in X1 are in the model. To illustrate the usefulness of this procedure, consider the model
y = 0 + 1x1 + 2 x2 + 3x3 + 
The sums of squares
SSR (1 0, 2, 3 ), SSR (2 0, 1, 3 ), SSR (3 0, 1, 2 )

HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION

91

are single-degree-of-freedom sums of squares that measure the contribution of each regressor xj, j = 1, 2, 3, to the model given that all of the other regressors were already in the model. That is, we are assessing the value of adding xj to a model that did not include this regressor. In general, we could find
SSR (j 0, 1, ... , j-1, j+1, ... , k ), 1  j  k
which is the increase in the regression sum of squares due to adding xj to a model that already contains x1, . . . , xj-1xj+1, . . . , xk. Some find it helpful to think of this as measuring the contribution of xj as if it were the last variable added to the model.
Appendix C3.35 formally shows the equivalence of the partial F test on a single variable xj and the t test in (3.29). However, the partial F test is a more general procedure in that we can measure the effect of sets of variables. In Chaper 10 we will show how the partial F test plays a major role in model building, that is, in searching for the best set of regressors to use in the model.
The extra-sum-of-squares method can be used to test hypotheses about any subset of regressor variables that seems reasonable for the particular problem under analysis. Sometimes we find that there is a natural hierarchy or ordering in the regressors, and this forms the basis of a test. For example, consider the quadratic polynomial

y = 0 + 1x1 + 2 x2 + 12 x1x2 + 11x12 + 22 x22 + 
Here we might be interested in finding
SSR (1, 2 0 )
which would measure the contribution of the first-order terms to the model, and
SSR (12, 11, 22 0, 1, 2 )
which would measure the contribution of adding second-order terms to a model that already contained first-order terms.
When we think of adding regressors one at a time to a model and examining the contribution of the regressor added at each step given all regressors added previously, we can partition the regression sum of squares into marginal single-degreeof-freedom components. For example, consider the model

y = 0 + 1x1 + 2 x2 + 3x3 +  with the corresponding analysis-of-variance identity
SST = SSR (1, 2, 3 0 ) + SSRes
We may decompose the three-degree-of-freedom regression sum of squares as follows:
SSR (1, 2, 3 0 ) = SSR (1 0 ) + SSR (2 1, 1 ) + SSR (3 1, 2, 0 )

92

MULTIPLE LINEAR REGRESSION

where each sum of squares on the right-hand side has one degree of freedom. Note that the order of the regressors in these marginal components is arbitrary. An alternate partitioning of SSR(1, 2, 3|0) is
SSR (1, 2, 3 0 ) = SSR (2 0 ) + SSR (1 2, 0 ) + SSR (3 1, 2, 0 )
However, the extra-sum-of-squares method does not always produce a partitioning of the regression sum of squares, since, in general,
SSR (1, 2, 3 0 )  SSR (1 2, 3, 0 ) + SSR (2 1, 3, 0 ) + SSR (3 1, 2, 0 )
Minitab Output The Minitab output in Table 3.4 provides a sequential partitioning of the regression sum of squares for x1 = cases and x2 = distance. The reported quantities are
SSR (1, 2 0 ) = SSR (1 0 ) + SSR (1, 2 0 )
5550.8 = 5382.4 + 168.4

Example 3.5 The Delivery Time Data

Consider the soft drink delivery time data in Example 3.1. Suppose that we wish to investigate the contribution of the variable distance (x2) to the model. The appropriate hypotheses are
H0: 2 = 0, H1: 2  0
To test these hypotheses, we need the extra sum of squares due to 2, or
SSR (2 1, 0 ) = SSR (1, 2, 0 ) - SSR (1, 0 ) = SSR (1, 2 0 ) - SSR (1 0 )

From Example 3.3 we know that

 n  2

SSR (1,

2

0 )

=

b^ Xy

-



i=1 yi  n

= 5550.8166 (2 degrees of freedom)

The reduced model y = 0 + 1x1 +  was fit in Example 2.9, resulting in y^ = 3.3208 + 2.1762x1 . The regression sum of squares for this model is
SSR (1 0 ) = ^1Sxy = (2.1762)(2473.3440) = 5382.4077 (1 degree of freedom)

Therefore, we have
SSR (2 1, 0 ) = 5550.8166 - 5382.4088 = 168.4078 (1 degree of freedom)

HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION

93

This is the increase in the regression sum of squares that results from adding x2 to a model already containing x1. To test H0: 2 = 0, form the test statistic

F0

=

SSR (2 1, 0 )
MSRes

1

=

168.4078 1 10.6239

=

15.85

Note that the MSRes from the full model using both x1 and x2 is used in the denominator of the test statistic. Since F0.05,1,22 = 4.30, we reject H0: 2 = 0 and conclude that

distance (x2) contributes significantly to the model.

Since this partial F test involves a single variable, it is equivalent to the t test. To

see this, recall that the t test on H0: 2 = 0 resulted in the test statistic t0 = 3.98. From

Section C.1, the square of a t random variable with v degrees of freedom is an F

random variable with one numerator and v denominator degrees of freedom, and

we have t02 = (3.98)2 = 15.84 F0.



3.3.3 Special Case of Orthogonal Columns in X Consider the model (3.31)

y = Xb + e = X1b1 + X2b2 + e

The extra-sum-of-squares method allows us to measure the effect of the regressors

in X2 conditional on those in X1 by computing SSR(2|1). In general, we cannot talk about finding the sum of squares due to 2, SSR(2), without accounting for the

dependence of this quantity on the regressors in X1. However, if the columns in X1 are orthogonal to the columns in X2, we can determine a sum of squares due to 2

that is free of any dependence on To demonstrate this, form the

the regressors in X1.
normal equations (X

X

)

b^

= Xy

for

the

model

(3.31). The normal equations are

 X1 X1 X2X1

X1 X 2 X2X2

  

  

b^1 b^ 2

  

=

X1y 

 

X2y

 

Now if the columns of X1 are orthogonal to the columns in X2, X1X2 = 0 and X2X1 = 0. Then the normal equations become
X1X1b^1 = X1y, X2X2b^2 = X2y

with solution

b^1 = (X1X1 )-1 X1y, b^2 = (X2X2 )-1 X2y

Note that the in the model,

alenadstt-hsqeulaeraesst-esqstuimaraetsoersotifma1tiosrb^o1freg2airsdbl^e2ssreogfawrdhleesths eorf

or not X2 is whether or

not X1 is in the model.

94

MULTIPLE LINEAR REGRESSION

The regression sum of squares for the full model is

SSR (b ) = b^ Xy

=

b^1,

b^ 2



X1y  X2y

= b^ X1y + b2X2y

= yX1(X1X1 )-1 X1y + yX2 (X2X2 )-1 X2y

(3.36)

However, the normal equations form two sets, and for each set we note that

SSR (b1 ) = b^1X1y = yX1(X1X1 )-1 X1y SSR (b2 ) = b^2 X2y = yX2 (X2X2 )-1 X2y

(3.37)

Comparing Eq. (3.37) with Eq. (3.36), we see that

SSR (b ) = SSR (b1 ) + SSR (b2 )

(3.38)

Therefore, and

SSR (b1 b2 ) = SSR (b ) - SSR (b2 )  SSR (b1 )

SSR (b2 b1 ) = SSR (b ) - SSR (b1 )  SSR (b2 )
Consequently, SSR(1) measures the contribution of the regressors in X1 to the model unconditionally, and SSR(2) measures the contribution of the regressors in X2 to the model unconditionally. Because we can unambiguously determine the effect of each regressor when the regressors are orthogonal, data collection experiments are often designed to have orthogonal variables.
As an example of a regression model with orthogonal regressors, consider the model y = 0 + 1x1 + 2x2 + 3x3 + , where the X matrix is

0 1 2 3

1 -1 -1 -1

1 1 -1 -1

1 -1 1 -1

X

=

1 1

-1 1

-1 1

1 -1

 1

1 -1

 1

1 -1 1 1

1 1 1 1

HYPOTHESIS TESTING IN MULTIPLE LINEAR REGRESSION

95

The levels of the regressors correspond to the 23 factorial design. It is easy to see that the columns of X are orthogonal. Thus, SSR(j), j = 1, 2, 3, measures the contribution of the regressor xj to the model regardless of whether any of the other regressors are included in the fit.

3.3.4 Testing the General Linear Hypothesis
Many hypotheses about regression coefficients can be tested using a unified approach. The extra-sum-of-squares method is a special case of this procedure. In the more general procedure the sum of squares used to test the hypothesis is usually calculated as the difference between two residual sums of squares. We will now outline the procedure. For proofs and further discussion, refer to Graybill [1976], Searle [1971], or Seber [1977].
Suppose that the null hypothesis of interest can be expressed as H0: T = 0, where T is an m × p matrix of constants, such that only r of the m equations in T = 0 are
independent. The full model is y = X + , with b^ = (XX)-1 Xy, and the residual
sum of squares for the full model is
SSRes (FM) = yy - b^ Xy (n - p degrees of freedom)
To obtain the reduced model, the r independent equations in T = 0 are used to solve for r of the regression coefficients in the full model in terms of the remaining p - r regression coefficients.This leads to the reduced model y = Z + , for example, where Z is an n × (p - r) matrix and  is a (p - r) × 1 vector of unknown regression coefficients. The estimate of  is
g^ = (ZZ)-1 Zy

and the residual sum of squares for the reduced model is

SSRes (RM) = yy - g^ Zy (n - p + r degrees of freedom)
The reduced model contains fewer parameters than the full model, so consequently SSRes(RM)  SSRes(FM). To test the hypothesis H0: T = 0, we use the difference in residual sums of squares

SSH = SSRes (RM ) - SSRes (FM )

(3.39)

with n - p + r - (n - p) = r degrees of freedom. Here SSH is called the sum of squares due to the hypothesis H0: T = 0. The test statistic for this hypothesis is

F0

=

SSH r
SSRes (FM ) (n

-

p)

(3.40)

We reject H0: T = 0 if F0 > F,r,n-p.

96

MULTIPLE LINEAR REGRESSION

Example 3.6 Testing Equality of Regression Coefficients

The general linear hypothesis approach can be used to test the equality of regression coefficients. Consider the model

y = 0 + 1x1 + 2 x2 + 3x3 + 

For the full model, SSRes(RM) has n - p = n - 4 degrees of freedom. We wish to test H0: 1 = 3. This hypothesis may be stated as H0: T = 0, where
T = [0, 1, 0, -1]

is a 1 × 4 row vector. There is only one equation in T = 0, namely, 1 - 3 = 0. Substituting this equation into the full model gives the reduced model

y = 0 + 1x1 + 2 x2 + 1x3 + 
= 0 + 1( x1 + x3 ) + 2x2 + 
=  0 +  1z1 +  2z2 + 
where 0 = 0, 1 = 1(= 3), z1 = x1 + x3, 2 = 2, and z2 = x2. We would find SSRes(RM) with n - 4 + 1 = n - 3 degrees of freedom by fitting the reduced model. The sum of squares due to hypothesis SSH = SSRes(RM) - SSRes(FM) has n - 3 - (n - 4) = 1 degree of freedom. The F ratio (3.40) is F0 = (SSH/1)[SSRes(RM)/(n - 4)]. Note that this hypothesis could also be tested by using the t statistic

( ) t0

=

^1 - ^3 se ^1 - ^3

=

^1 - ^3
^ 2 (C11 + C33 - 2C13 )

with n - 4 degrees of freedom. This is equivalent to the F test.



Example 3.7

Suppose that the model is

y = 0 + 1x1 + 2 x2 + 3x3 + 
and we wish to test H0: 1 = 3, 2 = 0. To state this in the form of the general linear hypothesis, let

T

=

0 0

1 0

0 1

-1 0 

There are now two equations in T = 0, 1 - 3 = 0 and 2 = 0. These equations give the reduced model

y = 0 + 1x1 + 1x3 + 
= 0 + 1( x1 + x3 ) + 
=  0 +  1z1 + 

CONFIDENCE INTERVALS IN MULTIPLE REGRESSION

97

In this example,SSRes(RM) has n - 2 degrees of freedom,so SSR has n - 2 - (n - 4) = 2

degrees of freedom. The F ratio (3.40) is F0 = (SSH/2)/[SSRes(FM)/(n - 4)].



The test statistic (3.40) for the general linear hypothesis may be written in another form, namely,

F0

=

b^ T T (XX)-1 T-1 Tb^ SSRes (FM) (n - p)

r

(3.41)

This form of the statistic could have been used to develop the test procedures illustrated in Examples 3.6 and 3.7.
There is a slight extension of the general linear hypothesis that is occasionally useful. This is

H0: Tb = c, H1: Tb  c

for which the test statistic is

( ) ( ) F0 =

Tb^ - c  T (XX)-1 T-1 Tb^ - c SSRes (FM) (n - p)

r

(3.42) (3.43)

Since under the null hypothesis T = c, the distribution of F0 in Eq. (3.43) is Fr,n-p, we would reject H0: T = c if F0 > F,r,n-p. That is, the test procedure is an upper onetailed F test. Notice that the numerator of Eq. (3.43) expresses a measure of squared distance between T and c standardized by the covariance matrix of Tb^ .
To illustrate how this extended procedure can be used, consider the situation described in Example 3.6, and suppose that we wish to test

H0: 1 - 3 = 2
Clearly T = [0, 1, 0, -1] and c = [2]. For other uses of this procedure, refer to Problems 3.21 and 3.22.
Finally, if the hypothesis H0: T = 0 (or H0: T = c) cannot be rejected, then it may be reasonable to estimate  subject to the constraint imposed by the null hypothesis. It is unlikely that the usual least-squares estimator will automatically satisfy the constraint. In such cases a constrained least-squares estimator may be useful. Refer to Problem 3.34.

3.4 CONFIDENCE INTERVALS IN MULTIPLE REGRESSION
Confidence intervals on individual regression coefficients and confidence intervals on the mean response given specific levels of the regressors play the same important role in multiple regression that they do in simple linear regression. This section develops the one-at-a-time confidence intervals for these cases. We also briefly introduce simultaneous confidence intervals on the regression coefficients.

98

MULTIPLE LINEAR REGRESSION

3.4.1 Confidence Intervals on the Regression Coefficients

To construct confidence interval estimates for the regression coefficients j, we

will continue to assume that the errors i are normally and independently distributed

with mean zero and variance 2. Therefore, the observations yi are normally

and independently distributed with least-squares estimator b^ is a linear

mean 0 +  combination

k j =1
of

j xij the

and variance 2. Since the observations, it follows that

b^ is normally distributed with mean vector  and covariance matrix 2(XX)-1. This

implies that the marginal distribution of any regression coefficient ^ j is normal with mean j and variance 2Cjj, where Cjj is the jth diagonal element of the (XX)-1

matrix. Consequently, each of the statistics

^ j - j , j = 0, 1,... , k ^ 2Cjj

(3.44)

is distributed as t with n - p degrees of freedom, where ^ 2 is the estimate of the error variance obtained from Eq. (3.18).
Based on the result given in Eq. (3.44), we may defme a 100(l - ) percent confidence interval for the regression coefficient j, j = 0, 1, . . . , k, as

^ j - t 2,n- p ^ 2Cjj  j  ^ j + t 2,n- p ^ 2Cjj

(3.45)

Remember that we call the quantity
( ) se ^ j = ^ 2Cjj
the standard error of the regression coefficient ^ j.
Example 3.8 The Delivery Time Data

(3.46)

We now find a 95% CI for the parameter 1 in Example 3.1. The point estimate of 1 is ^1 = 1.61591, the diagonal element of (XX)-1 corresponding to 1 is C11 = 0.00274378, and ^ 2 = 10.6239 (from Example 3.2). Using Eq. (3.45), we find that
^1 - t0.025,22 ^ 2C11  1  ^1 + t0.025,22 ^ 2C11
1.61591 - (2.074) (10.6239)(0.00274378)  1  1.61591 + (2.074) (10.6239)(0.00274378)
1.61591 - (2.074)(0.17073)  1  1.61591 + (2.074)(0.17073)

and the 95% CI on 1 is

1.26181  1  1.97001

Notice that the Minitab output in Table 3.4 gives the standard error of each regres-

sion coefficient. This makes the construction of these intervals very easy

in practice.



CONFIDENCE INTERVALS IN MULTIPLE REGRESSION

99

3.4.2 CI Estimation of the Mean Response
We may construct a CI on the mean response at a particular point, such as x01, x02, . . . , x0k. Define the vector x0 as

1

 

x01

 

x0 =  x02  



 x0k 

The fitted value at this point is

y^0 = x0b^

(3.47)

This is an unbiased estimator of E(y|x0), since E (y^0 ) = x0b = E (y x0 ), and the vari-
ance of y^0 is

Var ( y^0 ) =  2x0 (XX)-1 x0

(3.48)

Therefore, a 100(l - ) percent confidence interval on the mean response at the point x01, x02, . . . , x0k is

y^0 - t 2,n-p ^ 2x0 (XX)-1 x0  E ( y x0 )  y^0 + t 2,n-p ^ 2x0 (XX)-1 x0

(3.49)

This is the multiple regression generalization of Eq. (2.43).

Example 3.9 The Delivery Time Data

The soft drink bottler in Example 3.1 would like to construct a 95% CI on the mean delivery time for an outlet requiring x1 = 8 cases and where the distance x2 = 275 feet. Therefore,

 1

x0

=

 

8

275

The fitted value at this point is found from Eq. (3.47) as

2.34123
y^0 = x0b^ = [1 8 275]1.61591 = 19.22 minutes
0.01438 The variance of y^0 is estimated by

100 MULTIPLE LINEAR REGRESSION

^ 2x0 (XX)-1 x0 = 10.6239[1 8 275]
 0.11321518 -0.00444859 × -0.00444859 0.00274378
-0.00008367 -0.00004786
= 10.6239(0.05346) = 0.56794

-0.00008367  1

-0.00004786

 

8

0.00000123 275

Therefore, a 95% CI on the mean delivery time at this point is found from Eq. (3.49) as

19.22 - 2.074 0.56794  E (y x0 )  19.22 + 2.074 0.56794
which reduces to

17.66  E (y x0 )  20.78

Ninety-five percent of such intervals will contain the true delivery time.



The length of the CI or the mean response is a useful measure of the quality of the regression model. It can also be used to compare competing models. To illustrate, consider the 95% CI on the the mean delivery time when x1 = 8 cases and x2 = 275 feet. In Example 3.9 this CI is found to be (17.66, 20.78), and the length of this interval is 20.78 - 17.16 = 3.12 minutes. If we consider the simple linear regression model with x1 = cases as the only regressor, the 95% CI on the mean delivery time with x1 = 8 cases is (18.99, 22.97). The length of this interval is 22.47 - 18.99 = 3.45 minutes. Clearly, adding cases to the model has improved the precision of estimation. However, the change in the length of the interval depends on the location of the point in the x space. Consider the point x1 = 16 cases and x2 = 688 feet. The 95% CI for the multiple regression model is (36.11, 40.08) with length 3.97 minutes, and for the simple linear regression model the 95% CI at x1 = 16 cases is (35.60, 40.68) with length 5.08 minutes. The improvement from the multiple regression model is even better at this point. Generally, the further the point is from the centroid of the x space, the greater the difference will be in the lengths of the two CIs.

3.4.3 Simultaneous Confidence Intervals on Regression Coefficients
We have discussed procedures for constructing several types of confidence and prediction intervals for the linear regression model. We have noted that these are one-at-a-time intervals, that is, they are the usual type of confidence or prediction interval where the confidence coefficient 1 -  indicates the proportion of correct statements that results when repeated random samples are selected and the appropriate interval estimate is constructed for each sample. Some problems require that several confidence or prediction intervals be constructed using the same sample data. In these cases, the analyst is usually interested in specifying a confidence coefficient that applies simultaneously to the entire set of interval estimates. A set of confidence or prediction intervals that are all true simultaneously with probability 1 -  are called simultaneous or joint confidence or joint prediction intervals.

CONFIDENCE INTERVALS IN MULTIPLE REGRESSION

101

As an example, consider a simple linear regression model. Suppose that the analyst wants to draw inferences about the intercept 0 and the slope 1. One possibility would be to construct 95% (say) CIs about both parameters. However, if these interval estimates are independent, the probability that both statements are correct is (0.95)2 = 0.9025. Thus, we do not have a confidence level of 95% associated with both statements. Furthermore, since the intervals are constructed using the same set of sample data, they are not independent. This introduces a further complication into determining the confidence level for the set of statements.
It is relatively easy to define a joint confidence region for the multiple regression model parameters . We may show that

( ) ( ) b^ - b  XX b^ - b

pMSRes

~ Fp,n- p

and this implies that

( ) ( ) 

P

 



b^ - b  XX b^ - b pMSRes





F ,n-

p

 

=

1

-









Consequently, a 100(1 - ) percent joint confidence region for all of the parameters in  is

( ) ( ) b^ - b  XX b^ - b

pMSRes

 F ,p,n- p

(3.50)

This inequality describes an elliptically shaped region. Construction of this joint confidence region is relatively straightforward for simple linear regression (p = 2). It is more difficult for p = 3 and would require special three-dimensional graphics software.

Example 3.10 The Rocket Propellant Data

For the case of simple linear regression, we can show that Eq. (3.50) reduces to

n

n

( )  ( )( )  ( ) n

^0 - 0

2
+2

xi ^0 - 0 ^1 - 1 +

xi2 ^1 - 1 2

i=1

i=1

2MSRes

 F ,2,n-2

To illustrate the construction of this confidence region, consider the rocket propel-

lant data in Example 2.1. We will find a 95% confidence region for 0 and 1.

^ 0

= 2627.82,

^1

= -37.15,



20 i=1

xi2

= 4677.69,

MSRes

=

9244.59,

and

F0.05,2,18

=

3.55,

we

may substitute into the above equation, yielding

20(2627.82 - 0 )2 + 2(267.25)(2627.82 - 0 )(-37.15 - 1 ) + (4677.69)(-37.15 - 1 )2  [2(9244.59)] = 3.55

as the boundary of the ellipse.

102 MULTIPLE LINEAR REGRESSION

0 2500 2600 0 2700

2800

-25

> >

-30

1 -35 1
-40

-45 Figure 3.8 Joint 95% confidence region for 0 and 1 for the rocket propellant data.

The joint confidence region is shown in Figure 3.8. Note that this ellipse is not

parallel to the 1 axis. The tilt of the ellipse is a function of the covariance between ^0 and ^1, which is -x 2 Sxx. A positive covariance implies that errors in the point estimates of 0 and 1 are likely to be in the same direction, while a negative covari-
ance indicates that these errors are likely to be in opposite directions. In our example
( ) x is positive so Cov ^0, ^1 is negative. Thus, if the estimate of the slope is too steep

(1 is overestimated), the estimate of the intercept is likely to be too small (0 is underestimated). The elongation of the region depends on the relative sizes of the

variances of 0 and 1. Generally, if the ellipse is elongated in the 0 direction (for

example), this implies that 0 is not estimated as precisely as 1. This is the case in

our example.



There is another general approach for obtaining simultaneous interval estimates of the parameters in a linear regression model. These CIs may be constructed by using

( ) ^ j ±  se ^ j , j = 0, 1,... , k

(3.51)

where the constant  is chosen so that a specified probability that all intervals are correct is obtained.
Several methods may be used to choose  in (3.51). One procedure is the Bonferroni method. In this approach, we set  = t/2p,n-p so that (3.51) becomes

( ) ^ j ± t 2 p,n-p se ^ j , j = 0, 1, ... , k

(3.52)

The probability is at least 1 -  that all intervals are correct. Notice that the Bonferroni confidence intervals look somewhat like the ordinary one-at-a-time CIs based on the t distribution, except that each Bonferroni interval has a confidence coefficient 1 - /p instead of 1 - .

CONFIDENCE INTERVALS IN MULTIPLE REGRESSION

103

Example 3.11 The Rocket Propellant Data

We may find 90% joint CIs for 0 and 1 for the rocket propellant data in Example 2.1 by constructing a 95% CI for each parameter. Since
( ) ^0 = 2627.822, se ^0 = 44.184 ( ) ^1 = -37.154, se ^1 = 2.889

and t0.05/2,18 = t0.025,18 = 2.101, the joint CIs are
( ) ( ) ^0 - t0.0125,18 se ^0  0  ^0 + t0.0125,18 se ^0
2627.822 - (2.445)(44.184)  0  2627.822 + (2.445)(44.184)
2519.792  0  2735.852

and
( ) ( ) ^1 - t0.0125,18 se ^1  1  ^1 - t0.0125,18 se ^1
-37.154 - (2.445)(2.889)  1  -37.154 + (2.445)(2.889)
-44.218  1  -30.090

We conclude with 90% confidence that this procedure leads to correct interval

estimates for both parameters.



The confidence ellipse is always a more efficient procedure than the Bonferroni method because the volume of the ellipse is always less than the volume of the space covered by the Bonferroni intervals. However, the Bonferroni intervals are easier to construct.
Constructing Bonferroni CIs often requires significance levels not listed in the usual t tables. Many modern calculators and software packages have values of t,v on call as a library function.
The Bonferroni method is not the only approach to choosing  in (3.51). Other approaches include the Scheffé S-method (see Scheffé [1953, 1959]), for which
( )  = 2F,p,n- p 1 2
and the maximum modulus t procedure (see Hahn [1972] and Hahn and Hendrickson [1971]), for which
 = u ,p,n- p
where u,p,n-p is the upper -tail point of the distnbution of the maximum absolute value of two independent student t random variables each based on n - 2 degrees of freedom. An obvious way to compare these three techniques is in terms of the lengths of the CIs they generate. Generally the Bonferroni intervals are shorter than the Scheffé intervals and the maximum modulus t intervals are shorter than the Bonferroni intervals.

104 MULTIPLE LINEAR REGRESSION

3.5 PREDICTION OF NEW OBSERVATIONS

The regression model can be used to predict future observations on y corresponding
to particular values of the regressor variables, for example, x01, x02, . . . , x0k. If
x0 = [1, x01, x02,... , x0k ], then a point estimate of the future observation y0 at the point
x01, x02, . . . , x0k is

y^0 = x0b^

(3.53)

A 100(1 - ) percent prediction interval for this future observation is
( ) ( ) y^0 - t 2,n-p ^ 2 1 + x0 (XX)-1 x0  y0  y^0 + t 2,n-p ^ 2 1 + x0 (XX)-1 x0 (3.54)

This is a generalization of the prediction interval for a future observation in simple linear regression, (2.45).

Example 3.12 The Delivery Time Data

Suppose that the soft drink bottler in Example 3.1 wishes to construct a 95% predic-
tion interval on the delivery time at an outlet where x1 = 8 cases are delivered and
the distance walked by the deliveryman is x2 = 275 feet. Note that x0 = [1, 8, 275],
and the point estimate of the delivery time is y^0 = x0 = 19.22 minutes. Also, in
Example 3.9 we calculated x0 (XX)-1 x0 = 0.05346. Therefore, from (3.54) we have

19.22 - 2.074 10.6239(1 + 0.05346)  y0  19.22 + 2.074 10.6239(1 + 0.05346)

and the 95% prediction interval is

12.28  y0  26.16



3.6 A MULTIPLE REGRESSION MODEL FOR THE PATIENT SATISFACTION DATA
In Section 2.7 we introduced the hospital patient satisfaction data and built a simple linear regression model relating patient satisfaction to a severity measure of the patient's illness. The data used in this example is in Table B17. In the simple linear regression model the regressor severity was significant, but the model fit to the data wasn't entirely satisfactory. Specifically, the value of R2 was relatively low, approximately 0.43, We noted that there could be several reasons for a low value of R2, including missing regressors. Figure 3.9 is the JMP output that results when we fit a multiple linear regression model to the satisfaction response using severity and patient age as the predictor variables.
In the multiple linear regression model we notice that the plot of actual versus predicted response is much improved when compared to the plot for the simple linear regression model (compare Figure 3.9 to Figure 2.7). Furthermore, the model is significant and both variables, age and severity, contribute significantly to the model. The R2 has increased from 0.43 to 0.81. The mean square error in the multiple

A MULTIPLE REGRESSION MODEL FOR THE PATIENT SATISFACTION DATA

105

Satisfaction Actual

Response Satisfaction Whole Model Actual by Predicted Plot
110 100
90 80 70 60 50 40 30 20
20 30 40 50 60 70 80 90 100 110 Satisfaction Predicted
P<.0001 RSq=0.81 RMSE=9.682

Summary of Fit
RSquare RSquare Adj Root Mean Square Error Mean of Response Observations (or Sum Wgts)

0.809595 0.792285 9.681956
66.72 25

Analysis of Variance

Source

DF

Model

2

Error

22

C. Total

24

Sum of Squares 8768.754 2062.286
10831.040

Mean Square 4384.38 93.74

F Ratio 46.7715 Prob > F <.0001*

Parameter Estimates

Term

Estimate

Intercept

139.92335

Age

-1.046154

Severity

-0.435907

Std Error 8.100194 0.157263 0.178754

t Ratio 17.27 -6.65 -2.44

Prob>|t| <.0001* <.0001* 0.0233*

Figure 3.9 JMP output for the multiple linear regression model for the patient satisfaction data.

linear regression model is 90.74, considerably smaller than the mean square error in the simple linear regression model, which was 270.02. The large reduction in mean square error indicates that the two-variable model is much more effective in explaining the variability in the data than the original simple linear regression model. This reduction in the mean square error is a quantitative measure of the improvement we qualitatively observed in the plot of actual response versus the predicted response when the predictor age was added to the model. Finally, the response is predicted with better precision in the multiple linear model. For example, the standard deviation of the predicted response for a patient that is 42 year old with a severity index

106 MULTIPLE LINEAR REGRESSION
of 30 is 3.10 for the multiple linear regression model while it is 5.25 for the simple linear regression model that includes only severity as the predictor. Consequently the prediction interval would be considerably wider for the simple linear regression model. Adding an important predictor to a regression model (age in this example) can often result in a much better fitting model with a smaller standard error and as a consequence narrow confidence intervals on the mean response and narrower prediction intervals.
3.7 USING SAS AND R FOR BASIC MULTIPLE LINEAR REGRESSION
SAS is an important statistical software package. Table 3.7 gives the source code to analyze the delivery time data that we have been analyzing throughout this chapter. The statement PROC REG tells the software that we wish to perform an ordinary least-squares linear regression analysis. The "model" statement gives the specific model and tells the software which analyses to perform. The commands for the
TABLE 3.7 SAS Code for Delivery Time Data
date delivery; input time cases distance; cards; 16.68 7 560 11.50 3 220 12.03 3 340 14.88 4 80 13.75 6 150 18.11 7 330
8.00 2 110 17.83 7 210 79.24 30 1460 21.50 5 605 40.33 16 688 21.00 10 215 13.50 4 255 19.75 6 462 24.00 9 448 29.00 10 776 15.35 6 200 19.00 7 132
9.50 3 36 35.10 17 770 17.90 10 140 52.32 26 810 18.75 9 450 19.83 8 635 10.75 4 150 proc reg; model time = cases distance/p clm cli; run;

HIDDEN EXTRAPOLATION IN MULTIPLE REGRESSION

107

optional analyses appear after the solidus. PROC REG always produces the analysis-of-variance table and the information on the parameter estimates. The "p clm cli" options on the model statement produced the information on the predicted values. Specifically, "p" asks SAS to print the predicted values, "clm" (which stands for confidence limit, mean) asks SAS to print the confidence band, and "cli" (which stands for confidence limit, individual observations) asks to print the prediction band. Table 3.8 gives the resulting output, which is consistent with the Minitab analysis.
We next illustrate the R code required to do the same analysis. The first step is to create the data set. The easiest way is to input the data into a text file using spaces for delimiters. Each row of the data file is a record. The top row should give the names for each variable. All other rows are the actual data records. Let delivery.txt be the name of the data file. The first row of the text file gives the variable names:

time cases distance

The next row is the first data record, with spaces delimiting each data item:

16.68 7 560

The R code to read the data into the package is: deliver <- read.table("delivery.txt",header=TRUE, sep="")

The object deliver is the R data set, and "delivery.txt" is the original data file. The phrase, hearder=TRUE tells R that the first row is the variable names. The phrase sep="" tells R that the data are space delimited.

The commands
deliver.model <- lm(timecases+distance, data=deliver) summary(deliver.model)

tell R

· to estimate the model, and · to print the analysis of variance, the estimated coefficients, and their tests.

3.8 HIDDEN EXTRAPOLATION IN MULTIPLE REGRESSION
In predicting new responses and in estimating the mean response at a given point x01, x02, . . . , x0k one must be careful about extrapolating beyond the region containing the original observations. It is very possible that a model that fits well in the region of the original data will perform poorly outside that region. In multiple regression it is easy to inadvertently extrapolate, since the levels of the regressors

108

TABLE 3.8 SAS Output for the Analysis of Delivery Time Data

SAS System 1
The REG Procedure Model: MODEL1
Dependent Variable: time

Number of Observation Read

25

Number of Observations Used

25

Analysis of Variance

Source

DF

Sun of Squares

Mean Squire

Model

2

Error

22

Corrected

24

Total

5550.81092 233.73168
5784.54260

2775.40546 10.62417

Root MSE Dependent
Mean Coeff Var

3.25947 22.38400
14.56162

R- Square Adj R- Sq

0.9596 0.9559

Parameter Estimates

Variable

DF

Parameter Estimate

Standard Error

Intercept

1

Cases

1

Distance

1

2.34123 1.61591 0.01438

1.09673 0.17073 0.00361

The SAS System 2

The REG Procedure Model: MODEL1

F Value 261.24
t value 2.13 9.46 3.98

Pr > F <.0001
Pr > |t| 0.0442 <.0001 0.0006

Dependent Variable: time

Output Statistics

Dependent Predicted Std error

Obs

Variable Value Mean Predict 95% CL

Mean 95% CL Predict Residual

1

16.6800 21.7081 1.0400 19.5513

23.8649 14.6126 28.8036 -5.0281

2

11.5000 10.3536 0.8667

8.5562

12.1510 3.3590 17.3482

1.1464

3

12.0300 12.0798 1.0242

9.9557

14.2038 4.9942 19.1654 -0.0498

4

14.8800 9.9556 0.9524

7.9805

11.9308 2.9133 16.9980

4.9244

5

13.7500 14.1944 0.8927 12.3430

16.0458 7.1857 21.2031 -0.4444

6

18.1100 18.3996 0.6749 17.0000

19.7991 11.4965 25.3027 -0.2896

7

8.0000 7.1554 0.9322

5.2221

9.0887

0.1246 14.1861

0.8446

8

17.8300 16.6734 0.8228 14.9670

18.3798

9.7016 23.6452

1.1566

9

79.2400 71.8203 2.3009 67.0486

76.5920 63.5461 80.0945

7.4197

10

21.5000 19.1236 1.4441 16.1287

22.1185 11.7301 26.5171

2.3764

11

40.3300 38.0925 0.9566 36.1086

40.0764 31.0477 45.1373

2.2378

12

21.0000 21.5930 1.0989 19.3141

23.8719 14.4595 28.7266 -0.5930

13

13.5000 12.4730 0.8059 10.8018

14.1442

5.5097 19.4363

1.0270

14

19.7500 18.6825 0.9117 16.7916

20.5733 11.6633 25.7017

1.0675

15

24.0000 23.3288 0.6609 21.9582

24.6994 16.4315 30.2261

0.6712

16

29.0000 29.6629 1.3278 26.9093

32.4166 22.3639 36.9620 -0.6629

17

15.3500 14.9136 0.7946 13.2657

16.5616

7.9559 21.8713

0.4364

18

19.0000 15.5514 1.0113 13.4541

17.6486

8.4738 22.6290

3.4486

19

9.5000 7.7068 1.0123

5.6075

9.8061

0.6286 14.7850

1.7932

20

35.1000 40.8880 1.0394 38.7324

43.0435 33.7929 47.9831 -5.7880

21

17.9000 20.5142 1.3251 17.7661

23.2623 13.2172 27.8112 -2.6142

22

52.3200 56.0065 2.0396 51.7766

60.2365 48.0324 63.9807 -3.6865

23

18.7500 23.3576 0.6621 21.9845

24.7306 16.4598 30.2553 -4.6076

24

19.8300 24.4029 1.1320 22.0553

26.7504 17.2471 31.5586 -4.5729

25

10.7500 10.9626 0.8414 9.2175

12.7076

3.9812 17.9439 -0.2126

109

Sum of Residuals Sum of Squared Residuals Predicted Residual SS (PRESS)

0 233.73168 459.03931

110 MULTIPLE LINEAR REGRESSION x2

Original range for
x2
x02

Joint region of original
data

x01

x1

Original

range for

x1

Figure 3.10 An example of extrapolation in multiple regression.

(xi1, xi2, . . . , xik), i = 1, 2, . . . , n, jointly define the region containing the data. As an example, consider Figure 3.10, which illustrates the region containing the original data for a two-regressor model. Note that the point (x01, x02) lies within the ranges of both regressors x1 and x2 but outside the region of the original data. Thus, either predicting the value of a new observation or estimating the mean response at this point is an extrapolation of the original regression model.
Since simply comparing the levels of the x's for a new data point with the ranges of the original x's will not always detect a hidden extrapolation, it would be helpful to have a formal procedure to do so. We will define the smallest convex set containing all of the original n data points (xi1, xi2, . . . , xik), i = 1, 2, . . . , n, as the regressor variable hull (RVH). If a point x01, x02, . . . , x0k lies inside or on the boundary of the RVH, then prediction or estimation involves interpolation, while if this point lies outside the RVH, extrapolation is required.
The diagonal elements hii of the hat matrix H = X(XX)-1X are useful in detecting hidden extrapolation. The values of hii depend both on the Euclidean distance of the point xi from the centroid and on the density of the points in the RVH. In general, the point that has the largest value of hii, say hmax, will lie on the boundary of the RVH in a region of the x space where the density of the observations is relatively low. The set of points x (not necessarily data points used to fit the model) that satisfy
x (XX)-1 x  hmax
is an ellipsoid enclosing all points inside the RVH (see Cook [1979] and Weisberg [1985]). Thus, if we are interested in prediction or estimation at the point
x0 = [1, x01, x02,... , x0k ], the location of that point relative to the RVH is reflected by
h00 = x0 (XX)-1 x0

STANDARDIZED REGRESSION COEFFLCIENTS

111

Points for which h00 > hmax are outside the ellipsoid enclosing the RVH and are extrapolation points. However, if h00 < hmax, then the point is inside the ellipsoid and possibly inside the RVH and would be considered an interpolation point because it is close to the cloud of points used to fit the model. Generally the smaller the value of h00, the closer the point x0 lies to the centroid of the x space.
Weisberg [1985] notes that this procedure does not produce the smallest volume ellipsoid containing the RVH. This is called the minimum covering ellipsoid (MCE). He gives an iterative algorithm for generating the MCE. However, the test for extrapolation based on the MCE is still only an approximation, as there may still be regions inside the MCE where there are no sample points.

Example 3.13 Hidden Extrapolation--The Delivery Time Data

We illustrate detecting hidden extrapolation using the soft drink delivery time data in Example 3.1. The values of hii for the 25 data points are shown in Table 3.9. Note that observation 9, represented by  in Figure 3.11, has the largest value of hii. Figure 3.11 confirms that observation 9 is on the boundary of the RVH.
Now suppose that we wish to consider prediction or estimation at the following four points:

Symbols in

Point

Figure 3.10

x10

x20

h00

a



8

275

0.05346



b

20

250

0.58917

c

+

28

500

0.89874

d

×

8

1200

0.86736

All of these points lie within the ranges of the regressors x1 and x2. In Figure 3.11

point a (used in Examples 3.9 and 3.12 for estimation and prediction), for which

h00 = 0.05346, is an interpolation point since h00 = 0.05346 < hmax = 0.49829. The

remaining points b, c, and d are all extrapolation points, since their values of h00

exceed hmax. This is readily confirmed by inspection of Figure 3.11.



3.9 STANDARDIZED REGRESSION COEFFLCIENTS
It is usually difficult to directly compare regression coefficients because the maguitude of ^ j reflects the units of measurement of the regressor xj. For example, suppose that the regression model is
If hmax is much larger than the next largest value, the point is a severe outlier in x space. The presence of such an outlier may make the ellipse much larger than desirable. In these cases one could use the second largest value of hii as hmax. This approach may be useful when the most remote point has been severely downweighted, say by the robust fitting techniques discussed in Chapter 15.

112 MULTIPLE LINEAR REGRESSION

TABLE 3.9 Values of hii for the Delivery Time Data

Observation, i Cases, xi1 Distance, xi2

hii

1

7

560

0.10180

2

3

220

0.07070

3

3

340

0.09874

4

4

80

0.08538

5

6

150

0.07501

6

7

330

0.04287

7

2

110

0.08180

8

7

210

0.06373

9

30

1460

0.49829 = hmax

10

5

605

0.19630

11

16

688

0.08613

12

10

215

0.11366

13

4

255

0.06113

14

6

462

0.07824

15

9

448

0.04111

16

10

776

0.16594

17

6

200

0.05943

18

7

132

0.09626

19

3

36

0.09645

20

17

770

0.10169

21

10

140

0.16528

22

26

810

0.39158

23

9

450

0.04126

24

8

635

0.12061

25

4

150

0.06664

800 1200

Distance

400

0

5

10 15 20 25 30

Cases

Figure 3.11 Scatterplot of cases and distance for the delivery time data.

STANDARDIZED REGRESSION COEFFLCIENTS

113

y^ = 5 + x1 + 1000x2
and y is measured in liters, x1 is measured in milliliters, and x2 is measured in liters. Note that although ^2 is considerably larger than ^1, the effect of both regressors on y^ is identical, since a 1-liter change in either x1 or x2 when the other variable is held constant produces the same change in y^ . Generally the units of the regression coefficient j are units of y/units of xj. For this reason, it is sometimes helpful to work with scaled regressor and response variables that produce dimensionless regression coefficients. These dimensionless coefficients are usually called standardized regression coefficients. We now show how they are computed, using two popular scaling techniques.

Unit Normal Scaling The first approach employs unit normal scaling for the regressors and the response variable. That is,

zij

=

xij - sj

xj

,

i = 1, 2,... , n,

j = 1, 2,... , k

and

(3.55)

yi*

=

yi - sy

y

,

i = 1, 2,... , n

(3.56)

where

n
 (xij - xj )2
s2j = i=1 n - 1

is the sample variance of regressor xj and
n
(yi - y)2
sy2 = i=1 n - 1

is the sample variance of the response. Note the similarity to standardizing a normal random variable. All of the scaled regressors and the scaled responses have sample mean equal to zero and sample variance equal to 1.
Using these new variables, the regression model becomes

yi* = b1zi1 + b2zi2 + + bkzik + i, i = 1, 2, ... , n

(3.57)

Centering the regressor and response variables by subtracting xj and y removes the intercept from the model (actually the least-squares estimate of b0 is b^ = y* = 0). The least-squares estimator of b is

b^ = (ZZ)-1 Zy*

(3.58)

114 MULTIPLE LINEAR REGRESSION

Unit Length Scaling The second popular scaling is unit length scaling,

wij

=

xij - xj s1jj 2

,

i = 1, 2,... , n,

j = 1, 2,... , k

and

(3.59)

where

yi0

=

yi - y SST1 2

,

i = 1, 2,... , n

n
 Sjj = (xij - xj )2 i=1

(3.60)

is the corrected sum of squares for regressor xj. In this scaling, each new regressor
wj has mean wj = 0 and length ( in=1 wij - wj )2 = 1. In terms of these variables, the
regression model is

yi0 = bqwi1 + b2wi2 + + bkwik + i, i = 1, 2, ... , n The vector of least-squares regression coefficients is
b^ = (WW)-1 Wy0

(3.61) (3.62)

In the unit length scaling, the WW matrix is in the form of a correlation matrix, that is,

 1 r12 r13

r1k 

 

r12

1

r23

r2

k

 

WW = r13 r23 1

r3k 









r1k r2k r3k

1 

where

n

 ( ) (xui - xi ) xuj - xj

( ) ( ) rij = u=1

SiiSjj 1 2

=

Sij SiiSjj 1 2

is the simple correlation between regressors xi and xj. Similarly,

r1y 

r2

y

 

Wy0 = r3y  



rky 

STANDARDIZED REGRESSION COEFFLCIENTS

115

where

n

 rjy

=

u=1

( xuj - xj )( yu (SjjSST )1 2

-

y)

=

Sjy
(SjjSST

)1

2

is the simple correlation between the regressor xj and the response y. If unit normal scaling is used, the ZZ matrix is closely related to WW; in fact,

ZZ = (n - 1)WW

Consequently, the estimates of the regression coefficients in Eqs. (3.58) and (3.62)
are identical. That is, it does not matter which scaling we use; they both produce the same set of dimensionless regression coefficients b^ .
The regression coefficients b^ are usually called standardized regression coeffi-
cients. The relationship between the original and standardized regression coeffi-
cients is

^ j

=

b^ j

 

SST  1 Sjj 

2
,

j = 1, 2,... , k

(3.63)

and
k
 ^0 = y - ^ j xj j =1

(3.64)

Many multiple regression computer programs use this scaling to reduce problems arising from round-off errors in the (XX)-1 matrix. These round-off errors may be very serious if the original variables differ considerably in magnitude. Most computer programs also display both the original regression coefficients and the standardized regression coefficients, which are often referred to as "beta coefficients." In interpreting standardized regression coefficients, we must remember that they are still partial regression coefficients (i.e., bj measures the effect of xj given that other regressors xi, i  j, are in the model). Furthermore, the bj are affected by the range of values for the regressor variables. Consequently, it may be dangerous to use the magnitude of the b^j as a measure of the relative importance of regressor xj.
Example 3.14 The Delivery Time Data

We find the standardized regression coefficients for the delivery time data in Example 3.1. Since

SST = 5784.5426, S1y = 2473.3440,

S11 = 1136.5600 S22 = 2, 537,935.0330

It is customary to refer to riy and rjj as correlations even through the regressors are not necessarily random variables.

116 MULTIPLE LINEAR REGRESSION

S2y = 108, 038.6019, S12 = 44, 266.6800

we find (using the unit length scaling) that

r12

=

S12
(S11S22 )1 2

=

44, 266.6800

= 0.824215

(1136.5600)(2, 537,935.0303)

r1y

=

S1y
(S11SST )1 2

=

2473.3440

= 0.964615

(1136.5600 ) ( 5784.53426)

r2 y

=

S2 y
(S22SST )1 2

=

108, 038.6019

= 0.891670

(2, 537,935.0330)(5784.5426)

and the correlation matrix for this problem is

WW

=

1 0.824215

0.824215

1



The normal equations in terms of the standardized regression coefficients are

1 0.824215

0.824215

1



bb^^12

  

=

0.964615 0.891670

Consequently, the standardized regression coefficients are

bb^^12

  

=

1 0.824215

0.824215-1 0.964615

1

 0.891670

=

3.11841 -2.57023

-2.57023 0.964615 3.11841  0.891670

=

0.716267 0.301311

The fitted model is

y^ 0 = 0.716267w1 + 0.301311w2

Thus, increasing the standardized value of cases w1 by one unit increases the standardized value of time y^ 0 by 0.716267. Furthermore, increasing the standardized

value of distance w2 by one unit increases y^0 by 0.301311 unit. Therefore, it seems that the volume of product delivered is more important than the distance in that it

has a larger effect on delivery time in terms of the standardized variables. However, we should be somewhat cautious in reaching this conclusion, as b^1 and b^2 are still partial regression coefficients, and b^1 and b^2 are affected by the spread in the regressors. That is, if we took another sample with a different range of values for cases

and distance, we might draw different conclusions about the relative importance of

these regressors.



MULTICOLLINEARITY

117

3.10 MULTICOLLINEARITY

Regression models are used for a wide variety of applications. A serious problem that may dramatically impact the usefulness of a regression model is multicollinearity, or near-linear dependence among the regression variables. In this section we briefly introduce the problem and point out some of the harmful effects of multicollinearity. A more extensive presentation, including more information on diagnostics and remedial measures, is in Chapter 9.
Multicollinearity implies near-linear dependence among the regressors. The regressors are the columns of the X matrix, so clearly an exact linear dependence would result in a singular XX. The presence of near-linear dependencies can dramatically impact the ability to estimate regression coefficients. For example, consider the regression data shown in Figure 3.12.
In Section 3.8 we introduced standardized regression coefficients. Suppose we use the unit length scaling [Eqs. (3.59) and (3.60)] for the data in Figure 3.12 so that the XX matrix (called WW in Section 3.8) will be in the form of a correlation matrix. This results in

WW

=

1 0

0 1

and

(WW)-1

=

1 0

0 1

For the soft drink delivery time data, we showed in Example 3.14 that

WW

=

 1.00000 0.824215

0.824215 1.00000 

and

(WW)-1

=

 3.11841 -2.57023

-2.57023 3.11841 

Now consider the variances of the standardized regression coefficients b^1 and b^2 for the two data sets. For the hypothetical data set in Figure 3.12.

( ) ( ) Var b^1 = Var b^2 = 1

2

2

x1

x2

5

20

10

20

5

30

10

30

5

20

10

20

5

30

10

30

31

30

29

28

27

26

x2

25 24

23

22

21

20

19

4 5 6 7 8 9 10 11 x1
Figure 3.12 Data on two regressors.

118 MULTIPLE LINEAR REGRESSION

while for the soft drink delivery time data

( ) ( ) Var b^1 = Var b^2 = 3.11841

2

2

In the soft drink delivery time data the variances of the regression coefficients are inflated because of the multicollinearity. This multicollinearity is evident from the nonzero off-diagonal elements in WW. These off-diagonal elements are usually called simple correlations between the regressors, although the term correlation may not be appropriate unless the x's are random variables. The off-diagonals do provide a measure of linear dependency between regressors. Thus, multicollinearity can seriously affect the precision with which regression coefficients are estimated.
The main diagonal elements of the inverse of the XX matrix in correlation form [(WW)-1 above] are often called variance inflation factors (VIFs), and they are an important multicollinearity diagnostic. For the soft drink data,

VIF1 = VIF2 = 3.11841

while for the hypothetical regressor data above,

VIF1 = VIF2 = 1

implying that the two regressors x1 and x2 are orthogonal. We can show that, in general, the VIF for the jth regression coefficient can be written as

VIFj

=

1 1 - Rj2

where Rj2 is the coefficient of multiple determination obtained from regressing xj on the other regressor variables. Clearly, if xj is nearly linearly dependent on some of the other regressors, then Rj2 will be near unity and VIFj will be large. VIFs larger than 10 imply serious problems with multicollinearity. Most regression software computes and displays the VIFj.
Regression models fit to data by the method of least squares when strong multicollinearity is present are notoriously poor prediction equations, and the values of the regression coefficients are often very sensitive to the data in the particular sample collected. The illustration in Figure 3.13a will provide some insight regarding these effects of multicollinearity. Building a regression model to the (x1, x2, y) data in Figure 3.13a is analogous to placing a plane through the dots. Clearly this plane will be very unstable and is sensitive to relatively small changes in the data points. Furthermore, the model may predict y's at points similar to those observed in the sample reasonably well, but any extrapolation away from this path is likely to produce poor prediction. By contrast, examine the of orthogonal regressors in Figure 3.13b. The plane fit to these points will be more stable.
The diagnosis and treatment of multicollinearity is an important aspect of regression modeling. For a more in-depth treatment of the subject, refer to Chapter 9.

WHY DO REGRESSION COEFFICIENTS HAVE THE WRONG SIGN?

119

y

x1

x2

x1

x2

(a)

(b)

Figure 3.13 (a) A data set with multicollinearity. (b) Orthogonal regressors.

3.11 WHY DO REGRESSION COEFFICIENTS HAVE THE WRONG SIGN?
When using multiple regression, occasionally we find an apparent contradiction of intuition or theory when one or more of the regression coefficients seem to have the wrong sign. For example, the problem situation may imply that a particular regression coefficient should be positive, while the actual estimate of the parameter is negative. This "wrong"-sign problem can be disconcerting, as it is usually difficult to explain a negative estimate (say) of a parameter to the model user when that user believes that the coefficient should be positive. Mullet [1976] points out that regression coefficients may have the wrong sign for the following reasons:
1. The range of some of the regressors is too small. 2. Important regressors have not been included in the model. 3. Multicollinearity is present. 4. Computational errors have been made.
It is easy to see how the range of the x's can affect the sign of the regression coefficients. Consider the simple linear regression model. The variance of the regression
coefficient ^1 is Var (1 ) =  2 Sxx =  2 ( in=1 xi - x ). Note that the variance of ^1 is
inversely proportional to the "spread" of the regressor. Therefore, if the levels of x are all close together, the variance of ^1 will be relatively large. In some cases the variance of ^1 could be so large that a negative estimate (for example) of a regression coefficient that is really positive results. The situation is illustrated in Figure 3.14, which plots the sampling distribution of ^1. Examining this figure, we see that the probability of obtaining a negative estimate of ^1 depends on how close the true regression coefficient is to zero and the variance of ^1, which is greatly influenced by the spread of the x's.
In some situations the analyst can control the levels of the regressors. Although it is possible in these cases to decrease the variance of the regression coefficients by increasing the range of the x's, it may not be desirable to spread the levels of the regressors out too far. If the x's cover too large a range and the true response

> y
> >

120 MULTIPLE LINEAR REGRESSION
Probability that 1 < 0

V (1) =

2

n
(xi

-

x-

)2

i=1

0 1

1

Figure 3.14 Sampling distribution of ^1.

x1

x2

y

2

11

4

25

5

23

6

48

8

45

10

43

11

6 10

13

67

Values of x2 shown at each point

10

6

4 6

5

2

4

2

4

1

0

5

10

15

x1

Figure 3.15 Plot of y versus x1.

function is nonlinear, the analyst may have to develop a much more complex equation to adequately model the curvature in the system. Furthermore, many problems involve a region of x space of specific interest to the experimenter, and spreading the regressors out beyond this region of interest may be impractical or impossible. In general, we must trade off the precision of estimation, the likely complexity of the model, and the values of the regressors of practical interest when deciding how far to spread out the x's.
Wrong signs can also occur when important regressors have been left out of the model. In these cases the sign is not really wrong. The partial nature of the regression coefficients cause the sign reversal. To illustrate, consider the data in Figure 3.15.
Suppose we fit a model involving only y and x1. The equation is
y^ = 1.835 + 0.463x1
where ^1 = 0.463 is a "total" regression coefficient.That is, it measures the total effect of x1 ignoring the information content in x2. The model involving both x1 and x2 is
y^ = 1.036 - 1.222x1 + 3.649x2

PROBLEMS 121
Note that now ^1 = -1.222, and a sign reversal has occurred. The reason is that ^1 = -1.222 in the multiple regression model is a partial regression coefficient; it measures the effect of x1 given that x2 is also in the model.
The data from this example are plotted in Figure 3.15. The reason for the difference in sign between the partial and total regression coefficients is obvious from inspection of this figure. If we ignore the x2 values, the apparent relationship between y and x1 has a positive slope. However, if we consider the relationship between y and x1 for constant values of x2, we note that this relationship really has a negative slope. Thus, a wrong sign in a regression model may indicate that important regressors are missing. If the analyst can identify these regressors and include them in the model, then the wrong signs may disappear.
Multicollinearity can cause wrong signs for regression coefficients. In effect, severe multicollinearity inflates the variances of the regression coefficients, and this increases the probability that one or more regression coefficients will have the wrong sign. Methods for diagnosing and dealing with multicollinearity are summarized in Chapter 9.
Computational error is also a source of wrong signs in regression models. Different computer programs handle round-off or truncation problems in different ways, and some programs are more effective than others in this regard. Severe multicollinearity causes the XX matrix to be ill-conditioned, which is also a source of computational error. Computational error can cause not only sign reversals but regression coefficients to differ by several orders of magnitude. The accuracy of the computer code should be investigated when wrong-sign problems are suspected.
PROBLEMS
3.1 Consider the National Football League data in Table B.1. a. Fit a multiple linear regression model relating the number of games won to the team's passing yardage (x2), the percentage of rushing plays (x7), and the opponents' yards rushing (x8). b. Construct the analysis-of-variance table and test for significance of regression. c. Calculate t statistics for testing the hypotheses H0: 2 = 0, H0: 7 = 0, and H0: 8 = 0. What conclusions can you draw about the roles the variables x2, x7, and x8 play in the model? d. Calculate R2 and RA2 dj for this model. e. Using the partial F test, determine the contribution of x7 to the model. How is this partial F statistic related to the t test for 7 calculated in part c above?
3.2 Using the results of Problem 3.1, show numerically that the square of the simple correlation coefficient between the observed values yi and the fitted values y^i equals R2.
3.3 Refer to Problem 3.1. a. Find a 95% CI on 7. b. Find a 95% CI on the mean number of games won by a team when x2 = 2300, x7 = 56.0, and x8 = 2100.

122 MULTIPLE LINEAR REGRESSION
3.4 Reconsider the National Football League data from Problem 3.1. Fit a model to these data using only x7 and x8 as the regressors. a. Test for significance of regression. b. Calculate R2 and RA2 dj. How do these quantities compare to the values computed for the model in Problem 3.1, which included an additional regressor (x2)? c. Calculate a 95% CI on 7. Also find a 95% CI on the mean number of games won by a team when x7 = 56.0 and x8 = 2100. Compare the lengths of these CIs to the lengths of the corresponding CIs from Problem 3.3. d. What conclusions can you draw from this problem about the consequences of omitting an important regressor from a model?
3.5 Consider the gasoline mileage data in Table B.3. a. Fit a multiple linear regression model relatmg gasoline mileage y (miles per gallon) to engine displacement x1 and the number of carburetor barrels x6. b. Construct the analysis-of-variance table and test for significance of regression. c. Calculate R2 and RA2 dj for this model. Compare this to the R2 and the RA2 dj for the simple linear regression model relating mileage to engine displacement in Problem 2.4. d. Find a 95% CI for 1. e. Compute the t statistics for testing H0: 1 = 0 and H0: 6 = 0. What conclusions can you draw? f. Find a 95% CI on the mean gasoline mileage when x1 = 275 in.3 and x6 = 2 barrels. g. Find a 95% prediction interval for a new observation on gasoline mileage when x1 = 257 in.3 and x6 = 2 barrels.
3.6 In Problem 2.4 you were asked to compute a 95% CI on mean gasoline prediction interval on mileage when the engine displacement x1 = 275 in.3 Compare the lengths of these intervals to the lengths of the confidence and prediction intervals from Problem 3.5 above. Does this tell you anything about the benefits of adding x6 to the model?
3.7 Consider the house price data in Table B.4. a. Fit a multiple regression model relating selling price to all nine regressors. b. Test for significance of regression. What conclusions can you draw? c. Use t tests to assess the contribution of each regressor to the model. Discuss your findings. d. What is the contribution of lot size and living space to the model given that all of the other regressors are included? e. Is multicollinearity a potential problem in this model?
3.8 The data in Table B.5 present the performance of a chemical process as a function of sever controllable process variables. a. Fit a multiple regression model relating CO2 product (y) to total solvent (x6) and hydrogen consumption (x7).

PROBLEMS 123
b. Test for significance of regression. Calculate R2 and RA2 dj. c. Using t tests determine the contribution of x6 and x7 to the model. d. Construct 95% CIs on 6 and 7. e. Refit the model using only x6 as the regressor. Test for significance of
regression and calculate R2 and RA2 dj. Discuss your findings. Based on these statistics, are you satisfied with this model? f. Construct a 95% CI on 6 using the model you fit in part e. Compare the length of this CI to the length of the CI in part d. Does this tell you anything important about the contribution of x7 to the model? g. Compare the values of MSRes obtained for the two models you have fit (parts a and e). How did the MSRes change when you removed x7 from the model? Does this tell you anything importaut about the contributiou of x7 to the model?
3.9 The concentration of NbOCl3 in a tube-flow reactor as a function of several controllable variables is shown in Table B.6.
a. Fit a multiple regression model relating concentration of NbOCl3 (y) to concentration of COCl2, (x1) and mole fraction (x4).
b. Test for significance of regression. c. Calculate R2 and RA2 dj for this model. d. Using t tests, determine the contribution of x1 and x4 to the model. Are
both regressors x1 and x4 necessary? e. Is multicollinearity a potential concern in this model?
3.10 The quality of Pinot Noir wine is thought to be related to the properties of clarity, aroma, body, flavor, and oakiness. Data for 38 wines are given in Table B.11.
a. Fit a multiple linear regression model relating wine quality to these regressors.
b. Test for significance of regression. What conclusions can you draw?
c. Use t tests to assess the contribution of each regressor to the model. Discuss your findings.
d. Calculate R2 and RA2 dj for this model. Compare these values to the R2 and RA2 dj for the linear regression model relating wine quality to aroma and flavor. Discuss your results.
e. Find a 95 % CI for the regression coefficient for flavor for both models in part d. Discuss any differences.
3.11 An engineer performed an experiment to determine the effect of CO2 pressure, CO2 temperature, peanut moisture, CO2 flow rate, and peanut particle size on the total yield of oil per batch of peanuts. Table B.7 summarizes the experimental results.
a. Fit a multiple linear regression model relating yield to these regressors.
b. Test for significance of regression. What conclusions can you draw?
c. Use t tests to assess the contribution of each regressor to the model. Discuss your findings
d. Calculate R2 and RA2 dj for this model. Compare these values to the R2 and RA2 dj for the multiple linear regression model relating yield to temperature and particle size. Discuss your results.

124 MULTIPLE LINEAR REGRESSION
e. Find a 95% CI for the regression coefficient for temperature for both models in part d. Discuss any differences.
3.12 A chemical engineer studied the effect of the amount of surfactant and time on clathrate formation. Clathrates are used as cool storage media. Table B.8 summarizes the experimental results. a. Fit a multiple linear regression model relating clathrate formation to these regressors. b. Test for significance of regression. What conclusions can you draw? c. Use t tests to assess the contribution of each regressor to the model. Discuss your findings. d. Calculate R2 and RA2 dj for this model. Compare these values to the R2 and RA2 dj for the simple linear regression model relating clathrate formation to time. Discuss your results. e. Find a 95% CI for the regression coefficient for time for both models in part d. Discuss any differences.
3.13 An engineer studied the effect of four variables on a dimensionless factor used to describe pressure drops in a screen-plate bubble column. Table B.9 summarizes the experimental results. a. Fit a multiple linear regression model relating this dimensionless number to these regressors. b. Test for significance of regression. What conclusions can you draw? c. Use t tests to assess the contribution of each regressor to the model. Discuss your findings. d. Calculate R2 and RA2 dj for this model. Compare these values to the R2 and RA2 dj for the multiple linear regression model relating the dimensionless number to x2 and x3. Discuss your results. e. Find a 99% CI for the regression coefficient for x2 for both models in part d. Discuss any differences.
3.14 The kinematic viscosity of a certain solvent system depends on the ratio of the two solvents and the temperature. Table B.10 summarizes a set of experimental results. a. Fit a multiple linear regression model relating the viscosity to the two regressors. b. Test for significance of regression. What conclusions can you draw? c. Use t tests to assess the contribution of each regressor to the model. Discuss your findings. d. Calculate R2 and RA2 dj for this model. Compare these values to the R2 and RA2 dj for the simple linear regression model relating the viscosity to temperature only. Discuss your results. e. Find a 99% CI for the regression coefficient for temperature for both models in part d. Discuss any differences.
3.15 McDonald and Ayers [1978] present data from an early study that examined the possible link between air pollution and mortality. Table B.15 summarizes the data. The response MORT is the total age-adjusted mortality from all

PROBLEMS 125
causes, in deaths per 100,000 population. The regressor PRECIP is the mean annual precipitation ·(in inches), EDUC is the median number of school years completed for persons of age 25 years or older, NONWHITE is the percentage of the 1960 population that is nonwhite, NOX is the relative pollution potential of oxides of nitrogen, and SO2 is the relative pollution potential of sulfur dioxide."Relative pollution potential" is the product of the tons emitted per day per square kilometer and a factor correcting the SMSA dimensions and exposure. a. Fit a multiple linear regression model relating the mortality rate to these
regressors. b. Test for significance of regression. What conclusions can you draw? c. Use t tests to assess the contribution of each regressor to the model.
Discuss your findings. d. Calculate R2 and RA2 dj for this model. e. Find a 95% CI for the regression coefficient for SO2.
3.16 Rossman [1994] presents an interesting study of average life expectancy of 40 countries. Table B.16 gives the data. The study has three responses: LifeExp is the overall average life expectancy. LifeExpMale is the average life expectancy for males, and LifeExpFemale is the average life expectancy for females. The regressors are People-per-TV, which is the average number of people per television, and People-per-Dr, which is the average number of people per physician. a. Fit different multiple linear regression models for each response. b. Test each model for significance of regression. What conclusions can you draw? c. Use t tests to assess the contribution of each regressor to each model. Discuss your findings. d. Calculate R2 and RA2 dj for each model. e. Find a 95% CI for the regression coefficient for People-per-Dr in each model.
3.17 Consider the patient satisfaction data in Table B.17. For the purposes of this exercise, ignore the regressor "Medical-Surgical." Perform a thorough analysis of these data. Please discuss any differences from the analyses outlined in Sections 2.7 and 3.6.
3.18 Consider the fuel consumption data in Table B.18. For the purposes of this exercise, ignore regressor x1. Perform a thorough analysis of these data. What conclusions do you draw from this analysis?
3.19 Consider the wine quality of young red wines data in Table B.19. For the purposes of this exercise, ignore regressor x1. Perform a thorough analysis of these data. What conclusions do you draw from this analysis?
3.20 Consider the methanol oxidation data in Table B.20. Perform a thorough analysis of these data. What conclusions do you draw from this analysis?
3.21 A chemical engineer is investigating how the amount of conversion of a product from a raw material ( y) depends on reaction temperature (x1) and reaction time (x2). He has developed the following regression models:

126 MULTIPLE LINEAR REGRESSION

1. y^ = 100 + 0.2x1 + 4x2 2. y^ = 95 + 0.15x1 + 3x2 + 1x1x2

Both ]models have been built over the range 20  x1  50 (°C) and 0.5  x2  10 (hours).
a. Using both models, what is the predicted value of conversion when x2 = 2 in terms of x1? Repeat this calculation for x2 = 8. Draw a graph of the predicted values as a function of temperature for both conversion models. Comment on the effect of the interaction term in model 2.
b. Find the expected change in the mean conversion for a unit change in temperature x1 for model 1 when x2 = 5. Does this quantity depend on the specific value of reaction time selected? Why?
c. Find the expected change in the mean conversion for a unit change in temperature x1 for model 2 when x2 = 5. Repeat this calculation for x2 = 2 and x2 = 8. Does the result depend on the value selected for x2? Why?

3.22 Show that an equivalent way to perform the test for significance of regression in multiple linear regression is to base the test on R2 as follows: To test H0: 1 = 2 = . . . = k versus H1: at least one j  0, calculate

F0

=

R2(n - p)
k (1 - R2 )

and to reject H0 if the computed value of F0 exceeds F,k,n-p, where p = k + 1.
3.23 Suppose that a linear regression model with k = 2 regressors has been fit to n = 25 observations and R2 = 0.90. a. Test for significance of regression at  = 0.05. Use the results of the previous problem. b. What is the smallest value of R2 that would lead to the conclusion of a significant regression if  = 0.05? Are you surprised at how small this value of R2 is?
3.24 Show that an alternate computing formula for the regression sum of squares in a linear regression model is

n
 SSR = y^i2 - ny2 i=1

3.25 Consider the multiple linear regression model

y = 0 + 1x1 + 2 x2 + 3x3 + 4x4 + 

Using the procedure for testing a general linear hypothesis, show how to test
a. H0: 1 = 2 = 3 = 4 =  b. H0: 1 = 2, 3 = 4 c. H0 : 1 - 22 = 43
1 + 22 = 0

PROBLEMS 127

3.26 Suppose that we have two independent samples, say

yx

yx

y1 y2

x1 

x2

  

Sample

1

yn1 +1

xn1+1 

 

Sample

2





 yn1 xn1 

yn1+n2 xn1+n2 

Two models can be fit to these samples,

yi = 0 + 1xi + i, i = 1, 2, ... , n2 yi =  0 +  1xi + i, i = n1 + 1, n1 + 2, ... , n1 + n2

a. Show how these two separate models can be written as a single model. b. Using the result in part a, show how the general linear hypothesis can be
used to test the equality of slopes 1 and 1. c. Using the result in part a, show how the general linear hypothesis can be
used to test the equality of the two regression lines. d. Using the result in part a, show how the general linear hypothesis can be
used to test that both slopes are equal to a constant c.
3.27 Show that Var (y^ ) =  2H.
3.28 Prove that the matrices H and I - H are idempotent, that is, HH = H and (I - H)(I - H) = I - H.
3.29 For the simple linear regression model, show that the elements of the hat matrix are

hij

=

1 n

+

( xi

-

x)(xj
Sxx

-

x)

and

hii

=

1 n

+

( xi - x )2
Sxx

Discuss the behavior of these quantities as xi moves farther from x,
3.30 Consider the multiple linear regression model y = X + . Show that the leastsquares estimator can be written as

b^ = b + Re where R = (XX)-1 X

3.31 Show that the residuals from a linear regression model can be expressed as e = (I - H). [Hint: Refer to Eq. (3.15b).]
3.32 For the multiple linear regression model, show that SSR() = yHy.
3.33 Prove that R2 is the square of the correlation between y and y^ .
3.34 Constrained least squares. Suppose we wish to find the least-squares estimator of  in the model y = X +  subject to a set of equality constraints on , say T = c. Show that the estimator is

128 MULTIPLE LINEAR REGRESSION

( ) b = b^ + (XX)-1 T T (XX)-1 T-1 c - Tb^
where b^ = (XX)-1 Xy. Discuss situations in which this constrained estimator
might be appropriate. Find the residual sum of squares for the constrained estimator. Is it larger or smaller than the residual sum of squares in the unconstrained case?
3.35 Let xj be the jth row of X, and X-j be the X matrix with the jth row removed. Show that

Var ^ j  =  2 xjx j - xjX- j (X- jX- j )-1 X- jx j 

3.36 Consider the following two models where E() = 0 and Var() = 2I:

Model A: Model B: Show that

y = X11 +  y = X1b1 + X2b2 + e RA2  RB2 .

3.37 Suppose we fit the model y = X12 +  when the true model is actually given by y = X12 + X22 + . For both models, assume E() = 0 and Var() = 2I.
Find the expected value and variance of the ordinary least-squares estimate, b^1. Under what conditions is this estimate unbiased?

3.38 Consider a correctly specified regression model with p terms, including the intercept. Make the usual assumptions about . Prove that

n
 Var (y^i ) = p 2
i=1

3.39 Let Rj2 be the coefficient of determination when we regress the jth regressor on the other k - 1 regressors. Show that the jth variance inflation factor may
be expressed as

1 1 - Rj2
3.40 Consider the hypotheses for the general linear model, which are of the form

H0: Tb = c, H1: Tb  c
where T is a q × p matrix of rank q. Derive the appropriate F statistic under both the null and alternative hypothesis.

CHAPTER 4
MODEL ADEQUACY CHECKING
4.1 INTRODUCTION The major assumptions that we have made thus far in our study of regression analysis are as follows:
1. The relationship between the response y and the regressors is linear, at least approximately.
2. The error term  has zero mean. 3. The error term  has constant variance  2. 4. The errors are uncorrelated. 5. The errors are normally distributed. Taken together, assumptions 4 and 5 imply that the errors are independent random variables. Assumption 5 is required for hypothesis testing and interval estimation. We should always consider the validity of these assumptions to be doubtful and conduct analyses to examine the adequacy of the model we have tentatively entertained. The types of model inadequacies discussed here have potentially serious consequences. Gross violations of the assumptions may yield an unstable model in the sense that a different sample could lead to a totally different model with opposite conclusions. We usually cannot detect departures from the underlying assumptions by examination of the standard summary statistics, such as the t or F statistics, or R2. These are "global" model properties, and as such they do not ensure model adequacy.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
129

130 MODEL ADEQUACY CHECKING
In this chapter we present several methods useful for diagnosing violations of the basic regression assumptions. These diagnostic methods are primarily based on study of the model residuals. Methods for dealing with model inadequacies, as well as additional, more sophisticated diagnostics, are discussed in Chapters 5 and 6.

4.2 RESIDUAL ANALYSIS

4.2.1 Definition of Residuals

We have previously defined the residuals as

ei = yi - y^i , i = 1, 2, ... , n

(4.1)

where yi is an observation and y^i is the corresponding fitted value. Since a residual may be viewed as the deviation between the data and the fit, it is also a measure of the variability in the response variable not explained by the regression model. It is also convenient to think of the residuals as the realized or observed values of the model errors. Thus, any departures from the assumptions on the errors should show up in the residuals. Analysis of the residuals is an effective way to discover several types of model inadequacies. As we will see, plotting residuals is a very effective way to investigate how well the regression model fits the data and to check the assumptions listed in Section 4.1.
The residuals have several important properties. They have zero mean, and their approximate average variance is estimated by

n

n

  i=1

(ei - e )2
n- p

=

ei2
i=1
n- p

=

SSRes n- p

= MSRes

The residuals are not independent, however, as the n residuals have only n - p degrees of freedom associated with them. This nonindependence of the residuals has little effect on their use for model adequacy checking as long as n is not small relative to the number of parameters p.

4.2.2 Methods for Scaling Residuals
Sometimes it is useful to work with scaled residuals. In this section we introduce four popular methods for scaling residuals. These scaled residuals are helpful in finding observations that are outliers, or extreme values, that is, observations that are separated in some fashion from the rest of the data. See Figures 2.6­2.8 for examples of outliers and extreme values.

Standardized Residuals Since the approximate average variance of a residual is estimated by MSRes, a logical scaling for the residuals would be the standardized residuals

di = ei , i = 1, 2, ... , n MSRes

(4.2)

RESIDUAL ANALYSIS

131

The standardized residuals have mean zero and approximately unit variance. Consequently, a large standardized residual (di > 3, say) potentially indicates an outlier.
Studentized Residuals Using MSRes as the variance of the ith residual, ei is only an approximation. We can improve the residual scaling by dividing ei by the exact standard deviation of the ith residual. Recall from Eq. (3.15b) that we may write the vector of residuals as

e = (I-H)y

(4.3)

where H = X(XX)­lX is the hat matrix. The hat matrix has several useful properties. It is symmetric (H = H) and idempotent (HH = H). Similarly the matrix I - H is symmetric and idempotent. Substituting y = X +  into Eq. (4.3) yields

e = (I - H)(Xb + e ) = Xb - HXb + (I - H)e = Xb - X (XX)-1 XXb + (I - H)e = (I - H)e

(4.4)

Thus, the residuals are the same linear transformation of the observations y and the errors .
The covariance matrix of the residuals is

Var (e) = Var[(I - H) e ] = (I - H) Var (e )(I - H) =  2 (I - H)

(4.5)

since Var() =  2I and I - H is symmetric and idempotent. The matrix I - H is generally not diagonal, so the residuals have different variances and they are correlated.
The variance of the ith residual is

Var (ei ) =  2 (1 - hii )

(4.6)

where hii is the ith diagonal element of the hat matrix H. The covariance between residuals ei and ej is

Cov (ei , ej ) = - 2hij

(4.7)

where hij is the ijth element of the hat matrix. Now since 0  hii  1, using the residual mean square MSRes to estimate the variance of the residuals actually overestimates Var(ei). Furthermore, since hii is a measure of the location of the ith point in x space (recall the discussion of hidden extrapolation in Section 3.7), the variance of ei depends on where the point xi lies. Generally points near the center of the x space have larger variance (poorer least-squares fit) than residuals at more remote locations. Violations of model assumptions are more likely at remote points, and these violations may be hard to detect from inspection of the ordinary residuals ei (or the standardized residuals di) because their residuals will usually be smaller.
Most students find it very counter-intuitive that the residuals for data points remote in terms of the xs are small, and in fact go to 0 as the remote points get

132 MODEL ADEQUACY CHECKING

Scatterplot of y vs x 30

25

20

y

15

10

5

0

5

10

15

20

25

x

Figure 4.1 Example of a pure leverage point.

y

Scatterplot of y vs x 12

10

8

6

4

2

0

5

10

15

20

25

x

Figure 4.2 Example of an influential point.

further away from the center of the other points. Figures 4.1 and 4.2 help to illustrate this point. The only difference between these two plots occurs at x = 25. In Figure 4.1, the value of the response is 25, and in Figure 4.2, the value is 2. Figure 4.1 is a typical scatter plot for a pure leverage point. Such a point is remote in terms of the specific values of the regressors, but the observed value for the response is consistent with the prediction based on the other data values. The data point with x = 25 is an example of a pure leverage point. The line drawn on the figure is the actual ordinary least squares fit to the entire data set. Figure 4.2 is a typical scatter plot for an influential point. Such a data value is not only remote in terms of the specific values for the regressors, but the observed response is not consistent with the values that would be predicted based on only the other data points. Once again, the line drawn is the actual ordinary least squares fit to the entire data set. One can clearly see that the influential point draws the prediction equation to itself.

RESIDUAL ANALYSIS

133

A little mathematics provides more insight into this situation. Let yn be the observed response for the nth data point, let xn be the specific values for the regressors for this data point, let y^n* be the predicted value for the response based on the other n - 1 data points, and let  = yn - y^n* be the difference between the actually observed value for this response compared to the predicted value based on the other values. Please note that yn = y^n* +  . If a data point is remote in terms of the regressor values and || is large, then we have an influential point. In Figures 4.1 and 4.2, consider x = 25. Let yn be 2, the value from Figure 4.2. The actual predicted value
for the that response based on the other four data values is 25, which is the point
illustrated in Figure 4.1. In this case,  = ­23, and we see that it is a very influential point. Finally, let y^n be the predicted value for the nth response using all the data. It
can be shown that

y^n = y^n* + hnn
where hnn is the nth diagonal element of the hat matrix. If the nth data point is remote in terms of the space defined by the data values for the regressors, then hnn approaches 1, and y^n approaches yn. The remote data value "drags" the prediction to itself.
This point is easier to see within a simple linear regression example. Let x* be the average value for the other n - 1 regressors. It can be shown that

y^ n

=

y^ n*

+

 

1

 n

+



n- n

1

2

( xn

- x* Sxx

)2

  

.

Clearly, for even a moderate sample size, as the data point becomes more remote in terms of the regressors (as xn moves further away from x*, then the ordinary least squares estimate of yn appraoches the actually observed value for yn).
The bottom line is two-fold. As we discussed in Sections 2.4 and 3.4, the prediction variance for data points that are remote in terms of the regressors is large. However, these data points do draw the prediction equation to themselves. As a result, the variance of the residuals for these points is small. This combination presents complications for doing proper residual analysis.
A logical procedure, then, is to examine the studentized residuals

ri =

ei

,

MSRes (1 - hii )

i = 1, 2, ... , n

(4.8)

instead of ei (or di). The studentized residuals have constant variance Var(ri) = 1 regardless of the location of xi when the form of the model is correct. In many situations the variance of the residuals stabilizes, particularly for large data sets. In these cases there may be little difference between the standardized and studentized residuals. Thus, standardized and studentized residuals often convey equivalent information. However, since any point with a large residual and a large hii is potentially highly influential on the least-squares fit, examination of the studentized residuals is generally recommended.
Some of these points are very easy to see by examining the studentized residuals for a simple linear regression model. If there is only one regressor, it is easy to show that the studentized residuals are

134 MODEL ADEQUACY CHECKING

ri =

ei

,

MSRes

1 - 

  

1 n

+

(

xi - x Sxx

)2

  

i = 1, 2, ... , n

(4.9)

Notice that when the observation xi is close to the midpoint of the x data, xi - x will be small, and the estimated standard deviation of ei [the denominator of Eq. (4.9)] will be large. Conversely, when xi is near the extreme ends of the range of the x data, xi - x will be large, and the estimated standard deviation of ei will be small. Also,
when the sample size n is really large, the effect of ( xi - x )2 will be relatively small,
so in big data sets, studentized residuals may not differ dramatically from standard-
ized residuals.

PRESS Residuals The standardized and studentized residuals are effective in
detecting outliers. Another approach to making residuals useful in finding outliers is to examine the quantity that is computed from yi - y^(i), where y^(i) is the fitted value of the ith response based on all observations except the ith one. The logic behind
this is that if the ith observation yi is really unusual, the regression model based on all observations may be overly influenced by this observation. This could produce a fitted value y^i that is very similar to the observed value yi, and consequently, the ordinary residual ei will be small. Therefore, it will be hard to detect the outlier. However, if the ith observation is deleted, then y^(i) cannot be influenced by that observation, so the resulting residual should be likely to indicate the presence of
the outlier. If we delete the ith observation, fit the regression model to the remaining n - 1
observations, and calculate the predicted value of yi corresponding to the deleted observation, the corresponding prediction error is

e(i) = yi - y^(i)

(4.10)

This prediction error calculation is repeated for each observation i = 1, 2, . . . , n. These prediction errors are usually called PRESS residuals (because of their use in computing the prediction error sum of squares, discussed in Section 4.3). Some authors call the e(i) deleted residuals.
It would initially seem that calculating the PRESS residuals requires fitting n different regressions. However, it is possible to calculate PRESS residuals from the results of a single least-squares fit to all n observations. We show in Appendix C.7 how this is accomplished. It turns out that the ith PRESS residual is

e(i)

=

ei 1 - hii

,

i = 1, 2, ... , n

(4.11)

From Eq. (4.11) it is easy to see that the PRESS residual is just the ordinary residual weighted according to the diagonal elements of the hat matrix hii. Residuals associated with points for which hii is large will have large PRESS residuals. These points will generally be high influence points. Generally, a large difference between the ordinary residual and the PRESS residual will indicate a point where the model fits the data well, but a model built without that point predicts poorly. In Chapter 6, we discuss some other measures of influential observations.

RESIDUAL ANALYSIS

135

Finally, the variance of the ith PRESS residual is

Var

[e(i)

]

=

Var

 

1

ei - hii

 

=

(1

1 - hii

)2

[

2

(1

-

hii

)]

=

 1-

2
hii

so that a standardized PRESS residual is

e(i)

= ei / (1 - hii ) =

ei

Var[e(i) ]



2 i

(

1

-

hii

)

 2 (1 - hii )

which, if we use MSRes to estimate 2, is just the studentized residual discussed previously.

R-Student The studentized residual ri discussed above is often considered an outlier diagnostic. It is customary to use MSRes as an estimate of 2 in computing ri.
This is referred to as internal scaling of the residual because MSRes is an internally generated estimate of 2 obtained from fitting the model to all n observations. Another approach would be to use an estimate of 2 based on a data set with the ith observation removed. Denote the estimate of 2 so obtained by S(2i). We can show (see Appendix C.8) that

S(2i )

=

(n

-

p) MSRes - ei2
n- p-1

/ (1 - hii

)

(4.12)

The estimate of 2 in Eq. (4.12) is used instead of MSRes to produce an externally studentized residual, usually called R-student, given by

ti =

ei

,

S(2i) (1 - hii )

i = 1, 2, ... , n

(4.13)

In many situations ti will differ little from the studentized residual ri. However, if the ith observation is influential, then S(2i) can differ significantly from MSRes, and thus the R-student statistic will be more sensitive to this point.
It turns out that under the usual regression assumptions, ti will follow the tn­p­1 distribution. Appendix C.9 establishes a formal hypothesis-testing procedure for outlier detection based on R-student. One could use a Bonferroni-type approach and compare all n values of |ti| to t(/2n),n­p­1 to provide guidance regarding outliers. However, it is our view that a formal approach is usually not necessary and that only relatively crude cutoff values need be considered. In general, a diagnostic view as opposed to a strict statistical hypothesis-testing view is best. Furthermore, detection of outliers often needs to be considered simultaneously with detection of influential observations, as discussed in Chapter 6.

Example 4.1 The Delivery Time Data

Table 4.1 presents the scaled residuals discussed in this section using the model for the soft drink delivery time data developed in Example 3.1. Examining column

136 MODEL ADEQUACY CHECKING

1 of Table 4.1 (the ordinary residuals, originally calculated in Table 3.3) we note that one residual, e9 = 7.4197, seems suspiciously large. Column 2 shows that
the standardized residual is d9 = e9 / MSRes = 7.4197 / 10.6239 = 2.2763. All other standardized residuals are inside the ±2 limits. Column 3 of Table 4.1 shows the studentized residuals. The studentized residual at point 9 is
r9 = e9 / MSRes (1 - h9,9 ) = 7.4197 / 10.6239(1 - 0.49829) = 3.2138, which is substan-
tially larger than the standardized residual. As we noted in Example 3.13, point 9 has the largest value of x1 (30 cases) and x2 (1460 feet). If we take the remote location of point 9 into account when scaling its residual, we conclude that the model does not fit this point well. The diagonal elements of the hat matrix, which are used extensively in computing scaled residuals, are shown in column 4.
Column 5 of Table 4.1 contains the PRESS residuals. The PRESS residuals for points 9 and 22 are substantially larger than the corresponding ordinary residuals, indicating that these are likely to be points where the model fits reasonably well but does not provide good predictions of fresh data. As we have observed in Example 3.13, these points are remote from the rest of the sample.
Column 6 displays the values of R-student. Only one value, t9, is unusually large. Note that t9 is larger than the corresponding studentized residual r9, indicating that when run 9 is set aside, S(29) is smaller than MSRes, so clearly this run is influential. Note that S(29) is calculated from Eq. (4.12) as follows:

S(29)

=

(n

-

p) MSRes - e92 / (1 -
n- p-1

h9,9

)

= (22)(10.6239) - (7.4197)2 / (1 - 0.49829)

21

= 5.9046



4.2.3 Residual Plots
As mentioned previously, graphical analysis of residuals is a very effective way to investigate the adequacy of the fit of a regression model and to check the underlying assumptions. In this section, we introduce and illustrate the basic residual plots. These plots are typically generated by regression computer software packages. They should be examined routinely in all regression modeling problems. We often plot externally studentized residuals because they have constant variance.
Normal Probability Plot Small departures from the normality assumption do not affect the model greatly, but gross nonnormality is potentially more serious as the t or F statistics and confidence and prediction intervals depend on the normality assumption. Furthermore, if the errors come from a distribution with thicker or heavier tails than the normal, the least-squares fit may be sensitive to a small subset of the data. Heavy-tailed error distributions often generate outlier that "pull" the least-squares fit too much in their direction. In these cases other estimation techniques (such as the robust regression methods in Section 15.1 should be considered.
A very simple method of checking the normality assumption is to construct a normal probability plot of the residuals. This is a graph designed so that the cumula-

137

TABLE 4.1 Scaled Residuals for Example 4.1

Observation
Number, i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

ei = yi - y^i
(1)
­5.0281 1.1464 ­0.0498 4.9244 ­0.4444 ­0.2896 0.8446 1.1566 7.4197 2.3764 2.2375 ­0.5930 1.0270 1.0675 0.6712 ­0.6629 0.4364 3.4486 1.7932 ­5.7880 ­2.6142 ­3.6865 ­4.6076 ­4.5728 ­0.2126

di = ei / MSRes
(2)
­1.5426 0.3517 ­0.0153 1.5108 ­0.1363 ­0.0888 0.2501 0.3548 2.2763 0.7291 0.6865 ­0.1819 0.3151 0.3275 0.2059 ­0.2034 0.1339 1.0580 0.5502 ­1.7758 ­0.8020 ­1.1310 ­1.4136 ­1.4029 ­0.0652

ri = ei / MSRes (1 - hii )
(3)
­1.6277 0.349 ­0.0161 1.5798 ­0.1418 ­0.0908 0.2704 0.3667 3.2138 0.8133 0.7181 ­0.1932 0.3252 0.3411 0.2103 ­0.2227 0.1381 1.1130 0.5787 ­1.8736 ­0.8779 ­1.4500 ­1.4437 ­1.4961 ­0.0675

hii
(4)
0.10180 0.07070 0.09874 0.05838 0.07501 0.04287 0.08180 0.06373 0.49829 0.19630 0.08613 0.11366 0.06113 0.07824 0.04111 0.16594 0.05943 0.09626 0.09645 0.10169 0.16528 0.39158 0.04126 0.12061 0.06664

e(i) = ei/(1 - hii)
(5)
­5.5980 1.2336 ­0.0557 5.2297 ­0.4804 ­0.3025 0.9198 1.2353 14.7888 2.9568 2.4484 ­0.6690 1.0938 1.1581 0.7000 ­0.7948 0.4640 3.8159 1.9846 ­6.4432 ­3.1318 ­6.0591 ­4.8059 ­5.2000 ­0.2278

ti = ei / S(2i) (1 - hii )
(6)
­1.6956 0.3575 ­0.0157 1.6392 ­0.1386 ­0.0887 0.2646 0.3594 4.3108 0.8068 0.7099 ­0.1890 0.3185 0.3342 0.2057 ­0.2178 0.1349 1.1193 0.5698 ­1.9967 ­0.8731 ­1.4896 ­1.4825 ­1.5422 ­0.0660

[ei / (1 - hii )]2
(7)
31.3373 1.5218 0.0031 27.3499 0.2308 0.0915 0.8461 1.5260
218.7093 8.728 5.9946 0.4476 1.1965 1.3412 0.4900 0.6317 0.2153 14.5612 3.9387 41.5150 9.8084 36.7131 23.0966 27.0397 0.0519
PRESS = 457.4000

138 MODEL ADEQUACY CHECKING

tive normal distribution will plot as a straight line. Let t[1] < t[2] < . . . < t[n] be the

externally studentized residuals ranked in increasing order. If we plot t[i] against the

cumulative

probability

Pi

=

(i

-

1 2

)/

n,

i

=

1,

2,

.

.

.,

n,

on

the

normal

probability

plot,

the resulting points should lie approximately on a straight line. The straight line is

usually determined visually, with emphasis on the central values (e.g., the 0.33 and

0.67 cumulative probability points) rather than the extremes. Substantial departures

from a straight line indicate that the distribution is not normal. Sometimes normal

probability plots are constructed by plotting the ranked residual t[i] against the

"expected

normal

value"



-1

[(

i

-

1 2

)

/

n],

where



denotes

the

standard

normal

cumulative distribution. This follows from the fact that E (t[i] )

 -1

[(i

-

1 2

)

/

n]

.

Figure 4.3a displays an "idealized" normal probability plot. Notice that the points

lie approximately along a straight line. Panels b­e present other typical problems.

Panel b shows a sharp upward and downward curve at both extremes, indicating

that the tails of this distribution are too light for it to be considered normal. Con-

versely, panel c shows flattening at the extremes, which is a pattern typical of samples

from a distribution with heavier tails than the normal. Panels d and e exhibit pat-

terns associated with positive and negative skew, respectively.

Because samples taken from a normal distribution will not plot exactly as a

straight line, some experience is required to interpret normal probability plots.

Daniel and Wood [1980] present normal probability plots for sample sizes 8­384.

Study of these plots is helpful in acquiring a feel for how much deviation from the

1

1

1

Probability Probability Probability

0.5

0.5

0.5

0 ti
(a)
1

0 ti
(b)

0 ti
(c)

1

Probability Probability

0.5

0.5

0
ti (d)

0
ti (e)

Figure 4.3 Normal probability plots: (a) ideal; (b) light-tailed distribution; (c) heavy-tailed distribution; (d) positive skew; (e) negative skew.

These interpretations assume that the ranked residuals are plotted on the horizontal axis. If the residuals are plotted on the vertical axis, as some computer systems do, the interpretation is reversed.

RESIDUAL ANALYSIS

139

straight line is acceptable. Small sample sizes (n  16) often produce normal probability plots that deviate substantially from linearity. For larger sample sizes (n  32) the plots are much better behaved. Usually about 20 points are required to produce normal probability plots that are stable enough to be easily interpreted.
Andrews [1979] and Gnanadesikan [1977] note that normal probability plots often exhibit no unusual behavior even if the errors i are not normally distributed. This problem occurs because the residuals are not a simple random sample; they are the remnants of a parameter estimation process. The residuals are actually linear combinations of the model errors (the i). Thus, fitting the parameters tends to destroy the evidence of nonnormality in the residuals, and consequently we cannot always rely on the normal probability plot to detect departures from normality.
A common defect that shows up on the normal probability plot is the occurrence of one or two large residuals. Sometimes this is an indication that the corresponding observations are outliers. For additional discussion of outliers, refer to Section 4.4.

Example 4.2 The Delivery Time Data

Figure 4.4 presents a normal probability plot of the externally studentized residuals

from the regression model for the delivery time data from Example 3.1. The residu-

als are shown in columns 1 and 2 of Table 4.1.

The residuals do not lie exactly along a straight line, indicating that there may

be some problems with the normality assumption, or that there may be one or more

outliers in the data. From Example 4.1, we know that the studentized residual for

observation 9 is moderately large (r9 = 3.2138), as is the R-student residual

(t9 = 4.3108). However, there is no indication of a severe problem in the delivery

time data.



Plot of Residuals against the Fitted Values y^i A plot of the (preferrably the externally studentized residuals, ti) versus the corresponding fitted values y^i is useful for detecting several common types of model inadequacies. If this plot resembles Figure 4.5a, which indicates that the residuals can be contained in a horizontal band, then there are no obvious model defects. Plots of ti versus y^i that resemble any of the patterns in panels b­d are symptomatic of model deficiencies.
The patterns in panels b and c indicate that the variance of the errors is not constant. The outward-opening funnel pattern in panel b implies that the variance is an increasing function of y [an inward-opening funnel is also possible, indicating that Var() increases as y decreases]. The double-bow pattern in panel c often occurs when y is a proportion between zero and 1. The variance of a binomial proportion near 0.5 is greater than one near zero or 1. The usual approach for dealing with inequality of variance is to apply a suitable transformation to either the regressor or the response variable (see Sections 5.2 and 5.3) or to use the method of weighted least squares (Section 5.5). In practice, transformations on the response are generally employed to stabilize variance.
A curved plot such as in panel d indicates nonlinearity. This could mean that other regressor variables are needed in the model. For example, a squared term may

The residuals should be plotted versus the fitted values y^i and not the observed values yi because the ei and the y^i are uncorrelated while the ei and the yi are usually correlated. The proof of this statement in Appendix C.10.

140 MODEL ADEQUACY CHECKING

Normal Probability Plot (response is time)
99

Percent

95 90
80 70 60 50 40 30 20
10 5

1

­3 ­2 ­1 0

1

2

3

4

5

Externally Studentized Residual

Figure 4.4 Normal probability plot of the externally studentized residuals for the delivery time data.

ti 0
yi (a)

>

ti 0
yi (b)

>

ti 0

ti 0

>

>

yi

yi

(c)

(d)

Figure 4.5 Patterns for residual plots: (a) satisfactory; (b) funnel; (c) double bow; (d) nonlinear.

be necessary. Transformations on the regressor and/or the response variable may also be helpful in these cases.
A plot of the residuals against y^i may also reveal one or more unusually large residuals. These points are, of course, potential outliers. Large residuals that occur at the extreme y^i values could also indicate that either the variance is not constant or the true relationship between y and x is not linear. These possibilities should be investigated before the points are considered outliers.

RESIDUAL ANALYSIS

141

Example 4.3 The Delivery Time Data

Figure 4.6 presents the plot of the externally studentized residuals versus the fitted

values of delivery time. The plot does not exhibit any strong unusual pattern,

although the large residual t9 shows up clearly. There does seem to be a slight ten-

dency for the model to underpredict short delivery times and overpredict long

delivery times.



Plot of Residuals against the Regressor Plotting the residuals against the corresponding values of each regressor variable can also be helpful. These plots often exhibit patterns such as those in Figure 4.5, except that the horizontal scale is xij for the jth regressor rather than y^i. Once again an impression of a horizontal band containing the residuals is desirable. The funnel and double-bow patterns in panels b and c indicate nonconstant variance. The curved band in panel d or a nonlinear pattern in general implies that the assumed relationship between y and the regressor xj is not correct. Thus, either higher order terms in xj (such as x2j ) or a transformation should be considered.
In the simple linear regressor case, it is not necessary to plot residuals versus both y^i and the regressor variable. The reason is that the fitted values y^i are linear combinations of the regressor values xi, so the plots would only differ in the scale for the abscissa.

Externally Studentized Residual

Example 4.4 The Delivery Time Data
Figure 4.7 presents the plots of the externally studentized residuals ti from the delivery time problem in Example 3.1 versus both regressors. Panel a plots residuals versus cases and panel b plots residuals versus distance. Neither of these plots reveals any clear indication of a problem with either misspecification of the regressor (implying the need for either a transformation on the regressor or higher order
Versus Fits (response is time) 5 4 3 2 1 0 ­1 ­2 0 10 20 30 40 50 60 70 80
Fitted Value Figure 4.6 Plot of externally studentized residuals versus predicted for the delivery time data.

142 MODEL ADEQUACY CHECKING

terms in cases and/or distance) or inequality of variance, although the moderately

large residual associated with point 9 is apparent on both plots.

It is also helpful to plot residuals against regressor variables that are not currently

in the model but which could potentially be included. Any structure in the plot of

residuals versus an omitted variable indicates that incorporation of that variable

could improve the model.

Plotting residuals versus a regressor is not always the most effective way to reveal

whether a curvature effect (or a transformation) is required for that variable in the

model. In Section 4.2.4 we describe two additional residual plots that are more

effective in investigating the relationship between the response variable and the

regressors.



Externally Studentized Residual

5 4 3 2 1 0 ­1 ­2
0

Residuals Versus cases (response is time)

5

10

15

20

25

30

Cases (a)

Externally Studentized Residual

Residuals Versus dist (response is time)
5
4
3
2 1
0
­1 ­2
0 200 400 600 800 1000 1200 1400 1600 Dist (b)
Figure 4.7 Plot of externally studentized residuals versus the regressors for the delivery time data: (a) residuals versus cases; (b) residuals versus distance.

RESIDUAL ANALYSIS

143

0

0

ti ti

Time (a)

Time (b)

Figure 4.8 Prototype residual plots against time displaying autocorrelation in the errors: (a) positive autocorrelation; (b) negative autocorrelation.

Plot of Residuals in Time Sequence If the time sequence in which the data were collected is known, it is a good idea to plot the residuals against time order. Ideally, this plot will resemble Figure 4.5a; that is, a horizontal band will enclose all of the residuals, and the residuals will fluctuate in a more or less random fashion within this band. However, if this plot resembles the patterns in Figures 4.5b­d, this may indicate that the variance is changing with time or that linear or quadratic terms in time should be added to the model.
The time sequence plot of residuals may indicate that the errors at one time period are correlated with those at other time periods. The correlation between model errors at different time periods is called autocorrelation. A plot such as Figure 4.8a indicates positive autocorrelation, while Figure 4.8b is typical of negative autocorrelation. The presence of autocorrelation is a potentially serious violation of the basic regression assumptions. More discussion about methods for detecting autocorrelation and remedial measures are discussed in Chapter 14.

4.2.4 Partial Regression and Partial Residual Plots
We noted in Section 4.2.3 that a plot of residuals versus a regressor variable is useful in determining whether a curvature effect for that regressor is needed in the model. A limitation of these plots is that they may not completely show the correct or complete marginal effect of a regressor, given the other regressors in the model. A partial regression plot is a variation of the plot of residuals versus the predictor that is an enhanced way to study the marginal relationship of a regressor given the other variables that are in the model. This plot can be very useful in evaluating whether we have specified the relationship between the response and the regressor variables correctly. Sometimes the partial residual plot is called the added-variable plot or the adjusted-variable plot. Partial regression plots can also be used to provide information about the marginal usefulness of a variable that is not currently in the model.
Partial regression plots consider the marginal role of the regressor xj given other regressors that are already in the model. In this plot, the response variable y and the regressor xj are both regressed against the other regressors in the model and the residuals obtained for each regression. The plot of these residuals against each other provides information about the nature of the marginal relationship for regressor xj under consideration.

144 MODEL ADEQUACY CHECKING

To illustrate, suppose we are considering a first-order multiple regression model with two regressors variables, that is, y = 0 + 1x1 + 2x2 + . We are concerned about the nature of the marginal relationship for regressor x1--in other words, is the relationship between y and x1 correctly specified? First we would regress y on x2 and obtain the fitted values and residuals:

y^i ( x2 ) = ^0 + ^1xi2 ei ( y | x2 ) = yi - y^i ( x2 ), i = 1, 2, ... , n

(4.14)

Now regress x1 on x2 and calculate the residuals:

x^i1 ( x2 ) = ^0 + ^1xi2 ei ( x1 | x2 ) = xi1 - x^i1 ( x2 ), i = 1, 2, ... , n

(4.15)

The partial regression plot for regressor variable x1 is obtained by plotting the y residuals ei(y|x2) against the x1 residuals ei(x1|x2). If the regressor x1 enters the model linearly, then the partial regression plot should show a linear relationship, that is, the partial residuals will fall along a straight line with a nonzero slope. The slope of this line will be the regression coefficient of x1 in the multiple linear regression model. If the partial regression plot shows a curvilinear band, then higher order terms in x1 or a transformation (such as replacing x1 with 1/x1) may be helpful. When x1 is a candidate variable being considered for inclusion in the model, a horizontal band on the partial regression plot indicates that there is no additional useful information in x1 for predicting y.

Example 4.5 The Delivery Time Data

Figure 4.9 presents the partial regression plots for the delivery time data, with the

plot for x1 shown in Figure 4.9a and the plot for x2 shown in Figure 4.9b. The linear relationship between both cases and distance is clearly evident in both of these plots,

although, once again, observation 9 falls somewhat off the straight line that appar-

ently well-describes the rest of the data. This is another indication that point 9 bears

further investigation.



Some Comments on Partial Regression Plots
1. Partial regression plots need to be used with caution as they only suggest possible relationships between the regressor and the response. These plots may not give information about the proper form of the relationship if several variables already in the model are incorrectly specified. It will usually be necessary to investigate several alternate forms for the relationship between the regressor and y or several transformations. Residual plots for these subsequent models should be examined to identify the best relationship or transformation.
2. Partial regression plots will not, in general, detect interaction effects among the regressors.

Time Time

RESIDUAL ANALYSIS

145

15

12.5

10.0 10
7.5
5 5.0

0

2.5

0.0 -5
-2.5
-10 -5.0

-15 -8 -6 -4 -2 0 2 4 6 8 10 12
Cases (a)

-7.5 -400 -300 -200 -100 0 100 200 300 400
Distance (b)

Figure 4.9 Partial regression plots for the delivery time data.

3. The presence of strong multicollinearity (refer to Section 3.9 and Chapter 9) can cause partial regression plots to give incorrect information about the relationship between the response and the regressor variables.
4. It is fairly easy to give a general development of the partial regression plotting concept that shows clearly why the slope of the plot should be the regression coefficient for the variable of interest, say xj.

The partial regression plot is a plot of residuals from which the linear dependence of y on all regressors other than xj has been removed against regressor xj with its linear dependence on other regressors removed. In matrix form, we may write these quantities as e[y|X(j)] and e[xj|X(j)], respectively, where X(j) is the original X matrix with the jth regressor (xj) removed. To show how these quantities are defined, consider the model

y = Xb + e = X( j)b +  jx j + e Premultiply Eq. (4.16) by I - H(j) to give
(I - H(j) )y = (I - H(j) )X(j)b + j (I - H(j) )xj + (I - H(j) )e
and note that (I - H(j))X(j) = 0, so that
(I - H(j) )y = j (I - H(j) )xj + (I - H(j) )e

(4.16)

146 MODEL ADEQUACY CHECKING
or
e[y | X(j) ] = je[xj | X(j) ]+ e*
where * = (I - H(j)). This suggests that a partial regression plot should have slope j. Thus, if xj enters the regression in a linear fashion, the partial regression plot should show a linear relationship passing through the origin. Many computer programs (such as SAS and Minitab) will generate partial regression plots.
Partial Residual Plots A residual plot closely related to the partial regression plot is the partial residual plot. It is also designed to show the relationship between the response variable and the regressors. Suppose that the model contains the regressors x1, x2, . . . , xk. The partial residuals for regressor xj are defined as
ei* ( y| xj ) = ei + ^j xij , i = 1, 2, ... , n
where the ei are the residuals from the model with all k regressors included. When the partial residuals are plotted against xij, the resulting display has slope ^j , the regression coefficient associated with xj in the model. The interpretation of the partial residual plot is very similar to that of the partial regression plot. See Larsen and McCeary [1972], Daniel and Wood [1980], Wood [1973], Mallows [1986], Mansfield and Conerly [1987], and Cook [1993] for more details and examples.
4.2.5 Using Minitab®, SAS, and R for Residual Analysis
It is easy to generate the residual plots in Minitab. Select the "graphs" box. Once it is opened, select the "deleted" option to get the studentized residuals. You then select the residual plots you want.
Table 4.2 gives the SAS source code for SAS version 9 to do residual analysis for the delivery time data. The partial option provides the partial regression plots. A common complaint about SAS is the quality of many of the plots generated by its procedures. These partial regression plots are prime examples. Version 9, however, upgrades some of the more important graphics plots for PROC REG. The first plot statement generates the studentized residuals versus predicted values, the studentized residuals versus the regressors, and the studentized residuals by time plots (assuming that the order in which the data are given is the actual time order). The second plot statement gives the normal probability plot of the studentized residuals.
As we noted, over the years the basic plots generated by SAS have been improved. Table 4.3 gives appropriate source code for earlier versions of SAS that produce "nice" residual plots. This code is important when we discuss plots from other SAS procedures that still do not generate nice plots. Basically this code uses the OUTPUT command to create a new data set that includes all of the previous delivery information plus the predicted values and the studentized residuals. It then uses the SASGRAPH features of SAS to generate the residual plots. The code uses PROC CAPABILITY to generate the normal probability plot. Unfortunately, PROC CAPABILITY by default produces a lot of noninteresting information in the output file.

RESIDUAL ANALYSIS

147

TABLE 4.2 SAS Code for Residual Analysis of Delivery Time Data
date delivery; input time cases distance; cards; 16.68 7 560 11.50 3 220 12.03 3 340 14.88 4 80 13.75 6 150 18.11 7 330
8.00 2 110 17.83 7 210 79.24 30 1460 21.50 5 605 40.33 16 688 21.00 10 215 13.50 4 255 19.75 6 462 24.00 9 448 29.00 10 776 15.35 6 200 19.00 7 132
9.50 3 36 35.10 17 770 17.90 10 140 52.32 26 810 18.75 9 450 19.83 8 635 10.75 4 150 proc reg;
model time = cases distance / partial; plot rstudent.*(predicted. cases distance obs.); plot npp.*rstudent.; run;

We next illustrate how to use R to create appropriate residual plots. Once again, consider the delivery data. The first step is to create a space delimited file named delivery.txt. The names of the columns should be time, cases, and distance.
The R code to do the basic analysis and to create the appropriate residual plots based on the externally studentized residuals is:
deliver <- read.table("delivery.txt",header=TRUE, sep=" ") deliver.model <- lm(timecases+distance, data=deliver) summary(deliver.model) yhat <- deliver.model$fit t <- rstudent(deliver.model) qqnorm(t) plot(yhat,t) plot(deliver$x1,t) plot(deliver$x2,t)

148 MODEL ADEQUACY CHECKING
TABLE 4.3 Older SAS Code for Residual Analysis of Delivery Time Data
date delivery; input time cases distance; cards; 16.68 7 560 11.50 3 220 12.03 3 340 14.88 4 80 13.75 6 150 18.11 7 330
8.00 2 110 17.83 7 210 79.24 30 1460 21.50 5 605 40.33 16 688 21.00 10 215 13.50 4 255 19.75 6 462 24.00 9 448 29.00 10 776 15.35 6 200 19.00 7 132
9.50 3 36 35.10 17 770 17.90 10 140 52.32 26 810 18.75 9 450 19.83 8 635 10.75 4 150 proc reg;
model time = cases distance / partial; output out = delivery2 p = ptime rstudent= t; run; data delivery3; set delivery2; index = _n_; proc gplot data = delivery3; plot t*ptime t*cases t*distance t*index; run; proc capability data = delivery3; var t; qqplot t; run;

RESIDUAL ANALYSIS

149

Generally, the graphics in R require a great deal of work in order to be of suitable quality. The commands

deliver2 <- cbind(deliver,yhat,t) write.table(deliver2,"delivery_output.txt")

create a file "delivery_output.txt" which the user than can import into his/her favorite package for doing graphics.

4.2.6 Other Residual Plotting and Analysis Methods
In addition to the basic residual plots discussed in Sections 4.2.3 and 4.2.4, there are several others that are occasionally useful. For example, it may be very useful to construct a scatterplot of regressor xi against regressor xj. This plot may be useful in studying the relationship between regressor variables and the disposition of the data in x space. Consider the plot of xi versus xj in Figure 4.10. This display indicates that xi and xj are highly positively correlated. Consequently, it may not be necessary to include both regressors in the model. If two or more regressors are highly correlated, it is possible that multicollinearity is present in the data. As observed in Chapter 3 (Section 3.10), multicollinearity can seriously disturb the least-squares fit and in some situations render the regression model almost useless. Plots of xi versus xj may also be useful in discovering points that are remote from the rest of the data and that potentially influence key model properties. Anscombe [1973] presents several other types of plots between regressors. Cook and Weisberg [1994] give a very modern treatment of regression graphics, including many advanced techniques not considered in this book.
Figure 4.11 is a scatterplot of x1 (cases) versus x2 (distance) for delivery time data from Example 3.1 (Table 3.2). Comparing Figure 4.11 with Figure 4.10, we see that cases and distance are positively correlated. In fact, the simple correlation between x1 and x2 is r12 = 0.82. While highly correlated regressors can cause a number of serious problems in regression, there is no strong indication in this example that

30

Cases

20
xi 10

xj Figure 4.10 Plot of xi versus xj.

0

0

500

1000

1500

Distance

Figure 4.11 Plot of regressor x1 (cases) versus regressor x2 (distance for the delivery time data in Table 3.2.

150 MODEL ADEQUACY CHECKING
any problems have occurred. The scatterplot clearly reveals that observation 9 is unusual with respect to both cases and distance (x1 = 30, x2 = 1460); in fact, it is rather remote in x space from the rest of the data. Observation 22 (x1 = 26, x2 = 810) is also quite far from the rest of the data. Points remote in x space can potentially control some of the properties of the regression model. Other formal methods for studying this are discussed in Chapter 6.
The problem situation often suggests other types of residual plots. For example, consider the delivery time data in Example 3.1. The 25 observations in Table 3.2 were collected on truck routes in four different cities. Observations 1­7 were collected in San Diego, observations 8­17 in Boston, observations 18­23 in Austin, and observations 24 and 25 in Minneapolis. We might suspect that there is a difference in delivery operations from city to city due to such factors as different types of equipment, different levels of crew training and experience, or motivational factors influenced by management policies. These factors could result in a "site" effect that is not incorporated in the present equation. To investigate this, we plot the residuals by site in Figure 4.12. We see from this plot that there is some imbalance in the distribution of positive and negative residuals at each site. Specifically, there is an apparent tendency for the model to overpredict delivery times in Austin and underpredict delivery times in Boston. This could happen because of the site-dependent factors mentioned above or because one or more important regressors have been omitted from the model.
Statistical Tests on Residuals We may apply statistical tests to the residuals to obtain quantitative measures of some of the model inadequacies discussed above. For example, see Anscombe [1961, 1967], Anscombe and Tukey [1963], Andrews [1971], Looney and Gulledge [1985], Levine [1960], and Cook and Weisberg [1983]. Several formal statistical testing procedures for residuals are discussed in Draper and Smith [1998] and Neter, Kutner, Nachtsheim, and Wasserman [1996].

5

4

3

2

1

0

-1

-2 A

B

M

SD

Site

Figure 4.12 Plot of externally studentized residuals by site (city) for the delivery time data in Table 3.2.

PRESS STATISTIC 151
In our experience, statistical tests on regression model residuals are not widely used. In most practical situations the residual plots are more informative than the corresponding tests. However, since residual plots do require skill and experience to interpret, the statistical tests may occasionally prove useful. For a good example of the use of statistical tests in conjunction with plots see Feder [1974].

4.3 PRESS STATISTIC

In Section 4.2.2 we defined the PRESS residuals as e(i) = yi - y^(i), where y^(i) is the predicted value of the ith observed response based on a model fit to the remaining n - 1 sample points. We noted that large PRESS residuals are potentially useful in identifying observations where the model does not fit the data well or observations for which the model is likely to provide poor future predictions.
Allen [1971, 1974] has suggested using the prediction error sum of squares (or the PRESS statistic), defined as the sum of the squared PRESS residuals, as a measure of model quality. The PRESS statistic is

n
 PRESS = [yi - ] y^(i) 2

i=1

 =

n i=1

 

1

ei - hii

2 

(4.17)

PRESS is generally regarded as a measure of how well a regression model will perform in predicting new data. A model with a small value of PRESS is desired.

Example 4.6 The Delivery Time Data

Column 5 of Table 4.1 shows the calculations of the PRESS residuals for the delivery

time data of Example 3.1. Column 7 of Table 4.1 contains the squared PRESS

residuals, and the PRESS statistic is shown at the foot of this column. The value of

PRESS = 457.4000 is nearly twice as large as the residual sum of squares for this

model, SSRes = 233.7260. Notice that almost half of the PRESS statistic is contributed

by point 9, a relatively remote point in x space with a moderately large residual.

This indicates that the model will not likely predict new observations with large case

volumes and long distances particularly well.



R 2 for Prediction Based on PRESS The PRESS statistic can be used to compute an R2-like statistic for prediction, say

R2 prediction

=

1-

PRESS SST

(4.18)

This statistic gives some indication of the predictive capability of the regression model. For the soft drink delivery time model we find

152 MODEL ADEQUACY CHECKING

R2 prediction

=

1-

PRESS SST

= 1 - 457.4000 5784.5426

= 0.9209

Therefore, we could expect this model to "explain" about 92.09% of the variability in predicting new observations, as compared to the approximately 95.96% of the variability in the original data explained by the least-squares fit. The predictive capability of the model seems satisfactory, overall. However, recall that the individual PRESS residuals indicated that observations that are similar to point 9 may not be predicted well.

Using PRESS to Compare Models One very important use of the PRESS statistic is in comparing regression models. Generally, a model with a small value of PRESS is preferable to one where PRESS is large. For example, when we added x2 = distance to the regression model for the delivery time data containing x1 = cases, the value of PRESS decreased from 733.55 to 457.40. This is an indication that the two-regressor model is likely to be a better predictor than the model containing only x1 = cases.

4.4 DETECTION AND TREATMENT OF OUTLIERS
An outlier is an extreme observation; one that is considerably different from the majority of the data. Residuals that are considerably larger in absolute value than the others, say three or four standard deviations from the mean, indicate potential y space outliers. Outliers are data points that are not typical of the rest of the data. Depending on their location in x space, outliers can have moderate to severe effects on the regression model (e.g., see Figures 2.6­2.8). Residual plots against y^i and the normal probability plot are helpful in identifying outliers. Examining scaled residuals, such as the studentized and R-student residuals, is an excellent way to identify potential outliers. An excellent general treatment of the outlier problems is in Barnett and Lewis [1994]. Also see Myers [1990] for a good discussion.
Outliers should be carefully investigated to see if a reason for their unusual behavior can be found. Sometimes outliers are "bad" values, occurring as a result of unusual but explainable events. Examples include faulty measurement or analysis, incorrect recording of data, and failure of a measuring instrument. If this is the case, then the outlier should be corrected (if possible) or deleted from the data set. Clearly discarding bad values is desirable because least squares pulls the fitted equation toward the outlier as it minimizes the residual sum of squares. However, we emphasize that there should be strong nonstatistical evidence that the outlier is a bad value before it is discarded.
Sometimes we find that the outlier is an unusual but perfectly plausible observation. Deleting these points to "improve the fit of the equation" can be dangerous,

DETECTION AND TREATMENT OF OUTLIERS

153

as it can give the user a false sense of precision in estimation or prediction. Occasionally we find that the outlier is more important than the rest of the data because it may control many key model properties. Outliers may also point out inadequacies in the model, such as failure to fit the data well in a certain region of x space. If the outlier is a point of particularly desirable response (e.g., low cost, high yield), knowledge of the regressor values when that response was observed may be extremely valuable. Identification and follow-up analyses of outliers often result in process improvement or new knowledge concerning factors whose effect on the response was previously unknown.
Various statistical tests have been proposed for detecting and rejecting outliers. For example, see Barnett and Lewis [1994]. Stefansky [1971, 1972] has proposed an approximate test for identifying outliers based on the maximum normed residual ei / in=1 ei2 that is particularly easy to apply. Examples of this test and other related references are in Cook and Prescott [1981], Daniel [1976], and Williams [1973]. See also Appendix C.9. While these tests may be useful for identifying outliers, they should not be interpreted to imply that the points so discovered should be automatically rejected. As we have noted, these points may be important clues containing valuable information.
The effect of outliers on the regression model may be easily checked by dropping these points and refitting the regression equation. We may find that the values of the regression coefficients or the summary statistics such as the t or F statistic, R2, and the residual mean square may be very sensitive to the outliers. Situations in which a relatively small percentage of the data has a significant impact on the model may not be acceptable to the user of the regression equation. Generally we are happier about assuming that a regression equation is valid if it is not overly sensitive to a few observations. We would like the regression relationship to be embedded in all of the observations and not merely an artifice of a few points.

Example 4.7 The Rocket Propellant Data
Figure 4.13 presents the normal probability plot of the externally studentized residuals and the plot of the externally studentized residuals versus the predicted y^i for the rocket propellant data introduced in Example 2.1. We note that there are two large negative residuals that lie quite far from the rest (observations 5 and 6 in Table 2.1). These points are potential outliers. These two points tend to give the normal probability plot the appearance of one for skewed data. Note that observation 5 occurs at a relatively low value of age (5.5 weeks) and observation 6 occurs at a relatively high value of age (19 weeks). Thus, these two points are widely separated in x space and occur near the extreme values of x, and they may be influential in determining model properties. Although neither residual is excessively large, the overall impression from the residual plots (Figure 4.13) is that these two observations are distinctly from the others.
To investigate the influence of these two points on the model, a new regression equation is obtained with observations 5 and 6 deleted. A comparison of the summary statistics from the two models is given below.

154 MODEL ADEQUACY CHECKING

Percent

Normal Probability Plot (response is shear)
99

95 90
80 70 60 50 40 30 20
10 5

1

­3

­2

­1

0

1

2

3

Externally Studentized Residual

(a)

Versus Fits (response is shear)

1

Externally Studentized Residual

0

­1

­2

­3 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 Fitted Value (b)
Figure 4.13 Externally studentized residual plots for the rocket propellant data: (a) the normal probability plot; (b) residuals versus predicted y^i .

^0 ^1 R2
MSRes
se(^1 )

Observations 5 and 6 IN
2627.82 -37.15
0.9018 9244.59
2.89

Observations 5 and 6 OUT
2658.97 -37.69
0.9578 3964.63
1.98

Deleting points 5 and 6 has almost no effect on the estimates of the regression coef-
ficients. There has, however, been a dramatic reduction in the residual mean square, a moderate increase in R2, and approximately a one-third reduction in the standard error of ^1.

DETECTION AND TREATMENT OF OUTLIERS

155

Since the estimates of the parameters have not changed dramatically, we con-

clude that points 5 and 6 are not overly influential. They lie somewhat off the line

passing through the other 18 points, but they do not control the slope and intercept.

However, these two residuals make up approximately 56% of the residual sum of

squares. Thus, if these points are truly bad values and should be deleted, the preci-

sion of the parameter estimates would be improved and the widths of confidence

and prediction intervals could be substantially decreased.

Figure 4.14 shows the normal probability plot of the externally studentized resid-

uals and the plot of the externally studentized residuals versus y^i for the model with points 5 and 6 deleted. These plots do not indicate any serious departures from

assumptions.

Further examination of points 5 and 6 fails to reveal any reason for the unusually

low propellant shear strengths obtained. Therefore, we should not discard these two

points. However, we feel relatively confident that including them does not seriously

limit the use of the model.



Normal Probability Plot (response is shear)
99
95 90 80 70 60 50 40 30 20 10
5

Percent

1

­3

­2

­1

0

1

2

3

Externally Studentized Residual

(a)

Versus Fits (response is shear)
1.5
1.0

Externally Studentized Residual

0.5 0.0 ­0.5 ­1.0 ­1.5 ­2.0

­2.5 1600

1800

2000

2200

Fitted Value (b)

2400

2600

Figure 4.14 Residual plots for the rocket propellant data with observations 5 and 6 removed: (a) the normal probability plot; (b) residuals versus predicted y^i .

156 MODEL ADEQUACY CHECKING
4.5 LACK OF FIT OF THE REGRESSION MODEL
A famous quote attributed to George Box is "All models are wrong; some models are useful." This comment goes to heart of why tests for lack-of-fit are important. In basic English, lack-of-fit is "the terms that we could have fit to the model but chose not to fit." For example, only two distinct points are required to fit a straight line. If we have three distinct points, then we could fit a parabola (a second-order model). If we choose to fit only the straight line, then we note that in general the straight line does not go through all three points. We typically assume that this phenomenon is due to error. On the other hand, the true underlying mechanism could really be quadratic. In the process, what we claim to be random error is actually a systematic departure as the result of not fitting enough terms. In the simple linear regression context, if we have n distinct data points, we can always fit a polynomial of order up to n - 1. When we choose to fit a straight line, we give up n - 2 degrees of freedom to estimate the error term when we could have chosen to fit these other higher-order terms.
4.5.1 A Formal Test for Lack of Fit
The formal statistical test for the lack of fit of a regression model assumes that the normality, independence, and constant-variance requirements are met and that only the first-order or straight-line character of the relationship is in doubt. For example, consider the data in Figure 4.15. There is some indication that the straight-line fit is not very satisfactory. Perhaps, a quadratic term (x2) should be added, or perhaps another regressor should be added. It would be helpful to have a test procedure to determine if systematic lack of fit is present.
The lack-of-fit test requires that we have replicate observations on the response y for at least one level of x. We emphasize that these should be true replications, not just duplicate readings or measurements of y. For example, suppose that y is product viscosity and x is temperature. True replication consists of running ni separate experiments at x = xi and observing viscosity, not just running a single
30
25
y 20
15
10
0 01234567 x
Figure 4.15 Data illustrating lack of fit of the straight-line model.

LACK OF FIT OF THE REGRESSION MODEL

157

experiment at xi and measuring viscosity ni times. The readings obtained from the latter procedure provide information only on the variability of the method of measuring viscosity. The error variance 2 includes this measurement error and the
variability associated with reaching and maintaining the same temperature level in
different experiments. These replicated observations are used to obtain a modelindependent estimate of 2.
Suppose that we have ni observations on the response at the ith level of the regressor xi, i = 1, 2, . . . , m. Let yij denote the jth observation on the response at xi, i = 1, 2, . . . , m and j = 1, 2, . . . , ni. There are n = im=1ni total observations. The test procedure involves partitioning the residual sum of squares into two com-
ponents, say

SSRes = SSPE + SSLOF
where SSPE is the sum of squares due to pure error and SSLOF is the sum of squares due to lack of fit.
To develop this partitioning of SSRes, note that the (ij)th residual is

yij - y^i = ( yij - yi ) + ( yi - y^i )

(4.19)

where yi is the average of the ni observations at xi. Squaring both sides of Eq. (4.19) and summing over i and j yields

m ni

n ni

m

   ( yij - y^i )2 =

( yij - yi )2 + ni ( yi - y^i )2

i=1 j=1

i=1 j=1

i=1

(4.20)

since the cross-product term equals zero. The left-hand side of Eq. (4.20) is the usual residual sum of squares. The two
components on the right-hand side measure pure error and lack of fit. We see that the pure-error sum of squares

m ni

 SSPE =

( yij - yi )2

i=1 j=1

(4.21)

is obtained by computing the corrected sum of squares of the repeat observations at each level of x and then pooling over the m levels of x. If the assumption of constant variance is satisfied, this is a model-independent measure of pure error since only the variability of the y's at each x level is used to compute SSPE. Since there are ni - 1 degrees of freedom for pure error at each level xi, the total number of degrees of freedom associated with the pure-error sum of squares is

m
(ni - 1) = n - m
i=1

(4.22)

The sum of squares for lack of fit

158 MODEL ADEQUACY CHECKING

m
 SSLOF = ni ( yi - y^i )2 i=1

(4.23)

is a weighted sum of squared deviations between the mean response yi at each x level and the corresponding fitted value. If the fitted values y^i are close to the corresponding average responses yi, then there is a strong indication that the regression function is linear. If the y^i deviate greatly from the yi, then it is likely that the regression function is not linear.There are m - 2 degrees of freedom associated with SSLOF, since there are m levels of x and two degrees of freedom are lost because two
parameters must be estimated to obtain the yi. Computationally we usually obtain SSLOF by subtracting SSPE from SSRes.
The test statistic for lack of fit is

F0

=

SSLOF / (m - 2) SSPE / (n - m)

=

MSLOF MSPE

(4.24)

The expected value of MSPE is 2, and the expected value of MSLOF is

m
 ni [E ( yi ) - 0 - 1xi ]2

E (MSLOF ) =  2 + i=1

m-2

(4.25)

If the true regression function is linear, then E ( yi ) = 0 + 1xi, and the second term
of Eq. (4.25) is zero, resniting in E(MSLOF) = 2. However, if the true regression function is not linear, then E(yi)  0 + 1xi, and E(MSLOF) > 2. Furthermore, if the true regression function is linear, then the statistic F0 follows the Fm-2,n-m distribution. Therefore, to test for lack of fit, we would compute the test statistic F0 and conclude that the regression function is not linear if F0 > F,m-2,n-m.
This test procedure may be easily introduced into the analysis of variance conducted for significance of regression. If we conclude that the regression function is not linear, then the tentative model must be abandoned and attempts made to find a more appropriate equation. Alternatively, if F0 does not exceed F,m-2,n-m, there is no strong evidence of lack of fit, and MSPE and MSLOF are often combined to estimate 2.
Ideally, we find that the F ratio for lack of fit is not significant, and the hypothesis of significance of regression (H0: 1 = 0) is rejected. Unfortunately, this does not guarantee that the model will be satisfactory as a prediction equation. Unless the variation of the predicted values is large relative to the random error, the model is not estimated with sufficient precision to yield satisfactory predictions. That is, the model may have been fitted to the errors only. Some analytical work has been done on developing criteria for judging the adequacy of the regression model from a prediction point of view. See Box and Wetz [1973], Ellerton [1978], Gunst and Mason [1979], Hill, Judge, and Fomby [1978], and Suich and Derringer [1977]. The Box and Wetz work suggests that the observed F ratio must be at least four or five times the critical value from the F table if the regression model is to be useful as a predictor, that is, if the spread of predicted values is to be large relative to the noise.

LACK OF FIT OF THE REGRESSION MODEL

159

A relatively simple measure of potential prediction performance is found by comparing the range of the fitted values y^i (i.e., y^max - y^min) to their average standard error. It can be shown that, regardless of the form of the model, the average variance of the fitted values is

 Var( y^ )

=

1 n

n i=1

Var ( y^i )

=

p 2 n

(4.26)

where p is the number of parameters in the model. In general, the model is not
likely to be a satisfactory predictor unless the range of the fitted values y^i is large
relative to their average estimated standard error ( p^ 2 ) / n , where ^ 2 is a model-
independent estimate of the error variance.

Example 4.8 Testing for Lack of Fit The data from Figure 4.15 are shown below:

x 1.0

1.0

2.0

3.3

3.3

4.0

4.0

4.0

4.7

5.0

y 10.84 9.30 16.35 22.88 24.35 24.56 25.86 29.16 24.59 22.25

x 5.6

5.6

5.6

6.0

6.0

6.5

6.9

y 25.90 27.20 25.61 25.45 26.56 21.03 21.46

The straight-line fit is y^ = 13.301 + 2.108x, with SST = 487.6126, SSR = 234.7087, and SSRes = 252.9039. Note that there are 10 distinct levels of x, with repeat points at x = 1.0, x = 3.3, x = 4.0, x = 5.6, and x = 6.0. The pure-error sum of squares is
computed using the repeat points as follows:

Level of x
1.0 3.3 4.0 5.6 6.0 Total

 j ( yij - yi )2
1.1858 1.0805 11.2467 1.4341 0.6161 15.5632

Degrees of Freedom
1 1 2 2 1 7

The lack-of-fit sum of squares is found by subtraction as

SSLOF = SSRes - SSPE = 252.9039 - 15.5632 = 237.3407

with m - 2 = 10 - 2 = 8 degrees of freedom. The analysis of variance incorporating

the lack-of-fit test is shown in Table 4.4. The lack-of-fit test statistic is F0 = 13.34, and

since the P value is very small, we reject the hypothesis that the tentative model

adequately describes the data.



160 MODEL ADEQUACY CHECKING

TABLE 4.4 Analysis of Variance for Example 4.8

Source of Variation

Sum of Squares

Degrees of Freedom

Mean Square

Regression

234.7087

1

Residual

252.9039

15

(Lack of fit)

237.3407

8

(Pure error)

15.5632

7

Total

487.6126

16

234.7087 16.8603 29.6676
2.2233

F0 13.34

P Value 0.0013

Example 4.9 Testing for Lack of Fit in JMP

Some software packages will perform the lack of fit test automatically if there are

replicate observations in the data. In the patient satisfaction data of Appendix Table

B.17 there are replicate observations in the severity predictor (they occur at 30, 31,

38, 42, 28, and 50). Figure 4.16 is a portion of the JMP output that results from fitting

a simple linear regression model to these data. The F-test for lack of fit in Equation

4.24 is shown in the output. The P-value is 0.0874, so there is some mild indication

of lack of fit. Recall from Section 3.6 that when we added the second predictor (age)

to this model the quality of the overall fit improved considerably. As this example

illustrates, sometimes lack of fit is caused by missing regressors; it isn't always neces-

sary to add higher-order terms to the model.



4.5.2 Estimation of Pure Error from Near Neighbors
In Section 4.5.1 we described a test for lack of fit for the linear regression model. The procedure involved partitioning the error or residual sum of squares into a component due to "pure" error and a component due to lack of fit:
SSRes = SSPE + SSLOF
The pure-error sum of squares SSPE is computed using responses at repeat observations at the same level of x. This is a model-independent estimate of 2.
This general procedure can in principle be applied to any regression model. The calculation of SSPE requires repeat observations on the response y at the same set of levels on the regressor variables x1, x2, . . . , xk. That is, some of the rows of the X matrix must be the same. However, repeat observations do not often occur in multiple regression, and the procedure described in Section 4.5.1 is not often useful.
Daniel and Wood [1980] and Joglekar, Schuenemeyer, and La Riccia [1989] have investigated methods for obtaining a model-independent estimate of error when there are no exact repeat points. These procedures search for points in x space that are near neighbors, that is, sets of observations that have been taken with nearly

LACK OF FIT OF THE REGRESSION MODEL

161

Figure 4.16 JMP output for the simple linear regression model relating satisfaction to severity.

identical levels of x1, x2, . . . , xk. The responses yi from such near neighbors can be considered as repeat points and used to obtain an estimate of pure error. As a measure of the distance between any two points, for example, xi1, xi2, . . . , xik and xi1, xi 2, . . . , xik, we will use the weighted sum of squared distance (WSSD)

 Di2i

=

k j=1

  

^ j

( xij - xij
MSRes

)2
 

(4.27)

Pairs of points that have small values of Di2i are "near neighbors," that is, they are relatively close together in x space. Pairs of points for which Di2i is large (e.g., Di2i 1)
are widely separated in x space. The residuals at two points with a small value of Di2i can be used to obtain an estimate of pure error. The estimate is obtained from the range of the residuals at the points i and i, say

162 MODEL ADEQUACY CHECKING Ei = ei - ei

There is a relationship between the range of a sample from a normal population and the population standard deviation. For samples of size 2, this relationship is

^ = (1.128)-1 E = 0.886E

The quantity ^ so obtained is an estimate of the standard deviation of pure error. An efficient algorithm may be used to compute this estimate. A computer
program for this algorithm is given in Montgomery, Martin, and Peck [1980]. First arrange the data points xi1, xi2, . . . , xik in order of increasing y^i. Note that points with very different values of y^i cannot be near neighbors, but those with similar values of y^i could be neighbors (or they could be near the same contour of constant y^ but far apart in some x coordinates). Then:
1. Compute the values of Di2i for all n - 1 pairs of points with adjacent values of y^ . Repeat this calculation for the pairs of points separated by one, two, and three intermediate y^ values. This will produce 4n - 10 values of Di2i.
2. Arrange the 4n - 10 values of Di2i found in 1 above in ascending order. Let Eu, u = 1, 2, . . . , 4n - 10, be the range of the residuals at these points.
3. For the first m values of Eu, calculate an estimate of the standard deviation of pure error as

 ^

=

0.886 m

m u=1

Eu

(4.28)

Note that ^ is based on the average range of the residuals associated with the m smallest values of Di2i; m must be chosen after inspecting the values of Di2i. One should not include values of Eu in the calculations for which the weighted sum of squared distance is too large.

Example 4.10 The Delivery Time Data

We use the procedure described above to calculate an estimate of the standard

deviation of pure error for the soft drink delivery time data from Example 3.1. Table 4.5 displays the calculation of Di2i for pairs of points that, in terms of y^ , are adjacent,
one apart, two apart, and three apart. The R columns in this table identify the 15 smallest values of Di2i. The residuals at these 15 pairs of points are used to estimate . These calculations yield ^ = 1.969 and are summarized in Table 4.6. From Table

3.4, we find that MSRes = 10.6239 = 3.259. Now if there is no appreciable lack of

fit, we would expect to find that ^ = MSRes . In this case MSRes is about 65% larger

than ^ , indicating some lack of fit. This could be due to the effects of regressors not

presently in the model or the presence of one or more outliers.



TABLE 4.5 Calculation of Di2i for Example 4.10

Near-Neighbor Calculations Delta Residuals and the Weighted Standardized Squared Distances of Near Neighbors

Adjacent

1 Apart

2 Apart

3 Apart

Ordered

Observation Fitted, y Residual Delta

Di2i

Ra Delta

Di2i

R Delta

Di2i

R Delta

Di2i

R

7

7.155

.845

.949 .3524E + 00

4.080 .1001E + 01

.302 .4814E + 00

1.057 .1014E + 01

19

7.707 1.793 3.131 .2835E + 00 12 .647 .6594E + 00

2.006 .4989E + 00

1.843 .1800E + 01

4

9.956 4.924 3.778 .6275E + 00

5.137 .9544E - 01 3 4.974 .1562E + 01

3.897 .5965E + 00

2

10.354 1.146 1.359 .3412E + 00 15 1.196 .2805E + 00 11 .119 .2696E + 00 9 1.591 .2307E + 01

25

10.963 -.213

.163 .9489E + 00

1.240 .2147E + 00 6 .232 .9831E + 00

.649 .1032E + 01

3

12.080 -.050 1.077 .3865E + 00

.395 .2915E + 01

.486 .2594E + 01

3.498 .4775E + 01

13

12.473 1.027 1.471 .1198E + 01

.591 .1042E + 01

2.422 .2507E + 01

.130 .2251E + 01

5

14.194 -.444

.881 .4869E - 01 2 3.893 .2521E + 00 8 1.601 .3159E + 00 13 .155 .8768E + 00

17

14.914

.436 3.012 .3358E + 00 14 .720 .2477E + 00 7 .726 .5749E + 00

.631 .1337E + 01

18

15.551 3.449 2.292 .1185E + 00 5 3.738 .7636E + 00

2.381 .2367E + 01

1.072 .5341E + 01

8

16.673 1.157 1.446 .2805E + 00 10 .089 .1483E + 01

1.220 .4022E + 01

3.771 .2307E + 01

6

18.400 -.290 1.357 .5851E + 00

2.666 .2456E + 01

2.325 .2915E + 01

.303 .2470E + 01

14

18.682 1.068 1.309 .6441E + 00

3.682 .5952E + 01

1.661 .5121E + 01

6.096 .4328E + 00

10

19.124 2.376 4.991 .1036E + 02

2.969 .9107E + 01

7.404 .1023E + 01

1.705 .4412E + 01

21

20.514 -2.614 2.021 .1096E + 00 4 2.414 .5648E + 01

3.285 .2093E + 01

1.993 .2117E + 01

12

21.593 -.593 4.435 .4530E + 01

1.264 .1303E + 01

4.015 .1321E + 01

3.980 .4419E + 01

1

21.708 -5.028 5.699 .1227E + 01

.421 .1219E + 01

.455 .3553E + 00

4.365 .3121E + 01

15

23.329

.671 5.279 .7791E - 04 1 5.244 .9269E + 00

1.334 .2341E + 01

1.566 .1316E + 02

23

23.358 -4.608

.035 .9124E + 00

3.945 .2316E + 01

6.845 .1315E + 02

l.180 .1772E + 02

24

24.403 -4.573 3.910 .1370E + 01

6.810 .1578E + 02

1.215 .2026E + 02

.886 .8023E + 02

16

29.663 -.663 2.900 .8999E + 01

5.125 .1204E + 02

3.024 .6294E + 02

8.083 .1074E + 03

11

38.093 2.237 8.025 .3767E + 00

5.924 .2487E + 02

5.182 .5978E + 02

20

40.888 -5.788 2.101 .1994E + 02

13.208 .5081E + 02

22

56.007 -3.687 11.106 .1216E + 02

9

71.820 7.420

163

a Column R gives the rank order of the 15 smallest Di2i values.

164 MODEL ADEQUACY CHECKING

TABLE 4.6 Calculation of s^ for Example 4.10

Standard Deviation Estimated from Residuals of Neighboring Observations

Cumulative Standard

Ordered by Di2i

Number Deviation

Di2i

Observation Observation

1

.4677E + 01 .7791E - 04

15

23

2

.2729E + 01 .4859E - 01

5

17

3

.3336E + 01 .9544E - 01

4

25

4

.2950E + 01 .1096E + 00

21

12

5

.2766E + 01 .1185E + 00

18

8

6

.2488E + 01 .2147E + 00

25

13

7

.2224E + 01 .2477E + 00

17

8

8

.2377E + 01 .2521E + 00

5

18

9

.2125E + 01 .2696E + 00

2

13

10

.2040E + 01 .2805E + 00

8

6

11

.1951E + 01 .2805E + 00

2

3

12

.2020E + 01 .2835E + 00

19

4

13

.1973E + 01 .3159E + 00

5

8

14

.2023E + 01 .3358E + 00

17

18

15

.1969E + 01 .3412E + 00

2

25

16

.1898E + 01 .3524E + 00

7

19

17

.1810E + 01 .3553E + 00

1

24

18

.2105E + 01 .3767E + 00

11

20

19

.2044E + 01 .3865E + 00

3

13

20

.2212E + 01 .4328E + 00

14

1

21

.2119E + 01 .4814E + 00

7

2

22

.2104E + 01 .4989E + 00

19

25

23

.2040E + 01 .5749E + 00

17

6

24

.2005E + 01 .5851E + 00

6

14

25

.2063E + 01 .5965E + 00

4

13

26

.2113E + 01 .6275E + 00

4

2

27

.2077E + 01 .6441E + 00

14

10

28

.2024E + 01 .6594E + 00

19

2

29

.2068E + 01 .7636E + 00

18

6

30

.2004E + 01 .8768E + 00

5

6

31

.1940E + 01 .9124E + 00

23

24

32

.2025E + 01 .9269E + 00

15

24

33

.1968E + 01 .9489E + 00

25

3

34

.1916E + 01 .9831E + 00

25

5

35

.1964E + 01 .1001E + 01

7

4

36

.1936E + 01 .1014E + 01

7

25

37

.2061E + 01 .1023E + 01

10

1

38

.2022E + 01 .1032E + 01

25

17

39

.1983E + 01 .1042E + 01

13

17

40

.1966E + 01 .1198E + 01

13

5

Delta Residual
5.2788 0.8807 5.1369 2.0211 2.2920 1.2396 0.7203 3.8930 0.1194 1.4462 1.1962 3.1312 1.6010 3.0123 1.3590 0.9486 0.4552 8.0255 1.0768 6.0956 0.3018 2.0058 0.7259 1.3571 3.8973 3.7780 1.3089 0.6468 3.7382 0.1548 0.0347 5.2441 0.1628 0.2318 4.0797 1.0572 7.4045 0.6489 0.5907 1.4714

PROBLEMS 165
PROBLEMS
4.1 Consider the simple regression model fit to the National Football League team performance data in Problem 2.1. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Plot the residuals versus the team passing yardage, x2. Does this plot indicate that the model will be improved by adding x2 to the model?
4.2 Consider the multiple regression model fit to the National Football League team performance data in Problem 3.1. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Construct plots of the residuals versus each of the regressor variables. Do these plots imply that the regressor is correctly specified? d. Construct the partial regression plots for this model. Compare the plots with the plots of residuals versus regressors from part c above. Discuss the type of information provided by these plots. e. Compute the studentized residuals and the R-student residuals for this model. What information is conveyed by these scaled residuals?
4.3 Consider the simple linear regression model fit to the solar energy data in Problem 2.3. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response.
4.4 Consider the multiple regression model fit to the gasoline mileage data in Problem 3.5. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Construct and interpret the partial regression plots for this model. d. Compute the studentized residuals and the R-student residuals for this model. What information is conveyed by these scaled residuals?
4.5 Consider the multiple regression model fit to the house price data in Problem 3.7. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Construct the partial regression plots for this model. Does it seem that some variables currently in the model are not necessary? d. Compute the studentized residuals and the R-student residuals for this model. What information is conveyed by these scaled residuals?

166 MODEL ADEQUACY CHECKING
4.6 Consider the simple linear regression model fit to the oxygen purity data in Problem 2.7. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response.
4.7 Consider the simple linear regression model fit to the weight and blood pressure data in Problem 2.10. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Suppose that the data were collected in the order shown in the table. Plot the residuals versus time order and comment on the plot.
4.8 Consider the simple linear regression model fit to the steam plant data in Problem 2.12. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Suppose that the data were collected in the order shown in the table. Plot the residuals versus time order and comment on the plot.
4.9 Consider the simple linear regression model fit to the ozone data in Problem 2.13. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Plot the residuals versus time order and comment on the plot.
4.10 Consider the simple linear regression model fit to the copolyester viscosity data in Problem 2.14. a. Construct a normal probability plot of the unscaled residuals. Does there seem to be any problem with the normality assumption? b. Repeat part a using the studentized residuals. Is there any substantial difference in the two plots? c. Construct and interpret a plot of the residuals versus the predicted response.
4.11 Consider the simple linear regression model fit to the toluene­tetralin viscosity data in Problem 2.15. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response.
4.12 Consider the simple linear regression model fit to the tank pressure and volume data in Problem 2.16. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption?

PROBLEMS 167
b. Construct and interpret a plot of the residuals versus the predicted response. c. Suppose that the data were collected in the order shown in the table. Plot
the residuals versus time order and comment on the plot.
4.13 Problem 3.8 asked you to fit two different models to the chemical process data in Table B.5. Perform appropriate residual analyses for both models. Discuss the results of these analyses. Calculate the PRESS statistic for both models. Do the residual plots and PRESS provide any insight regarding the best choice of model for the data?
4.14 Problems 2.4 and 3.5 asked you to fit two different models to the gasoline mileage data in Table B.3. Calculate the PRESS statistic for these two models. Based on this statistic, which model is most likely to provide better predictions of new data?
4.15 In Problem 3.9, you were asked to fit a model to the tube-flow reactor data in Table B.6. a. Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. Construct the partial regression plots for this model. Does it seem that some variables currently in the model are not necessary?
4.16 In Problem 3.12, you were asked to fit a model to the clathrate formation data in Table B.8. a. Construct a normality plot of the residuals from the full model. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. In Problem 3.12, you were asked to fit a second model. Compute the PRESS statistic for both models. Based on this statistic, which model is most likely to provide better predictions of new data?
4.17 In Problem 3.14, you were asked to fit a model to the kinematic viscosity data in Table B.10. a. Construct a normality plot of the residuals from the full model. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response. c. In Problem 3.14, you were asked to fit a second model. Compute the PRESS statistic for both models. Based on this statistic, which model is most likely to provide better predictions of new data?
4.18 Coteron, Sanchez, Martinez, and Aracil ("Optimization of the Synthesis of an Analogue of Jojoba Oil Using a Fully Central Composite Design," Canadian Journal of Chemical Engineering, 1993) studied the relationship of reaction temperature x1, initial amount of catalyst x2, and pressure x3 on the yield of a synthetic analogue to jojoba oil. The following table summarizes the experimental results.

168 MODEL ADEQUACY CHECKING

xl

x2

x3

y

-1

-1

-1

17

1

-1

-1

44

-1

1

-1

19

1

1

-1

46

-1

-1

1

7

1

-1

1

55

-1

1

1

15

1

1

1

41

0

0

0

29

0

0

0

28.5

0

0

0

30

0

0

0

27

0

0

0

28

a. Perform a thorough analysis of the results including residual plots. b. Perform the appropriate test for lack of fit.
4.19 Derringer and Suich ("Simultaneous Optimization of Several Response Variables," Journal of Quality Technology, 1980) studied the relationship of an abrasion index for a tire tread compound in terms of three factors: x1, hydrated silica level; x2, silane coupling agent level; and x3, sulfur level. The following table gives the actual results.

x1

x2

x3

y

-1

-1

1

102

1

-1

-1

120

-1

1

-1

117

1

1

1

198

-1

-1

-1

103

1

-1

1

132

-1

1

1

132

1

1

-1

139

0

0

0

133

0

0

0

133

0

0

0

140

0

0

0

142

0

0

0

145

0

0

0

142

a. Perform a thorough analysis of the results including residual plots. b. Perform the appropriate test for lack of fit.
4.20 Myers Montgomery and Anderson-Cook (Response Surface Methodology 3rd edition, Wiley, New York, 2009) discuss an experiment to determine the influence of five factors:

PROBLEMS 169
x1--acid bath temperature x2--cascade acid concentration x3--water temperature x4--sulfide concentration x5--amount of chlorine bleach
on an appropriate measure of the whiteness of rayon (y). The engineers conducting this experiment wish to minimize this measure. The experimental results follow.

Acid Temp.
35 35 35 35 35 35 35 35 55 55 55 55 55 55 55 55 25 65 45 45 45 45 45 45 45 45

Acid Conc.
0.3 0.3 0.3 0.3 0.7 0.7 0.7 0.7 0.3 0.3 0.3 0.3 0.7 0.7 0.7 0.7 0.5 0.5 0.1 0.9 0.5 0.5 0.5 0.5 0.5 0.5

Water Temp.
82 82 88 88 82 82 88 88 82 82 88 88 82 82 88 88 85 85 85 85 79 91 85 85 85 85

Sulfide Conc.
0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.3 0.2 0.3 0.25 0.25 0.25 0.25 0.25 0.25 0.15 0.35 0.25 0.25

Amount of Bleach
0.3 0.5 0.5 0.3 0.5 0.3 0.3 0.5 0.5 0.3 0.3 0.5 0.3 0.5 0.5 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.2 0.6

y
76.5 76.0 79.9 83.5 89.5 84.2 85.7 99.5 89.4 97.5 103.2 108.7 115.2 111.5 102.3 108.1 80.2 89.1 77.2 85.1 71.5 84.5 77.5 79.2 71.0 90.2

a. Perform a thorough analysis of the results including residual plots. b. Perform the appropriate test for lack of fit.
4.21 Consider the test for lack of fit. Find E(MSPE) and E(MSLOF).
4.22 Table B.14 contains data on the transient points of an electronic inverter. Using only the regressors x1, . . . , x4, fit a multiple regression model to these data.

170 MODEL ADEQUACY CHECKING
a. Investigate the adequacy of the model. b. Suppose that observation 2 was recorded incorrectly. Delete this observa-
tion, refit the model, and perform a thorough residual analysis. Comment on the difference in results that you observe.
4.23 Consider the advertising data given in Problem 2.18. a. Construct a normal probability plot of the residuals from the full model. Does there seem to be any problem with the normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response.
4.24 Consider the air pollution and mortality data given in Problem 3.15 and Table B.15. a. Construct a normal probability plot of the residuals from the full model. Does there seem to be any problem with normality assumption? b. Construct and interpret a plot of the residuals versus the predicted response.
4.25 Consider the life expectancy data given in Problem 3.16 and Table B.16. a. For each model construct a normal probability plot of the residuals from the full model. Does there seem to be any problem with the normality assumption? b. For each model construct and interpret a plot of the residuals versus the predicted response.
4.26 Consider the multiple regression model for the patient satisfaction data in Section 3.6. Analyse the residuals from this model and comment on model adequacy.
4.27 Consider the fuel consumption data in Table B.18. For the purposes of this exercise, ignore regressor x1. Perform a thorough residual analysis of these data. What conclusions do you draw from this analysis?
4.28 Consider the wine quality of young red wines data in Table B.19. For the purposes of this exercise, ignore regressor x1. Perform a thorough residual analysis of these data. What conclusions do you draw from this analysis?
4.29 Consider the methanol oxidation data in Table B.20. Perform a thorough analysis of these data. What conclusions do you draw from this residual analysis?

CHAPTER 5
TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES
5.1 INTRODUCTION Chapter 4 presented several techniques for checking the adequacy of the linear regression model. Recall that regression model fitting has several implicit assumptions, including the following:
1. The model errors have mean zero and constant variance and are uncorrelated.
2. The model errors have a normal distribution--this assumption is made in order to conduct hypothesis tests and construct CIs--under this assumption, the errors are independent.
3. The form of the model, including the specification of the regressors, is correct. Plots of residuals are very powerful methods for detecting violations of these basic regression assumptions. This form of model adequacy checking should be conducted for every regression model that is under serious consideration for use in practice.
In this chapter, we focus on methods and procedures for building regression models when some of the above assumptions are violated. We place considerable emphasis on data transformation. It is not unusual to find that when the response and/or the regressor variables are expressed in the correct scale of measurement or metric, certain violations of assumptions, such as inequality of variance, are no longer present. Ideally, the choice of metric should be made by the engineer or scientist with subject-matter knowledge, but there are many situations where this
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
171

172

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

information is not available. In these cases, a data transformation may be chosen heuristically or by some analytical procedure.
The method of weighted least squares is also useful in building regression models in situations where some of the underlying assumptions are violated. We will illustrate how weighted least squares can be used when the equal-variance assumption is not appropriate. This technique will also prove essential in subsequent chapters when we consider other methods for handling nonnormal response variables.

5.2 VARIANCE-STABILIZING TRANSFORMATIONS
The assumption of constant variance is a basic requirement of regression analysis. A common reason for the violation of this assumption is for the response variable y to follow a probability distribution in which the variance is functionally related to the mean. For example, if y is a Poisson random variable in a simple linear regression model, then the variance of y is equal to the mean. Since the mean of y is related to the regressor variable x, the variance of y will be proportional to x. Variancestabilizing transformations are often useful in these cases. Thus, if the distribution of y is Poisson, we could regress y = y against x since the variance of the square root of a Poisson random variable is independent of the mean. As another example, if the response variable is a proportion (0  yi  1) and the plot of the residuals
( ) versus y^i has the double-bow pattern of Figure 4.5c, the arcsin transformation
y = sin-1 y is appropriate. Several commonly used variance-stabilizing transformations are summarized in
Table 5.1. The strength of a transformation depends on the amount of curvature that it induces. The transformations given in Table 5.1 range from the relatively mild square root to the relatively strong reciprocal. Generally speaking, a mild transformation applied over a relatively narrow range of values (e.g., ymax/ymin < 2, 3) has little effect. On the other hand, a strong transformation over a wide range of values will have a dramatic effect on the analysis.
Sometimes we can use prior experience or theoretical considerations to guide us in selecting an appropriate transformation. However, in many cases we have no a priori reason to suspect that the error variance is not constant. Our first indication of the problem is from inspection of scatter diagrams or residual analysis. In these cases the appropriate transformation may be selected empirically.

TABLE 5.1 Useful Variance-Stabilizing Transformations

Relationship of 2to E(y)

Transfonnation

2  constant 2  E(y)
2  E(y)[1 - E(y)] 2  [E(y)]2 2  [E(y)]3 2  [E(y)]4

y = y (no transformation)
y = y (square root; Poisson data)
( ) y = sin-1 y (arcsin; binomial proportions 0  yi  1)
y = ln(y)(log) y = y-1/2 (reciprocal square root) y = y-1(reciprocal)

VARIANCE-STABILIZING TRANSFORMATIONS

173

It is important to detect and correct a nonconstant error variance. If this problem is not eliminated, the least-squares estimators will still be unbiased, but they will no longer have the minimum-variance property. This means that the regression coefficients will have larger standard errors than necessary. The effect of the transformation is usually to give more precise estimates of the model parameters and increased sensitivity for the statistical tests.
When the response variable has been reexpressed, the predicted values are in the transformed scale. It is often necessary to convert the predicted values back to the original units. Unfortunately, applying the inverse transformation directly to the predicted values gives an estimate of the median of the distribution of the response instead of the mean. It is usually possible to devise a method for obtaining unbiased predictions in the original units. Procedures for producing unbiased point estimates for several standard transformations are given by Neyman and Scott [1960]. Miller [1984] also suggests some simple solutions to this problem. Confidence or prediction intervals may be directly converted from one metric to another, as these interval estimates are percentiles of a distribution and percentiles are unaffected by transformation. However, there is no assurance that the resulting intervals in the original units are the shortest possible intervals. For further discussion, see Land [1974].

Example 5.1 The Electric Utility Data

An electric utility is interested in developing a model relating peak-hour demand (y) to total energy usage during the month (x). This is an important planning problem because while most customers pay directly for energy usage (in kilowatthours), the generation system must be large enough to meet the maximum demand imposed. Data for 53 residential customers for the month of August are shown in Table 5.2, and a scatter diagram is given in Figure 5.1. As a starting point, a simple linear regression model is assumed, and the least-squares fit is

y^ = -0.8313 + 0.00368x

The analysis of variance is shown in Table 5.3. For this model R2 = 0.7046; that is, about 70% of the variability in demand is accounted for by the straight-line fit to energy usage. The summary statistics do not reveal any obvious problems with this model.
A plot of the R-student residuals versus the fitted values y^i is shown in Figure 5.2.The residuals form an outward-opening funnel, indicating that the error variance is increasing as energy consumption increases. A transformation may be helpful in correcting this model inadequacy. To select the form of the transformation, note that the response variable y may be viewed as a "count" of the number of kilowatts used by a customer during a particular hour. The simplest probabilistic model for count data is the Poisson distribution. This suggests regressing y* = y on x as a variancestabilizing transformation. The resulting least-squares fit is

y^* = 0.5822 + 0.0009529x

The R-student values from this least-squares fit are plotted against y^i* in Figure 5.3. The impression from examining this plot is that the variance is stable;

174

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

TABLE 5.2 Demand (y) and Energy Usage (x) Data for 53 Residential Cnstomers, August

Customer

x (kWh)

y (kW)

Customer

x (kWh)

1

679

0.79

27

837

2

292

0.44

28

1748

3

1012

0.56

29

1381

4

493

0.79

30

1428

5

582

2.70

31

1255

6

1156

3.64

32

1777

7

997

4.73

33

370

8

2189

9.50

34

2316

9

1097

5.34

35

1130

10

2078

6.85

36

463

11

1818

5.84

37

770

12

1700

5.21

38

724

13

747

3.25

39

808

14

2030

4.43

40

790

15

1643

3.16

41

783

16

414

0.50

42

406

17

354

0.17

43

1242

18

1276

1.88

44

658

19

745

0.77

45

1746

20

435

1.39

46

468

21

540

0.56

47

1114

22

874

1.56

48

413

23

1543

5.28

49

1787

24

1029

0.64

50

3560

25

710

4.00

51

1495

26

1434

0.31

52

2221

53

1526

y(kW)
4.20 4.88 3.48 7.58 2.63 4.99 0.59 8.19 4.79 0.51 1.74 4.10 3.94 0.96 3.29 0.44 3.24 2.14 5.71 0.64 1.90 0.51 8.33 14.94 5.11 3.85 3.93

Demand

15 14 13 12 11 10
9 8 7 6 5 4 3 2 1 0
0

1000 2000 3000 Usage

4000

Figure 5.1 Scatter diagram of the energy demand (kW) versus energy usage (kWh), Example 5.1.

VARIANCE-STABILIZING TRANSFORMATIONS

175

TABLE 5.3 Analysis of Variance for Regression of y on x for Example 5.1

Source of Variation
Regression Residual Total

Sum of Squares
303.6331 126.8660 429.4991

Degrees of Freedom
1 51 52

Mean Square
302.6331 2.4876

F0 121.66

P Value <0.0001

3
2
1 ti 0
-1
-2
-3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 yi
Figure 5.2 Plot of R-student values ti versus fitted values y^i, Example 5.1.

>

2

1

0 ti -1

-2

-3

-4

0

1

2

3

4

>

y *i

Figure 5.3 Plot of R-student values ti versus fitted values y^i* for the transformed data, Example 5.1.

consequently, we conclude that the transformed model is adequate. Note that there

is one suspiciously large residual (customer 26) and one customer whose energy

usage is somewhat large (customer 50). The effect of these two points on the fit

should be studied further before the model is released for use.



176

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

5.3 TRANSFORMATIONS TO LINEARIZE THE MODEL

The assumption of a linear relationship between y and the regressors is the usual starting point in regression analysis. Occasionally we find that this assumption is inappropriate. Nonlinearity may be detected via the lack-of-fit test described in Section 4.5 or from scatter diagrams, the matrix of scatterplots, or residual plots such as the partial regression plot. Sometimes prior experience or theoretical considerations may indicate that the relationship between y and the regressors is not linear. In some cases a nonlinear function can be linearized by using a suitable transformation. Such nonlinear models are called intrinsically or transformably linear.
Several linearizable functions are shown in Figure 5.4. The corresponding nonlinear functions, transformations, and resulting linear forms are shown in Table 5.4. When the scatter diagram of y against x indicates curvature, we may be able to match the observed behavior of the plot to one of the curves in Figure 5.4 and use the linearized form of the function to represent the data.
To illustrate a nonlinear model that is intrinsically linear, consider the exponential function
y = 0e1x
This function is intrinsically linear since it can be transformed to a straight line by a logarithmic transformation

ln y = ln 0 + 1x + ln  or

y = 0 + 1x +  
as shown in Table 5.4. This transformation requires that the transformed error terms  = ln  are normally and independently distributed with mean zero and variance 2. This implies that the multiplicative error  in the original model is log normally distributed. We should look at the residuals from the transformed model to see if the assumptions are valid. Generally if x and/or y are in the proper metric, the usual least-squares assumptions are more likely to be satisfied, although it is no unusual to discover at this stage that a nonlinear model is preferable (see Chapter 12).
Various types of reciprocal transformations are also useful. For example, the model

y

=

0

+

1 

1 x



+



can be linearized by using the reciprocal transformation x = 1/x. The resulting linearized model is

y = 0 + 1x + 

(0, 1,x all > 0) y
(a) 1 >1
1 =1

(0, x > 0, 1 < 0) y

- 1 < 1 < 0

(b)

1 = -1

1 < -1

y (e)

(1 > 0 )

(1 < 0 ) y
(f)

0 < 1 < 1

1

0

0

x

0

x

0

0

1

x

1

x

y (c)

(1 > 0 )

(1 < 0 )

(1 > 0 )

y

y

(d)

(g)

(1 < 0 ) (h)
y

0e 0 /e

1/0

0

x

0 0
1/1

0

x

1/1

1/0

0

x

1/0

x

1/0

Positive curvature

Nagative curvature

Figure 5.4 Linearizable functions. (From Daniel and Wood [1980], used with permission of the publisher.)

177

178

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

TABLE 5.4 Linearizable Functions and Corresponding Linear Form

Figure

Linearizable Function

Transformation

5.4a, b 5.4c, d 5.4e, f
5.4g, h

y = 0x1 y = 0e1x y = 0 + 1log x y= x
0x - 1

y = log y, x = log x

y = ln y

x = log x

y = 1 , x = 1

y

x

Linear Form
y = log 0 + 1x y = ln 0 + 1x y = 0 + 1x
y = 0 - 1x

Other models that can be linearized by reciprocal transformations are

1 y

=

0

+

1x

+



and

y=

x

0x - 1 + 

This last model is illustrated in Figures 5.4g, h. When transformations such as those described above are employed, the leastsquares estimator has least-squares properties with respect to the transformed data, not the original data. For additional reading on transformations, see Atkinson [1983, 1985], Box, Hunter, and Hunter [1978], Carroll and Ruppert [1985], Dolby [1963], Mosteller and Tukey [1977, Chs. 4­6], Myers [1990], Smith [1972], and Tukey [1957].

Example 5.2 The Windmill Data
A research engineer is investigating the use of a windmill to generate electricity. He has collected data on the DC output from his windmill and the corresponding wind velocity. The data are plotted in Figure 5.5 and listed in Table 5.5.
Inspection of the scatter diagram indicates that the relationship between DC output (y) and wind velocity (x) may be nonlinear. However, we initially fit a straight-line model to the data. The regression model is
y^ = 0.1309 + 0.2411x
The summary statistics for this model are R2 = 0.8745, MSRes = 0.0557, and F0 = 160.26 (the P value is <0.0001). Column A of Table 5.6 shows the fitted values and residuals obtained from this model. In Table 5.6 the observations are arranged in order of increasing wind speed. The residuals show a distinct pattern, that is, they move systematically from negative to positive and back to negative again as wind speed increases.
A plot of the residuals versus y^i is shown in Figure 5.6. This residual plot indicates model inadequacy and implies that the linear relationship has not captured all of the information in the wind speed variable. Note that the curvature that was

TRANSFORMATIONS TO LINEARIZE THE MODEL

179

3.0

DC output, y

2.0

1.0

0.0 02

4

6

8

10

Wind velocity, x

Figure 5.5 Plot of DC output y versus wind velocity x for the windmill data.

TABLE 5.5 Observed Values yi and Regressor Variable xi for Example 5.2

Observation Number, i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

Wind Velocity, xi (mph)
5.00 6.00 3.40 2.70 10.00 9.70 9.55 3.05 8.15 6.20 2.90 6.35 4.60 5.80 7.40 3.60 7.85 8.80 7.00 5.45 9.10 10.20 4.10 3.95 2.45

DC Output, yi
1.582 1.822 1.057 0.500 2.236 2.386 2.294 0.558 2.166 1.866 0.653 1.930 1.562 1.737 2.088 1.137 2.179 2.112 1.800 1.501 2.303 2.310 1.194 1.144 0.123

180

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

0.4

0.2

0.0

ei >

-0.2

-0.4

-0.6

Figure 5.6

0.4

0.8

1.2

1.6

2.0

2.4

yi

Plot of residuals ei versus fitted values y^i for the windmill data.

apparent in the scatter diagram of Figure 5.5 is greatly amplified in the residual plot. Clearly some other model form must be considered.
We might initially consider using a quadratic model such as

y = 0 + 1x + 2x2 + 

to account for the apparent curvature. However, the scatter diagram Figure 5.5 suggests that as wind speed increases, DC output approaches an upper limit of approximately 2.5. This is also consistent with the theory of windmill operation. Since the quadratic model will eventually bend downward as wind speed increases, it would not be appropriate for these data. A more reasonable model for the windmill data that incorporates an upper asymptote would be

y

=

0

+

1 

1 x



+



Figure 5.7 is a scatter diagram with the transformed variable x = 1/x. This plot appears linear, indicating that the reciprocal transformation is appropriate. The fitted regression model is

y^ = 2.9789 - 6.9345x

The summary statistics for this model are R2 = 0.9800, MSRes = 0.0089, and F0 = 1128.43 (the P value is <0.0001).
The fitted values and corresponding residuals from the transformed model are
shown in column B of Table 5.6. A plot of R-student values from the transformed model versus y^ is shown in Figure 5.8. This plot does not reveal any serious problem

TRANSFORMATIONS TO LINEARIZE THE MODEL

181

with inequality of variance. Other residual plots are satisfactory, and so because

there is no strong signal of model inadequacy, we conclude that the transformed

model is satisfactory.



3.0

DC output, y

2.0

1.0

0.0 0.10 0.20 0.30 0.40 0.50

x'

=

1 x

Figure 5.7 Plot of DC output versus x = 1/x for the windmill data.

TABLE 5.6 Observations yi Ordered by Increasing Wind Velocity, Fitted Values y^i, and Residuals ei for Both Models for Example 5.2

A. Straight-Line Model y^ = ^0 + ^1x

B. Transformed Model
y^ = ^0 + ^1(1 x)

Wind Velocity, xi DC Output yi

y^ i

ei

yi

ei

2.45 2.70 2.90 3.05 3.40 3.60 3.95 4.10 4.60 5.00 5.45 5.80 6.00 6.20 6.35 7.00 7.40 7.85 8.15 8.80 9.10 9.55 9.70 10.00 10.20

0.123 0.500 0.653 0.558 1.057 1.137 1.144 1.194 1.562 1.582 1.501 1.737 1.822 1.866 1.930 1.800 2.088 2.179 2.166 2.112 2.303 2.294 2.386 2.236 2.310

0.7217 0.7820 0.8302 0.8664 0.9508 0.9990 1.0834 1.1196 1.2402 1.3366 1.4451 1.5295 1.5778 1.6260 1.6622 1.8189 1.9154 2.0239 2.0962 2.2530 2.3252 2.4338 2.4700 2.5424 2.5906

-0.5987 -0.2820 -0.1772 -0.3084
0.1062 0.1380 0.0606 0.0744 0.3218 0.2454 0.0559 0.2075 0.2442 0.2400 0.2678 -0.0189 0.1726 0.1551 0.0698 -0.1410 -0.0223 -0.1398 -0.0840 -0.3064 -0.2906

0.1484 0.4105 0.5876 0.7052 0.9393 1.0526 1.2233 1.2875 1.4713 1.5920 1.7065 1.7832 1.8231 1.8604 1.8868 1.9882 2.0418 2.0955 2.1280 2.1908 2.2168 2.2527 2.2640 2.2854 2.2990

-0.0254 0.0895 0.0654
-0.1472 0.1177 0.0844
-0.0793 -0.0935
0.0907 -0.0100 -0.2055 -0.0462 -0.0011
0.0056 0.0432 -0.1882 0.0462 0.0835 0.0380 -0.0788 0.0862 -0.1472 0.1220 -0.0494 0.0110

182

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

2

1

0 t i -1

-2

-3

0

1

2

3

y^i

Figure 5.8 Plot of R-student values ti versus fitted values y^i for the transformed model for the windmill data.

5.4 ANALYTICAL METHODS FOR SELECTING A TRANSFORMATION
While in many instances transformations are selected empirically, more formal, objective techniques can be applied to help specify an appropriate transformation. This section will discuss and illustrate analytical procedures for selecting transformations on both the response and regressor variables.

5.4.1 Transformations on y: The Box-Cox Method

Suppose that we wish to transform y to correct nonnormality and/or nonconstant

variance. A useful class of transformations is the power transformation y, where 

is

a

parameter

to

be

determined

(e.g., 

=

1 2

means

use

y as the response). Box and

Cox [1964] show how the parameters of the regression model and  can be estimated

simultaneously using the method of maximum likelihood.

In thinking about the power transformation y a difficulty arises when  = 0;

namely, as  approaches zero, y approaches unity. This is obviously a problem, since

it is meaningless to have all of the response values equal to a constant. One approach

to solving this difficulty (we call this a discontinuity at  = 0) is to use (y - 1)/ as

the response variable. This· solves the discontinuity problem, because as  tends to

zero, (y - 1)/ goes to a limit of ln y. However, there is still a problem, because as

 changes, the values of (y - 1)/ change dramatically, so it would be difficult to

compare model summary statistics for models with different values of .

The appropriate procedure is to use

y( )

=

  

y - 1  y-1

,

y ln y,

0 =0

(5.1)

where

y

=

[ ln-1 1

n



n i=1

ln

yi ]

is

the

geometric

mean

of

the

observations,

and

fit

the

model

y() = Xb + e

(5.2)

ANALYTICAL METHODS FOR SELECTING A TRANSFORMATION

183

by least squares (or maximum likelihood). The divisor y-1 turns out to be related to the Jacobian of the transformation converting the response variable y into y(). It is, in effect, a scale factor that ensures that residual sums of squares for models with different values of  are comparable.
Computational Procedure The maximum-likelihood estimate of  corresponds to the value of  for which the residual sum of squares from the fitted model SSRes() is a minimum. This value of  is usually determined by fitting a model to y() for various values of , plotting the residual sum of squares SSRes() versus , and then reading the value of  that minimizes SSRes() from the graph. Usually 10­20 values of  are sufficient for estimation of the optimum value. A second iteration can be performed using a finer mesh of values if desired. As noted above, we cannot select  by directly comparing residual sums of squares from the regressions of y on x because for each  the residual sum of squares is measured on a different scale. Equation (5.1) scales the responses so that the residual sums of squares are directly comparable. We recommend that the analyst use simple choices for , as the practical difference in the fits for  = 0.5 and  = 0.596 is likely to be small, but the former is much easier to interpret.
Once a value of  is selected, the analyst is now free to fit the model using y as the response if   0. If  = 0, then use ln y as the response. It is entirely acceptable to use y() as the response for the final model--this model will have a scale difference and an origin shift in comparison to the model using y (or ln y). In our experience, most engineers and scientists prefer using y (or ln y) as the response.
An Approximate Confidence Interval for  We can also find an approximate CI for the transformation parameter . This CI can be useful in selecting the final value for ; for example, if ^ = 0.596 is the minimizing value for the residual sum of squares, but if  = 0.5 is in the CI, then one might prefer to use the square-root transformation on the basis that it is easier to explain. Furthermore, if  = 1 is in the CI, then no transformation may be necessary.
In applying the method of maximum likelihood to the regression model, we are essentially maximizing

L

(

)

=

-

1 2

n

ln

[SSRes

(

)]

(5.3)

or equivalently, we are minimizing the residual-sum-of-squares function SSRes(). An approximate 100(1 - ) percent CI for  consists of those values of  that satisfy the inequality

( ) L

^

-

L()



1 2

2 ,1

n

(5.4)

where 2,1 is the upper  percentage point of the chi-square distribution with one degree of freedom. To actually construct the CI, we would draw, on a plot of L() versus  a horizontal line at height

( ) L

^

-

1 2

2 ,1

184

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

on the vertical scale. This line would cut the curve of L() at two points, and the location of these two points on the  axis defines the two end points of the approximate CI. If we are minimizing the residual sum of squares and plotting SSRes() versus , then the line must be plotted at height

( ) SS* = SSRes ^ e2,1 n

(5.5)

Remember that ^ is the value of  that minimizes the residual sum of squares.

In actually applying the CI procedure, one is likely to find that the factor

exp(2,1 n) on the right-hand side of Eq. (5.5) is replaced by either 1 + z2 2 n or 1 + t2 2, n or 1 + 2,1 n, or perhaps either 1 + z2 2  or 1 + t2 2,  or 1 + 2,1  , where
 is the number of residual degrees of freedom. These are based on the expansion

of

exp(x)

=

1

+

x

+

x2/2!

+

x3/3!

+

.

.

.

=

1

+

x

and

the

fact

that



2 1

=

z2



t 2

unless

the

number of residual degrees of freedom  is small. It is perhaps debatable whether

we should use n or , but in most practical cases, there will be very little difference

between the CIs that result.

Example 5.3 The Electric Utility Data

Recall the electric utility data introduced in Example 5.1. We use the Box-Cox procedure to select a variance-stabilizing transformation. The values of SSRes() for various values of  are shown in Table 5.7. This display indicates that  = 0.5 (the square-root transformation) is very close to the optimum value. Notice that we have used a finer "grid" on  in the vicinity of the optimum. This is helpful in locating the optimum  more precisely and in plotting the residual-sum-of-squares function.
A graph of the residual sum of squares versus  is shown in Figure 5.9. If we take  = 0.5 as the optimum value, then an approximate 95% CI for  may be found by calculating the critical sum of squares SS* from Eq. (5.5) as follows:
( ) SS* = SSRes ^ e02.05,1 n
= 96.9495e3.84 53
= 96.9495(1.0751)
= 104.23

The horizontal line at this height is shown in Figure 5.9. The corresponding values

of - = 0.26 and + = 0.80 read from the curve give the lower and upper confidence

limits for , respectively. Since these limits do not include the value 1 (implying

no transformation), we conclude that a transformation is helpful. Furthermore,

the square-root transformation that was used in Example 5.1 has an analytic

justification.



5.4.2 Transformations on the Regressor Variables
Suppose that the relationship between y and one or more of the regressor variables is nonlinear but that the usual assumptions of normally and independently distributed responses with constant variance are at least approximately satisfied. We want

ANALYTICAL METHODS FOR SELECTING A TRANSFORMATION

185

TABLE 5.7 Values of the Residual Sum of Squares for Various Values of , Example 5.3



SSRes()

-2 -1 -0.5 0 0.125 0.25 0.375 0.5 0.625 0.75 1 2

34,101.0381 986.0423 291.5834 134.0940 118.1982 107.2057 100.2561 96.9495 97.2889 101.6869 126.8660
1,275.5555

300

SSRes()

200
SS* = 104.62 100

0

-2

-1

0

1

2



Figure 5.9 Plot of residual sum of squares SSRes() versus .

to select an appropriate transformation on the regressor variables so that the relationship between y and the transformed regressor is as simple as possible. Box and Tidwell [1962] describe an analylical procedure for determining the form of the transformation on x. While their procedure may be used in the general regression situation, we will present and illustrate its application to the simple linear regression model.
Assume that the response variable y is related to a power of the regressor, say  = xa, as

186

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

E (y) = f (, 0, 1 ) = 0 + 1

where



=

x 

,

ln x

a0 a=0

and 0, 1, and  are unknown parameters. Suppose that 0 is an initial guess of the constant . Usually this first guess is 0 = 1, so that 0 = x0 = x, or that no transformation at all is applied in the first iteration. Expanding about the initial guess in a
Taylor series and ignoring terms of higher than first order gives

{ } E (y) = f (0, 0, 1 ) + ( - 0 )

df (, 0, 1 )
d

=0

 =0

{ } = 0 + 1x + ( - 1)

df (, 0, 1 )
d

=0

 =0

(5.6)

Now if the term in braces in Eq. (5.6) were known, it could be treated as an additional regressor variable, and it would be possible to estimate the parameters 0, 1, and  in Eq. (5.6) by least squares. The estimate of  could be taken as an improved estimate of the transformation parameter. The term in braces in Eq. (5.6) can be written as

{ } { } df (, 0, 1) d

=0  =0

=

 df  

(, 0,
d

1

)

   =0

d d  =0

and since the form of the transformation is known, that is,  = x, we have d/d = x ln x. Furthermore,

 df  

(, 0,
d

1

)

  

= 0

=

d(0 + 1x)
dx

= 1

This parameter may be conveniently estimated by fitting the model

y^ = ^0 + ^1x

(5.7)

by least squares. Then an "adjustment" to the initial guess 0 = 1 may be computed by defining a second regressor variable as w = x ln x, estimating the parameters in

E ( y) = 0* + 1*x + ( - 1)1w = 0* + 0*x +  w

(5.8)

by least squares, giving

y^ = ^0* + ^1* + ^w

(5.9)

and taking

ANALYTICAL METHODS FOR SELECTING A TRANSFORMATION

187

1

=

^ ^ 1

+

1

(5.10)

as the revised estimate of . Note that ^1 is obtained from Eq. (5.7) and ^ from Eq. (5.9); generally ^1 and ^1* will differ. This procedure may now be repeated using a new regressor x = x1 in the calculations. Box and Tidwell [1962] note that this procedure usually converges quite rapidly, and often the first-stage result  is a satisfactory estimate of . They also caution that round-off error is potentially a problem and successive values of  may oscillate wildly unless enough decimal
places are carried. Convergence problems may be encountered in cases where the error standard deviation  is large or when the range of the regressor is very small
compared to its mean. This situation implies that the data do not support the need
for any transformation.

Example 5.4 The Windmill Data

We will illustrate this procedure using the windmill data in Example 5.2. The scatter diagram in Figure 5.5 suggests that the relationship between DC output (y) and wind speed (x) is not a straight line and that some transformation on x may be appropriate.
We begin with the initial guess 0 = 1 and fit a straight-line model, giving y^ = 0.1309 + 0.2411x. Then defining w = x ln x, we fit Eq. (5.8) and obtain
y^ = ^0* + ^1*x + ^w = -2.4168 + 1.5344x - 0.4626w

From Eq. (5.10) we calcnlate

1

=

^ ^ 1

+

1

=

-0.4626 0.2411

+

1

=

-0.92

as the improved estimate of . Note that this estimate of  is very close to -1, ·so that the reciprocal transformation on x actually used in Example 5.2 is supported by the Box-Tidwell procedure.
To perform a second iteration, we would define a new regressor variable x = x-0.92 and fit the model

y^ = ^0 + ^1x = 3.1039 - 6.6784x

Then a second regressor w = x ln x is formed and we fit

y^ = ^0* + ^1*x + ^w = 3.2409 - 6.445x + 0.5994w

The second-step estimate of  is thus

2

=

^ ^ 1

+ 1

=

0.5994 -6.6784

=

(-0.92)

=

-1.01

which again supports the use of the reciprocal transformation on x.



188

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

5.5 GENERALIZED AND WEIGHTED LEAST SQUARES

Linear regression models with nonconstant error variance can also be fitted by the method of weighted least squares. In this method of estimation the deviation between the observed and expected values of yi is multiplied by a weight wi chosen inversely proportional to the variance of yi. For the case of simple linear regression, the weighted least-squares function is

n
 S (0, 1 ) = wi ( yi - 0 - 1xi )2 i=1

(5.11)

The resulting least-squares normal equations are

n

n

n

   ^0 wi + ^1 wi xi = wi yi

i=1

i=1

i=1

n

n

n

   0 wi xi + ^1 wi xi2 = wi xi yi

i=1

i=1

i=1

(5.12)

Solving Eq. (5.12) will produce weighted least-squares estimates of 0 and 1. In this section we give a development of weighted least squares for the multiple regression model. We begin by considering a slightly more general situation concerning the structure of the model errors.

5.5.1 Generalized Least Squares
The assumptions usually made concerning the linear regression model y = X +  are that E() = 0 and that Var() = 2I. As we have observed, sometimes these assumptions are unreasonable, so that we will now consider what modifications to these in the ordinary least-squares procedure are necessary when Var() = 2V, where V is a known n × n matrix. This situation has an easy interpretation; if V is diagonal but with unequal diagonal elements, then the observations y are uncorrelated but have unequal variances, while if some of the off-diagonal elements of V are nonzero, then the observations are correlated.
When the model is

y = Xb + e
E (e ) = 0, Var (e ) =  2V

(5.13)

the ordinary least-squares estimator b^ = (XX)-1 Xy is no longer appropriate. We
will approach this problem by transforming the model to a new set of observations
that satisfy the standard least-squares assumptions. Then we will use ordinary least squares on the transformed data. Since 2V is the covariance matrix of the errors, V must be nonsingular and positive definite, so there exists an n × n nonsingular symmetric matrix K, where KK = KK = V. The matrix K is often called the square

GENERALIZED AND WEIGHTED LEAST SQUARES

189

root of V. Typically, 2 is unknown, in which case V represents the assumed structure of the variances and covariances among the random errors apart from a constant.
Define the new variables

z = K-1y, B = K-1X, g = K-1e

(5.14)

so that the regression model y = X +  becomes K-1y = K-1X + K-1, or

z = Bb + g

(5.15)

The errors in this transformed model have zero expectation, that is, E(g) = K-1E() = 0. Furthermore, the covariance matrix of g is

Var (g) = {[g - E (g)][g - E (g)]} = E (gg)
= E (K-1e e K-1 )
= K-1E (e e )K-1
=  2K-1VK-1 =  2K-1KKK-1
= 2I

(5.16)

Thus, the elements of g have mean zero and constant variance and are uncorrelated. Since the errors g in the model (5.15) satisfy the usual assumptions, we may apply ordinary least squares. The least-squares function is

S (b ) = gg = e V-1e = (y - Xb )V-1 (y - Xb )

(5.17)

The least-squares normal equations are
(XV-1X) b^ = XV-1y

(5.18)

and the solution to these equations is

( ) b^ = XV-1X -1 XV-1y

(5.19)

Here b^ is called the generalized least-squares estimator of . It is not difficult to show that b^ is an unbiased estimator of . The covariance
matrix of b^ is

( ) Var b^ ( ) =  2 (BB)-1 =  2 XV-1X -1

(5.20)

Appendix C.11 shows that b^ is the best linear unbiased estimator of . The analysis of variance in terms of generalized least squares is summarized in Table 5.8.

190

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

TABLE 5.8 Analysis of Variance for Generalized Least Squares

Source

Sum of Squares

Degrees of Freedom

Mean Square

Regression Error Total

SSR = b^ Bz
( ) = yV-1X XV-1X -1 XV-1y
SSRes = zz - b^ Bz
= yV-1y
( ) - yV-1X XV-1X -1 XV-1y
zz = yV-1y

p

SSR/p

n-p

SSRes/ (n - p)

n

F0 MSR/MSRes

5.5.2 Weighted Least Squares

When the errors  are uncorrelated but have unequal variances so that the covariance matrix of  is

1

 

w1

0

 





1





2

V

=



2

 

w2

 





 0

1

 



wn 

say, the estimation procedure is usually called weighted least squares. Let W = V-1. Since V is a diagonal matrix, W is also diagonal with diagonal elements or weights w1, w2 . . . , wn. From Eq. (5.18), the weighted least-squares normal equations are

(XWX) b^ = XWy

This is the multiple regression analogue of the weighted least-squares normal equations for simple linear regression given in Eq. (5.12). Therefore,

b^ = (XWX)-1 XWy

is the weighted least-squares estimator. Note that observations with large variances will have smaller weights than observations with small variances.
Weighted least-squares estimates may be obtained easily from an ordinary leastsquares computer program. If we multiply each of the observed values for the ith observation (including the 1 for the intercept) by the square root of the weight for that observation, then we obtain a transformed set of data:

1 w1 

B

=

1 

w2



1 wn

x11 w1 x21 w2
xn1 wn

x1k w1  

x2k

w2

 

,



xnk wn 

 y1 w1 





z

=

 

y2

w2  





 yn wn 

GENERALIZED AND WEIGHTED LEAST SQUARES

191

Now if we apply ordinary least squares to these transformed data, we obtain
b^ = (BB)-1 Bz = (XWX)-1 XWy

the weighted least-squares estimate of . Both JMP and Minitab will perform weighted least squares. SAS will do weighted
least squares. The user must specify a "weight" variable, for example, w. To perform weighted least squares, the user adds the following statement after the model statement:

weight w;

5.5.3 Some Practical Issues

To use weighted least squares, the weights wi must be known. Sometimes prior knowledge or experience or information from a theoretical model can be used to determine the weights (for an example of this approach, see Weisberg [1985]). Alternatively, residual analysis may indicate that the variance of the errors may be a function of one of the regressors, say Var(i) = 2xij, so that wi = 1/xij. In some cases yi is actually an average of ni observations at xi and if all original observations have constant variance 2, then the variance of yi is Var(yi) = Var(i) = 2/ni, and we would choose the weights as wi = ni. Sometimes the primary source of error is measurement error and different observations are measured by different instruments of unequal but known (or well-estimated) accuracy. Then the weights could be chosen inversely proportional to the variances of measurement error. In many practical cases we may have to guess at the weights, perform the analysis, and then reestimate the weights based on the results. Several iterations may be necessary.
Since generalized or weighted least squares requires making additional assumptions regarding the errors, it is of interest to ask what happens when we fail to do this and use ordinary least squares in a situation where Var() = 2V with V  I. If
ordinary least squares is used in this case, the resulting estimator b^ = (XX)-1 Xy is
still unbiased. However, the ordinary least-squares estimator is no longer a minimumvariance estimator. That is, the covariance matrix of the ordinary least-squares estimator is

( ) Var b^ =  2 (XX)-1 XVX (XX)-1

(5.21)

and the covariance matrix of the generalized least-squares estimator (5.20) gives smaller variances for the regression coefficients. Thus, generalized or weighted least squares is preferable to ordinary least squares whenever V  I.

Example 5.5 Weighted Least Squares
The average monthly income from food sales and the corresponding annual advertising expenses for 30 restaurants are shown in columns a and b of Table 5.9. Management is interested in the relationship between these variables, and so a linear regression model relating food sales y to advertising expense x is fit by ordinary least squares, resulting in y^ = 49, 443.3838 + 8.0484x. The residuals from this

192

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

TABLE 5.9 Restaurant Food Sales Data

Obs. i (a) Income, Yi

(b) Advertising Expense, Xi

(c) x

1

81,464

2

72,661

3

72,344

4

90,743

5

98,588

6

96,507

7

126,574

8

114,133

9

115,814

10

123,181

11

131,434

12

140,564

13

151,352

14

146,926

15

130,963

16

144,630

17

147,041

18

179,021

19

166,200

20

180,732

21

178,187

22

185,304

23

155,931

24

172,579

25

188,851

26

192,424

27

203,112

28

192,482

29

218,715

30

214,317

3,000 3,150 3,085 5,225 5,350 6,090 8,925 9,015 8,885 8,950 9,000 1,1345 12,275 12,400 12,525 12,310 13,700 15,000 15,175 14,995 15,050 15,200 15,150 16,800 16,500 17,830 19,500 19,200 19,000 19,350

3,078.3 5,287.5 8,955.0 12,171.0
15,095.0
16,650.0 19,262.5

(d) sy2 26,794,616 30,772,013 52,803,695
59,646,475
120,571,061
132,388,992 138,856,871

(e) Weights, wi
6.21771 E-08 5.79507 E-08 5.97094 E-08 2.98667 E-08 2,90195 E-08 2.48471 E-08 1.60217 E-08 1.58431 E-08 1.61024 E-08 1.59717 E-08 1.58726 E-08 1.22942 E-08 1.12852 E-08 1.11621 E-08 1.10416 E-08 1.12505 E-08 1.00246 E-08 9.09750 E-09 8.98563 E-09 9.10073 E-09 9.06525 E-09 8.96987 E-09 9.00144 E-09 8.06478 E-09 8.22030 E-09 7.57287 E-09 6.89136 E-09 7.00460 E-09 7.08218 E-09 6.94752 E-09

least-squares fit are plotted against y^i in Figure 5.10. This plot indicates violation of the constant-variance assumption. Consequently, the ordinary least-squares fit is inappropriate.
To correct this inequality-of-variance problem, we must know the weights wi. We note from examining the data in Table 5.9 that there are several sets of x values that are "near neighbors," that is, that have approximate repeat points on x. We will assume that these near neighbors are close enough to be considered repeat points and use the variance of the responses at those repeat points to investigate how
Var(y) changes with x. Columns c and d of Table 5.9 show the average x value (x)
for each cluster of near neighbors and the sample variance of the y's in each cluster. Plotting sy2 against the corresponding x implies that sy2 increases approximately linearly with x. A least-squares fit gives

20000

GENERALIZED AND WEIGHTED LEAST SQUARES

193

10000

ei

0

-10000

-20000 50000

>

100000 150000 200000 250000 yi

Figure 5.10 Plot of ordinary least-squares residuals versus fitted values, Example 5.5.

s^y2 = -9, 226, 002 + 7781.626x
Substituting each xi value into this equation will give an estimate of the variance of the corresponding observation yi. The inverse of these fitted values will be reasonable estimates of the weights wi. These estimated weights are shown in column e of Table 5.9.
Applying weighted least squares to the data using the weights in Table 5.9 gives the fitted model

y^ = 50, 974.564 + 7.92224x

We must now examine the residuals to determine if using weighted least squares

has improved the fit. To do this, plot the weighted residuals wi1 2ei = wi1 2 (yi - y^ ),
where y^i comes from the weighted least-squares fit, against wi1 2 y^i. This plot is shown in Figure 5.11 and is much improved when compared to the previous plot for the

ordinary least-squares fit. We conclude that weighted least squares has corrected

the inequality-of-variance problem.

Two other points concerning this example should be made. First, we were fortu-

nate to have several near neighbors in the x space. Furthermore, it was easy to

identify these clusters of points by inspection of Table 5.9 because there was only

one regressor involved. With several regressors visual identification of these clusters

would be more difficult. Recall that an analytical procedure for finding pairs of

points that are close together in x space was presented in Section 4.5.3. The second

point involves the use of a regression equation to estimate the weights. The analyst

should carefully check the weights produced by the equation to be sure that they

are reasonable. For example, in our problem a sufficiently small x value could result

in a negative weight, which is clearly unreasonable.



194

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

2

1

0 wi1/2 ei
-1

-2

-3

15

16

17

18

19

>

ui1/2 yi

Figure 5.11 Plot of weighted residuals wi1 2ei versus weighted fitted values wi1 2y^i, Example 5.5.

5.6 REGRESSION MODELS WITH RANDOM EFFECTS

5.6.1 Subsampling
Random effects allow the analyst to take into account multiple sources of variability. For example, many people use simple paper helicopters to illustrate some of the basic principles of experimental design. Consider a simple experiment to determine the effect of the length of the helicopter's wings to the typical flight time. There often is quite a bit of error associated with measuring the time for a specific flight of a helicopter, especially when the people who are timing the flights have never done this before. As a result, a popular protocol for this experiment has three people timing each flight to get a more accurate idea of its actual flight time. In addition, there is quite a bit of variability from helicopter to helicopter, particularly in a corporate short course where the students have never made these helicopters before. This particular experiment thus has two sources of variability: within each specific helicopter and between the various helicopters used in the study.
A reasonable model for this experiment is

yij = 0 + 1xi + i + ij (i = 1, 2, ... , m and j = 1, 2, ... , ri )

(5.22)

where m is the number of helicopters, ri is the number of measured flight times for

the ith helicopter, yij is the flight time for the jth flight of the ith helicopter, xi is the

length of the wings for the ith helicopter, i is the error term associated with the ith

helicopter, and ij is the random error associated with the jth flight of the ith helicopter.

The key point is that there are two sources of variability represented by i and ij.

Typically, we would assume that the is are independent and normally distributed

with

a

mean

of

0

and

a

constant

variance



2 

,

that

the

ijs

are

independent

and

nor-

mally distributed with mean 0 and constant variance 2, and that the is and the ijs

REGRESSION MODELS WITH RANDOM EFFECTS

195

are independent. Under these assumptions, the flight times for a specific helicopter

are correlated. The flight times across helicopters are independent.

Equation (5.22) is an example of a mixed model that contains fixed effects, in

this case the xis, and random effects, in this case the is and the ijs. The units used

for a specific random effect represent a random sample from a much larger popula-

tion of possible units. For example, patients in a biomedical study often are random

effects. The analyst selects the patients for the study from a large population of

possible people. The focus of all statistical inference is not on the specific patients

selected; rather, the focus is on the population of all possible patients. The key point

underlying all random effects is this focus on the population and not on the specific

units selected for the study. Random effects almost always are categorical.

The data collection method creates the need for the mixed model. In some sense,

our standard regression model y = X +  is a mixed model with  representing the

fixed effects and  representing the random effects. More typically, we restrict the

term mixed model to the situations where we have more than one error term.

Equation (5.22) is the standard model when we have multiple observations on a

single unit. Often we call such a situation subsampling. The experimental protocol

creates the need for two separate error terms. In most biomedical studies we have

several observations for each patient. Once again, our protocol creates the need for

two error terms: one for the observation-to-observation differences within a patient

and another error term to explain the randomly selected patient-to-patient

differences.

In the subsampling situation, the total number of observations in the study,

n

=



m i=1

ri .

Equation

(5.22)

in

matrix

form

is

y = Xb + Zd + e

where Z is a n × m "incidence" matrix and  is a m × 1 vector of random helicopterto-helicopter errors. The form of Z is

1r1 0 ... 0 

Z

=

 

0

1r2 ...

0

 





 0 0 ... 1rm 

where 1i is a ri × 1 vector of ones. We can establish that

Var

(y

)

=



2

I

+



2 

ZZ.

The matrix ZZ is block diagonal with each block consisting of a ri × ri matrix of ones. The net consequence of this model is that one should use generalized least squares to estimate . In the case that we have balanced data, where there are the same number of observations per helicopter, then the ordinary least squares estimate of  is exactly the same as the generalized least squares estimate and is the best linear unbiased estimate. As a result, ordinary least squares is an excellent way to estimate the model. However, there are serious issues with any inference based on the usual ordinary least squares methodology because it does not reflect the

196

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

helicopter-to-helicopter variability. This important source of error is missing from the usual ordinary least squares analysis. Thus, while it is appropriate to use ordinary least squares to estimate the model, it is not appropriate to do the standard ordinary least squares inference on the model based on the original flight times. To do so would be to ignore the impact of the helicopter-to-helicopter error term. In the balanced case and only in the balanced case, we can construct exact F and t tests. It can be shown (see Exercise 5.19) that the appropriate error term is based on
SSsubsample = y Z (ZZ)-1 Z - X (XX)-1 X y,
which has m - p degrees of freedom. Basically, this error term uses the average flight times for each helicopter rather than the individual flight times. As a result, the generalized least squares analysis is exactly equivalent to doing an ordinary least squares analysis on the average flight time for each helicopter. This insight is important when using the software, as we illustrate in the next example.
If we do not have balance, then we recommend residual maximum likelihood, also known as restricted maximum likelihood (REML) as the basis for estimation and inference (see Section 5.6.2). In the unbalanced situation there are no best linear unbiased estimates of . The inference based on REML is asymptotically efficient.

Example 5.6 The Helicopter Subsampling Study
Table 5.10 summarizes data from an industrial short course on experimental design that used the paper helicopter as a class exercise. The class conducted a simple 22 factorial experiment replicated a total of twice. As a result, the experiment required a total of eight helicopters to see the effect of "aspect," which was the length of the body of a paper helicopter, and "paper," which was the weight of the paper, on the flight time. Three people timed the each helicopter flight, which yields three flight times for each flight. The variable Rep is necessary to do the proper analysis on the original flight times. The table gives the data in the actual run order.
The Minitab analysis of the original flight times requires three steps. First, we can do the ordinary least squares estimation of the model to get the estimates of the model coefficients. Next, we need to re-analyze the data to get the estimate of the proper error variance. The final step requires us to update the t statistics from the first step to reflect the proper error term.
Table 5.11 gives the analysis for the first step. The estimated model is correct. However, the R2, the t statistics, the F statistics and their associated P values are all incorrect because they do not reflect the proper error term.
The second step creates the proper error term. In so doing, we must use the General Linear Model functionality within Minitab. Basically, we treat the factors and their interaction as categorical. The model statement to generate the correct error term is:
aspect paper aspect*paper rep(aspect paper)
One then must list rep as a random factor. Table 5.12 gives the results. The proper error term is the mean squared for rep(aspect paper).

REGRESSION MODELS WITH RANDOM EFFECTS

197

TABLE 5.10 The Helicopter Subsampling Data

Helicopter Aspect Paper Interaction Rep

1

1

-1

-1

1

1

1

-1

-1

1

1

1

-1

-1

1

2

-1

-1

1

1

2

-1

-1

1

1

2

-1

-1

1

1

3

-1

1

-1

1

3

-1

1

-1

1

3

-1

1

-1

1

4

-1

1

-1

2

4

-1

1

-1

2

4

-1

1

-1

2

5

1

1

1

1

5

1

1

1

1

5

1

1

1

1

6

1

-1

-1

2

6

1

-1

-1

2

6

1

-1

-1

2

7

1

1

1

2

7

1

1

1

2

7

1

1

1

2

8

-1

-1

1

2

8

-1

-1

1

2

8

-1

-1

1

2

Time
3.60 3.85 3.98 6.44 6.37 6.78 6.84 6.90 7.18 6.37 6.38 6.58 3.44 3.43 3.75 3.75 3.73 4.10 4.59 4.64 5.02 6.50 6.33 6.92

TABLE 5.11 Minitab Analysis for the First Step of the Helicopter Subsampling Data

The regression equation is time = 5.31 - 1.32 aspect + 0.115 paper + 0.0396 inter

Predictor Constant aspect paper inter

Coef 5.31125 -1.32125 0.11542 0.03958

SE Coef 0.08339 0.08339 0.08339 0.08339

T 63.69 -15.84
1.38 0.47

P 0.000 0.000 0.182 0.640

S = 0.408541

R-Sq = 92.7%

R-Sq(adj) = 91.6%

Analysis of Variance

Source

DF

Regression 3

Residual 20

Error

Total

23

SS 42.254
3.338
45.592

MS

F

P

14.085 84.39 0.000

0.167

198

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

TABLE 5.12 Minitab Analysis for the Second Step of the Helicopter Subsampling Data

Analysis of Variance for time, using Adjusted SS for Tests

Source

DF Seq SS Adj SS Adj MS

aspect

1 41.8968 41.8968 41.8968

paper

1 0.3197 0.3197 0.3197

aspect*paper

1 0.0376 0.0376 0.0376

rep(aspect paper) 4 2.6255 2.6255 0.6564

Error

16 0.7126 0.7126 0.0445

Total

23 45.5923

S = 0.211039 R-Sq = 98.44% R-Sq(adj) = 97.75%

F 63.83
0.49 0.06 14.74

P 0.001 0.524 0.823 0.000

The third step is to correct the t statistics from the first step. The mean squared residual from the first step is 0.167. The correct error variance is 0.6564. Both of these values are rounded, which is all right, but it will lead to small differences when we do a correct one-step procedure based on the average flight times. Let tj be the t-statistic for the first-step analysis for the jth estimated coefficient, and let tc,j be the corrected statistic given by

tc, j =

0.167 0.6564 t j .

These t statistics have the degrees of freedom associated with rep(aspect paper), which in this case is 4. Table 5.13 gives the correct t statistics and P values. We note that the correct t statistics are smaller in absolute value than for the first-step analysis. This result reflects the fact that the error variance in the first step is too small since it ignores the helicopter-to-helicopter variability. The basic conclusion is that aspect seems to be the only important factor, which is true in both the first-step analysis and the correct analysis. It is important to note, however, that this equivalence does not hold in general. Regressors that appear important in the first-step analysis often are not statistically significant in the correct analysis.
An easier way to do this analysis in Minitab recognizes that we do have a balanced situation here because we have exactly three times for each helicopter's flight. As a result, we can do the proper analysis using the average time for each helicopter flight. Table 5.14 summarizes the data. Table 5.15 gives the analysis from Minitab, which apart from rounding reflects the same values as Table 5.12. We can do a full
 residual analysis of these data, which we leave as an exercise for the reader.

5.6.2 The General Situation for a Regression Model with a Single Random Effect
The balanced subsampling problem discussed in Section 5.6.1 is common. This section extends these ideas to the more general situation when there is a single random effect in our regression model.
For example, suppose an environmental engineer postulates that the amount of a particular pollutant in lakes across the Commonwealth of Virginia depends upon the water temperature. She takes water samples from various randomly selected

REGRESSION MODELS WITH RANDOM EFFECTS

199

TABLE 5.13 Correct t Statistics and P Values for the Helicopter Subsampling Data

Factor

t

P Value

Constant aspect paper Aspect*paper

32.12515 -7.98968 0.60607
0.237067

0.000 0.001 0.525 0.824

TABLE 5.14 Average Flight Times for the Helicopter Subsampling Data

Helicopter Aspect Paper Interaction Average Time

1

1

-1

-1

3.810

2

-1

-1

1

6.530

3

-1

1

-1

6.973

4

-1

1

-1

6.443

5

1

1

1

3.540

6

1

-1

-1

3.860

7

1

1

1

4.750

8

-1

-1

1

6.583

TABLE 5.15 Final Minitab Analysis for the Helicopter Experiment in Table 5.14

The regression equation is Average Time = 5.31 - 1.32 Aspect + 0.115 Paper + 0.040 Aspect*Paper

Predictor Constant Aspect Paper Aspect*Paper
S = 0.467748

Coef 5.3111 -1.3211 0.1154 0.0396
R-Sq = 94.1%

SE Coef 0.1654 0.1654 0.1654 0.1654
R-Sq(adj) = 89.8%

T 32.12 -7.99
0.70 0.24

P 0.000 0.001 0.524 0.822

Analysis of Variance

Source

DF

Regression

3

Residual Error

4

Total

7

SS 14.0820
0.8752 14.9572

MS 4.6940 0.2188

F 21.45

P 0.006

locations for several randomly selected lakes in Virginia. She records the water temperature at the time of the sample was taken. She then sends the water sample to her laboratory to determine the amount of the particular pollutant present. There are two sources of variability: location-to-location within a lake and lake-to-lake. This point is important. A heavily polluted lake is likely to have much higher amount of the pollutant across all of its locations than a lightly polluted lake.
The model given by Equation (5.22) provides a basis for analyzing these data. The water temperature is a fixed regressor. There are two components to the

200

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

variability in the data: the random lake effect and the random location within

the

lake

effect.

Let

2

be

the

variance

of

the

location

random

effect,

and

let



2 

be

the variance of the lake random effect.

Although we can use the same model for this lake pollution example as

the subsampling experiment, the experimental contexts are very different. In the

helicopter experiment, the helicopter is the fundamental experimental unit, which

is the smallest unit to which we can apply the treatment. However, we realize

that there is a great deal of variability in the flight times for a specific helicopter.

Thus, flying the helicopter several times gives us a better idea about the typical

flying time for that specific helicopter.The experimental error looks at the variability

among the experimental units. The variability in the flying times for a specific

helicopter is part of the experimental error, but it is only a part. Another component

is the variability in trying to replicate precisely the levels for the experimental

factors. In the subsampling case, it is pretty easy to ensure that the number of

subsamples (in the helicopter case, the flights) is the same, which leads to the

balanced case.

In the lake pollution case, we have a true observational study. The engineer is

taking a single water sample at each location. She probably uses fewer randomly

selected locations for smaller lakes, and more randomly selected lakes from larger-

lakes. In addition, it is not practical for her to sample from every lake in Virginia.

On the other hand, it is very straightforward for her to select randomly a series of

lakes for testing. As a result, we expect to have different number of locations for

each lake; hence, we expect to see an unbalanced situation.

We recommend the use of REML for the unbalanced case. REML is a very

general method for analysis of statistical models with random effects represented

by the model terms i and ij in Equation (5.22). Many software packages use REML

to estimate the variance components associated with the random effects in mixed

models like the model for the paper helicopter experiment. REML then uses an

iterative procedure to pursue a weighted least squares approach for estimating the

model. Ultimately, REML uses the estimated variance components to perform

statistical tests and construct confidence intervals for the final estimated model.

REML operates by dividing the parameter estimation problem into two parts. In

the first stage the random effects are ignored and the fixed effects are estimated,

usually by ordinary least squares. Then a set of residuals from the model is con-

structed and the likelihood function for these residuals is obtained. In the second

stage the maximum likelihood estimates for the variance components are obtained

by maximizing the likelihood function for the residuals. The procedure then takes

the estimated variance components to produce an estimate of the variance of y,

which it then uses to reestimate the fixed effects. It then updates the residuals and

the estimates of the variance components. The procedure continues to some con-

vergence criterion. REML always assumes that the observations are normally dis-

tributed because this simplifies setting up the likelihood function.

REML estimates have all the properties of maximum likelihood. As a result, they

are asymptotically unbiased and minimum variance. There are several ways to

determine the degrees of freedom for the maximum likelihood estimates in REML,

and some controversy about the best way to do this, but a full discussion of these

issues is beyond the scope of this book. The following example illustrates the use of

REML for a mixed effects regression model.

REGRESSION MODELS WITH RANDOM EFFECTS

201

Parameter Estimates

Term

Estimate Std Error DFDen t Ratio Prob>|t|

Intercept 2.0754319 1.356914 6.667 1.53 0.1721

cases 1.7148234 0.1868 21.97 9.18 <.0001*

dist

0.0120317 0.003797 21.9 3.17 0.0045*

REML Variance Component Estimates

Random

Var

Effect Var Ratio Component Std Error 95% Lower 95% Upper Pct of Total

city

0.2946232 2.5897428 3.4964817 ­4.263235 9.442721

22.757

Residual

8.7900161 2.8169566 5.1131327 18.546137

77.243

Total

11.379759

100.000

­2 LogLikelihood = 136.68398351

Figure 5.12 JMP results for the delivery time data treating city as a random effects.

Example 5.7 The Delivery Time Data Revisited
We introduced the delivery time data in Example 3.1. In Section 4.2.6 we observed that the first seven observations were collected from San Diego, observations 8­17 from Boston, observations 18­23 from Austin, and observations 24 and 25 from Minneapolis.
It is not unreasonable to assume that the cities used in this study represent a random sample of cities across the country. Ultimately, our interest is the impact of the number of cases deliveryed and the distance required to make the delivery on the delivery times over the entire country. As a result, a proper analysis needs to consider the impact of the random city effect on this analysis.
Figure 5.12 summarizes the analysis from JMP. We see few differences in the parameter estimates between the mixed model analysis that did not include the city's factor, given in Example 3.1. The P values for cases and distance are larger but only slightly so. The intercept P value is quite a bit larger. Part of this change is due to the significant decrease in the effective degrees of freedom for the intercept effect as the result of using the city information. The plot of the actual delivery times versus the predicted shows that the model is reasonable. The variance component for city is approximately 2.59. The variance for the residual error is 8.79. In the original analysis of Example 3.1 is 10.6. Clearly, part of the variability from the Example 3.1 analysis considered purely random is due to systematic variability due to the various cities, which the REML reflects through the cities' variance component.
The SAS code to analyze these data is:
proc mixed cl; class city; model time = cases distance / ddfm=kenwardroger s; random city;
run;
The following R code assumes that the data are in the object deliver. Also, one must load the package nlme in order to perform this analysis.

202

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

deliver.model <- lme(timecases+dist, random=1|city, data=deliver) print(deliver.model)

R reports the estimated standard deviations rather than the variances. As a result,

one needs to square the estimates to get the same results as SAS and JMP.



5.6.3 The Importance of the Mixed Model in Regression
Classical regression analysis always has assumed that there is only one source of variability. However, the analysis of many important experimental designs often has required the use of multiple sources of variability. Consequently, analysts have been using mixed models for many years to analyze experimental data. However, in such cases, the investigator typically planned a balanced experiment, which made for a straightforward analysis. REML evolved as a way to deal with imbalance primarily for the analysis of variance (ANOVA) models that underlie the classical analysis of experimental designs.
Recently, regression analysts have come to understand that there often are multiple sources of error in their observational studies. They have realized that classical regression analysis falls short in taking these multiple error terms in the analysis. They have realized that the result often is the use of an error term that understates the proper variability.The resulting analyses have tended to identify more significant factors than the data truly justifies.
We intend this section to be a short introduction to the mixed model in regression analysis. It is quite straightforward to extend what we have done here to more complex mixed models with more error terms. We hope that this presentation will help readers to appreciate the need for mixed models and to see how to modify the classical regression model and analysis to accommodate more complex error structures. The modification requires the use of generalized least squares; however, it is not difficult to do.

PROBLEMS

5.1 Byers and Williams ("Viscosities of Binary and Ternary Mixtures of Polyaromatic Hydrocarbons," Journal of Chemical and Engineering Data, 32, 349­354, 1987) studied the impact of temperature (the regressor) on the viscosity (the response) of toluene-tetralin blends. The following table gives the data for blends with a 0.4 molar fraction of toluene.

Temperature (°C)
24.9 35.0 44.9 55.1 65.2 75.2 85.2 95.2

Viscosity (mPa · s)
1.133 0.9772 0.8532 0.7550 0.6723 0.6021 0.5420 0.5074

PROBLEMS 203

a. Plot a scatter diagram. Does it seem likely that a straight-line model will be adequate?
b. Fit the straight-line model. Compute the summary statistics and the residual plots. What are your conclusions regarding model adequacy?
c. Basic principles of physical chemistry suggest that the viscosity is an exponential function of the temperature. Repeat part b using the appropriate transformation based on this information.
5.2 The following table gives the vapor pressure of water for various temperatures.

Temperature (°K)
273 283 293 303 313 323 333 343 353 363 373

Vapor Pressure (mm Hg)
4.6 9.2 17.5 31.8 55.3 92.5. 149.4 233.7 355.1 525.8 760.0

a. Plot a scatter diagram. Does it seem likely that a straight-line model will be adequate?
b. Fit the straight-line model. Compute the summary statistics and the residual plots. What are your conclusions regarding model adequacy?
c. From physical chemistry the Clausius-Clapeyron equation states that

ln

(

p

)



-

1 T

Repeat part b using the appropriate transformation based on this information.
5.3 The data shown below present the average number of surviving bacteria in a canned food product and the minutes of exposure to 300°F heat.

Number of Bacteria
175 108 95 82 71 50

Minutes of Exposure
1 2 3 4 5 6
(Continued)

204

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

Number of Bacteria
49 31 28 17 16 11

Minutes of Exposure
7 8 9 10 11 12

a. Plot a scatter diagram. Does it seem likely that a straight-line model will be adequate?
b. Fit the straight-line model. Compute the summary statistics and the residual plots. What are your conclusions regarding model adequacy?
c. Identify an appropriate transformed model for these data. Fit this model to the data and conduct the usual tests of model adequacy.
5.4 Consider the data shown below. Construct a scatter diagram and suggest an appropriate form for the regression model. Fit this model to the data and conduct the standard tests of model adequacy.

x 10

15

18

12

9

8

11

6

y

0.17

0.13

0.09

0.15

0.20

0.21

0.18 0.24

5.5 A glass bottle manufacturing company has recorded data on the average number of defects per 10,000 bottles due to stones (small pieces of rock embedded in the bottle wall) and the number of weeks since the last furnace overhaul. The data are shown below.

Defects per 10,000
13.0 16.1 14.5 17.8 22.0 27.4 16.8

Weeks
4 5 6 7 8 9 10

Defects per 10,000
34.2 65.6 49.2 66.2 81.2 87.4 114.5

Weeks
11 12 13 14 15 16 17

a. Fit a straight-line regression model to the data and perform the standard tests for model adequacy.
b. Suggest an appropriate transformation to eliminate the problems encountered in part a. Fit the transformed model and check for adequacy.
5.6 Consider the fuel consumption data in Table B.18. For the purposes of this exercise, ignore regressor x1. Recall the thorough residual analysis of these data from Exercise 4.27. Would a transformation improve this analysis? Why or why not? If yes, perform the transformation and repeat the full analysis.

PROBLEMS 205

5.7 Consider the methanol oxidation data in Table B.20. Perform a thorough analysis of these data. Recall the thorough residual analysis of these data from Exercise 4.29. Would a transformation improve this analysis? Why or why not? If yes, perform the transformation and repeat the full analysis.
5.8 Consider the three models a. y = 0 + 1(1/x) +  b. 1/y = 0 + 1x +  c. y = x/(0 - 1x) +  All of these models can be linearized by reciprocal transformations. Sketch the behavior of y as a function of x. What observed characteristics in the scatter diagram would lead you to choose one of these models?
5.9 Consider the clathrate formation data in Table B.8. a. Perform a thorough residual analysis of these data. b. Identify the most appropriate transformation for these data. Fit this model and repeat the residual analysis.
5.10 Consider the pressure drop data in Table B.9. a. Perform a thorough residual analysis of these data. b. Identify the most appropriate transformation for these data. Fit this model and repeat the residual analysis.
5.11 Consider the kinematic viscosity data in Table B.10. a. Perform a thorough residual analysis of these data. b. Identify the most appropriate transformation for these data. Fit this model and repeat the residual analysis.
5.12 Vining and Myers ("Combining Taguchi and Response Surface Philosophies: A Dual Response Approach," Journal of Quality Technology, 22, 15­22, 1990) analyze an experiment, which originally appeared in Box and Draper [1987]. This experiment studied the effect of speed (x1), pressure (x2), and distance (x3) on a printing machine's ability to apply coloring inks on package labels. The following table summarizes the experimental results.

i

xi x2 x3

yi1

yi2

yi3

yi

si

1 -1 -1 -1 34 10

28

24.0 12.5

2 0 -1 -1 115 116 130 120.3 8.4

3 1 -1 -1 192 186 263 213.7 42.8

4 -1 0 -1 82 88

88

86.0

3.7

5 0 0 -1 44 178 188 136.7 80.4

6 1 0 -1 322 350 350 340.7 16.2

7 -1 1 -1 141 110

86 112.3 27.6

8 0 1 -1 259 251 259 256.3 4.6

9 1 1 -1 290 280 245 271.7 23.6

10 -1 -1 0 81 81 81 81.0 0.0

11 0 -1 0 90 122 93 101.7 17.7

12 1 -1 0 319 376 376 357.0 32.9

(Continued)

206

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

i

xi x2 x3

yi1

yi2

yi3

yi

si

13 -1 0 14 0 0 15 1 0 16 -1 1 17 0 1 18 1 1 19 -1 -1 20 0 -1 21 1 -1 22 -1 0 23 0 0 24 1 0 25 -1 1 26 0 1 27 1 1

0 180 180 154 171.3 15.0 0 372 372 372 372.0 0.0 0 541 568 396 501.7 92.5 0 288 192 312 264.0 63.5 0 432 336 513 427.0 88.6 0 713 725 754 730.7 21.1 1 364 99 199 220.7 133.8 1 232 221 266 239.7 23.5 1 408 415 443 422.0 18.5 1 182 233 182 199.0 29.4 1 507 515 434 485.3 44.6 1 846 535 640 673.7 158.2 1 236 126 168 176.7 55.5 1 660 440 403 501.0 138.9 1 878 991 1161 1010.0 142.5

a. Fit an appropriate modal to each respone and conduct the residual analysis.
b. Use the sample variances as the basis for weighted least-squares estimation of the original data (not the sample means).
c. Vining and Myers suggest fitting a linear model to an appropriate transformation of the sample variances. Use such a model to develop the appropriate weights and repeat part b.
5.13 Schubert et al. ("The Catapult Problem: Enhanced Engineering Modeling Using Experimental Design," Quality Engineering, 4, 463­473, 1992) conducted an experiment with a catapult to determine the effects of hook (x1), arm length (x2), start angle (x3), and stop angle (x4) on the distance that the catapult throws a ball. They threw the ball three times for each setting of the factors. The following table summarizes the experimental results.

x1 x2 x3 x4

y

-1 -1 -1 -1 28.0 27.1 26.2 -1 -1 1 1 46.3 43.5 46.5 -1 1 -1 1 21.9 21.0 20.1 -1 1 1 -1 52.9 53.7 52.0 1 -1 -1 1 75.0 73.1 74.3 1 -1 1 -1 127.7 126.9 128.7 1 1 -1 -1 86.2 86.5 87.0 1 1 1 1 195.0 195.9 195.7

a. Fit a first-order regression model to the data and conduct the residual analysis.
b. Use the sample variances as the basis for weighted least-squares estimation of the original data (not the sample means).

PROBLEMS 207
c. Fit an appropriate model to the sample variances (note: you will require an appropriate transformation!). Use this model to develop the appropriate weights and repeat part b.
5.14 Consider the simple linear regression model yi = 0 + 1xi + i, where the vari-
ance of i is proportional to xi2, that is, Var (i ) =  2 xi2.
a. Suppose that we use the transformations y = y/x and x = l/x. Is this a variance-stabilizing transformation?
b. What are the relationships between the parameters in the original and transformed models?
c. Suppose we use the method of weighted least squares with wi = 1 xi2. Is this equivalent to the transformation introduced in part a?
5.15 Suppose that we want to fit the no-intercept model y = x +  using weighted least squares. Assume that the observations are uncorrelated but have unequal variances. a. Find a general formula for the weighted least-squares estimator of . b. What is the variance of the weighted least-squares estimator? c. Suppose that Var(yi) = cxi, that is, the variance of yi is proportional to the corresponding xi. Using the results of parts a and b, find the weighted leastsquares estimator of  and the variance of this estimator.
d. Suppose that Var (yi ) = cxi2, that is, the variance of yi is proportional to
the square of the corresponding xi. Using the results of parts a and b, find the weighted least-squares estimator of  and the variance of this estimator.
5.16 Consider the model
y = X1b1 + X2b2 + e
where E() = 0 and Var() = 2V. Assume that 2 and V are known. Derive an appropriate test statistic for the hypotheses
H0 : b2 = 0, H1 : b2  0
Give the distribution under both the null and alternative hypotheses.
5.17 Consider the model
y = Xb + e
where E() = 0 and Var() = 2V. Assume that V is known but not 2. Show that
( ) ( ) yV-1y - yV-1X XV-1X -1 XV-1y (n - p)
is an unbiased estimate of 2.

208

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

5.18 Table B.14 contains data on the transient points of an electronic inverter. Delete the second observation and use x1 - x4 as regressors. Fit a multiple regression model to these data. a. Plot the ordinary residuals, the studentized residuals, and R-student versus the predicted response. Comment on the results. b. Investigate the utility of a transformation on the response variable. Does this improve the model? c. In addition to a transformation on the response, consider transformations on the regressors. Use partial regression or partial residual plots as an aid in this study.
5.19 Consider the following subsampling model:

yij = 0 + 1xi + i + ij (i = 1, 2, ... , m and j = 1, 2, ... , r)

(5.22)

where m is the number of helicopters, r is the number of measured flight times

for each helicopter, yij is the flight time for the jth flight of the ith helicopter, xi is the length of the wings for the ith helicopter, i is the error term associated with the ith helicopter, and ij is the random error associated with the jth flight of the ith helicopter. Assume that the is are independent and normally dis-

tributed

with

a

mean

of

0

and

a

constant

variance



2 

,

that

the

ijs

are

inde-

pendent and normally distributed with mean 0 and constant variance 2, and

that the is and the ijs are independent. The total number of observations in

this

study

is

n

=



m i=1

ri .

This

model

in

matrix

form

is

y = Xb + Zd + e

where Z is a n × m "incidence" matrix and  is a m × 1 vector of random helicopter-to-helicopter errors. The form of Z is

1r 0 ... 0 

Z

=

 

0

1r

...

0

 





 0 0 ... 1r 

where 1r is a r × 1 vector of ones. a. Show that

Var

(y

)

=



2

I

+



2 

ZZ.

b. Show that the ordinary least squares estimates of  are the same as the generalized least squares estimates.
c. Derive the appropriate error term for testing the regression coefficients.

5.20 The fuel consumption data in Appendix B.18 is actually a subsampling problem. The batches of oil are divided into two. One batch went to the bus, and the other batch went to the truck. Perform the proper analysis of these data.

PROBLEMS 209
5.21 A construction engineer studied the effect of mixing rate on the tensile strength of portland cement. From each batch she mixed, the engineer made four test samples. Of course, the mix rate was applied to the entire batch. The data follow. Perform the appropriate analysis.

Mix Rate(rpm)
150 175 200 225

3129 3200 2800 2600

Tensile Strength(lb/in2)

3000 3300 2900 2700

3065 2975 2985 2600

3190 3150 3050 2765

5.22 A ceramic chemist studied the effect of four peak kiln temperatures on the density of bricks. Her test kiln could hold five bricks at a time. Two samples, each from different peak temperatures, broke before she could test their density. The data follow. Perform the appropriate analysis.

Temp.

Density

900

21.8 21.9 21.7 21.6 21.7

910

22.7 22.4 22.5 22.4

920

23.9 22.8 22.8 22.6 22.5

930

23.4 23.2 23.3 22.9

5.23 A paper manufacturer studied the effect of three vat pressures on the strength of one of its products. Three batches of cellulose were selected at random from the inventory. The company made two production runs for each pressure setting from each batch. As a result, each batch produced a total of six production runs. The data follow. Perform the appropriate analysis.

Batch
A A A A A A B B B B B B

Pressure
400 400 500 500 600 600 400 400 500 500 600 600

Strength
198.4 198.6 199.6 200.4 200.6 200.9 197.5 198.1 198.7 198.0 199.6 199.0
(Continued)

210

TRANSFORMATIONS AND WEIGHTING TO CORRECT MODEL INADEQUACIES

Batch
C C C C C C

Pressure
400 400 500 500 600 600

Strength
197.6 198.4 197.0 197.8 198.5 199.8

5.24 French and Schultz ("Water Use Efficiency of Wheat in a Mediterranean-type Environment, I The Relation between Yield, Water Use, and Climate," Australian Journal of Agricultural Research, 35, 743­64) studied the impact of water use on the yield of wheat in Australia. The data below are from 1970 for several locations assumed to be randomly selected for this study. The response, y, is the yield of what in kg/ha. The regressors are:
· x1 the amount of rain in mm for the period October to April. · x2 is the number of days in the growing season. · x3 is the amount of rain in mm during the growing season. · x4 is the water use in mm for the growing season. · x5 is the pan evaporation in mm during the growing season. Perform a thorough analysis of these data.

Location

x1

x2

x3

x4

x5

y

A

227 145 196 203 727

810

B

243 194 193 226 810 1500

B

254 183 195 268 790 1340

C

296 179 239 327 711 2750

C

296 181 239 304 705 3240

D

327 196 358 388 641 2860

E

441 189 363 465 663 4970

F

356 186 340 441 846 3780

G

419 195 340 387 713 2740

H

293 182 235 306 638 3190

I

274 182 201 220 638 2350

J

363 194 316 370 766 4440

K

253 189 255 340 778 2110

CHAPTER 6
DIAGNOSTICS FOR LEVERAGE AND INFLUENCE
6.1 IMPORTANCE OF DETECTING INFLUENTIAL OBSERVATIONS
When we compute a sample average, each observation in the sample has the same weight in determining the outcome. In the regression situation, this is not the case. For example, we noted in Section 2.9 that the location of observations in x space can play an important role in determining the regression coefficients (refer to Figures 2.8 and 2.9). We have also focused attention on outliers, or observations that have unusual y values. In Section 4.4 we observed that outliers are often identified by unusually large residuals and that these observations can also affect the regression results. The material in this chapter is an extension and consolidation of some of these issues.
Consider the situation illustrated in Figure 6.1. The point labeled A in this figure is remote in x space from the rest of the sample, but it lies almost on the regression line passing through the rest of the sample points. This is an example of a leverage point; that is, it has an unusual x value and may control certain model properties. Now this point does not affect the estimates of the regression coefficients, but it certainly will have a dramatic effect on the model summary statistics such as R2 and the standard errors of the regression coefficients. Now consider the point labeled A in Figure 6.2. This point has a moderately unusual x coordinate, and the y value is unusual as well. This is an influence point, that is, it has a noticeable impact on the model coefficients in that it "pulls" the regression model in its direction.
We sometimes find that a small subset of the data exerts a disproportionate influence on the model coefficients and properties. In an extreme case, the parameter estimates may depend more on the influential subset of points than on the majority of the data. This is obviously an undesirable situation; we would like for a regression
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
211

212

DIAGNOSTICS FOR LEVERAGE AND INFLUENCE

A

y

y

A

X
Figure 6.1 An example of a leverage point.

X
Figure 6.2 An example of an influential observation.

model to be representative of all of the sample observations, not an artifact of a few. Consequently, we would like to find these influential points and assess their impact on the model. If these influential points are indeed "bad" values, then they should be eliminated from the sample. On the other hand, there may be nothing wrong with these points, but if they control key model properties, we would like to know it, as it could affect the end use of the regression model.
In this chapter we present several diagnostics for leverage and influence. These diagnostics are available in most multiple regression computer packages. It is important to use these diagnostics in conjunction with the residual analysis techniques of Chapter 4. Sometimes we find that a regression coefficient may have a sign that does not make engineering or scientific sense, a regressor known to be important may be statistically insignificant, or a model that fits the data well and that is logical from an application­environment perspective may produce poor predictions. These situations may be the result of one or perhaps a few influential observations. Finding these observations then can shed considerable light on the problems with the model.

6.2 LEVERAGE

As observed above, the location of points in x space is potentially important in determining the properties of the regression model. In particular, remote points potentially have disproportionate impact on the parameter estimates, standard errors, predicted values, and model summary statistics. The hat matrix

H = X (XX)-1 X

(6.1)

plays an important role in identifying influential observations. As noted earlier,
H determines the variances and covariances of y^ and e, since Var (y^ ) =  2H
and Var(e) = 2(I - H). The elements hij of the matrix H may be interpreted
as the amount of leverage exerted by the ith observation yi on the jth fitted value y^ j.

LEVERAGE 213

We usually focus attention on the diagonal elements hii of the hat matrix H, which may be written as

hii = xi (XX )-1 xi

(6.2)

where xi is the ith row of the X matrix. The hat matrix diagonal is a standardized measure of the distance of the ith observation from the center (or centroid) of the x space. Thus, large hat diagonals reveal observations that are potentially influential because they are remote in x space from the rest of the sample. It turns out that the
average size of a hat diagonal is h = p / n [because in=1hii = rank(H) = rank(X) = p],
and we traditionally assume that any observation for which the hat diagonal exceeds twice the average 2p/n is remote enough from the rest of the data to be considered a leverage point.
Not all leverage points are going to be influential on the regression coefficients. For example, recall point A in Figure 6.1. This point will have a large hat diagonal and is assuredly a leverage point, but it has almost no effect on the regression coefficients because it lies almost on the line passing through the remaining observations. Because the hat diagonals examine only the location of the observation in x space, some analysts like to look at the studentized residuals or R-student in conjunction with the hii. Observations with large hat diagonals and large residuals are likely to be influential. Finally, note that in using the cutoff value 2p/n we must also be careful to assess the magnitudes of both p and n. There will be situations where 2p/n > 1, and in these situations, the cutoff does not apply.

Example 6.1 The Delivery Time Data
Column a of Table 6.1 shows the hat diagonals for the soft drink delivery time data Example 3.1. Since p = 3 and n = 25, any point for which the hat diagonal hii exceeds 2p/n = 2(3)/25 = 0.24 is a leverage point. This criterion would identify observations 9 and 22 as leverage points. The remote location of these points (particularly point 9) was previously noted when we examined the matrix of scatterplots in Figure 3.4 and when we illustrated interpolation and extrapolation with this model in Figure 3.11.
In Example 4.1 we calculated the scaled residuals for the delivery time data. Table 4.1 contains the studentized residuals and R-student. These residuals are not unusually large for observation 22, indicating that it likely has little influence on the fitted model. However, both scaled residuals for point 9 are moderately large, suggesting that this observation may have moderate influence on the model. To illustrate the effect of these two points on the model, three additional analyses were performed: one deleting observation 9, a second deleting observation 22, and the third deleting both 9 and 22. The results of these additional runs are shown in the following table:

Run
9 and 22 in 9 out 22 out 9 and 22 out

^0
2.341 4.447 1.916 4.643

^1
1.616 1.498 1.786 1.456

^2
0.014 0.010 0.012 0.011

MSRes
10.624 5.905 10.066 6.163

R2
0.9596 0.9487 0.9564 0.9072

214

TABLE 6.1 Statistics for Detecting Influential Observations for the Soft Drink Delivery Time Data

Observation i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(a) hii
0.10180 0.07070 0.09874 0.08538 0.07501 0.04287 0.08180 0.06373 0.49829 0.19630 0.08613 0.11366 0.06113 0.07824 0.04111 0.16594 0.05943 0.09626 0.09645 0.10169 0.16528 0.39158 0.04126 0.12061 0.06664

(b) Di
0.10009 0.00338 0.00001 0.07766 0.00054 0.00012 0.00217 0.00305 3.41835 0.05385 0.01620 0.00160 0.00229 0.00329 0.00063 0.00329 0.00040 0.04398 0.01192 0.13246 0.05086 0.45106 0.02990 0.10232 0.00011

(c) DFFITSi
-0.5709 0.0986 -0.0052 0.5008 -0.0395 -0.0188 0.0790 0.0938 4.2961 0.3987 0.2180 -0.0677 0.0813 0.0974 0.0426
-0.0972 0.0339 0.3653 0.1862 -0.6718 -0.3885 -1.1950 -0.3075 -0.5711 -0.0176

(d) Intercept DFBETAS(),i
-0.1873 0.0898 -0.0035 0.4520 -0.0317 -0.0147 0.0781 0.0712 -2.5757 0.1079 -0.0343 -0.0303 0.0724 0.0495 0.0223
-0.0027 0.0289 0.2486 0.1726 0.1680 -0.1619 0.3986 -0.1599 -0.1197 -0.0168

(e) Cases DFBETAS1,i
0.4113 -0.0478
0.0039 0.0883 -0.0133 0.0018 -0.0223 0.0334 0.9287 -0.3382 0.0925 -0.0487 -0.0356 -0.0671 -0.0048 0.0644 0.0065 0.1897 0.0236 -0.2150 -0.2972 -1.0254 0.0373 0.4046 0.0008

(f) Distance DFBETAS2,i
-0.4349 0.0144 -0.0028 -0.2734 0.0242 0.0011 -0.0110 -0.0538 1.5076 0.3413 -0.0027 0.0540 0.0113 0.0618 0.0068
-0.0842 -0.0157 -0.2724 -0.0990 -0.0929
0.3364 0.5731 -0.0527 -0.4654 0.0056

(g) COVRATIOi
0.8711 1.2149 1.2757 0.8760 1.2396 1.1999 1.2398 1.2056 0.3422 1.3054 1.1717 1.2906 1.2070 1.2277 1.1918 1.3692 1.2192 1.0692 1.2153 0.7598 1.2377 1.3981 0.8897 0.9476 1.2311

MEASURES OF INFLUENCE: COOK'S D

215

Deleting observation 9 produces only a minor change in ^1 but results in approximately a 28% change in ^2 and a 90% change in ^0. This illustrates that observation 9 is off the plane passing through the other 24 points and exerts a moderately strong

influence on the regression coefficient associated with x2 (distance). This is not surprising considering that the value of x2 for this observation (1460 feet) is very different from the other observations. In effect, observation 9 may be causing curvature

in the x2 direction. If observation 9 were deleted, then MSRes would be reduced to 5.905. Note that 5.905 = 2.430, which is not too different from the estimate of pure

error ^ = 1.969 found by the near-neighbor analysis in Example 4.10. It seems that

most of the lack of fit noted in this model in Example 4.11 is due to point 9's large

residual. Deleting point 22 produces relative smaller changes in the regression coef-

ficients and model summary statistics. Deleting both points 9 and 22 produces

changes similar to those observed when deleting only 9.



The SAS code to generate its influence diagnostics is:

model time = cases dist / influence;

The R code is:

deliver.model <- lm(timecases+dist, data=deliver) summary(deliver.model) print(influence.measures(deliver.model))

6.3 MEASURES OF INFLUENCE: COOK'S D

We noted in the previous section that it is desirable to consider both the location of the point in the x space and the response variable in measuring influence. Cook [1977, 1979] has suggested a way to do this, using a measure of the squared distance between the least-squares estimate based on all n points b^ and the estimate obtained by deleting the ith point, say b^(i). This distance measure can be expressed in a general form as

( ) ( ) Di = (M, c) =

b^(i) - b^  M b(i) - b^ c

,

i = 1, 2, ... , n

(6.3)

The usual choices of M and c are M = X´X and c = pMSRes, so that Eq. (6.3) becomes

( ) ( ) Di (XX, pMSRes )  Di =

b^(i) - b^  XX b^(i) - b^ pMSRes

,

i = 1, 2, ... , n

(6.4)

Points mates

with b^ .

large

values

of

Di

have

considerable

influence

on

the

least-squares

esti-

The magnitude of Di is usually assessed by comparing it to F,p,n-p. If Di = F0.5,p,n-p, then deleting point i would move b^(i) to the boundary of an approximate 50% confidence region for  based on the complete data set. This is a large displacement

216

DIAGNOSTICS FOR LEVERAGE AND INFLUENCE

and indicates that the least-squares estimate is sensitive to the ith data point. Since F0.5,p,n-p 1, we usually consider points for which Di > 1 to be influential. Ideally we would like each estimate b^(i) to stay within the boundary of a 10 or 20% confidence
region. This recommendation for a cutoff is based on the similarity of Di to the equation for the normal-theory confidence ellipsoid [Eq. (3.50)]. The distance measure Di is not an F statistic. However, the cutoff of unity works very well in practice.
The Di statistic may be rewritten as

Di

=

ri2 p

Var ( y^i ) Var (ei )

=

ri2 p

hii 1 - hii

,

i = 1, 2, ... , n

(6.5)

Thus, we see that, apart from the constant p, Di is the product of the square of the ith studentized residual and hii/(1 - hii). This ratio can be shown to be the distance from the vector xi to the centroid of the remaining data. Thus, Di is made up of a component that reflects how well the model fits the ith observation yi and a component that measures how far that point is from the rest of the data. Either component (or both) may contribute to a large value of Di. Thus, Di combines residual magnitude for the ith observation and the location of that point in x space to assess influence.
Because Xb^(i) - Xb^ = y^ (i) - y^ , another way to write Cook's distance measure is

Di

=

(y^ (i)

- y^ ) (y^ (i)
pMSRes

-

y^ )

(6.6)

Therefore, another way to interpret Cook's distance is that it is the squared Euclidean distance (apart from pMSRes) that the vector of fitted values moves when the ith observation is deleted.

Example 6.2 The Delivery Time Data

Column b of Table 6.1 contains the values of Cook's distance measure for the soft drink delivery time data. We illustrate the calculations by considering the first observation. The studentized residuals for the delivery time data in Table 4.1, and r1 = -1.6277. Thus,

D1

=

r12 p

h11 1 - h11

=

( -1.6277 )2
3

0.10180 1 - 0.10180

=

0.10009

The largest value of the Di statistic is D9 = 3.41835, which indicates that deletion
of observation 9 would move the least-squares estimate to approximately the boundary of a 96% confidence region around b^ . The next largest value is

D22 = 0.45106, and deletion of point 22 will move the estimate of  to approximately

the edge of a 35% confidence region.Therefore, we would conclude that observation

9 is definitely influential using the cutoff of unity, and observation 22 is not influential.

Notice that these conclusions agree quite well with those reached in Example 6.1

by examining the hat diagonals and studentized residuals separately.



MEASURES OF INFLUENCE: DFFITS AND DFBETAS

217

6.4 MEASURES OF INFLUENCE: DFFITS AND DFBETAS

Cook's distance measure is a deletion diagnostic, that is, it measures the influence of the ith observation if it is removed from the sample. Belsley, Kuh, and Welsch [1980] introduced two other useful measures of deletion influence. The first of these is a statistic that indicates how much the regression coefficient ^j changes, in standard deviation units, if the ith observation were deleted. This statistic is

DFBETASj,i

=

^ j - ^ j(i) S(2i)C jj

(6.7)

where Cjj is the jth diagonal element of (X´X)-1 and ^j(i) is the jth regression coefficient computed without use of the ith observation. A large (in magnitude) value
of DFBETASj,i indicates that observation i has considerable influence on the jth regression coefficient. Notice that DFBETASj,i is an n × p matrix that conveys similar information to the composite influence information in Cook's distance
measure. The computation of DFBETASj,i is interesting. Define the p × n matrix

R = (XX)-1 X

The n elements in the jth row of R produce the leverage that the n observations in the sample have on ^j. If we let rj denote the jth row of R, then we can show (see Appendix C.13) that

DFBETASj,i =

rj,i

ei

=

rjrj S(i) (1 - hii )

rj,i rjrj

ti 1 - hii

(6.8)

where ti is the R-student residual. Note that DFBETASj,i measures both leverage (rj,i/ rjrj is a measure of the impact of the ith observation on ^j) and the effect of
a large residual. Belsley, Kuh, and Welsch [1980] suggest a cutoff of 2 / n for
DFBETASj,i; that is, if DFBETASj,i > 2 / n, then the ith observation warrants examination.
We may also investigate the deletion influence of the ith observation on the predicted or fitted value. This leads to the second diagnostic proposed by Belsley, Kuh, and Welsch:

DFFITSi

=

y^i - y^(i) S(2i ) hii

,

i = 1, 2, ... , n

(6.9)

where y^(i) is the fitted value of yi obtained without the use of the ith observation.
The denominator is just a standardization, since Var ( y^i ) =  2hii. Thus, DFFITSi is
the number of standard deviations that the fitted value y^i changes if observation i
is removed.
Computationally we may find (see Appendix C.13 for details)

DFFITSi

=

 

1

hii - hii

1 2 

ei
S(i) (1 - hii )1 2

=

 

1

hii - hii

1 

2

ti

( 6.10)

218

DIAGNOSTICS FOR LEVERAGE AND INFLUENCE

where ti is R-student. Thus, DFFITSi is the value of R-student multiplied by the leverage of the ith observation [hii/(1 - hii)]1/2. If the data point is an outlier, then R-student will be large in magnitude, while if the data point has high leverage,
hii will be close to unity. In either of these cases DFFITSi can be large. However, if hii 0, the effect of R-student will be moderated. Similarly a near-zero R-student combined with a high leverage point could produce a small value of DFFITSi. Thus, DFFITSi is affected by both leverage and prediction error. Belsley, Kuh, and Welsch suggest that any observation for which DFFITSi > 2 p / n warrants attention.

A Remark on Cutoff Values In this section we have provided recommended cutoff values for DFFITSi and DFBETASj,i. Remember that these recommendations are only guidelines, as it is very difficult to produce cutoffs that are correct for all cases. Therefore, we recommend that the analyst utilize information about both what the diagnostic means and the application environment in selecting a cutoff. For example, if DFFITSi = 1.0, say, we could translate this into actual response units to determine just how much y^i is affected by removing the ith observation. Then DFBETASj,i could be used to see whether this observation is responsible for the significance (or perhaps nonsignificance) of particular coefficients or for changes of sign in a regression coefficient. Diagnostic DFBETASj,i can also be used to determine (by using the standard error of the coefficient) how much change in actual problem-specific units a data point has on the regression coefficient. Sometimes these changes will be important in a problem-specific context even though the diagnostic statistics do not exceed the formal cutoff.
Notice that the recommended cutoffs are a function of sample size n. Certainly, we believe that any formal cutoff should be a function of sample size; however, in our experience these cutoffs often identify more data points than an analyst may wish to analyze. This is particularly true in small samples. We believe that the cutoffs recommended by Belsley, Kuh, and Welsch make sense for large samples, but when n is small, we prefer the diagnostic view discussed previously.

Example 6.3 The Delivery Time Data

Columns c­f of Table 6.1 present the values of DFFITSi and DFBETASj,i for the soft drink delivery time data. The formal cutoff value for DFFITSi is 2 p / n = 2 3 / 25 = 0.69. Inspection of Table 6.1 reveals that both points 9 and 22 have values of DFFITSi that exceed this value, and additionally DFFITS20 is close to the cutoff.
Examining DFBETASj,i and recalling that the cutoff is 2 / 25 = 0.40, we immediately notice that points 9 and 22 have large effects on all three parameters. Point 9 has a very large effect on the intercept and smaller effects on ^1 and ^2, while point 22 has its largest effect on ^1. Several other points produce effects on the coefficients that are close to the formal cutoff, including 1 (on ^1 and ^2), 4 (on ^0), and 24 (on ^1 and ^2). These points produce relatively small changes in comparison to point 9.
Adopting a diagnostic view, point 9 is clearly influential, since its deletion results in a displacement of every regression coefficient by at least 0.9 standard deviation. The effect of point 22 is much smaller. Furthermore, deleting point 9 displaces the

A MEASURE OF MODEL PERFORMANCE

219

predicted response by over four standard deviations. Once again, we have a clear

signal that observation 9 is influential.



6.5 A MEASURE OF MODEL PERFORMANCE

The diagnostics Di, DFBETASj,i, and DFFITSi provide insight about the effect of observations on the estimated coefficients ^j and fitted values y^i. They do not provide any information about overall precision of estimation. Since it is fairly common practice to use the determinant of the covariance matrix as a convenient scalar measure of precision, called the generalized variance, we could define the generalized variance of b^ as
( ) ( ) GV b^ = Var b^ =  2 (XX)-1

To express the role of the ith observation on the precision of estimation, we could define

COVRATIOi

=

( ) X(i)X(i) -1 S(2i)
(XX )-1 MSRes

,

i = 1, 2, ... , n

(6.11)

Clearly if COVRATIOi > 1, the ith observation improves the precision of estimation, while if COVRATIOi < 1, inclusion of the ith point degrades precision. Computationally

( ) COVRATIOi

=

S(2i) p MSRpes

 

1 1 - hii

 

( 6.12)

Note that [1/(1 - hii)] is the ratio of |(X(i)X(i))-1| to |(XX)-1|, so that a high leverage point will make COVRATIOi large. This is logical, since a high leverage point will improve the precision unless the point is an outlier in y space. If the ith observation is an outlier, S(2i) /MSRes will be much less than unity.
Cutoff values for COVRATIO are not easy to obtain. Belsley, Kuh, and Welsch [1980] suggest that if COVRATIOi > 1 + 3p/n or if COVRATIOi < 1 - 3p/n, then the ith point should be considered influential.The lower bound is only appropriate when n > 3p. These cutoffs are only recommended for large samples.

Example 6.4 The Delivery Time Data

Column g of Table 6.1 contains the values of COVRATIOi for the soft drink delivery time data.The formal recommended cutoff for COVRATIOi is 1 ± 3p/n = 1 ± 3(3)/25,

or 0.64 and 1.36. Note that the values of COVRATIO9 and COVRATIO22 exceed these limits, indicating that these points are influential. Since COVRATIO9 < 1, this observation degrades precision of estimation, while since COVRATIO22 > 1, this

observation tends to improve the precision. However, point 22 barely exceeds its

cutoff, so the influence of this observation, from a practical viewpoint, is fairly small.

Point 9 is much more clearly influential.



220

DIAGNOSTICS FOR LEVERAGE AND INFLUENCE

6.6 DETECTING GROUPS OF INFLUENTIAL OBSERVATIONS

We have focused on single-observation deletion diagnostics for influence and leverage. Now obviously, there could be situations where a group of points have high leverage or exert undue influence on the regression model. Very good discussions of this problem are in Belsley, Kuh, and Welsch [1980] and Rousseeuw and Leroy [1987].
In principle, we can extend the single-observation diagnostics to the multiple observation case. In fact, there are several strategies for solving the multiple-outlier influential observation problem. For example, see Atkinson [1994], Hadi and Simonoff [1993], Hawkings, Bradu, and Kass [1984], Pena and Yohai [1995], and Rousseeuw and van Zomeren [1990]. To show how we could extend Cook's distance measure to assess the simultaneous influence of a group of m observations, let i denote the m × 1 vector of indices specifying the points to be deleted, and define

( ) ( ) Di (XX, pMSRes ) = Di =

b^(i) - b^  XX b^(i) - b^ pMSRes

Obviously, Di is a multiple-observation version of Cook's distance measure. The interpretation of Di is similar to the single-observation statistic. Large values of Di indicate that the set of m points are influential. Selection of the subset of points to include in m is not obvious, however, because in some data sets subsets of points are jointly influential but individual points are not. Furthermore, it is not practical to investigate all possible combinations of the n sample points taken m = 1, 2, . . . , n points at a time.
Sebert, Montgomery, and Roilier [1998] investigate the use of cluster analysis to find the set of influential observations in regression. Cluster analysis is a multivariate technique for finding groups of similar observations.The procedure consists of defining a measure of similarity between observations and then using a set of rules to classify the observations into groups based on their interobservation similarities. They use a single-linkage clustering procedure (see Johnson and Wichern [1992] and Everitt [1993]) applied to the least-squares residuals and fitted values to cluster n - m observations into a "clean" group and a potentially influential group of m observations. The potentially influential group of observations are then evaluated in subsets of size 1, 2, . . . , m using the multiple-observation version of Cook's distance measure. The authors report that this procedure is very effective in finding the subset of influential observations. There is some "swamping," that is, identifying too many observations as potentially influential, but the use of Cook's distance efficiently eliminates the noninfluential observations. In studying nine data sets from the literature, the authors report no incidents of "masking," that is, failure to find the correct subset of influential points. They also report successful results from an extensive performance study conducted by Monte Carlo simulation.

6.7 TREATMENT OF INFLUENTIAL OBSERVATIONS
Diagnostics for leverage and influence are an important part of the regression model-builder's arsenal of tools. They are intended to offer the analyst insight about

PROBLEMS 221
the data and to signal which observations may deserve more scrutiny. How much effort should be devoted to study of these points? It probably depends on the number of influential points identified, their actual impact on the model, and the importance of the model-building problem. If you have spent a year collecting 30 observations, then it seems probable that a lot of follow-up analysis of suspect points can be justified. This is particularly true if an unexpected result occurs because of a single influential observation.
Should influential observations ever be discarded? This question is analogous to the question regarding discarding outliers. As a general rule, if there is an error in recording a measured value or if the sample point is indeed invalid or not part of the population that was intended to be sampled, then discarding the observation is appropriate. However, if analysis reveals that an influential point is a valid observation, then there is no justification for its removal.
A "compromise" between deleting an observation and retaining it is to consider using an estimation technique that is not impacted as severely by influential points as least squares. These robust estimation techniques essentially downweight observations in proportion to residual magnitude or influence, so that a highly influential observation will receive less weight than it would in a least-squares fit. Robust regression methods are discussed in Section 15.1.
PROBLEMS
6.1 Perform a thorough influence analysis of the solar thermal energy test data given in Table B.2. Discuss your results.
6.2 Perform a thorough influence analysis of the property valuation data given in Table B.4. Discuss your results.
6.3 Perform a thorough influence analysis of the Belle Ayr liquefaction runs given in Table B.5. Discuss your results.
6.4 Perform a thorough influence analysis of the tube-flow reactor data given in Table B.6. Discuss your results.
6.5 Perform a thorough influence analysis of the NFL team performance data given in Table B.1. Discuss your results.
6.6 Perform a thorough influence analysis of the oil extraction data given in Table B.7. Discuss your results.
6.7 Perform a thorough influence analysis of the clathrate formation data given in Table B.8. Perform any appropriate transformations. Discuss your results.
6.8 Perform a thorough influence analysis of the pressure drop data given in Table B.9. Perform any appropriate transformations. Discuss your results.
6.9 Perform a thorough influence analysis of kinematic viscosity data given in Table B.10. Perform any appropriate transformations. Discuss your results.
6.10 Formally show that

222

DIAGNOSTICS FOR LEVERAGE AND INFLUENCE

Di

=

ri p

hii 1 - hii

6.11 Formally show that

COVRATIOi

=

 S(2i)  MSRes

p 

 

1 1 - hii

 

6.12 Table B.11 contains data on the quality of Pinot Noir wine. Fit a regression model using clarity, aroma, body, flavor, and oakiness as the regressors. Investigate this model for influential observations and comment on your findings.
6.13 Table B.12 contains data collected on heat treating of gears. Fit a regression model to these data using all of the regressors. Investigate this model for influential observations and comment on your findings.
6.14 Table B.13 contains data on the thrust of a jet turbine engine. Fit a regression model to these data using all of the regressors. Investigate this model for influential observations and comment on your findings.
6.15 Table B.14 contains data concerning the transient points of an electronic inverter. Fit a regression model to all 25 observations but only use x1 - x4 as the regressors. Investigate this model for influential observations and comment on your findings.
6.16 Perform a thorough influential analysis of the air pollution and mortality data given in Table B.15. Perform any appropriate transformations. Discuss your results.
6.17 For each model perform a thorough influence analysis of the life expectancy data given in Table B.16. Perform any appropriate transformations. Discuss your results.
6.18 Consider the patient satisfaction data in Table B.17. Fit a regression model to the satisfaction response using age and security as the predictors. Perform an influence analysis of the date and comment on your findings.
6.19 Consider the fuel consumption data in Table B.18. For the purposes of this exercise, ignore regressor x1. Perform a thorough influence analysis of these data. What conclusions do you draw from this analysis?
6.20 Consider the wine quality of young red wines data in Table B.19. For the purposes of this exercise, ignore regressor x1. Perform a thorough influence analysis of these data. What conclusions do you draw from this analysis?
6.21 Consider the methanol oxidation data in Table B.20. Perform a thorough influence analysis of these data. What conclusions do you draw from this analysis?

CHAPTER 7
POLYNOMIAL REGRESSION MODELS
7.1 INTRODUCTION The linear regression model y = X +  is a general model for fitting any relationship that is linear in the unknown parameters . This includes the important class of polynomial regression models. For example, the second-order polynomial in one variable
y = 0 + 1x + 2x2 +  and the second-order polynomial in two variables
y = 0 + 1x1 + 2 x2 + 11x12 + 22 x22 + 12 x1x2 +  are linear regression models.
Polynomials are widely used in situations where the response is curvilinear, as even complex nonlinear relationships can be adequately modeled by polynomials over reasonably small ranges of the x's. This chapter will survey several problems and issues associated with fitting polynomials.
7.2 POLYNOMIAL MODELS IN ONE VARIABLE 7.2.1 Basic Principles As an example of a polynomial regression model in one variable, consider
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
223

224

POLYNOMIAL REGRESSION MODELS

E(y )

10 9 8 7 6 5 4 E ( y ) = 5 - 2x + .25x2 3 2 1 0 0 1 2 3 4 5 6 7 8 9 10 x
Figure 7.1 An example of a quadratic polynomial.

y = 0 + 1x + 2x2 + 

(7.1)

This model is called a second-order model in one variable. It is also sometimes called a quadratic model, since the expected value of y is

E(y) = 0 + 1x + 2x2

which describes a quadratic function. A typical example is shown in Figure 7.1. We often call 1 the linear effect parameter and 2 the quadratic effect parameter. The parameter 0 is the mean of y when x = 0 if the range of the data includes x = 0. Otherwise 0 has no physical interpretation.
In general, the kth-order polynomial model in one variable is

y = 0 + 1x + 2x2 + + k xk + 

(7.2)

If we set xj = xj, j = 1, 2, . . . , k, then Eq. (7.2) becomes a multiple linear regression model in the k regressors x1, x2, . . . xk. Thus, a polynomial model of order k may be fitted using the techniques studied previously.
Polynomial models are useful in situations where the analyst knows that curvilinear effects are present in the true response function. They are also useful as approximating functions to unknown and possibly very complex nonlinear relationships. In this sense, the polynomial model is just the Taylor series expansion of the unknown function. This type of application seems to occur most often in practice.
There are several important considerations that arise when fitting a polynomial in one variable. Some of these are discussed below.

1. Order of the Model It is important to keep the order of the model as low as possible. When the response function appears to be curvilinear, transformations should be tried to keep the model first order. The methods discussed in Chapter 5 are useful in this regard. If this fails, a second-order polynomial

POLYNOMIAL MODELS IN ONE VARIABLE

225

should be tried. As a general rule the use of high-order polynomials (k > 2) should be avoided unless they can be justified for reasons outside the data. A low-order model in a transformed variable is almost always preferable to a high-order model in the original metric. Arbitrary fitting of high-order polynomials is a serious abuse of regression analysis. One should always maintain a sense of parsimony, that is, use the simplest possible model that is consistent with the data and knowledge of the problem environment. Remember that in an extreme case it is always possible to pass a polynomial of order n - 1 through n points so that a polynomial of sufficiently high degree can always be found that provides a "good" fit to the data. In most cases, this would do nothing to enhance understanding of the unknown function, nor will it likely be a good predictor.
2. Model-Building Strategy Various strategies for choosing the order of an approximating polynomial have been suggested. One approach is to successively fit models of increasing order until the t test for the highest order term is nonsignificant. An alternate procedure is to appropriately fit the highest order model and then delete terms one at a time, starting with the highest order, until the highest order remaining term has a significant t statistic. These two procedures are called forward selection and backward elimination, respectively. They do not necessarily lead to the same model. In light of the comment in 1 above, these procedures should be used carefully. In most situations we should restrict our attention to first- and second-order polynomials.
3. Extrapolation Extrapolation with polynomial models can be extremely hazardous. For example, consider the second-order model in Figure 7.2. If we extrapolate beyond the range of the original data, the predicted response turns downward. This may be at odds with the true behavior of the system. In general, polynomial models may turn in unanticipated and inappropriate directions, both in interpolation and in extrapolation.

E(y)

7

6

E ( y ) = 2 + 2x - .25x 2

5

4

3

2

1

0 0 1 2 3 4 5 6 7 8 9x

Region of original data

Extrapolation

Figure 7.2 Danger of extrapolation.

226

POLYNOMIAL REGRESSION MODELS

4. Ill-Conditioning I As the order of the polynomial increases, the XX matrix becomes ill-conditioned. This means that the matrix inversion calculations will be inaccurate, and considerable error may be introduced into the parameter estimates. For example, see Forsythe [1957]. Nonessential ill-conditioning caused by the arbitrary choice of origin can be removed by first centering the regressor variables (i.e., correcting x for its average x), but as Bradley and Srivastava [1979] point out, even centering the data can still result in large sample correlations between certain regression coeffcients. One method for dealing with this problem will be discussed in Section 7.5.
5. Ill-Conditioning II If the values of x are limited to a narrow range, there can be significant ill-conditioning or multicollinearity in the columns of the X matrix. For example, if x varies between 1 and 2, x2 varies between 1 and 4, which could create strong multicollinearity between x and x2.
6. Hierarchy The regression model

y = 0 + 1x + 2x2 + 3x3 + 

is said to be hierarchical because it contains all terms of order 3 and lower. By contrast, the model

y = 0 + 1x + 3x3 + 

is not hierarchical. Peixoto [1987, 1990] points out that only hierarchical models are invariant under linear transformation and suggests that all polynomial models should have this property (the phrase "a hierarchically wellformulated model" is frequently used). We have mixed feelings about this as a hard-and-fast rule. It is certainly attractive to have the model form preserved following a linear transformation (such as fitting the model in coded variables and then converting to a model in the natural variables), but it is purely a mathematical nicety. There are many mechanistic models that are not hierarchical; for example, Newton's law of gravity is an inverse square law, and the magnetic dipole law is an inverse cube law. Furthermore, there are many situations in using a polynomial regression model to represent the results of a designed experiment where a model such as
y = 0 + 1x1 + 12 x1x2 + 
would be supported by the data, where the cross-product term represents a two-factor interaction. Now a hierarchical model would require the inclusion of the other main effect x2. However, this other term could really be entirely unnecessary from a statistical significance perspective. It may be perfectly logical from the viewpoint of the underlying science or engineering to have an interaction in the model without one (or even in some cases either) of the individual main effects. This occurs frequently when some of the variables involved in the interaction are categorical. The best advice is to fit a model that has all terms significant and to use discipline knowledge rather than an arbitrary rule as an additional guide in model formulation. Generally, a hierarchical model is usually easier to explain to a "customer" that is not familiar

POLYNOMIAL MODELS IN ONE VARIABLE

227

with statistical model-building, but a nonhierarchical model may produce better predictions of new data.
We now illustrate some of the analyses typically associated with fitting a polynomial model in one variable.

Example 7.1 The Hardwood Data

Table 7.1 presents data concerning the strength of kraft paper and the percentage of hardwood in the batch of pulp from which the paper was produced. A scatter diagram of these data is shown in Figure 7.3. This display and knowledge of the production process suggests that a quadratic model may adequately describe the relationship between tensile strength and hardwood concentration. Following the suggestion that centering the data may remove nonessential ill-conditioning, we will fit the model
y = 0 + 1(x - x ) + 2 (x - x )2 + 
Since fitting this model is equivalent to fitting a two-variable regression model, we can use the general approach in Chapter 3. The fitted model is
y^ = 45.295 + 2.546(x - 7.2632) - 0.635(x - 7.2632)2

The analysis of variance for this model is shown in Table 7.2. The observed value of F0 = 79.434 and the P value is small, so the hypothesis H0: 1 = 2 = 0 is rejected. We

TABLE 7.1 Hardwood Concentration in Pulp and Tensile Strength of Kraft Paper, Example 7.1

Hardwood Concentration, xi (%)

Tensile Strength, (psi) y, (psi)

1

6.3

1.5

11.1

2

20.0

3

24.0

4

26.1

4.5

30.0

5

33.8

5.5

34.0

6

38.1

6.5

39.9

7

42.0

8

46.1

9

53.1

10

52.0

11

52.5

12

48.0

13

42.8

14

27.8

15

21.9

228

POLYNOMIAL REGRESSION MODELS

60

50

Tensile strength

40

30

20

10

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Hardwood concentration (%)
Figure 7.3 Scatterplot of data, Example 7.1.

TABLE 7.2 Analysis of Variance for the Qnadratic Model for Example 7.1

Source of

Sum of

Degrees of

Variation

Squares

Freedom

Mean Square

F0

Regression

3104.247

2

Residual

312.638

16

Total

3416.885

18

1552.123 19.540

79.434

P Value 4.91 × 10-9

conclude that either the linear or the quadratic term (or both) contribute significantly to the model. The other summary statistics for this model are R2 = 0.9085,
( ) ( ) se ^1 = 0.254, and se ^2 = 0.062.
The plot of the residuals versus y^i is shown in Figure 7.4. This plot does not reveal any serious model inadequacy. The normal probability plot of the residuals, shown in Figure 7.5, is mildly disturbing, indicating that the error distribution has heavier tails than the normal. However, at this point we do not seriously question the normality assumption.
Now suppose that we wish to investigate the contribution of the quadratic term to the model. That is, we wish to test
H0 : 2 = 0, H1 : 2  0
We will test these hypotheses using the extra-sum-of-squares method. If 2 = 0, then
the reduced model is the straight line y = 0 + 1(x - x ) + . The least-squares fit is
y^ = 34.184 + 1.771(x - 7.2632)

POLYNOMIAL MODELS IN ONE VARIABLE

229

ei >
Expanded normal value

6
4
2
0
-2
-4
-6 5 15 25 35 45 10 20 30 40 50 yi
Figure 7.4 Plot of residuals ei, versus fitted values y^i, Example 7.1.

2.0 1.5 1.0 .50 0.0 -.50 -1.0 -1.5 -2.0
-4.5 -2.7 -.90 .90 2.7 4.5 6.3 -5.4 -3.6 -1.8 0.0 1.8 3.6 5.4
ei
Figure 7.5 Normal probability plot of the residuals, Example 7.1.

( ) The summary statistics for this model are MSRes = 139.615,R2 = 0.3054,se ^1 = 0.648,
and SSR(1|0) = 1043.427. We note that deleting the quadratic term has
( ) substantially affected R2, MSRes, and se ^1 . These summary statistics are much
worse than they were for the quadratic model. The extra sum of squares for testing H0: 2 = 0 is
SSR (2 1, 0 ) = SSR (1, 2 0 ) - SSR (1 0 )
= 3104.247 - 1043.427 = 2060.820

with one degree of freedom. The F statistic is

F0

=

SSR (2 1, 0 )
MSRes

1

=

2060.820 19.540

1

=

105.47

and since F0.01,1,16 = 8.53. we conclude that 2  0. Thus, the quadratic term contrib-

utes significantly to the model.



7.2.2 Piecewise Polynomial Fitting (Splines)
Sometimes we find that a low-order polynomial provides a poor fit to the data, and increasing the order of the polynomial modestly does not substantially improve the situation. Symptoms of this are the failure of the residual sum of squares to stabilize or residual plots that exhibit remaining unexplained structure. This problem may occur when the function behaves differently in different parts of the range of x. Occasionally transformations on x and/or y eliminate this problem. The usual approach, however, is to divide the range of x into segments and fit an appropriate

230

POLYNOMIAL REGRESSION MODELS

curve in each segment. Spline functions offer a useful way to perform this type of piecewise polynomial fitting.
Splines are piecewise polynomials of order k. The joint points of the pieces are usually called knots. Generally we require the function values and the first k - 1 derivatives to agree at the knots, so that the spline is a continuous function with k - 1 continuous derivatives. The cubic spline (k = 3) is usually adequate for most practical problems.
A cubic spline with h knots, t1 < t2 < · · · < th, with continuous first and second derivatives can be written as

3

h

  E (y) = S (x) = 0 j x j + i (x - ti )+3

j=0

i=1

(7.3)

where

(x - ti )+

=

(x - ti )

 

0

if x - ti > 0 if x - ti  0

We assume that the positions of the knots are known. If the knot positions are parameters to be estimated, the resulting problem is a nonlinear regression problem. When the knot positions are known, however, fitting Eq. (7.3) can be accomplished by a straightforward application of linear least squares.
Deciding on the number and position of the knots and the order of the polynomial in each segment is not simple. Wold [1974] suggests that there should be as few knots as possible, with at least four or five data points per segment. Considerable caution should be exercised here because the great flexibility of spline functions makes it very easy to "overfit" the data. Wold also suggests that there should be no more than one extreme point (maximum or minimum) and one point of inflection per segment. Insofar as possible, the extreme points should be centered in the segment and the points of inflection should be near the knots. When prior information about the data-generating process is available, this can sometimes aid in knot positioning.
The basic cubic spline model (7.3) can be easily modified to fit polynomials of different order in each segment and to impose different continuity restrictions at the knots. If all h + 1 polynomial pieces are of order 3, then a cubic spline model with no continuity restrictions is

3

h3

   E (y) = S (x) = 0j xj +

ij (x - ti )+j

j=0

i=1 j=0

(7.4)

where (x - t)0+ equals 1 if x > t and 0 if x  t. Thus, if a term ij (x - ti )+j is in the model,
this forces a discontinuity at ti in the jth derivative of S(x). If this term is absent, the jth derivative of S(x) is continuous at ti The fewer continuity restrictions required, the better is the fit because more parameters are in the model, while the more
continuity restrictions required, the worse is the fit but the smoother the final curve
will be. Determining both the order of the polynomial segments and the continuity

POLYNOMIAL MODELS IN ONE VARIABLE

231

restrictions that do not substantially degrade the fit can be done using standard multiple regression hypothesis-testing methods.
As an illustration consider a cubic spline with a single knot at t and no continuity restrictions; for example,
E ( y) = S (x) = 00 + 01x + 02 x2 + 03 x3 + 10 (x - t)0+ + 11 (x - t)1+ + 12 (x - t)+2 + 13 (x - t)+3
Note that S(x), S(x), and S(x) are not necessarily continuous at t because of the presence of the terms involving 10, 11, and 12 in the model. To determine whether imposing continuity restrictions reduces the quality of the fit, test the hypotheses H0: 10 = 0 [continuity of S(x)], H0: 10 = 11 = 0 [continuity of S(x) and S(x)], and H0: 10 = 11 = 12 = 0 [continuity of S(x), S(x), and S(x)]. To determine whether the cubic spline fits the data better than a single cubic polynomial over the range of x, simply test H0: 10 = 11 = 12 = 13 = 0.
An excellent description of this approach to fitting splines is in Smith [1979]. A potential disadvantage of this method is that the XX matrix becomes ill-conditioned if there are a large number of knots. This problem can be overcome by using a different representation of the spline called the cubic B-spline. The cubic B-splines are defined in terms of divided differences









i
Bi (x) = 
j=i-4   

(
i

x

-
(t

t
j

)3
j+
- tm

)

 ,  

 m=i-4 m j



i = 1, 2, ... , h + 4

(7.5)

and

h+4
 E(y) = S(x) =  iBi (x) i=1

(7.6)

where i, i = 1, 2, . . . , h + 4, are parameters to be estimated. In Eq. (7.5) there are eight additional knots, t-3 < t-2 < t-1 < t0 and th+1 < th+2 < th+3 < th+4. We usually take t0 = xmin and th+1 = xmin; the other knots are arbitrary. For further reading on splines, see Buse and Lim [1977], Curry and Schoenberg [1966], Eubank [1988], Gallant and
Fuller [1973], Hayes [1970, 1974], Poirier [1973, 1975], and Wold [1974].

Example 7.2 Voltage Drop Data
The battery voltage drop in a guided missile motor observed over the time of missile flight is shown in Table 7.3. The scatterplot in Figure 7.6 suggests that voltage drop behaves differently in different segments of time, and so we will model the data with a cubic spline using two knots at t1 = 6.5 and t2 = 13 seconds after launch, respectively. This placement of knots roughly agrees with course changes by the missile

232

POLYNOMIAL REGRESSION MODELS

TABLE 7.3 Voltage Drop Data

Observation, i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

Time, xi (seconds)
0.0 05 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 95

Voltage Drop, yi
8.33 823
7.17 7.14 7.31 7.60 7.94 8.30 8.76 8.71 9.71 10.26 10.91 11.67 11.76 12.81 13.30 13.88 14.59 14.05

Observation, i
21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41

Time, xi (seconds)
10.0 105 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0 15.5 16.0 16.5 17.0 17.5 18.0 18.5 19.0 19.5 20.0

Voltage Drop, yi
14.48 14.92 14.37 14.63 15.18 14.51 14.34 13.81 13.79 13.05 13.04 12.60 12.05 11.15 11.15 10.14 10.08 9.78 9.80 9.95 9.51

15

Voltage drop, y

10

5

0

0

5

10

15

20

Time (sec), x

Figure 7.6 Scatterplot of voltage drop data.

(with associated changes in power requirements), which are known from trajectory data. The voltage drop model is intended for use in a digital-analog simulation model of the missile.
The cubic spline model is
y = 00 + 01x + 02 x2 + 03 x3 + 1 (x - 6.5)3+ + 2 (x - 13)3+ + 

POLYNOMIAL MODELS IN ONE VARIABLE

233

TABLE 7.4 Summary Statistics for the Cubic Spline Model of the Voltage Drop Data

Source of Variation
Regression Residual Total

Sum of Squares
260.1784 2.5102
262.6886

Degrees of Freedom
5 35 40

Mean Square
52.0357 0.0717

F0 725.52

P Value <0.0001

Parameter
00 01 02 03 1 2

Estimate
8.4657 -1.4531 0.4899 -0.0295 0.0247 0.0271

Standard Error
0.2005 0.1816 0.0430 0.0028 0.0040 0.0036
R2 = 0.9904

t Value for H0:  = 0
42.22 -8.00 11.39 -10.54
6.18 7.53

P Value
<0.0001 <0.0001 <0.0001 <0.0001 <0.0001 <0.0001

ei >
ei >

0.75
0.50
0.25
0.00
-0.25
-0.50 5 6 7 8 9 10 11 12 13 14 15 16 yi
Figure 7.7 Plot of residuals ei, versus fitted values y^i for the cubic spline model.

1.5
1.0
0.5
0.0
-0.5
-1.0
-1.5 5 6 7 8 9 10 11 12 13 14 15 16 yi
Figure 7.8 Plot of residuals ei, versus fitted values y^i for the cubic polynomial model.

and the least-squares fit is

y^ = 8.4657 - 1.4531x + 0.4899x2 - 0.0295x3 + 0.0247(x - 6.5)+3 + 0.0271(x - 13)3+

The model summary statistics are displayed in Table 7.4. A plot of the residuals

versus y^ is shown in Figure 7.7. This plot (and other residual plots) does not reveal

any serious departures from assumptions, so we conclude that the cubic spline model

is an adequate fit to the voltage drop data.



234

POLYNOMIAL REGRESSION MODELS

We may easily compare the cubic spline model fit from Example 7.2 with a sample cubic polynomial over the entire time of missile flight; for example,

y^ = 6.4910 + 0.7032x + 0.0340x2 - 0.0033x3

This is a simpler model containing fewer parameters and would be preferable to the cubic spline model if it provided a satisfactory fit. The residuals from this cubic polynomial are plotted versus y^ in Figure 7.8. This plot exhibits strong indication of curvature, and on the basis of this remaining unexplained structure we conclude that the simple cubic polynomial is an inadequate model for the voltage drop data.
We may also investigate whether the cubic spline model improves the fit by testing the hypothesis H0: 1 = 2 = 0 using the extra-sum-of-squares method. The regression sum of squares for the cubic polynomial is
SSR (01, 02, 03 00 ) = 230.4444
with three degrees of freedom. The extra sum of squares for testing H0: 1 = 2 = 0 is
SSR (1, 2 00, 01, 02, 03 ) = SSR (01, 02, 03, 2, 2 00 ) - SSR (01, 02, 03 00 )
= 260.1784 - 230.4444 = 29.7340

with two degrees of freedom. Since

F0

=

SSR (1, 2

00, 01, MSRes

02,

03 )

2

=

29.7340 2 0.0717

=

207.35

which would be referred to the F2, 35 distribution, we reject the hypothesis that H0: 1 = 2 = 0. We conclude that the cubic spline model provides a better fit.

Example 7.3 Piecewise Linear Regression

An important special case of practical interest involves fitting piecewise linear regression models. This can be treated easily using linear splines. For example, suppose that there is a single knot at t and that there could be both a slope change and a discontinuity at the knot. The resulting linear spline model is

E ( y) = S (x) = 00 + 01x + 10 (x - t)0+ + 11 (x - t)1+
Now if x  t, the straight-line model is

and if x > t, the model is

E ( y) = 00 + 01x

E ( y) = 00 + 01x + 10 (1) + 11 (x - t) = (00 + 10 - 11t) + (01 + 11 ) x

POLYNOMIAL MODELS IN ONE VARIABLE

235

01+11

y
00+10-11t 00

10 01
t (a)

y
00 00-11t x

01
t (b)

01+11 x

Figure 7.9 Piecewise linear regression: (a) discontinuity at the knot; (b) continuous piecewise linear regression model.

That is, if x  t, the model has intercept 00 and slope 01, while if x > t, the intercept is 00 + 10 - 11t and the slope is 01 + 11. The regression function is shown in Figure 7.9a. Note that the parameter 10 represents the difference in mean response at the
knot t.
A smoother function would result if we required the regression function to be
continuous at the knot. This is easily accomplished by deleting the term 10 (x - t)0+
from the original model, giving

E ( y) = S (x) = 00 + 01x + 11 (x - t)1+
Now if x  t, the model is

and if x > t, the model is

E ( y) = 00 + 01x

E ( y) = 00 + 01x + 11 (x - t) = (00 - 11t) + (01 + 11 ) x

The two regression functions are shown in Figure 7.9b.



7.2.3 Polynomial and Trigonometric Terms
It is sometimes useful to consider models that combine both polynomial and trigonometric terms as alternatives to models that contain polynomial terms only. In particular, if the scatter diagram indicates that there may be some periodicity or cyclic behavior in the data, adding trigonometric terms to the model may be very beneficial, in that a model with fewer terms may result than if only polynomial terms were employed. This benefit has been noted by both Graybill [1976] and Eubank and Speckman [1990].
The model for a single regressor x is

d

r

  y = 0 + i xi + [ j sin( jx) +  j cos( jx)] + 

i=1

j=1

236

POLYNOMIAL REGRESSION MODELS

If the regressor x is equally spaced, then the pairs of terms sin( jx) and cos( jx) are orthogonal. Even without exactly equal spacing, the correlation between these terms will usually be quite small.
Eubank and Speckman [1990] use the voltage drop data of Example 7.2 to illustrate fitting a polynomial-trigonometric regression model. They first rescale the regressor x (time) so that all of the observations are in the interval (0, 2) and fit the model above with d = 2 and r = 1 so that the model is quadratic in time and has a pair of sine-cosine terms. Thus, their model has only four terms, whereas our spline regression model had five. Eubank and Speckman obtain R2 = 0.9895 and MSRes = 0.0767, results that are very similar to those found for the spline model (refer to Table 7.4). Since the voltage drop data exhibited some indication of periodicity in the scatterplot (Figure 7.6), the polynomial-trigonometric regression model is certainly a good alternative to the spline model. It has one fewer term (always a desirable property) but a slightly larger residual mean square. Working with a rescaled version of the regressor variable might also be considered a potential disadvantage by some users.

7.3 NONPARAMETRIC REGRESSION
Closely related to piecewise polynomial regression is nonparametric regression. The basic idea of nonparametric regression is to develop a model-free basis for predicting the response over the range of the data. The early approaches to nonparametric regression borrow heavily from nonparametric density estimation. Most of the nonparametric regression literature focuses on a single regressor; however, many of the basic ideas extend to more than one.
A fundamental insight to nonparametric regression is the nature of the predicted value. Consider standard ordinary least squares. Recall

y^ = Xb^ = X (XX)-1 Xy

= Hy

 h11 h12 ... h1n   y1 

=

 

h21

h22

...

h2

n

 

 

y2

 



 

hn1 hn2 ... hnn   yn 

As a result,

n
 y^i = hij yj j=1

In other words, the predicted value for the ith response is simply a linear combination of the original data.

NONPARAMETRIC REGRESSION

237

7.3.1 Kernel Regression
One of the first alternative nonparametric approaches is the kernel smoother, which uses a weighted average of the data. Let yi be the kernel smoother estimate of the ith response. For a kernel smoother,

n
 yi = wij yj j=1

where



n j=1

w ij

=

1. As

a

result,

y = Sy

where S = [wij] is the "smoothing" matrix. Typically, the weights are chosen such that wij  0 for all yi's outside of a defined "neighborhood" of the specific location of interest. These kernel smoothers use a bandwidth, b, to define this neighborhood of interest. A large value for b results in more of the data being used to predict the response at the specific location. Consequently, the resulting plot of predicted values becomes much smoother as b increases. Conversely, as b decreases, less of the data are used to generate the prediction, and the resulting plot looks more "wiggly" or bumpy.
This approach is called a kernel smoother because it uses a kernel function, K, to specify the weights. Typically, these kernel functions have the following properties:
· K(t)  0 for all t

·  K (t)dt = 1 -
· K(-t) = K(t) (symmetry)

These are also the properties of a symmetric probability density function, which emphasizes the relationship back to nonparametric density estimation. The specific weights for the kernel smoother are given by

 wij

=

K

 

xi

- b

xj

 

n k=1

K

 

xi

- xk b

 

Table 7.5 summarizes the kernels used in S-PLUS. The properties of the kernel smoother depend much more on the choice of the bandwidth than the actual kernel function.

7.3.2 Locally Weighted Regression (Loess)
Another nonparametric alternative is locally weighted regression, often called loess. Like kernel regression, loess uses the data from a neighborhood around the specific

238

POLYNOMIAL REGRESSION MODELS

TABLE 7.5 Snmmary of the Kernel Functions Used in S-PLUS

Box Triangle
Parzen Normal

K

(t)

=

1, 0,

t  0.5 t > 0.5

K (t)

=

1 - 

t c

,

 

0,

t 1 c
t >1 c

  

k1 - k2

t2

,

K (t) =

 t2

 

k3

- k4

t

+ k5,



0,



t  C1 C1 < t  C2
t > C2

K (t) =

1 2

k6

exp

  

-t2 2k62

  

location. Typically, the neighborhood is defined as the span, which is the fraction of the total points used to form neighborhoods. A span of 0.5 indicates that the closest half of the total data points is used as the neighborhood. The loess procedure then uses the points in the neighborhood to generate a weighted least-squares estimate of the specific response. The weighted least-squares procedure uses a low-order polynomial, usually simple linear regression or a quadratic regression model. The weights for the weighted least-squares portion of the estimation are based on the distance of the points used in the estimation from the specific location of interest. Most software packages use the tri-cube weighting function as its default. Let x0 be the specific location of interest, and let (x0) be the distance the farthest point in the neighborhood lies from the specific location of interest. The tri-cube weight function is

W

 

x0 - xj
 (x0 )

 

where

W (t) = (1 - t3 )3 for 0  t < 1

0

elsewhere

We can summarize the loess estimation procedure by

y = Sy

where S is the smoothing matrix created by the locally weighted regression. The concept of sum of squared residuals carries over to nonparametric regression
directly. In particular,

NONPARAMETRIC REGRESSION

239

n
 SSRes = ( yi - yi )2 i=1 = (y - Sy) (y - Sy)
= y[I - S][I - S]y
= y[I - S - S + SS]y

Asymptotically, these smoothing procedures are unbiased. As a result, the asymptotic expected value for SSRes is
trace[(I - S - S + SS) s2I]
=  2trace[I - S - S + SS] =  2 [trace(I) - trace(S) - trace(S) + trace(SS)]

It is important to note that S is a square n × n matrix. As a result, trace[S] = trace[S]; thus,

E (SSRes ) =  2 [n - 2 trace(S) + trace(SS)]

In some sense, [2 trace(S) - trace(SS)] represents the degrees of freedom associated with the total model. In some packages, [2 trace(S) - trace(SS)] is called the
equivalent number of parameters and represents a measure of the complexity of the estimation procedure. A common estimate of 2 is

 (yi - yi )2

s2

=

n-2

i=1
trace(S) + trace(SS)

Finally, we can define a version of R2 by

R2 = SST - SSRes SST
whose interpretation is the same as before in ordinary least squares. All of this extends naturally to the multiple regression case, and S-PLUS has this capability.

Example 7.4 Applying Loess Regression to the Windmill Data
In Example 5.2, we discussed the data collected by an engineer who investigated the relationship of wind velocity and the DC electrical output for a windmill. Table 5.5 summarized these data. Ultimately in this example, we developed a simple linear regression model involving the inverse of the wind velocity. This model provided a nice basis for modeling the fact that there is a true upper bound to the DC output the windmill can generate.
An alternative approach to this example uses loess regression. The appropriate SAS code to analyze the windmill data is:

240

POLYNOMIAL REGRESSION MODELS

Output Residuals

2.5

2.0

1.5

1.0

0.5

0.0 2

4

6

8

10

Velocity

Figure 7.10 The loess fit to the windmill data.

0.1
0.0
-0.1
-0.2 0.0 0.5 1.0 1.5 2.0 2.5 Fit
Figure 7.11 The residuals versus fitted values for the loess fit to the windmill data.

TABLE 7.6 SAS Output for Loess Fit to Windmill Data
The LOESS Procedure Selected Smoothing Parameter: 0.78
Dependent Variable: output

Fit Summary Fit Method Blending Number of Observations Number of Fitting Points kd Tree Bucket Size Degree of Local Polynomials Smoothing Parameter Points in Local Neighborhood Residual Sum of Squares Trace[L] GCV AICC AICC1 Deltal Delta2 Equivalent Number of Parameters Lookup Degrees of Freedom Residual Standard Error

kd Tree Linear 25 10 3 2
0.78000 19
0.22112 4.56199 0.00052936 -3.12460 -77.85034 20.03324 19.70218 4.15723 20.36986 0.10506

proc loess; model output = velocity / degree = 2 dfmethod = exact residual;
Figure 7.10 gives the loess fit to the data using SAS's default settings, and Table 7.6 summarizes the resulting SAS report. Figure 7.11, which gives the residuals versus fitted values, shows no real problems. Figure 7.12 gives the normal probability plot, which, although not perfect, does not indicate any serious problems.

NONPARAMETRIC REGRESSION

241

99

Percent

95

90

80

70

60

50

40

30 20

Mean 0.009086 StDev 0.09554

10

N

25

5

AD

0.366

P Value 0.408

1 ­0.3 ­0.2 ­0.1 0.0 0.1 0.2 0.3

Residual

Figure 7.12 The normal probability plot of the residuals for the loess fit to the windmill data.

The loess fit to the data is quite good and compares favorably with the fit we

generated earlier using ordinary least squares and the inverse of the wind

velocity.

The report indicates an R2 of 0.98, which is the same as our final simple linear

regression model.Although the two R2 values are not directly comparable, they both

indicate a very good fit. The loess MSRes is 0.1017, compared to a value of 0.0089 for

the simple linear regression model. Clearly, both models are competitive with one

another. Interestingly, the loess fit requires an equivalent number of parameters of

4.4, which is somewhere between a cubic and quartic model. On the other hand, the

simple linear model using the inverse of the wind velocity requires only two param-

eters; hence, it is a much simpler model. Ultimately, we prefer the simple linear

regression model since it is simpler and corresponds to known engineering

theory. The loess model, on the other hand, is more complex and somewhat of a

"black box."



The R code to perform the analysis of these data is:

windmill <- read.table("windmill_loess.txt", header=TRUE, sep=" ") wind.model <- loess(outputvelocity, data=windmill)
summary(wind.model) yhat <- predict(wind.model)
plot(windmill$velocity,yhat)

7.3.3 Final Cautions
Parametric and nonparametric regression analyses each have their advantages and disadvantages. Often, parametric models are guided by appropriate subject area theory. Nonparametric models almost always reflect pure empiricism.

242

POLYNOMIAL REGRESSION MODELS

One should always prefer a simple parametric model when it provides a reasonable and satisfactory fit to the data.The complexity issue is not trivial. Simple models provide an easy and convenient basis for prediction. In addition, the model terms often have important interpretations. There are situations, like the windmill data, where transformations of either the response or the regressor are required to provide an appropriate fit to the data. Again, one should prefer the parametric model, especially when subject area theory supports the transformation used.
On the other hand, there are many situations where no simple parametric model yields an adequate or satisfactory fit to the data, where there is little or no subject area theory to guide the analyst, and where no simple transformation appears appropriate. In such cases, nonparametric regression makes a great deal of sense. One is willing to accept the relative complexity and the black-box nature of the estimation in order to give an adequate fit to the data.

7.4 POLYNOMIAL MODELS IN TWO OR MORE VARIABLES

Fitting a polynomial regression model in two or more regressor variables is a straightforward extension of the approach in Section 7.2.1. For example, a secondorder polynomial model in two variables would be

y = 0 + 1x1 + 2 x2 + 11x12 + 22 x22 + 12 x1x2 + 

(7.7)

Note that this model contains two linear effect parameters 1 and 2 two quadratic effect parameters 11 and 21 and an interaction effect parameter 12.
Fitting a second-order model such as Eq. (7.7) has received considerable attention, both from researchers and from practitioners. We usually call the regression function

E ( y) = 0 + 1x1 + 2 x2 + 11x12 + 22 x22 + 12 x1x2

a response surface. We may represent the two-dimensional response surface graphically by drawing the x1 and x2 axes in the plane of the paper and visualizing the E( y) axis perpendicular to the plane of the paper. Plotting contours of constant expected response E(y) produces the response surface. For example, refer to Figure 3.3, which shows the response surface

E ( y) = 800 + 10x1 + 7x2 - 8.5x12 - 5x22 + 4x1x2

Note that this response surface is a hill, containing a point of maximum response. Other possibilities include a valley containing a point of minimum response and a saddle system. Response surface methodology (RSM) is widely applied in industry for modeling the output response(s) of a process in terms of the important controllable variables and then finding the operating conditions that optimize the response. For a detailed treatment of response surface methods see Box and Draper [1987], Box, Hunter, and Hunter [1978], Khuri and Cornell [1996], Montgomery [2009], and Myers, Montgomery and Anderson Cook [2009].
We now illustrate fitting a second-order response surface in two variables. Panel A of Table 7.7 presents data from an experiment that was performed to study the

POLYNOMIAL MODELS IN TWO OR MORE VARIABLES

243

effect of two variables, reaction temperature (T ) and reactant concentration (C), on the percent conversion of a chemical process (y). The process engineers had used an approach to improving this process based on designed experiments. The first experiment was a screening experiment involving several factors that isolated temperature and concentration as the two most important variables. Because the experimenters thought that the process was operating in the vicinity of the optimum, they elected to fit a quadratic model relating yield to temperature and concentration.
Panel A of Table 7.7 shows the levels used for T and C in the natural units of measurements. Panel B shows the levels in terms of coded variables x1 and x2.
Figure 7.13 shows the experimental design in Table 7.5 graphically. This design is called a central composite design, and it is widely used for fitting a second-order

TABLE 7.7 Central Composite Design for Chemical Process Example

A

B

Observation Run Order Temperature (°C) T Cone. (%) C x1

x2

y

1

4

200

15

-1

-1

43

2

12

250

15

1

-1

78

3

11

200

25

-1

1

69

4

5

250

25

1

1

73

5

6

189.65

20

-1.414 0

48

6

7

260.35

20

1.414 0

76

7

1

225

12.93

0

-1.414 65

8

3

225

27.07

0

1.414 74

9

8

225

20

0

0

76

10

10

225

20

0

0

79

11

9

225

20

0

0

83

12

2

225

20

0

0

81

+2

30

x2 Concentration, C (%)

+1

25

0

20

-1

15

-2

10

175

200

225

250

275

Temperature, T (°C)

-2

-1

0

+1

+2

x1

Figure 7.13 Central composite design for the chemical process example.

244

POLYNOMIAL REGRESSION MODELS

response surface. Notice that the design consists of four runs at the comers of a
square plus four runs at the center of this square plus four axial runs, In terms of the coded variables the comers of the square are (x1, x2) = (-1, -1), (1, -1), (-1, 1), (1, 1); the center points are at (x1, x2) = (0, 0); and the axial runs are at (x1, x2) = (-1.414, 0), (1.414, 0), (0, -1.414), (0, 1.414).
We fit the second-order model

y = 0 + 1x1 + 2 x2 + 11x12 + 22 x22 + 12 x1x2 + 
using the coded variables, as that is the standard practice in RSM work. The X matrix and y vector for this model are

1

1

1

1

1



X

=

1 1

1

1

1

1

1

x1 -1
1 -1
1 -1.414
1.414 0 0 0 0 0 0

x2 -1 -1
1 1 0 0 -1.414 1.414 0 0 0 0

x12 x22 x1x2

1 1 1

1

1

-1

 

1 1 -1 

11

1

 

2 0 0



20 02

0 0

  

,

0 2 0

00

0

 

0 0 0

00

0

 

0 0 0 

43

78

69

73

48

y

=

76 65

74

76

79

83

81

Notice that we have shown the variables associated with each column above that column in the X matrix. The entries in the columns associated with x12 and x22 are found by squaring the entries in columns x1 and x2, respectively, and the entries in the x1x2 column are found by multiplying each entry from x1 by the corresponding entry from x2. The XX matrix and Xy vector are

12 0 0 8 8 0

 845.000 

 

0

8

0

0

0 0

 

78.592

 0 0 8 0 0 0

 33.726

XX =  

8

0

0

12

4

0 ,

Xy

=

 

511.000

 8 0 0 4 12 0

 541.000 





 0 0 0 0 0 4

-31.000

and from b^ = (XX)-1 Xy we obtain

POLYNOMIAL MODELS IN TWO OR MORE VARIABLES

245

TABLE 7.8 Analysis of Variance for the Chemical Process Example

Source of Variation
Regression SSR(1, 2|0) SSR(11, 22, 12|0, 1, 2)
Residual Lack of fit Pure error
Total R2 = 0.9800

Sum of Squares
1733.6 (914.4) (819.2)
35.3 (8.5) (26.8) 1768.9

Degrees of Freedom
5 (2) (3)
6 (3) (3) 11 Ra2dj = 0.9634

Mean Square
346.71 (457.20) (273.10)
5.89 (2.83) (8.92)

F0 58.86

P Value <0.0001

0.3176

0.8120

PRESS = 108.7

 79.75

 

9.83

b^

=

 4.22 -8.88

 -5.13

 

-7.75 

Therefore, the fitted model for percent conversion is

y^ = 79.75 + 9.83x1 + 4.22x2 - 8.88x12 - 5.13x22 - 7.75x1x2

In terms of the natural variables, the model is

y^ = -1105.56 + 8.0242T + 22.994C + 0.0142T 2 + 0.20502C2 + 0.062TC

Table 7.8 shows the analysis of variance for this model. Because the experimental design has four replicate runs, the residual sum of squares can be partitioned into pure-error and lack-of-fit components. The lack-of-fit test in Table 7.8 is testing the lack of fit for the quadratic model. The P value for this test is large (P = 0.8120), implying that the quadratic model is adequate. Therefore, the residual mean square with six degrees of freedom is used for the remaining analysis. The F test for significance of regression is F0 = 58.86; and because the P value is very small, we would reject the hypothesis H0: 1 = 2 = 11 = 22 = 12 = 0, concluding that at least some of these parameters are nonzero. This table also shows the sum of squares for testing the contribution of only the linear terms to the model [SSR(1, 2|0) = 918.4 with two degrees of freedom] and the sum of squares for testing the contribution of the quadratic terms given that the model already contains the linear terms [SSR(11, 22, 12|0, 1, 2) = 819.2 with three degrees of freedom]. Comparing both of the corresponding mean squares to the residual mean square gives the following F statistics

246

POLYNOMIAL REGRESSION MODELS

TABLE 7.9 Tests on the Individual Variables, Chemical Process Quadratic Model

Variable

Coefficient Estimate

Standard Error

t for H0 Coefficient = 0

P Value

Intercept

79.75

1.21

xl

9.83

0.86

x2

4.22

0.86

x12

-8.88

0.96

x22

-5.13

0.96

xlx2

-7.75

1.21

65.72 11.45
4.913
-9.250
-5.341 -6.386

0.0001 0.0027
0.0001
0.0018 0.0007

F0

=

SSR (1, 2 0 )
MSRes

2

=

914.4 2 5.89

=

457.2 5.89

=

77.62

for which P = 5.2 × 10-5 and

F0

=

SSR (11, 22, 12 0,
MSRes

1, 2 )

3

=

819.2 3 5.89

=

273.1 5.89

=

46.37

for which P = 0.0002. Therefore, both the linear and quadratic terms contribute

significantly to the model.

Table 7.9 shows t tests on each individual variable. All t values are large enough

for us to conclude that there are no nonsignificant terms in the model. If some of

these t statistics had been small, some analysts would drop the nonsignificant vari-

ables for the model, resulting in a reduced quadratic model for the process. Gener-

ally, we prefer to fit the full quadratic model whenever possible, unless there are

large differences between the full and reduced model in terms of PRESS and

adjusted R2. Table 7.8 indicates that the R2 and adjusted R2 values for this model

are

satisfactory.

R , 2 prediction

based

on

PRESS,

is

R2 prediction

=

1-

PRESS SST

=

1-

108.7 1768.9

=

0.9385

indicating that the model will probably explain a high percentage (about 94%) of the variability in new data.
Table 7.10 contains the observed and predicted values of percent conversion, the residuals, and other diagnostic statistics for this model. None of the studentized residuals or the values of R-student are large enough to indicate any potential problem with outliers. Notice that the hat diagonals hii take on only two values, either 0.625 or 0.250. The values of hii = 0.625 are associated with the four runs at the corners of the square in the design and the four axial runs. All eight of these points are equidistant from the center of the design; this is why all of the hii values are identical. The four center points all have hii = 0.250. Figures 7.14, 7.15, and 7.16 show a normal probability plot of the studentized residuals, a plot of the studentized residuals versus the predicted values y^i, and a plot of the studentized residuals versus run order. None of these plots reveal any model inadequacy.

POLYNOMIAL MODELS IN TWO OR MORE VARIABLES

247

TABLE 7.10 Observed Values, Predicted Values, Residuals, and Other Diagnostics for the Cbemical Process Example

Observed Value
1 2 3 4 5 6 7 8 9 10 11 12

Actual Value
43.00 78.00 69.00 73.00 48.00 76.00 65.00 74.00 76.00 79.00 83.00 81.00

Predicted Value
43.96 79.11 67.89 72.04 48.11 75.90 63.54 75.46 79.75 79.75 79.75 79.75

Residual
-0.96 -1.11
1.11 0.96 -0.11 0.10 1.46 -1.46 -3.75 -0.75 3.25 1.25

hii
0.625 0.625 0.625 0.625 0.625 0.625 0.625 0.625 0.250 0.250 0.250 0.250

Studentized Residual
-0.643 -0.745
0.748 0.646 -0.073 -0.073 0.982 -0.985 -1.784 -0.357 1.546 0.595

Cook's D
0.115 0.154 0.155 0.116 0.001 0.001 0.268 0.269 0.177 0.007 0.133 0.020

R-Student
-0.609 -0.714
0.717 0.612 -0.067 -0.067 0.979 -0.982 -2.377 -0.329 1.820 0.560

Normal % probability Student residuals

99
95 90 80 70 50 30 20 10
5 1
­1.784 ­0.674 0.436 1.546 ­1.229 0.119 0.991 Studentized residual
Figure 7.14 Normal probability plot of the studentized residuals, chemical process example.

1.546
0.991
0.436
­0.119
­0.674
­1.229
­1.784 43.96 55.89 67.82 79.75 49.92 61.85 73.78 Predicted as conversion in percent
Figure 7.15 Plot of studentized residuals versus predicted conversion, chemical process example.

Student residuals

1.546 0.991 0.436 ­0.119 ­0.674 ­1.229 ­1.784

1 2 3 4 5 6 7 8 9 10 11 12 Run number

Figure 7.16 Plot of the studentized residuals run order, chemical process example.

248

POLYNOMIAL REGRESSION MODELS

Conversion Concentration, C (%)

82.47 60.45

1.414 1

38.43 16.40

x2 0

1.414

27.07 24.24
1Coxn2ce0n2tr1a-.t41io11n8, .C5195(%.7)162.93
-1.414

260.4
189.7203T.2e8m17p.e92r-3a12tu.r12e,4T06.(2x°C1)1 -1.414
(a)

-1 1.414 -1.414

27.07 24.71

75.00 60.00 70.00

22.36

20.00

80.00 75.00

17.64

70.00 60.00

15.29

50.00 40.00

30.00 12.93
189.7 201.4 213.2 225.0 236.8 248.6 260.4

Ttemperature, T (°C)

-1.414 -1

0 x1
(b)

1 1.414

Figure 7.17 (a) Response surface of predicted conversion. (b) Contour plot of predicted conversion.

Plots of the conversion response surface and the contour plot, respectively, for the fitted model are shown in panels a and b of Figure 7.17. The response surface plots indicate that the maximum percent conversion occurs at about 245°C and 20% concentration.
In many response surface problems the experimenter is interested in predicting the response y or estimating the mean response at a particular point in the process variable space. The response surface plots in Figure 7.17 give a graphical display of these quantities. Typically, the variance of the prediction is also of interest, because this is a direct measure of the likely error associated with the point estimate produced by the model. Recall that the variance of the estimate of the mean response
at the point x0 is given by Var[y^ (x0 )] =  2x0 (XX)-1 x0. Plots of Var[y^ (x0 )], with
2 estimated by the residual mean square MSRes = 5.89 for this model for all values of x0 in the region of experimentation, are presented in panels a and b of Figure 7.18. Both the response surface in Figure 7.18a and the contour plot of constant
Var[y^ (x0 )] in Figure 7.18b show that the Var[y^ (x0 )] is the same for all points
x0 that are the same distance from the center of the design. This is a result of the spacing of the axial runs in the central composite design at 1.414 units from the origin (in the coded variables) and is a design property called rotatability. This is a very important property for a second-order response surface design and is discussed in detail in the references given on RSM.
7.5 ORTHOGONAL POLYNOMIALS
We have noted that in fitting polynomial models in one variable, even if nonessential ill-conditioning is removed by centering, we may still have high levels of multicollinearity. Some of these difficulties can be eliminated by using orthogonal polynomials to fit the model.

ORTHOGONAL POLYNOMIALS

249

3.640 2.810

1.414 1

27.07 2.810

2.400

24.71

2.000

22.36

2.810 2.400 2.000

SE mean Concentration, C (%)

1.981

x2 0

1.414

1.151
27.07 1Cxo2n2ce40n.2t2r4a1-t.i1o41n1,8C.51(95%.7)162.93
-1.414

260.4
1892.07T3e.28m17pe.29ra3-t21u.r12e,4T60.(x2°C1 )1 -1.414

-1 1.414
-1.414

(a)

20.00

17.64

1.200

2.000

15.29 2.400 12.93 2.810

1.300 1.200 1.500

2.000 2.810

189.7 201.4 213.2 225.0 236.8 248.6 260.4

Temperature, T (°C)

-1.414

-1

0

x1

(b)

1

1.414

Figure 7.18 (a) Response surface plot of Var[y^ (x0 )]. (b) Contour plot of Var[y^ (x0 )].

Suppose that the model is

yi = 0 + 1xi + 2 xi2 + + k xik + i, i = 1, 2, ... , n

(7.8)

Generally the columns of the X matrix will not be orthogonal. Furthermore, if we increase the order of the polynomial by adding a term k+1xk+1, we must recompute (XX)-1 and the estimates of the lower order parameters ^0, ^1,... , ^k will
change.
Now suppose that we fit the model

yi = 0 P0 (xi ) + 1P1 (xi ) + 2 P2 (xi ) + k Pk (xi ) + i, i = 1, 2, ... , n

(7.9)

where Pu(xi) is a uth-order orthogonal polynomial defined such that

n
 Pr (xi ) Ps (xi ) = 0,
i=1
P0 (xi ) = 1

r  s,

r, s = 0, 1, 2,... , k

Then the model becomes y = X + , where the X matrix is

 P0 (x1 )

X

=

 

P0

(

x2

)



P0 (xn )

P1(x1 ) P1(x2 )
P1(xn )

Pk (x1 ) Pk (x2 )

Pk (xn )

250

POLYNOMIAL REGRESSION MODELS

Since this matrix has orthogonal columns, the XX matrix is




n

P02 (xi )

 i=1



 XX = 

0







 

0

0
n
 P12 (xi )
i=1
0



0





0

 





n



i=1 Pk2 (xi )

The least-squares estimators of  are found from (XX)-1Xy as

n
 Pj (xi ) yi

^ j =

i=1 n

,

 Pj2 (xi )

i=1

j = 0, 1, ... , k

(7.10)

Since P0(xi) is a polynomial of degree zero, we can set P0(xi) = 1, and consequently

^ 0 = y^

The residual sum of squares is

  SSRes (k) = SST -

k

^

j

 

n

Pj

(

xi

)

yi

 

 j=1

i=1



(7.11)

The regression sum of squares for any model parameter does not depend on the other parameters in the model. This regression sum of squares is

n
 SSR ( j ) = ^ j Pj (xi ) yi i=1

(7.12)

If we wish to assess the significance of the highest order term, we should test H0: k = 0 [this is equivalent to testing H0: k = 0 in Eq. (7.4)]; we would use

n

 F0

=

SSR (k ) SSRes (k) (n - k

-

1)

=

^ k Pk (xi ) yi
i=1
SSRes (k) (n - k -

1)

(7.13)

as the F statistic. Furthermore, note that if the order of the model is changed to k + r, ouly the r new coefficients must be computed. The coefficients ^ 0, ^ 1,... , ^ k

ORTHOGONAL POLYNOMIALS

251

do not change due to the orthogonality property of the polynomials. Thus, sequential fitting of the model is computationally easy.
The orthogonal polynomials Pj(xi) are easily constructed for the case where the levels of x are equally spaced. The first five orthogonal polynomials are

P0 (xi ) = 1

P1

(

xi

)

=

1

 

xi

- d

x

 

P2

( xi

)

=

2



( xi

- d

x )

2

-

 

n2 - 12

1 

 

P3

( xi

)

=

3

 

xi

- d

x

 

3

-

 

xi

- d

x

 

 

3n2 - 20

7

 

 

P4 (xi

)

=

4



xi

- d

x

 4

-



xi

- d

x

 2

 

3n2 - 14

13 

+

3(n2

- 1)(n2
560

-

9)
 

where d is the spacing between the levels of x and the {j} are constants chosen so that the polynomials will have integer values. A brief table of the numerical values of these orthogonal polynomials is given in Table A.5. More extensive tables are found in DeLury [1960] and Pearson and Hartley [1966]. Orthogonal polynomials can also be constructed and used in cases where the x's are not equally spaced. A survey of methods for generating orthogonal polynomials is in Seber [1977, Ch. 8].

Example 7.5 Orthogonal Polynomials
An operations research analyst has developed a computer simulation model of a single item inventory system. He has experimented with the simulation model to investigate the effect of various reorder quantities on the average annual cost of the inventory. The data are shown in Table 7.11.

TABLE 7.11 Inventory Simnlatian Ontpnt far Example 7.5

Reorder Quantity, xi

Average Annual Cost, yi

50

$335

75

326

100

316

125

313

150

311

175

314

200

318

225

328

250

337

275

345

252

POLYNOMIAL REGRESSION MODELS

TABLE 7.12 Coefficients of Orthogonal Polynomials for Example 7.5

i

P0(xi)

P1(xi)

1

1

-9

2

1

-7

3

1

-5

4

1

-3

5

1

-1

6

1

1

7

1

3

8

1

5

9

1

7

10

1

9

10
 P02 (xi ) = 10
i=1

10
 P12 (xi ) = 330
i=1
1 = 2

P2(xi)
6 2 -1 -3 -4 -4 -3 -1 2 6

10
 P22 (xi ) = 132
i=1

2

=

1 2

Since we know that average annual inventory cost is a convex function of the reorder quantity, we suspect that a second-order polynomial is the highest order model that must be considered. Therefore, we will fit

yi = 0 P0 (xi ) + 1P1 (xi ) + 2 P2 (xi ) + i, i = 1, 2, ... , 10

The coefficients of the orthogonal polynomials P0(xi), P1(xi), and P2(xi), obtained from Table A.5, are shown in Table 7.12.
Thus,




10

P02 (xi )

0

 0

 i=1



10

 

10

0

0

XX

=

 

0



 

0

 P12 (xi )
i=1

0

 

=

 

0

330

0

  0 0 132

0

10 P22 (xi )



i=1






10

P0

(

xi

)

yi

 

 i=1  10

 

3243

 Xy = 

 

i=1

P1 (xi

)

yi

 

=

 

 

245 369 




10

P2

(

xi

)

yi

 

 i=1



ORTHOGONAL POLYNOMIALS

253

TABLE 7.13 Analysis of Variance for the Quadratic Model in Example 7.5

Source of Variation
Regression Linear, 1 Quadratic, 2
Residual Total

Sum of Squares
1213.43 (181.89) (1031.54) 26.67 1240.10

Degrees of Freedom
2 1 1 7 9

Mean Square
606.72 181.89 1031.54
3.81

F0
159.24 47.74 270.75

P Value
<0.0001 <0.0002 <0.0001

and

b^

=

(XX)-1 Xy

=

  

1 10
0

0
1 330

0  3243 324.3000

0

 

=

 

245

=

 

0.7424

 0

0

1 132



 369



2.7955

The fitted model is

y^ = 324.30 + 0.7424P1(x) + 2.7955P2 (x)
The regression sum of squares is

  SSR (1, 2 ) =

2

^ j

 

10

Pj

(

xi

)

yi

 

 j = 1

i=1



= 0.7424(245) + 2.7955(369)

= 181.89 + 1031.54 = 1213.43

The analysis of variance is shown in Table 7.13. Both the linear and quadratic terms contribute significantly to the model. Since these terms account for most of the variation in the data, we tentatively adopt the quadratic model subject to a satisfactory residual analysis.
We may obtain a fitted equation in terms of the original regressor by substituting for Pj(xi) as follows:

y^ = 324.30 + 0.7424P1(x) + 2.7955P2 (x)

=

324.30

+

0.7424 ( 2) 

x

-21562.5

+

2.7955

1 2



x

-

162.5 25



2

-

(10)2 -
12

1  

= 312.7686 + 0.0594(x - 162.5) + 0.0022(x - 162.5)2

This form of the model should be reported to the user.



254

POLYNOMIAL REGRESSION MODELS

PROBLEMS

7.1 Consider the values of x shown below: x = 1.00, 1.70, 1.25, 1.20, 1.45, 1.85, 1.60, 1.50, 1.95, 2.00

Suppose that we wish to fit a second-order model using these levels for the regressor variable x. Calculate the correlation between x and x2. Do you see any potential difficulties in fitting the model?
7.2 A solid-fuel rocket propellant loses weight after it is produced. The following data are available:

Months since Production, x
0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50

Weight Loss, y (kg)
1.42 1.39 1.55 1.89 2.43 3.15 4.05 5.15 6.43 7.89

a. Fit a second-order polynomial that expresses weight loss as a function of the number of months since production.
b. Test for significance of regression. c. Test the hypothesis H0: 2 = 0. Comment on the need for the quadratic term
in this model. d. Are there any potential hazards in extrapolating with this model?
7.3 Refer to Problem 7.2. Compute the residuals for the second-order model. Analyze the residuals and comment on the adequacy of the model.
7.4 Consider the data shown below:

x

y

x

y

4.00

24.60

6.50

67.11

4.00

24.71

6.50

67.24

4.00

23.90

6.75

67.15

5.00

39.50

7.00

77.87

5.00

39.60

7.10

80.11

6.00

57.12

7.30

84.67

a. Fit a second-order polynomial model to these data. b. Test for significance of regression.

PROBLEMS 255
c. Test for lack of fit and comment on the adequacy of the second-order model.
d. Test the hypothesis H0: 2 = 0. Can the quadratic term be deleted from this equation?
7.5 Refer to Problem 7.4. Compute the residuals from the second-order model. Analyze the residuals and draw conclusions about the adequacy of the model.
7.6 The carbonation level of a soft drink beverage is affected by the temperature of the product and the filler operating pressure. Twelve observations were obtained and the resulting data are shown below.

Carbonation, y
2.60 2.40 17.32 15.60 16.12 5.36 6.19 10.17 2.62 2.98 6.92 7.06

Temperature, x1
31.0 31.0 31.5 31.5 31.5 30.5 31.5 30.5 31.0 30.5 31.0 30.5

Pressure, x2
21.0 21.0 24.0 24.0 24.0 22.0 22.0 23.0 21.5 21.5 22.5 22.5

a. Fit a second-order polynomial. b. Test for significance of regression. c. Test for lack of fit and draw conclusions. d. Does the interaction term contribute significantly to the model? e. Do the second-order terms contribute significantly to the model?
7.7 Refer to Problem 7.6. Compute the residuals from the second-order model. Analyze the residuals and comment on the adequacy of the model.
7.8 Consider the data in Problem 7.2. a. Fit a second-order model to these data using orthogonal polynomials. b. Suppose that we wish to investigate the addition of a third-order term to this model. Comment on the necessity of this additional term. Support your conclusions with an appropriate statistical analysis.
7.9 Suppose we wish to fit the piecewise quadratic polynomial with a knot at x = t:

E ( y) = S (x) = 00 + 01x + 02 x2 + 10 (x - t)0+ + 11 (x - t)1+ + 12 (x - t)2+

a. Show how to test the hypothesis that this quadratic spline model fits the data significantly better than an ordinary quadratic polynomial.

256

POLYNOMIAL REGRESSION MODELS

b. The quadratic spline polynomial model is not continuous at the knot t. How can the model be modified so that continuity at x = t is obtained?
c. Show how the model can be modified so that both E(y) and dE(y)/dx are continuous at x = t.
d. Discuss the significance of the continuity restrictions on the model in parts b and c. In practice, how would you select the type of continuity restrictions to impose?

7.10 Consider the delivery time data in Example 3.1. Is there any indication that a complete second-order model in the two regressions cases and distance is preferable to the first-order model in Example 3.1?
7.11 Consider the patient satisfaction data in Section 3.6. Fit a complete secondorder model to those data. Is there any indication that adding these terms to the model is necessary?
7.12 Suppose that we wish to fit a piecewise polynomial model with three segments: if x < t1, the polynomial is linear; if t1  x < t2, the polynomial is quadratic; and if x > t2, the polynomial is linear. Consider the model
E ( y) = S (x) = 00 + 01x + 02 x2 + 10 (x - t1 )0+ + 11 (x - t1 )1+ + 12 (x - t1 )+2 + 20 (x - t2 )+2 + 21 (x - t2 )1+ + 22 (x - t2 )+2
a. Does this segmented polynomial satisfy our requirements? If not, show how it can be modified to do so.
b. Show how the segmented model would be modified to ensure that E(y) is continuous at the knots t1 and t2.
c. Show how the segmented model would be modified to ensure that both E(y) and dE(y)/dx are continuous at the knots t1 and t2.
7.13 An operations research analyst is investigating the relationship between production lot size x and the average production cost per unit y. A study of recent operations provides the following data:

x 100 120 140 160 180 200 220 240 260 280 300 y $9.73 9.61 8.15 6.98 5.87 4.98 5.09 4.79 4.02 4.46 3.82

The analyst suspects that a piecewise linear regression model should be fit to these data. Estimate the parameters in such a model assuming that the slope of the line changes at x = 200 units. Do the data support the use of this model?
7.14 Modify the model in Problem 7.13 to investigate the possibility that a discontinuity exists in the regression function at x = 200 units. Estimate the parameters in this model. Test appropriate hypotheses to determine if the regression function has a change in both the slope and the intercept at x = 200 units.
7.15 Consider the polynomial model in Problem 7.13. Find the variance inflation factors and comment on multicollinearity in this model.

PROBLEMS 257
7.16 Consider the data in Problem 7.2. a. Fit a second-order model y = 0 + 1x + 11x2 +  to the data. Evaluate the variance inflation factors.
b. Fit a second-order model y = 0 + 1(x - x ) + 11(x - x )2 +  to the data.
Evaluate the variance inflation factors. c. What can you conclude about the impact of centering the x's in a polyno-
mial model on multicollinearity?
7.17 Chemical and mechanical engineers often need to know the vapor pressure of water at various temperatures (the "infamous" steam tables can be used for this). Below are data on the vapor pressure of water (y) at various temperatures.

Vapor Pressure, y (mmHg)
9.2 17.5 31.8 55.3 92.5 149.4

Temperature, x (°C)
10 20 30 40 50 60

a. Fit a first-order model to the data. Overlay the fitted model on the scatterplot of y versus x. Comment on the apparent fit of the model.
b. Prepare a scatterplot of predicted y versus the observed y. What does this suggest about model fit?
c. Plot residuals versus the fitted or predicted y. Comment on model adequacy.
d. Fit a second-order model to the data. Is there evidence that the quadratic term is statistically significant?
e. Repeat parts a­c using the second-order model. Is there evidence that the second-order model provides a better fit to the vapor pressure data?
7.18 An article in the Journal of Pharmaceutical Sciences (80, 971­977, 1991) presents data on the observed mole fraction solubility of a solute at a constant temperature, along with x1 = dispersion partial solubility, x2 = dipolar partial solubility, and x3 = hydrogen bonding Hansen partial solubility. The response y is the negative logarithm of the mole fraction solubility. a. Fit a complete quadratic model to the data. b. Test for significance of regression, and construct t statistics for each model parameter. Interpret these results. c. Plot residuals and comment on model adequacy. d. Use the extra-sum-of-squares method to test the contribution of all secondorder terms to the model.

258

POLYNOMIAL REGRESSION MODELS

Observation

Number

y

x1

x2

x3

1

0.22200

7.3 0.0

0.0

2

0.39500

8.7 0.0

0.3

3

0.42200

8.8 0.7

1.0

4

0.43700

8.1 4.0

0.2

5

0.42800

9.0 0.5

1.0

6

0.46700

8.7 1.5

2.8

7

0.44400

9.3 2.1

1.0

8

0.37800

7.6 5.1

3.4

9

0.49400 10.0 0.0

0.3

10

0.45600

8.4 3.7

4.1

11

0.45200

9.3 3.6

2.0

12

0.11200

7.7 2.8

7.1

13

0.43200

9.8 4.2

2.0

14

0.10100

7.3 2.5

6.8

15

0.23200

8.5 2.0

6.6

16

0.30600

9.5 2.5

5.0

17

0.09230

7.4 2.8

7.8

18

0.11600

7.8 2.8

7.7

19

0.07640

7.7 3.0

8.0

20

0.43900 10.3 1.7

4.2

21

0.09440

7.8 3.3

8.5

22

0.11700

7.1 3.9

6.6

23

0.07260

7.7 4.3

9.5

24

0.04120

7.4 6.0 10.9

25

0.25100

7.3 2.0

5.2

26

0.00002

7.6 7.8 20.7

7.19 Consider the quadratic regression model from Problem 7.18. Find the variance inflation factors and comment on multicollinearity in this model.
7.20 Consider the solubility data from Problem 7.18. Suppose that a point of interest is x1 = 8.0, x2 = 3.0, and x3 = 5.0. a. For the quadratic model from Problem 7.18, predict the response at the point of interest and find a 95% confidence interval on the mean response at that point. b. Fit a model that includes only the main effects and two-factor interactions to the solubility data. Use this model to predict the response at the point of interest. Find a 95% confidence interval on the mean response at that point. c. Compare the lengths of the confidence intervals in parts a and b. Can you draw any conclusions about the best model from this comparison?
7.21 Below are data on y = green liquor (g/l) and x = paper machine speed (ft/min) from a kraft paper machine. (The data were read from a graph in an article in the Tappi Journal, March 1986.)

PROBLEMS 259

y 16.0 x 1700 y 14.0 x 1760

15.8 1720 13.5 1770

15.6 1730 13.0 1780

15.5 1740 12.0 1790

14.8 1750 11.0 1795

a. Fit the model y = 0 + 1x + 2x2 +  to the data. b. Test for significance of regression using  = 0.05. What are your
conclusions?
c. Test the contribution of the quadratic term to the model, the contribution of the linear term, using an F statistic. If  = 0.05, what conclusion can you draw?
d. Plot the residuals from the model. Does the model fit seem satisfactory?

7.22 Reconsider the data from Problem 7.21. Suppose that it is important to predict the response at the points x = 1750 and x = 1775.
a. Find the predicted response at these points and the 95% prediction intervals for the future observed response at these points.
b. Suppose that a first-order model is also being considered. Fit this model and find the predicted response at these points. Calculate the 95% prediction intervals for the future observed response at these points. Does this give any insight about which model should be preferred?

CHAPTER 8

INDICATOR VARIABLES

8.1 GENERAL CONCEPT OF INDICATOR VARIABLES

The variables employed in regression analysis are often quantitative variables, that is, the variables have a well-defined scale of measurement. Variables such as temperature, distance, pressure, and income are quantitative variables. In some situations it is necessary to use qualitative or categorical variables as predictor variables in regression. Examples of qualitative or categorical variables are operators, employment status (employed or unemployed), shifts (day, evening, or night), and sex (male or female). In general, a qualitative variable has no natural scale of measurement. We must assign a set of levels to a qualitative variable to account for the effect that the variable may have on the response. This is done through the use of indicator variables. Sometimes indicator variables are called dummy variables.
Suppose that a mechanical engineer wishes to relate the effective life of a cutting tool ( y) used on a lathe to the lathe speed in revolutions per minute (x1) and the type of cutting tool used. The second regressor variable, tool type, is qualitative and has two levels (e.g., tool types A and B). We use an indicator variable that takes on the values 0 and 1 to identify the classes of the regressor variable "tool type." Let

x2

=

0  1

if the observation is from tool type A if the observation is from tool type B

The choice of 0 and 1 to identify the levels of a qualitative variable is arbitrary. Any two distinct values for x2 would be satisfactory, although 0 and 1 are usually best.

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
260

GENERAL CONCEPT OF INDICATOR VARIABLES

261

Assuming that a first-order model is appropriate, we have

y = 0 + 1x1 + 2 x2 + 

(8.1)

To interpret the parameters in this model, consider first tool type A, for which x2 = 0. The regression model becomes

y = 0 + 1x1 + 2 (0) + 
= 0 + 1x1 + 

(8.2)

Thus, the relationship between tool life and lathe speed for tool type A is a straight line with intercept 0 and slope 1. For tool type B, we have x2 = 1, and

y = 0 + 1x1 + 2 (1) +  = (0 + 2 ) + 1x1 + 

(8.3)

That is, for tool type B the relationship between tool life and lathe speed is also a straight line with slope 1 but intercept 0 + 2.
The two response functions are shown in Figure 8.1. The models (8.2) and (8.3) describe two parallel regression lines, that is, two lines with a common slope 1 and different intercepts. Also the variance of the errors  is assumed to be the same for both tool types A and B. The parameter 2 expresses the difference in heights between the two regression lines, that is, 2 is a measure of the difference in mean tool life resulting from changing from tool type A to tool type B.
We may generalize this approach to qualitative factors with any number of levels.
For example, suppose that three tool types, A, B, and C, are of interest. Two indicator

Tool life, y (hours)

50

0+2
2 0

E (y | x2 = 1) = (0 + 2) +1x, tool type B 1

1

E (y | x2 = 0) = 0 + 1x1, tool type A

0 500

Lathe speed, x1 (RPM)

1000

Figure 8.1 Response functions for the tool life example.

262 INDICATOR VARIABLES
variables, such as x2 and x3, will be required to incorporate the three levels of tool type into the model. The levels of the indicator variables are

x2

x3

0

0

if the observation is from tool type A

1

0

if the observation is from tool type B

0

1

if the observation is from tool type C

and the regression model is
y = 0 + 1x1 + 2 x2 + 3 x3 + 
In general, a qualitative variable with a levels is represented by a - 1 indicator variables, each taking on the values 0 and 1.

Example 8.1 The Tool Life Data
Twenty observations on tool life and lathe speed are presented in Table 8.1, and the scatter diagram is shown in Figure 8.2. Inspection of this scatter diagram indicates that two different regression lines are required to adequately model these data, with the intercept depending on the type of tool used. Therefore, we fit the model
y = 0 + 1x1 + 2 x2 + 

TABLE 8.1 Data, Fitted Values, and Residuals for Example 8.1

i

yi (hours)

Xi1 (rpm)

Tool Type

y^ i

1

18.73

2

14.52

3

17.43

4

14.54

5

13.44

6

24.39

7

13.34

8

22.71

9

12.68

10

19.32

11

30.16

12

27.09

13

25.40

14

26.05

15

33.49

16

35.62

17

26.07

18

36.78

19

34.95

20

43.67

610 950 720 840 980 530 680 540 890 730 670 770 880 1000 760 590 910 650 810 500

A

20.7552

A

11.7087

A

17.8284

A

14.6355

A

10.9105

A

22.8838

A

18.8927

A

22.6177

A

13.3052

A

17.5623

B

34.1630

B

31.5023

B

28.5755

B

25.3826

B

31.7684

B

36.2916

B

27.7773

B

34.6952

B

30.4380

B

38.6862

ei
-2.0252 2.8113
-0.3984 -0.0955
2.5295 1.5062 -5.5527 0.0923 -0.6252 1.7577 -4.0030 -4.4123 -3.1755 0.6674 1.7216 -0.6716 -1.7073 2.0848 4.5120 4.9838

GENERAL CONCEPT OF INDICATOR VARIABLES

263

45

A

Scatterplot of TRES1 vs FITS1

40

B

2

type A

35

1

B

Tool life, y TRES1

30
25
20
15
10 500 600 700 800 900 1000 Speed, x1

0
­1
­2 10 15 20 25 30 35 40 FITS1

Figure 8.2 Plot of tool life y versus lathe speed x1 for tool types A and B.

Figure 8.3 Plot of externally studentized residuals t versus fitted values y^i, Example
8.1.

where the indicator variable x2 = 0 if the observation is from tool type A and x2 = 1 if the observation is from tool type B. The X matrix and y vector for fitting this model are

1 610 0

18.73

1 950 0

14.52

1 720 0

17.43

1 840 0

14.54

1 980 0





1 530 0

13.44  24.39

1 680 0

13.34

1 540 0

22.71

1 890 0

12.68

X

=

1 1

730 670

0 1

and

y

=

19.32 30.16

1 770 1





1 880 1

27.09  25.40

1 1000 1

26.05

1 760 1

33.49

1 590 1

35.62

1 910 1

26.07

1 650 1

36.78

1 810 1

34.95

1 500 1

43.67

264 INDICATOR VARIABLES

TABLE 8.2 Summary Statistics for the Regression Model in Example 8.1

Source of

Sum of

Degrees of

Mean

Variation

Squares

Freedom

Square

F0

Regression

1418.034

2

709.017

76.75

Residual

157.055

17

9.239

Total

1575.089

19

Coefficient
0 1 2

Estimate
36.986 -0.027 15.004 R2 = 0.9003

Standard Error
0.005 1.360

t0
-5.887 11.035

P Value 3.12 × 10-9
P Value 8.97 × 10-6 1.79 × 10-9

The least-squares fit is

y^ = 36.986 - 0.027x1 + 15.004x2

The analysis of variance and other summary statistics for this model are shown in Table 8.2. Since the observed value of F0 has a very small P value, the hypothesis of significance of regression is rejected, and since the t statistics for 1 and 2 have small P values, we conclude that both regressors x1 (rpm) and x2 (tool type) contribute to the model. The parameter 2 is the change in mean tool life resulting from a change from tool type A to tool type B. The 95 % confidence interval on 2 is
( ) ( ) ^2 - t0.025,17se ^2  2  ^2 + t0.025,17se ^2
15.004 - 2.110(1.360)  2  15.004 + 2.110(1.360)

or

12.135  2  17.873

Therefore, we are 95% confident that changing from tool type A to tool type B

increases the mean tool life by between 12.135 and 17.873 hours.

The fitted values y^i and the residuals ei from this model are shown in the last two columns of Table 8.1. A plot of the residuals versus y^i is shown in Figure 8.3. The

residuals in this plot are identified by tool type (A or B). If the variance of the errors

is not the same for both tool types, this should show up in the plot. Note that

the "B" residuals in Figure 8.3 exhibit slightly more scatter than the "A" residuals,

implying that there may be a mild inequality-of-variance problem. Figure 8.4 is the

normal probability plot of the residuals. There is no indication of serious model

inadequacies.



GENERAL CONCEPT OF INDICATOR VARIABLES

265

Normal Probability Plot (response is life)
99

Percent

95 90 80 70 60 50 40 30 20
10 5

1

­3

­2

­1

0

1

2

3

Externally Studentized Residual

Figure 8.4 Normal probability plot of externally studentized residuals, Example 8.1.

Since two different regression lines are employed to model the relationship between tool life and lathe speed in Example 8.1, we could have initially fit two separate straight-line models instead of a single model with an indicator variable. However, the single-model approach is preferred because the analyst has only one final equation to work with instead of two, a much simpler practical result. Furthermore, since both straight lines are assumed to have the same slope, it makes sense to combine the data from both tool types to produce a single estimate of this common parameter. This approach also gives one estimate of the common error variance  2 and more residual degrees of freedom than would result from fitting two separate regression lines.
Now suppose that we expect the regression lines relating tool life to lathe speed to differ in both intercept and slope. It is possible to model this situation with a single regression equation by using indicator variables. The model is

y = 0 + 1x1 + 2 x2 + 3 x1x2 + 

(8.4)

Comparing Eq. (8.4) with Eq. (8.1) we observe that a cross product between lathe speed x1 and the indicator variable denoting tool type x2 has been added to the model.To interpret the parameters in this model, first consider tool type A, for which x2 = 0. Model (8.4) becomes

y = 0 + 1x1 + 2 (0) + 3x1 (0) + 
= 0 + 1x1 + 

(8.5)

which is a straight line with intercept 0 and slope 1. For tool type B, we have x2 = 1, and

y = 0 + 1x1 + 2 (1) + 3x1 (1) +  = (0 + 2 ) + (1 + 3 ) x1 + 

(8.6)

266 INDICATOR VARIABLES 50

Tool life, y ( hours)

0+2

1 + 3

E (y | x2 = 1) = (0 + 2) + (1 + 3)x1, tool type B

0 E (y | x2 = 0) = 0 + 1x1, tool type A
1

0 500

Lathe speed, x1 (RPM)

Figure 8.5 Response functions for Eq. (8.4).

1000

This is a straight-line model with intercept 0 + 2 and slope 1 + 3. Both regression functions are plotted in Figure 8.5. Note that Eq. (8.4) defines two regression lines with different slopes and intercepts. Therefore, the parameter 2 reflects the change in the intercept associated with changing from tool type A to tool type B (the classes 0 and 1 for the indicator variable x2), and 3 indicates the change in the slope associated with changing from tool type A to tool type B.
Fitting model (8.4) is equivalent to fitting two separate regression equations. An advantage to the use of indicator variables is that tests of hypotheses can be performed directly using the extra-sum-of-squares method. For example, to test whether or not the two regression models are identical, we would test
H0: 2 = 3 = 0
H1: 2  0 and or 3  0
If H0: 2 = 3 = 0 is not rejected, this would imply that a single regression model can explain the relationship between tool life and lathe speed. To test that the two regression lines have a common slope but possibly different intercepts, the hypotheses are
H0: 3 = 0, H1: 3  0
By using model (8.4), both regression lines can be fitted and these tests performed with one computer run, provided the program produces the sums of squares SSR(1|0), SSR(2|0, 1), and SSR(3|0, 1, 2).
Indicator variables are useful in a variety of regression situations. We will now present three further typical applications of indicator variables.

GENERAL CONCEPT OF INDICATOR VARIABLES

267

Example 8.2 The Tool Life Data

We will fit the regression model

y = 0 + 1x1 + 2 x2 + 3 x1x2 +  to the tool life data in Table 8.1. The X matrix and y vector for this model are

x1 x2 x1x2

1 610 0 0

18.73

1 950 0

0

14.52

1 720 0 0

17.43

1 840 0

0

14.54

1 980 0 0

13.44





1 530 0 0

 24.39

1 680 0

0

13.34

1 540 0 0

22.71

1 890 0

0

12.68

X

=

1 1

730 0 670 1

0 670

and

y

=

19.32 30.16

1 770 1 770





27.09 

1 880 1 880

25.40

1 1000 1 1000

26.05

1 760 1 760

33.49

1 590 1 590

35.62

1 910 1 910

26.07

1 650 1 650

36.78

1 810 1 810

34.95

1 500 1 500

43.67

The fitted regression model is

y^ = 32.775 - 0.021x1 + 23.971x2 - 0.012x1x2

The summary analysis for this model is presented in Table 8.3. To test the hypothesis that the two regression lines are identical (H0: 2 = 3 = 0), use the statistic

F0

=

SSR (2, 3 1,
MSRes

0 )

2

268 INDICATOR VARIABLES

Since

SSR (2, 3 1, 0 ) = SSR (1, 2, 3 0 ) - SSR (1 0 )
= 1434.112 - 293.005 = 1141.107

the test statistic is

F0

=

SSR (2, 3 1, 0 )
MSRes

2

=

1141.107 8.811

2

=

64.75

and since for this statistic P = 2.14 × 10-8, we conclude that the two regression lines are not identical. To test the hypothesis that the two lines have different intercepts and a common slope (H0: 3 = 0), use the statistic

F0

=

SSR (3 2, 1,
MSRes

0 )

1

=

16.078 8.811

=

1.82

and since for this statistic P = 0.20, we conclude that the slopes of the two straight

lines are the same. This can also be determined by using the t statistics for 2 and

3 in Table 8.3.



TABLE 8.3 Summary Analysis for the Tool Life Regression Model in Example 8.2

Source of Variation
Regression Error Total

Sum of Squares
1434.112 140.976 1575.008

Degrees of Freedom
3 16 19

Mean Square
478.037 8.811

F0 54.25

P Value 1.32 × 10-9

Coefficient
0 1 2 3

Estimate
32.775 -0.021 23.971 -0.012

Standard Error

t0

0.0061

-3.45

6.7690

3.45

0.0088

-1.35

R2 = 0.9105

Sum of Squares
SSR(1|0) = 293.005 SSR(2|1, 0) = 1125.029 SSR(3|2, 1, 0) = 16.078

Example 8.3 An Indicator Variable with More Than Two Levels
An electric utility is investigating the effect of the size of a single-family house and the type of air conditioning used in the house on the total electricity consumption during warm-weather months. Let y be the total electricity consumption (in kilowatt-hours) during the period June through September and x1 be the size of the

GENERAL CONCEPT OF INDICATOR VARIABLES

269

house (square feet of floor space). There are four types of air conditioning systems: (1) no air conditioning, (2) window units, (3) heat pump, and (4) central air conditioning. The four levels of this factor can be modeled by three indicator variables, x2, x3, and x4, defined as follows:

Type of Air Conditioning x2 x3 x4

No air conditioning

000

Window units

100

Heat pump

010

Central air conditioning

00

1

The regression model is y = 0 + 1x1 + 2 x2 + 3 x3 + 4 x4 + 
If the house has no air conditioning, Eq. (8.7) becomes y = 0 + 1x1 + 
If the house has window units, then
y = (0 + 2 ) + 1x1 + 
If the house has a heat pump, the regression model is

(8.7)

y = (0 + 3 ) + 1x1 + 

while if the house has central air conditioning, then

y = (0 + 4 ) + 1x1 + 
Thus, model (8.7) assumes that the relationship between warm-weather electricity consumption and the size of the house is linear and that the slope does not depend on the type of air conditioning system employed. The parameters 2, 3, and 4 modify the height (or intercept) of the regression model for the different types of air conditioning systems. That is, 2, 3, and 4 measure the effect of window units, a heat pump, and a central air conditioning system, respectively, compared to no air conditioning. Furthermore, other effects can be determined by directly comparing the appropriate regression coefficients. For example, 3 - 4 reflects the relative efficiency of a heat pump compared to central air conditioning. Note also the

270 INDICATOR VARIABLES
assumption that the variance of energy consumption does not depend on the type of air conditioning system used. This assumption may be inappropriate.
In this problem it would seem unrealistic to assume that the slope of the regression function relating mean electricity consumption to the size of the house does not depend on the type of air conditioning system. For example, we would expect the mean electricity consumption to increase with the size of the house, but the rate of increase should be different for a central air conditioning system than for window units because central air conditioning should be more efficient than window units for larger houses. That is, there should be an interaction between the size of the house and the type of air conditioning system. This can be incorporated into the model by expanding model (8.7) to include interaction terms. The resulting model is

y = 0 + 1x1 + 2 x2 + 3 x3 + 4 x4 + 5 x1x2 + 6 x1x3 + 7 x1x4 + 

(8.8)

The four regression models corresponding to the four types of air conditioning systems are as follows:

y = 0 + 1x1 + 1
y = (0 + 2 ) + (1 + 5 ) x1 +  y = (0 + 3 ) + (1 + 6 ) x1 +  y = (0 + 4 ) + (1 + 7 ) x1 + 

(no air conditioning) (window units) (heat pump) (central air conditioning)

Note that model (8.8) implies that each type of air conditioning system can have a

separate regression line with a unique slope and intercept.



Example 8.4 More Than One Indicator Variable

Frequently there are several different qualitative variables that must be incorporated into the model. To illustrate, suppose that in Example 8.1 a second qualitative factor, the type of cutting oil used, must be considered. Assuming that this factor has two levels, we may define a second indicator variable, x3, as follows:

x3

=

0 1

if low-viscosity oil used if medium-viscosity oil used

A regression model relating tool life ( y) to cutting speed (x1), tool type (x2), and type of cutting oil (x3) is

y = 0 + 1x1 + 2 x2 + 3 x3 + 

(8.9)

Clearly the slope 1 of the regression model relating tool life to cutting speed does not depend on either the type of tool or the type of cutting oil. The intercept of the regression line depends on these factors in an additive fashion.

GENERAL CONCEPT OF INDICATOR VARIABLES

271

Various types of interaction effects may be added to the model. For example, suppose that we consider interactions between cutting speed and the two qualitative factors, so that model (8.9) becomes

y = 0 + 1x1 + 2 x2 + 3 x3 + 4 x1x2 + 5 x1x3 +  This implies the following situation:

(8.10)

Tool Type
A B A B

Cutting Oil
Low viscosity Low viscosity Medium viscosity Medium viscosity

Regression Model
y = 0 + 1x1 +  y = (0 + 2) + (1 + 4)x1 +  y = (0 + 3) + (1 + 5)x1 +  y = (0 + 2 + 3) + (1 + 4 + 5)x1 + 

Notice that each combination of tool type and cutting oil results in a separate regression line, with different slopes and intercepts. However, the model is still additive with respect to the levels of the indicator variables. That is, changing from lowto medium-viscosity cutting oil changes the intercept by 3 and the slope by 5 regardless of the type of tool used.
Suppose that we add a cross-product term involving the two indicator variables x2 and x3 to the model, resulting in

y = 0 + 1x1 + 2 x2 + 3 x3 + 4 x1x2 + 5 x1x3 + 6 x2 x3 + 

(8.11)

We then have the following:

Tool Type
A B A B

Cutting Oil
Low viscosity Low viscosity Medium viscosity Medium viscosity

Regression Model
y = 0 + 1x1 +  y = (0 + 2) + (1 + 4)x1 +  y = (0 + 3) + (1 + 5)x1 +  y = (0 + 2 + 3 + 6) + (1 + 4 + 5)x1 + 

The addition of the cross-product term 6x2x3 in Eq. (8.11) results in the effect of

one indicator variable on the intercept depending on the level of the other indicator

variable. That is, changing from low- to medium-viscosity cutting oil changes the

intercept by 3 if tool type A is used, but the same change in cutting oil changes the intercept by 3 + 6 if tool type B is used. If an interaction term 7x1x2x3 were

added to model (8.11), then changing from low- to medium-viscosity cutting oil

would have an effect on both the intercept and the slope, which depends on the

type of tool used.

Unless prior information is available concerning the anticipated effect of tool

type and cutting oil viscosity on tool life, we will have to let the data guide us in

selecting the correct form of the model. This may generally be done by testing

hypotheses about individual regression coefficients using the partial F test. For

example, testing H0: 6 = 0 for model (8.11) would allow us to discriminate between

the two candidate models (8.11) and (8.10).



272 INDICATOR VARIABLES

Example 8.5 Comparing Regression Models

Consider the case of simple linear regression where the n observations can be formed into M groups, with the mth group having nm observations. The most general model consists of M separate equations, such as

y = 0m + 1m x + , m = 1, 2, ... , M

(8.12)

It is often of interest to compare this general model to a more restrictive one. Indicator variables are helpful in this regard. We consider the following cases:

a. Parallel Lines In this situation all M slopes are identical, 11 = 12 = ··· = 1M, but the intercepts may differ. Note that this is the type of problem encountered in Example 8.1 (where M = 2), leading to the use of an additive indicator variable. More generally we may use the extra-sum-of squares method to test the hypothesis H0: 11 = 12 = ··· = 1M. Recall that this procedure involves fitting a full model (FM) and a reduced model (RM) restricted to the null hypothesis and computing the F statistic:

F0

=

[SSRes (RM ) - SSRes (FM )] (dfRM SSRes (FM ) dfFM

-

dfFM

)

(8.13)

If the reduced model is as satisfactory as the full model, then F0 will be small com-

pared to F . ,dfRM -dfFM,dfFM Large values of F0 imply that the reduced model is inadequate.

To fit the full model (8.12), sinIply fit M separate regression equations. Then

SSRes(FM) is found by adding the residual sums of squares from each separate

regression.

The

degrees

of

freedom

for

SSRes(FM)

is

dfFM

=

( 

M m=1

nm

- 2)

=

n - 2M.

To fit the reduced model, define M - 1 indicator variables D1, D2, . . . , DM-1 corre-

sponding to the M groups and fit

y = 0 + 1x + 2D1 + 3D2 + + M DM-1 + 
The residual sum of squares from this model is SSRes(RM) with dfRM = n - (M + 1) degrees of freedom.
If the F test (8.13) indicates that the M regression models have a common slope, then ^1 from the reduced model is an estimate of this parameter found by pooling or combining all of the data. This was illustrated in Example 8.1. More generally, analysis of covariance is used to pool the data to estimate the common slope. The analysis of covariance is a special type of linear model that is a combination of a regression model (with quantitative factors) and an analysis-of-variance model (with qualitative factors). For an introduction to analysis of covariance, see Montgomery [2009].

b. Concurrent Lines In this section, all M intercepts are equal, 01 = 02 = ··· = 0M, but the slopes may differ. The reduced model is

y = 0 + 1x + 2Z1 + 2Z2 + + M DM-1 + 

COMMENTS ON THE USE OF INDICATOR VARIABLES

273

where Zk = xDk, k = 1, 2, . . . , M - 1. The residual sum of squares from this model is SSRes(RM) with dfRM = n - (M + 1) degrees of freedom. Note that we are assuming concurrence at the origin. The more general case of concurrence at an arbitrary
point x0 is treated by Graybill [1976] and Seber [1977].

c. Coincident Lines In this case both the M slopes and the M intercepts are the same, 01 = 02 = ··· = 0M, and 11 = 12 = ··· = 1M. The reduced model is simply

y = 0 + 1x + 

and the residual sum of squares SSRes(RM) has dfRM = n - 2 degrees of freedom.

Indicator variables are not necessary in the test of coincidence, but we include this

case for completeness.



8.2 COMMENTS ON THE USE OF INDICATOR VARIABLES
8.2.1 Indicator Variables versus Regression on Allocated Codes
Another approach to the treatment of a qualitative variable in regression is to measure the levels of the variable by an allocated code. Recall Example 8.3, where an electric utility is investigating the effect of size of house and type of air conditioning system on residential electricity consumption. Instead of using three indicator variables to represent the four levels of the qualitative factor type of air conditioning system, we could use one quantitative factor, x2, with the following allocated code:

Type of Air Conditioning System

x2

No air conditioning

1

Window units

2

Heat pumps

3

Central air conditioning

4

We may now fit the regression model
y = 0 + 1x1 + 2 x2 + 
where x1 is the size of the house. This model implies that
E (y x1, no air conditioning) = 0 + 1x1 + 2 E ( y x1, window units) = 0 + 1x1 + 22 E ( y x1, heat pump) = 0 + 1x1 + 32
E (y x1, central air conditioning) = 0 + 1x1 + 42

(8.14)

274 INDICATOR VARIABLES
A direct consequence of this is that
E (y x1, central air conditioning) - E (y x1, heat pump) = E (y x1, heat pump) - E (y x1, window units) = E (y x1, window units) - E (y x1, no air conditioning)
= 2
which may be quite unrealistic. The allocated codes impose a particular metric on the levels of the qualitative factor. Other choices of the allocated code would imply different distances between the levels of the qualitative factor, but there is no guarantee that any particular allocated code leads to a spacing that is appropriate.
Indicator variables are more informative for this type problem because they do not force any particular metric on the levels of the qualitative factor. Furthermore, regression using indicator variables always leads to a larger R2 than does regression on allocated codes (e.g., see Searle and Udell [1970]).
8.2.2 Indicator Variables as a Substitute for a Quantitative Regressor
Quantitative regressors can also be represented by indicator variables. Sometimes this is necessary because it is difficult to collect accurate information on the quantitative regressor. Consider the electric power usage study in Example 8.3 and suppose that a second quantitative regressor, household income, is included in the analysis. Because it is difficult to obtain this information precisely, the quantitative regressor income may be collected by grouping income into classes such as
$0 to $19, 999
$20, 000 to $39, 999
$40, 000 to $59, 999
$60, 000 to $79, 999
$80, 000 and over
We may now represent the factor "income" in the model by using four indicator variables.
One disadvantage of this approach is that more parameters are required to represent the information content of the quantitative factor. In general, if the quantitative regressor is grouped into a classes, a - 1 parameters will be required, while only one parameter would be required if the original quantitative regressor is used. Thus, treating a quantitative factor as a qualitative one increases the complexity of the model. This approach also reduces the degrees of freedom for error, although if the data are numerous, this is not a serious problem. An advantage of the indicator variable approach is that it does not require the analyst to make any prior assumptions about the functional form of the relationship between the response and the regressor variable.

REGRESSION APPROACH TO ANALYSIS OF VARIANCE

275

8.3 REGRESSION APPROACH TO ANALYSIS OF VARIANCE

The analysis of variance is a technique frequently used to analyze data from planned or designed experiments. Although special computing procedures are generally used for analysis of variance, any analysis-of-variance problem can also be treated as a linear regression problem. Ordinarily we do not recommend that regression methods be used for analysis of variance because the specialized computing techniques are usually quite efficient. However, there are some analysis-of-variance situations, particularly those involving unbalanced designs, where the regression approach is helpful. Furthermore, many analysts are unaware of the close connection between the two procedures. Essentially, any analysis-of-variance problem can be treated as a regression problem in which all of the regressors are indicator variables.
In this section we illustrate the regression alternative to the one-way classification or single-factor analysis of variance. For further examples of the relationship between regression and analysis of variance, see Draper and Smith [1998], Montgomery [2009], Schilling [1974a, b], and Seber [1977].
The model for the one-way classification analysis of variance is

yij =  + i + ij, i = 1, 2, ... , k, j = 1, 2, ... , n

(8.15)

where Yij is the jth observation for the ith treatment or factor level,  is a parameter common to all k treatments (usually called the grand mean), i is a parameter that represents the effect of the ith treatment, and ij is an NID(0,  2) error component. It is customary to define the treatment effects in the balanced case (i.e., an equal
number of observations per treatment) as

1 +2 + +k = 0
Furthermore, the mean of the ith treatment is i =  + i, i = 1, 2, . . . , k. In the fixed-effects (or model I) case, the analysis of variance is used to test the hypothesis that all k population means are equal, or equivalently,

H0: 1 = 2 = = k = 0 H1: i  0 for at least one i

(8.16)

Table 8.4 displays the usual single-factor analysis of variance. We have a true error term in this case, as opposed to a residual term, because the replication allows a model-independent estimate of error. The test statistic F0 is compared to F . ,k-1,k(n-1) If F0 exceeds this critical value, the null hypothesis H0 in Eq. (8.16) is rejected; that is, we conclude that the k treatment means are not identical. Note that in Table 8.4 we have employed the usual "dot subscript" notation associated with analysis of variance. That is, the average of the n observations in the ith treatment is

 yi.

=

1 n

n j=1

yij,

i = 1, 2, ... , k

276 INDICATOR VARIABLES

TABLE 8.4 One-Way Analysis of Variance

Degrees of Variation

Sum of Squares

Degrees of Freedom

Treatments Error Total

k
 n (yi. - y.. )2 i=1
kn
  (yij - )yi. 2
i=1 j=1
kn
  (yij - )y.. 2
i=1 j=1

k-1 k(n - 1) kn - 1

Mean Square
SSTreatments k-1
SSRes
k (n - 1)

F0
MSTreatments MSRes

and the grand average is

  yi..

=

1 kn

k i=1

n
yij
j=1

To illustrate the connection between the single-factor fixed-effects analysis of variance and regression, suppose that we have k = 3 treatments, so that Eq. (8.15) becomes

yij =  + i + ij, i = 1, 2, 3, j = 1, 3, ... , n

(8.17)

These three treatments may be viewed as three levels of a qualitative factor, and they can be handled using indicator variables. Specifically a qualitative factor with three levels would require two indicator variables defined as follows:

x1

=

1 0

if the observation is from treatment 1 otherwise

x2

=

1 0

if the observation is from treatment 2 otherwise

Therefore, the regression model becomes

yij = 0 + 1x1j + 2 x2 j + ij, i = 1, 2, 3, j = 1, 2, ... , n

(8.18)

where x1j is the value of the indicator variable x1 for observation j in treatment i and x2j is the value of x2 for observation j in treatment i.
The relationship between the parameter u (u = 0, 1, 2) in the regression model and the parameters  and i (i = 1, 2, . . . , k) in the analysis-of-variance model is easily determined. Consider the observations from treatment 1, for which

x1j = 1 and x2 j = 0

REGRESSION APPROACH TO ANALYSIS OF VARIANCE

277

The regression model (8.18) becomes
y1j = 0 + 1 (1) + 2 (0) + 1j = 0 + 1 + 1j
Since in the analysis-of-variance model an observation from treatment 1 is represented by y1j =  + 1 + 1j = 1 + 1j, this implies that
0 + 1 = 1
Similarly, if the observations are from treatment 2, then x1j = 0, x2j = 1, and
y2 j = 0 + 1 (0) + 2 (1) + 2 j = 0 + 2 + 2 j
Considering the analysis-of-variance model, y2j =  + 2 + 2j = 2 + 2j, so
0 + 2 = 2
Finally, consider observations from treatment 3. Since x1j = x2j = 0 the regression model becomes
y3j = 0 + 1 (0) + 2 (0) + 3j = 0 + 3j
The corresponding analysis-of-variance model is y3j =  + 3 + 3j = 3 + 3j, so that
0 = 3
Thus, in the regression model formulation of the single-factor analysis of variance, the regression coefficients describe comparisons of the first two treatment means 1 and 2 with the third treatment mean 3. That is,
0 = 3, 1 = 1 - 3, 2 = 2 - 3
In general, if there are k treatments, the regression model for the single-factor analysis of variance will require k - 1 indicator variables, for example,

yij = 0 + 1x1j + 2 x2 j + +  x k-1 k-1, j + ij i = 1, 2, ... , k, j = 1, 2, ... , n (8.19) where

xij

=

1 0

if observation j is from treatment i otherwise

The relationship between the parameters in the regression and analysis-of-variance models is
0 = k i = i - k, i = 1, 2, ... , k - 1

278 INDICATOR VARIABLES

Thus, 0 always estimates the mean of the kth treatment and i estimates the differences in means between treatment i and treatment k.
Now consider fitting the regression model for the one-way analysis of variance. Once again, suppose that we have k = 3 treatments and now let there be n = 3 observations per treatment. The X matrix and y vector are as follows:

 y11 

 

y12

 

 y13 

 

y21

 

y

=

 

y22

 

,

 y23 

 

y31

 

 y32 

 y33 

1 1 1 1 X = 1  1 1 1 1

x1 x2 1 0 1 0 1 0 0 1 0 1
 0 1 0 0 0 0 0 0

Notice that the X matrix consists entirely of 0's and 1's. This is a characteristic of the regression formulation of any analysis-of-variance model. The least-squares normal equations are
(XX) b^ = Xy

or

9 3 3

3 3 0

3 0 3

^^01 ^ 2

    

=

   

y.. y1. y2.

   

where yi. is the total of all observations in treatment i and y.. is the grand total of all nine observations (i.e., y.. = y1. + y2. + y3.). The solution to the normal equations is

^0 = y.. - y1. - y2. = y3., ^1 = y1. - y3., ^ 2 = y2. - y3.

The extra-sum-of-squares method may be used to test for differences in treatment means. For the full model the regression sum of squares is

 y.. 

( ) SSR ^0, ^1, ^2

=

b^

Xy

=

[ y3.,

y1.

-

y3.,

y2.

-

y3.

]

 

y1.

 

 y2. 

= y.. y3. + y1. ( y1. - y3. ) + y2. ( y2. - y3. )

= ( y1. + y2. + y3. ) y3. + y1. ( y1. - y3. ) + y2. ( y2. - y3. )

= y1. y1. + y2. y2. + y3. y3.

 = 3 yi2. i=1 3

REGRESSION APPROACH TO ANALYSIS OF VARIANCE

279

with three degrees of freedom. The residual error sum of squares for the full model is

33

  SSRes =

yi2j - SSR (0, 1, 2 )

i=1 j=1

  =

3 i=1

3

3

yi2j -

j=1

i=1

yi2. 3

33

  =

( ) yij - yi. 2

i=1 j=1

(8.20)

with 9 - 3 = 6 degrees of freedom. Note that Eq. (8.20) is the error sum of squares in the analysis-of-variance table (Table 8.4) for k = n = 3.
Testing for differences in treatment means is equivalent to testing
H0: 1 = 2 = 3 = 0 H1: at least one i  0
If H0 is true, the parameters in the regression model become
0 = , 1 = 0, 2 = 0
Therefore, the reduced model contains only one parameter, that is,

yij = 0 + ij

The estimate of 0 in the reduced model is ^0 = y.., and the single-degree-of-freedom regression sum of squares for this model is

SSR (0 ) =

y.2. 9

The sum of squares for testing for equality of treatment means is the difference in regression sums of squares between the full and reduced models, or

SSR (1, 2 0 ) = SSR (0, 1, 2 ) - SSR (0 )
 = 3 yi2. - y.2. i=1 3 9
3
 = 3 ( yi. - y.. )2 j =1

(8.21)

This sum of squares has 3 - 1 = 2 degrees of freedom. Note that Eq. (8.21) is the treatment sum of squares in Table 8.4 assuming that k = n = 3. The appropriate test statistic is

280 INDICATOR VARIABLES

F0

=

SSR (1, 2
SSRes

0 )
6

2

3
3 ( yi. - y.. )2 2

=

i=1 33

  ( ) yij - yi. 2 6

i=1 j=1

= MSTreatments MSRes

If H0: 1 = 2 = 3 = 0 is true, then F0 follows the F2,6 distribution. This is the same test statistic given in the analysis-of-variance table (Table 8.4). Therefore, the regression approach is identical to the one-way analysis-of-variance procedure outlined in Table 8.4.

PROBLEMS
8.1 Consider the regression model (8.8) described in Example 8.3. Graph the response function for this model and indicate the role the model parameters play in determining the shape of this function.
8.2 Consider the regression models described in Example 8.4. a. Graph the response function associated with Eq. (8.10). b. Graph the response function associated with Eq. (8.11).
8.3 Consider the delivery time data in Example 3.1. In Section 4.2.5 noted that these observations were collected in four cities, San Diego, Boston, Austin, and Minneapolis. a. Develop a model that relates delivery time y to cases x1, distance x2, and the city in which the delivery was made. Estimate the parameters of the model. b. Is there an indication that delivery site is an important variable? c. Analyze the residuals from this model. What conclusions can you draw regarding model adequacy?
8.4 Consider the automobile gasoline mileage data in Table B.3. a. Build a linear regression model relating gasoline mileage y to engine displacement x1 and the type of transmission x11. Does the type of transmission significantly affect the mileage performance? b. Modify the model developed in part a to include an interaction between engine displacement and the type of transmission. What conclusions can you draw about the effect of the type of transmission on gasoline mileage? Interpret the parameters in this model.

PROBLEMS 281
8.5 Consider the automobile gasoline mileage data in Table B.3. a. Build a linear regression model relating gasoline mileage y to vehicle weight x10 and the type of transmission x11. Does the type of transmission significantly affect the mileage performance? b. Modify the model developed in part a to include an interaction between vehicle weight and the type of transmission. What conclusions can you draw about the effect of the type of transmission on gasoline mileage? Interpret the parameters in this model.
8.6 Consider the National Football League data in Table B.1. Build a linear regression model relating the number of games won to the yards gained rushing by opponents x8, the percentage of rushing plays x7, and a modification of the turnover differential x5. Specifically let the turnover differential be an indicator variable whose value is determined by whether the actual turnover differential is positive, negative, or zero. What conclusions can you draw about the effect of turnovers on the number of games won?
8.7 Piecewise Linear Regression. In Example 7.3 we showed how a linear regression model with a change in slope at some point t (xmin < t < xmax) could be fitted using splines. Develop a formulation of the piecewise linear regression model using indicator variables. Assume that the function is continuous at point t.
8.8 Continuation of Problem 8.7. Show how indicator variables can be used to develop a piecewise linear regression model with a discontinuity at the join point t.
8.9 Suppose that a one-way analysis of variance involves four treatments but that a different number of observations (e.g., ni) has been taken under each treatment. Assuming that n1 = 3, n2 = 2, n3 = 4, and n4 = 3, write down the y vector and X matrix for analyzing these data as a multiple regression model. Are any complications introduced by the unbalanced nature of these data?
8.10 Alternate Coding Schemes for tbe Regression Approach to Analysis of Variance. Consider Eq. (8.18), which represents the regression model corresponding to an analysis of variance with three treatments and n observations per treatment. Suppose that the indicator variables x1 and x2 are defined as
1 if observation is from treatment 1 x1 = -1 if observation is from treatment 2
0 otherwise
1 if observation is from treatment 2 x2 = -1 if observation is from treatment 3
0 otherwise
a. Show that the relationship between the parameters in the regression and analysis-of-variance models is

282 INDICATOR VARIABLES

0

=

1

+ 2 3

+ 3

=



1 = 1 - , 2 = 2 - 

b. Write down the y vector and X matrix.
c. Develop an appropriate sum of squares for testing the hypothesis H0: 1 = 2 = 3 = 0. Is this the usual treatment sum of squares in the oneway analysis of variance?
8.11 Montgomery [2009] presents an experiment concerning the tensile strength of synthetic fiber used to make cloth for men's shirts: The strength is thought to be affected by the percentage of cotton in the fiber. The data are shown below.

Percentage of Cotton

Tensile Strength

15

7

7 15 11

9

20

12

17

12

18

18

25

14

18

18

19

19

30

19

25

22

19

23

35

7

10

11

15

11

a. Write down the y vector and X matrix for the corresponding regression model.
b. Find the least-squares estimates of the model parameters. c. Find a point estimate of the difference in mean strength between 15% and
25% cotton. d. Test the hypothesis that the mean tensile strength is the same for all five
cotton percentages.
8.12 Two-Way Analysis of Variance. Suppose that two different sets of treatments are of interest. Let yijk be the kth observation level i of the first treatment type and level j of the second treatment type. The two-way analysis-ofvariance model is
yijk =  + i +  j + (  )ij + ijk
i = 1, 2, ... , a, j = 1, 2, ... , b, k = 1, 2, ... , n
where 1 is the effect of level i of the first treatment type, j is the effect of level j of the second treatment type, ()ij is an interaction effect between the two treatment types, and ijk is an NID(0,  2) random-error component. a. For the case a = b = n = 2, write down a regression model that corresponds
to the two-way analysis of variance. b. What are the y vector and X matrix for this regression model? c. Discnss how the regression model could be used to test the hypotheses
H0: 1 = 2 = 0 (treatment type 1 means are equal), H0: 1 = 2 = 0 (treatment

PROBLEMS 283

type 2 means are equal), and H0: ( )11 = ( )12 = ( )22 = 0 (no interaction between treatment types).
8.13 Table B.11 presents data on the quality of Pinot Noir wine. a. Build a regression model relating quality y to flavor x4 that incorporates the region information given in the last column. Does the region have an impact on wine quality? b. Perform a residual analysis for this model and comment on model adequacy. c. Are there any outliers or influential observations in this data set? d. Modify the model in part a to include interaction terms between flavor and the region variables. Is this model superior to the one you found in part a?
8.14 Using the wine quality data from Table B.11, fit a model relating wine quality y to flavor x4 using region as an allocated code, taking on the values shown in the table (1,2,3). Discuss the interpretation of the parameters in this model. Compare the model to the one you built using indicator variables in Problem 8.13.
8.15 Consider the life expectancy data given in Table B.16. Create an indicator variable for gender. Perform a thorough analysis of the overall average life expectancy. Discuss the results of this analysis relative to your previous analyses of these data.
8.16 Smith et al. [1992] discuss a study of the ozone layer over the Antarctic. These scientists developed a measure of the degree to which oceanic phytoplankton production is inhibited by exposure to ultraviolet radiation (UVB). The response is INHIBIT. The regressors are UVB and SURFACE, which is depth below the ocean's surface from which the sample was taken. The data follow.

Location
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17

INHIBIT
0.00 1.00 6.00 7.00 7.00 7.00 9.00 9.50 10.00 11.00 12.50 14.00 20.00 21.00 25.00 39.00 59.00

UVB
0.00 0.00 0.01 0.01 0.02 0.03 0.04 0.01 0.00 0.03 0.03 0.01 0.03 0.04 0.02 0.03 0.03

SURFACE
Deep Deep Deep Surface Surface Surface Surface Deep Deep Surface Surface Deep Deep Surface Deep Deep Deep

Perform an analysis of these data. Discuss your results.

284 INDICATOR VARIABLES
8.17 Table B.17 contains hospital patient satisfaction data. Fit an appropriate regression model to the satisfaction response using age and severity as the regressors and account for the medical versus surgical classification of each patient with an indicator variable. Has adding the indicator variable improved the model? Is that any evidence to support a claims that medical and surgical patients differ in their satisfaction?
8.18 Consider the fuel consumption data in Table B.18. Regressor x1 is an indicator variable. Perform a thorough analysis of these data. What conclusions do you draw from this analysis?
8.19 Consider the wine quality of young red wines data in Table B.19. Regressor x1 is an indicator variable. Perform a thorough analysis of these data. What conclusions do you draw from this analysis?
8.20 Consider the methanol oxidation data in Table B.20. Perform a thorough analysis of these data. What conclusions do you draw from this analysis?

CHAPTER 9
MULTICOLLINEARITY
9.1 INTRODUCTION The use and interpretation of a multiple regression model often depends explicitly or implicitly on the estimates of the individual regression coefficients. Some examples of inferences that are frequently made include the following:
1. Identifying the relative effects of the regressor variables 2. Prediction and/or estimation 3. Selection of an appropriate set of variables for the model If there is no linear relationship between the regressors, they are said to be orthogonal. When the regressors are orthogonal, inferences such as those illustrated above can be made relatively easily. Unfortunately, in most applications of regression, the regressors are not orthogonal. Sometimes the lack of orthogonality is not serious. However, in some situations the regressors are nearly perfectly linearly related, and in such cases the inferences based on the regression model can be misleading or erroneous. When there are near-linear dependencies among the regressors, the problem of multicollinearity is said to exist. This chapter will extend the preliminary discussion of multicollinearity begun in Chapter 3 and discuss a variety of problems and techniques related to this problem. Specifically we will examine the causes of multicollinearity, some of its specific effects on inference, methods of detecting the presence of multicollinearity, and some techniques for dealing with the problem.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
285

286 MULTICOLLINEARITY

9.2 SOURCES OF MULTICOLLINEARITY

We write the multiple regression model as

y = Xb + e

where y is an n × 1 vector of responses, X is an n × p matrix of the regressor variables,  is a p × 1 vector of unknown constants, and  is an n × 1 vector of random errors, with i  NID(0, 2). It will be convenient to assume that the regressor variables and the response have been centered and scaled to unit length, as in Section 3.9. Consequently, XX is a p × p matrix of correlations between the regressors and Xy is a p × 1 vector of correlations between the regressors and the respouse.
Let the jth column of the X matrix be denoted Xj, so that X = [X1, X2, . . . , Xp]. Thus, Xj contains the n levels of the jth regressor variable. We may formally define multicollinearity in terms of the linear dependence of the columns of X. The vectors
X1, X2, . . . , Xp are linearly dependent if there is a set of constants t1, t2, . . . , tp, not all zero, such that

p
tjXj = 0
j=1

(9.1)

If Eq. (9.1) holds exactly for a subset of the columns of X, then the rank of the XX matrix is less than p and (XX)-1 does not exist. However, suppose that Eq. (9.1) is approximately true for some subset of the columns of X. Then there will be a nearlinear dependency in XX and the problem of multicollinearity is said to exist. Note that multicollinearity is a form of ill-conditioning in the XX matrix. Furthermore, the problem is one of degree, that is, every data set will suffer from multicollinearity to some extent unless the columns of X are orthogonal (XX is a diagonal matrix). Generally this will happen only in a designed experiment. As we shall see, the presence of multicollinearity can make the usual least-squares analysis of the regression model dramatically inadequate.
There are four primary sources of multicollinearity:

1. The data collection method employed 2. Constraints on the model or in the population 3. Model specification 4. An overdefined model

It is important to understand the differences among these sources of multicollinearity, as the recommendations for analysis of the data and interpretation of the resulting model depend to some extent on the cause of the problem (see Mason, Gunst, and Webster [1975] for further discussion of the source of multicollinearity).

It is customary to refer to the off-diagonal elements of XX as correlation coefficients, although the regressors are not necessarily random variables. If the regressors are not centered, then 0 in Eq. (9.1) becomes a vector of constants m, not all necessarily equal to 0.

SOURCES OF MULTICOLLINEARITY

287

The data collection method can lead to multicollinearity problems when the analyst samples only a subspace of the region of the regressors defined (approximately) by Eq. (9.1). For example, consider the soft drink delivery time data discussed in Example 3.1. The space of the regressor variables "cases" and "distance," as well as the subspace of this region that has been sampled, is shown in the matrix of scatterplots, Figure 3.4. Note that the sample (cases, distance) pairs fall approximately along a straight line. In general, if there are more than two regressors, the data will lie approximately along a hyperplace defined by Eq. (9.1). In this example, observations with a small number of cases generally also have a short distance, while observations with a large number of cases usually also have a long distance. Thus, cases and distance are positively correlated, and if this positive correlation is strong enough, a multicollinearity problem will occur. Multicollinearity caused by the sampling technique is not inherent in the model or the population being sampled. For example, in the delivery time problem we could collect data with a small number of cases and a long distance. There is nothing in the physical structure of the problem to prevent this..
Constraints on the model or in the population being sampled can cause multicollinearity. For example, suppose that an electric utility is investigating the effect of family income (x1) and house size (x2) on residential electricity consumption. The levels of the two regressor variables obtained in the sample data are shown in Figure 9.1. Note that the data lie approximately along a straight line, indicating a potential multicollinearity problem. In this example a physical constraint in the population has caused this phenomenon, namely, families with higher incomes generally have larger homes than families with lower incomes. When physical constraints such as this are present, multicollinearity will exist regardless of the sampling method employed. Constraints often occur in problems involving production or chemical processes, where the regressors are the components of a product, and these components add to a constant.

Family income, x1 ($/year)

52,000 48,000 44,000 40,000 36,000 32,000 28,000 24,000 20,000 16,000 12,000
8,000 4,000
0

1,000

2,000

3,000

4,000

House size, x2 (square feet)

Figure 9.1 Levels of family income and house size for a study on residential electricity consumption.

288 MULTICOLLINEARITY
Multicollinearity may also be induced by the choice of model. For example, we know from Chapter 7 that adding polynomial terms to a regression model causes ill-conditioning in XX. Furthermore, if the range of x is small, adding an x2 term can result in significant multicollinearity.We often encounter situations such as these where two or more regressors are nearly linearly dependent, and retaining all these regressors may contribute to multicollinearity. In these cases some subset of the regressors is usually preferable from the standpoint of multicollinearity.
An overdefined model has more regressor variables than observations. These models are sometimes encountered in medical and behavioral research, where there may be only a small number of subjects (sample units) available, and information is collected for a large number of regressors on each subject. The usual approach to dealing with multicollinearity in this context is to eliminate some of the regressor variables from consideration. Mason, Gunst, and Webster [1975] give three specific recommendations: (1) redefine the model in terms of a smaller set of regressors, (2) perform preliminary studies using only subsets of the original regressors, and (3) use principal-component-type regression methods to decide which regressors to remove from the model.The first two methods ignore the interrelationships between the regressors and consequently can lead to unsatisfactory results. Principalcomponent regression will be discussed in Section 9.5.4, although not in the context of overdefined models.

9.3 EFFECTS OF MULTICOLLINEARITY

The presence of multicollinearity has a number of potentially serious effects on the least-squares estimates of the regression coefficients. Some of these effects may be easily demonstrated. Suppose that there are ouly two regressor variables, x1 and x2. The model, assuming that x1, x2, and y are scaled to unit length, is

y = 1x1 + 2 x2 + 

and the least-squares normal equations are

(XX) b^ = Xy

1 r12

r12 1

 

^^21

  

=

 r1 y r2 y

 

where r12 is the simple correlation between x1 and x2 and rjy is the simple correlation between xj and y, j = 1, 2. Now the inverse of (XX) is

1

C

=

( X  X )-1

=

 

1

-

r122

 -r12

 1 - r122

-r12 

1

-

r122

 

1

1 - r122 

(9.2)

and the estimates of the regression coefficients are

EFFECTS OF MULTICOLLINEARITY

289

^ 1

=

r1y - r12r2 y 1 - r122

,

^ 2

=

r2 y - r12r1y 1 - r122

( ) If there is strong multicollinearity between x1 and x2, then the correlation coefficient
r12 will be large. From Eq. (9.2) we see that as |r12|  1, Var ^ j = Cjj 2   and
( ) Cov ^1, ^2 = C12 2  ± depending on whether r12  +1 or r12  -1. Therefore,
strong multicollinearity between x1 and x2 results in large variances and covariances for the least-squares estimators of the regression coefficients. This implies that dif-
ferent samples taken at the same x levels could lead to widely different estimates
of the model parameters.
When there are more than two regressor variables, multicollinearity produces similar effects. It can be shown that the diagonal elements of the C = (XX)-1 matrix are

C jj

=

1 1 - Rj2

,

j = 1, 2, ... , p

(9.3)

where Rj2 is the coefficient of multiple determination from the regression of xj on the remaining p - 1 regressor variables. If there is strong multicollinearity between

( ) xj
to

and any subset of the other unity. Since the variance of

p ^ j

- 1, regressors, then is Var ^ j = Cjj 2 =

the
(1 -

value of
) Rj2 -1 2,

Rj2 will be close strong multicol-

linearity implies that the variance of the least-squares estimate of the regression coefficient j is very large. Generally, the covariance of ^i and ^ j will also be large

if the regressors xi and xj are involved in a multicollinear relationship. Multicollinearity also tends to produce least-squares estimates ^ j that are too
large in absolute value. To see this, consider the squared distance from b^ to the true

parameter vector , for example,

( ) ( ) L21 = b^ - b  b^ - b
The expected squared distance, E (L21 ), is

( ) ( )  ( ) E (L21 ) = E b^ - b  b^ - b = p E ^ j - j 2

j=1

p

 ( ) = Var ^ j =  2Tr (XX)-1

(9.4)

j=1

where the trace of a matrix (abbreviated Tr) is just the sum of the main diagonal elements. When there is multicollinearity present, some of the eigenvalues of XX will be small. Since the trace of a matrix is also equal to the sum of its eigenvalues, Eq. (9.4) becomes

( )  E

L21

p
= 2

1

j=1 j

(9.5)

Multlcollinearity is not the only cause of large variances and covariances of regression coefficients.

290 MULTICOLLINEARITY

where j > 0, j = 1, 2, . . . , p, are the eigenvalues of XX. Thus, if the XX matrix is ill-conditioned because of multicollinearity, at least one of the j will be small, and Eq. (9.5) implies that the distance from the least-squares estimate b^ to the true parameters  may be large. Equivalently we can show that
( ) ( ) ( ) E (L21 ) = E b^ - b  b^ - b = E b^ b^ - 2b^ b + b b

or
( ) E b^ b^ = b b +  2Tr (XX)-1

That is, the vector b^ is generally longer than the vector . This implies that the

method of least squares produces estimated regression coefficients that are too large

in absolute value.

While the method of least squares will generally produce poor estimates of the

individual model parameters when strong multicollinearity is present, this does not

necessarily imply that the fitted model is a poor predictor. If predictions are confined

to regions of the x space where the multicollinearity holds approximately, the fitted

model often produces satisfactory predictions. This can occur because the linear

combination



p j=1



j

xij

may

be

estimated

quite

well,

even

though

the

individual

parameters j are estimated poorly. That is, if the original data lie approximately

along the hyperplane defined by Eq. (9.1), then future observations that also lie near

this hyperplane can often be precisely predicted despite the inadequate estimates

of the individual model parameters.

Example 9.1 The Acetylene Data
Table 9.1 presents data concerning the percentage of conversion of n-heptane to acetylene and three explanatory variables (Himmelblau [1970], Kunugi, Tamura, and Naito [1961], and Marquardt and Snee [1975]). These are typical chemical process data for which a full quadratic response surface in all three regressors is often considered to be an appropriate tentative model. A plot of contact time versus reactor temperature is shown in Figure 9.2. Since these two regressors are highly correlated, there are potential multicollinearity problems in these data.
The full quadratic model for the acetylene data is
P =  0 +  1T +  2 H +  3C +  12TH +  13TC +  23HC +  11T 2 +  22 H 2 +  33C 2 + 
where
P = percentage of conversion T = temperature - 1212.50
80.623
H = H2 (n-heptane) - 12.44
5.662

EFFECTS OF MULTICOLLINEARITY

291

TABLE 9.1 Acetylene Data for Example 9.1

Observation

Conversion of n-Heptane to Acetylene (%)

Reactor Temperature (°C)

1

49.0

1300

2

50.2

1300

3

50.5

1300

4

48.5

1300

5

47.5

1300

6

44.5

1300

7

28.0

1200

8

31.5

1200

9

34.5

1200

10

35.0

1200

11

38.0

1200

12

38.5

1200

13

15.0

1100

14

17.0

1100

15

20.5

1100

16

29.5

1100

Ratio of H2 to n-Heptane (mole ratio)
7.5 9.0 11.0 13.5 17.0 23.0 5.3 7.5 11.0 13.5 17.0 23.0 5.3 7.5 11.0 17.0

Contact Time (sec)
0.0120 0.0120 0.0115 0.0130 0.0135 0.0120 0.0400 0.0380 0.0320 0.0260 0.0340 0.0410 0.0840 0.0980 0.0920 0.0860

0.10

0.08

Contact time (seconds)

0.06

0.04

0.02

0

1100

1200

1300

Reactor temperature (°C)

Figure 9.2 Contact time versus reactor temperature, acetylene data. (From Marquardt and Snee [1975], with permission of the publisher.)

292 MULTICOLLINEARITY
and
C = contact time - 0.0403 0.03164
Each of the original regressors has been scaled using the unit normal scaling of Section 3.9 [subtracting the average (centering) and dividing by the standard deviation. The squared and cross-product terms are generated from the scaled linear terms. As we noted in Chapter 7, centering the linear terms is helpful in removing nonessential ill-conditioning when fitting polynomials. The least-squares fit is
P^ = 35.897 + 4.019T + 2.781H - 8.031C - 6.457TH - 26.982TC - 3.768HC - 12.54T 2 - 0.973H 2 - 11.594C 2

The summary statistics for this model are displayed in Table 9.2. The regression

coefficients are reported in terms of both the original centered regressors and stan-

dardized regressors.

The fitted values for the six points (A, B, E, F, I, and J) that define the boundary

of the regressor variable hull of contact time and reactor temperature are shown in

Figure 9.3 along with the corresponding observed values of percentage of conver-

sion. The predicted and observed values agree very closely; consequently, the model

seems adequate for interpolation within the range of the original data. Now consider

using the model for extrapolation. Figure 9.3 (points C, D, G, and H) also shows

predictions made at the corners of the region defined by the range of the original

data. These points represent relatively mild extrapolation, since the original ranges

of the regressors have not been exceeded. The predicted conversions at three of the

four extrapolation points are negative, an obvious impossibility. It seems that the

least-squares model fits the data reasonably well but extrapolates very poorly. A

likely cause of this in view of the strong apparent correlation between contact time

and reactor temperature is multicollinearity. In general, if a model is to extrapolate

well, good estimates of the individual coefficients are required. When multicollinear-

ity is suspected, the least-squares estimates of the regression coefficients may be

very poor. This may seriously limit the usefulness of the regression model for infer-

ence and prediction.



9.4 MULTICOLLINEARITY DIAGNOSTICS
Several techniques have been proposed for detecting multicollinearity. We will now discuss and illustrate some of these diagnostic measures. Desirable characteristics of a diagnostic procedure are that it directly reflect the degree of the multicollinearity problem and provide information helpful in determining which regressors are involved.

9.4.1 Examination of the Correlation Matrix
A very simple measure of multicollinearity is inspection of the off-diagonal elements rij in XX. If regressors xi and xj are nearly linearly dependent, then |rij| will

MULTICOLLINEARITY DIAGNOSTICS

293

TABLE 9.2 Summary Statistics for the Least-Squares Acetylene Model

Term
Intercept T H C TH TC HC T2 H2 C2

Regression Coefficient
35.8971 4.0187 2.7811 -8.0311 -6.4568 -26.9818 -3.7683 -12.5237 -0.9721 -11.5943

Standard Error
1.0903 4.5012 0.3074 6.0657 1.4660 21.0224 1.6554 12.3239 0.3746 7.7070

t0
32.93 0.89 9.05 -1.32 -4.40 -1.28 -2.28 -1.02 -2.60 -1.50

Standardized Regression Coefficient
0.3377 0.2337 -0.6749 -0.4799 -2.0344 -0.2657 -0.8346 -0.0904 -1.0015

MSRes = 0.8126, R2 = 0.998, F0 = 289.72. When the response is standardized, MSRes = 0.00038 for the least-squares model.

0.10

A

p = 17.0

>

p = 16.33

B 0.08 p = 15.0
p = 14.84

>

D

H

>

>

p = - 19.74 p = - 78.34

Contact time (seconds)

0.06

E p = 38.5

>

0.04

p = 34.40

F p = 35.0

>>

0.02

p = 35.34

>

I p = 47.5

C p = - 33.78 G

p = 47.33

>

p = 22.15 J p = 50.5

>

p = 50.01

0

1100

1200

1300

Reactor temperature (°C)

Figure 9.3 Predictions of percentage of conversion within the range of the data and extrapolation for the least-squares acetylene model. (Adapted from Marquardt and Snee [1975], with permission of the publisher.)

294 MULTICOLLINEARITY

be near unity. To illustrate this procedure, consider the acetylene data from Example 9.1. Table 9.3 shows the nine regressor variables and the response in standardized form; that is, each of the variables has been centered by subtracting the mean for that variable and dividing by the square root of the corrected sum of squares for that variable. The XX matrix in correlation form for the acetylene data is

1.000 0.224

 

1.000









XX = 













Symmetric

-0.958 -0.240
1.000

-0.132 0.039 0.194 1.000

0.443 0.192 -0.661 -0.265 1.000

0.205 -0.023 -0.274 -0.975
0.323 1.000

-0.271 -0.148
0.501 0.246 -0.972 -0.279 1.000

0.031 0.498 -0.018 0.398 0.126 -0.374 -0.124 1.000

-0.577

-0.224

 

0.765

0.274

 

-0.972

 0.358

0.874

 

-0.158

1.000

 



The XX matrix reveals the high correlation between reactor temperature (x1) and contact time (x3) suspected earlier from inspection of Figure 9.2, since r13 = -0.958.

Furthermore, there are other large correlation coefficients between x1x2 and x2x3, x1x3 and x12, and x12 and x32. This is not surprising as these variables are generated
from the linear terms and they involve the highly correlated regressors x1 and x3.

Thus, inspection of the correlation matrix indicates that there are several near-linear

dependencies in the acetylene data.

Examining the simple correlations rij between the regressors is helpful in detect-

ing near-linear dependence between pairs of regressors only. Unfortunately, when

more than two regressors are involved in a near-linear dependence, there is no

assurance that any of the pairwise correlations rij will be large. As an illustration,

consider the data in Table 9.4. These data were artificially generated by Webster,

Gunst,

and

Mason

[1974].

They

required

that



4 j=1

xij

= 10

for

observations

2­12,

while



4 j=1

x1

j

=

11

for

observation

1.

Regressors

5

and

6

were

obtained

from

a

table

of normal random numbers. The responses yi were generated by the relationship

yi = 10 + 2.0xi1 + 1.0xi2 + 0.2xi3 - 2.0xi4 + 3.0xi5 + 10.0xi6 + i

where i  N(0, 1). The XX matrix in correlation form for these data is

1.000 0.052

 

1.000



XX

=

 







Symmetric

-0.343 -0.432
1.000

-0.498 -0.371 -0.355
1.000

0.417 0.485 -0.505 -0.215 1.000

-0.192

-0.317

 

0.494

-0.087

 

-0.123

 1.000



295

TABLE 9.3 Standardized Acetylene Dataa

Observation, i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

y
.27979 .30583 .31234 .26894 .24724 .18214 .17590 -.09995 -.03486 -.02401 .04109 .05194 .45800 .41460 -.33865 -.14335

x1
.28022 .28022 .28022 .28022 .28022 -.04003 -.04003 -.04003 -.04003 -.04003 -.04003 -.04003 -.36029 -.36029 -.36029 -.36029

x2
-.22554 -.15704 -.06584
.04817 .20777 .48139 -.32577 -.22544 -.06584 .04817 .20777 .48139 -.32577 -.22544 -.06584 .20777

x3
-.23106 -.23106 -.23514 -.22290 -.21882 -.23106 -.00255 -.01887 -.06784 -.11680 -.05152
.00561 .35653 .47078 .42187 .37285

x1x2
-.33766 -.25371 -.14179
.00189 .19398 .52974 -.00413 -.02171 -.04970 -.06968 -.09766 -.14563 .45252 .29423 .04240 -.38930

x1x3
-.02085 -.02085 -.02579 -.01098 -.00605 -.02085
.25895 .26177 .27023 .27869 .26741 .25754 -.29615 -.47384 -.39769 -.32153

x2x3
.30952 .23659 .14058 .01960 -.14065 -.44415 .07300 .08884 .08985 .04328 .01996 .08202 -.46678 -.42042 -.05859 -.42738

aThe standardized data were constructed from the centered and scaled form of the original data in Table 9.1.

x12
.07829 .07829 .07829 .07829 .07829 .07829 -.29746 -.29746 -.29746 -.29746 -.29746 -.29746 .32879 .32879 .32879 .32879

x22
-.04116 -.13270 -.20378 -.21070 -.06745
.59324 .15239 -.04116 -.20378 -.21070 -.06745 .59329 .15239 -.04116 -.20378 -.06745

x32
-.03452 -.03452 -.02735 -.04847 -.05526 -.03452 -.23548 -.23418 -.21822 -.18419 -.22554 -.23538
.24374 .60000 .43527 .28861

296 MULTICOLLINEARITY

TABLE 9.4 Unstandardized Regressor and Response Variables from Webster, Gunst, and Mason [1974]

Observation, i
1 2 3 4 5 6 7 8 9 10 11 12

yi
10.006 9.737 15.087 8.422 8.625 16.289 5.958 9.313 12.960 5.541 8.756 10.937

xi1
8.000 8.000 8.000 0.000 0.000 0.000 2.000 2.000 2.000 0.000 0.000 0.000

xi2
1.000 1.000 1.000 0.000 0.000 0.000 7.000 7.000 7.000 0.000 0.000 0.000

xi3
1.000 1.000 1.000 9.000 9.000 9.000 0.000 0.000 0.000 0.000 0.000 0.000

xi4
1.000 0.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000 10.000 10.000 10.000

xi5
0.541 0.130 2.116 -2.397 -0.046 0.365 1.996 0.228 1.380 -0.798 0.257 0.440

xi6
-0.099 0.070 0.115 0.252 0.017 1.504
-0.865 -0.055
0.502 -0.399
0.101 0.432

None of the pairwise correlations rij are suspiciously large, and consequently we have no indication of the near-linear dependence among the regressors. Generally, inspection of the rij is not sufficient for detecting anything more complex than pairwise multicollinearity.

9.4.2 Variance Inflation Factors
We observed in Chapter 3 that the diagonal elements of the C = (XX)-1 matrix are very useful in detecting multicollinearity. Recall from Eq. (9.3) that Cjj, the jth
diagonal element of C, can be written as Cjj = (1 - ) Rj2 -1, where Rj2 is the coefficient
of determination obtained when xj is regressed on the remaining p - 1 regressors. If xj is nearly orthogonal to the remaining regressors, Rj2 is small and Cjj is close to unity, while if xj is nearly linearly dependent on some subset of the remaining regressors, Rj2 is near unity and Cjj is large. Since the variance of the jth regression coefficients is Cjj2, we can view Cjj as the factor by which the variance of ^ j is increased due to near-linear dependences among the regressors. In Chapter 3 we called
( ) VIFj = Cjj = 1 - Rj2 -1
the variance inflation factor. This terminology is due to Marquardt [1970]. The VIF for each term in the model measures the combined effect of the dependences among the regressors on the variance of that term. One or more large VIFs indicate multicollinearity. Practical experience indicates that if any of the VIFs exceeds 5 or 10, it is an indication that the associated regression coefficients are poorly estimated because of multicollinearity.
The VIFs have another interesting interpretation. The length of the normal theory confidence interval on the jth regression coefficient may be written as

( ) Lj

=2

C jj 2

t 1 2  2,n- p-1

MULTICOLLINEARITY DIAGNOSTICS

297

and the length of the corresponding interval based on an orthogonal reference

design with the same sample size and root-mean-square (rms) values [i.e.,

( ) rms

=



n i=1

xij - xj

2

n is a measure of the spread of the regressor xj] as the original

design is

L* = 2 t 2,n- p-1

The ratio of these two confidence intervals is Lj

L*

=

C

1 jj

2.

Thus,

the

square

root

of

the jth VIF indicates how much longer the confidence interval for the jth regression

coefficient is because of multicollinearity.

The VIFs for the acetylene data are shown in panel A of Table 9.5. These VIFs

are the main diagonal elements of (XX)-1, assuming that the linear terms in the

model are centered and the second-order terms are generated directly from the

linear terms. The maximum VIF is 6565.91, so we conclude that a multicollinearity

problem exists. Furthermore, the VIFs for several of the other cross-product and

squared variables involving x1 and x3 are large. Thus, the VIFs can help identify which regressors are involved in the multicollinearity. Note that the VIFs in poly-

nomial models are affected by centering the linear terms. Panel B of Table 9.5 shows

the VIFs for the acetylene data, assuming that the linear terms are not centered.

These VIFs are much larger than those for the centered data. Thus centering the

linear terms in a polynomial model removes some of the nonessential ill-conditioning

caused by the choice of origin for the regressors.

The VIFs for the Webster, Gunst, and Mason data are shown in panel C of Table

9.5. Since the maximum VIF is 297.14, multicollinearity is clearly indicated. Once

again, note that the VIFs corresponding to the regressors involved in the multicol-

linearity are much larger than those for x5 and x6.

9.4.3 Eigensystem Analysis of XX
The characteristic roots or eigenvalues of XX, say 1, 2, . . . , p, can be used to measure the extent of multicollinearity in the data. If there are one or more

TABLE 9.5 VIFs for Acetylene Data and Webster, Gunst, and Mason Data

Data, (A) Acetylene Centered Term VIF

Data, (B) Acetylene Uncentered
Term VIF

Data, (C) Webster, Gunst, and
Mason Term VIF

x1 = 374 x2 = 1.74 x3 = 679.11 x1x2 = 31.03 x1x3 = 6565.91 x2x3 = 35.60
x12 = 1762.58 x22 = 3.17 x32 = 1158.13 Maximum VIF = 6565.91

x1 = 2,856,749 x2 = 10,956.1 x3 = 2,017,163 x1x2 = 2,501,945 x1x3 = 65.73 x2x3 = 12,667.1
x12 = 9802.9 x22 = 1, 428,092 x32 = 240.36 Maximum VIF = 2,856,749

x1 = 181.83 x2 = 161.40 x3 = 265.49 x4 = 297.14 x5 = 1.74 x6 = 1.44
Maximum VIF = 297.14

Recall that the eigenvalues of a p × p matrix A are the p roots of the equation |A - I| = 0. Eigenvalues are almost always calculated by computer routines. Methods for computing eigenvalues and eigenvectors are discussed in Smith et al. [1974], Stewart [1973], and Wilkinson [1965].

298 MULTICOLLINEARITY

near-linear dependences in the data, then one or more of the characteristic roots will be small. One or more small eigenvalues imply that there are near-linear dependences among the columns of X. Some analysts prefer to examine the condition number of XX, defined as

 = max min

(9.6)

This is just a measure of the spread in the eigenvalue spectrum of XX. Generally, if the condition number is less than 100, there is no serious problem with multicollinearity. Condition numbers between 100 and 1000 imply moderate to strong multicollinearity, and if  exceeds 1000, severe multicollinearity is indicated.
The condition indices of the XX matrix are

j

=

max j

,

j = 1, 2, ... , p

Clearly the largest condition index is the condition number defined in Eq. (9.6). The number of condition indices that are large (say  1000) is a useful measure of the number of near-linear dependences in XX.
The eigenvalues of XX for the acetylene data are 1 = 4.2048, 2 = 2.1626, 3 = 1.1384, 4 = 1.0413, 5 = 0.3845, 6 = 0.0495, 7 = 0.0136, 8 = 0.0051, and 9 = 0.0001. There are four very small eigenvalues, a symptom of seriously illconditioned data. The condition number is

 = max = 4.2048 = 42, 048 min 0.0001

which indicates severe multicollinearity. The condition indices are

1

=

4.2048 4.2048

=

1,

2

=

4.2048 2.1626

=

1.94,

3

=

4.2048 1.1384

=

3.69

4

=

4.2048 1.0413

=

4.04,

5

=

4.2048 0.3845

=

10.94,

6

=

4.2048 0.0495

=

84

7

=

4.2048 0.0136

=

309.18,

8

=

4.2048 0.0051

=

824.47,

9

=

4.2048 0.0001

=

42, 048

Since one of the condition indices exceeds 1000 (and two others exceed 100), we
conclude that there is at least one strong near-linear dependence in the acetylene
data. Considering that x1 is highly correlated with x3 and the model contains both quadratic and cross-product terms in x1 and x3, this is, of course, not surprising.
The eigenvalues for the Webster, Gunst, and Mason data are 1 = 2.4288, 2 = 1.5462, 3 = 0.9221, 4 = 0.7940, 5 = 0.3079, and 6 = 0.0011. The small eigenvalue indicates the near-linear dependence in the data. The condition number is

 = max = 2.4288 = 2188.11 min 0.0011

MULTICOLLINEARITY DIAGNOSTICS

299

which also indicates strong multicollinearity. Only one condition index exceeds 1000, so we conclude that there is only one near-linear dependence in the data.
Eigensystem analysis can also be used to identify the nature of the near-linear dependences in data. The XX matrix may be decomposed as

XX = TLT

where  is a p × p diagonal matrix whose main diagonal elements are the eigenvalues j (j = 1, 2, . . . , p) of XX and T is a p × p orthogonal matrix whose columns are the eigenvectors of XX. Let the columns of T be denoted by t1, t2, . . . , tp. If the eigenvalue j is close to zero, indicating a near-linear dependence in the data, the elements of the associated eigenvector tj describe the nature of this linear dependence. Specifically the elements of the vector tj are the coefficients t1, t2, . . . , tp in Eq. (9.1).
Table 9.6 displays the eigenvectors for the Webster, Gunst, and Mason data. The smallest eigenvalue is 6 = 0.0011, so the elements of the eigenvector t6 are the coefficients of the regressors in Eq. (9.1). This implies that

-0.44768x1 - 0.42114x2 - 0.54169x3 - 0.57337x4 - 0.00605x5 - 0.00217x6 = 0
Assuming that -0.00605 and -0.00217 are approximately zero and rearranging terms gives

x1 -0.941x2 - 1.120x3 - 1.281x4
That is, the first four regressors add approximately to a constant. Thus, the elements of t6 directly reflect the relationship used to generate x1, x2, x3, and x4.
Belsley, Kuh, and Welsch [1980] propose a similar approach for diagnosing multicollinearity. The n × p X matrix may be decomposed as
X = UDT
where U is n × p, T is p × p, UU = I, TT = I, and D is a p × p diagonal matrix with nonnegative diagonal elements , j = 1, 2, . . . , p. The j are called the singular values of X and X = UDT is called the singular-value decomposition of X. The singularvalue decomposition is closely related to the concepts of eigenvalues and eigenvectors, since XX = (UDT)UDT = TD2T = TT, so that the squares of the singular values of X are the eigenvalues of XX. Here T is the matrix of eigenvectors of XX

TABLE 9.6 Eigenvectors for the Webster, Gunst, and Mason Data

t1
-.39072 -.45560
.48264 .18766 -.49773 .35195

t2
-.33968 -.05392 -.45333
.73547 -.09714 -.35476

t3
.67980 -.70013 -.16078
.13587 -.03185 -.04864

t4
.07990 .05769 .19103 -.27645 -.56356 -.74818

t5
-.25104 -.34447
.45364 .01521 .65128 -.43375

t6
-.44768 -.42114 -.54169 -.57337 -.00605 -.00217

300 MULTICOLLINEARITY

defined earlier, and U is a matrix whose columns are the eigenvectors associated with the p nonzero eigenvalues of XX.
Ill-conditioning in X is reflected in the size of the singular values. There will be one small singular value for each near-linear dependence. The extent of illconditioning depends on how small the singular value is relative to the maximum singular value max. SAS follows Belsley, Kuh, and Welsch [1980] and defines the condition indices of the X matrix as

j

=

max j

,

j = 1, 2, ... , p

The largest value for j is the condition number of X. Note that this approach deals directly with the data matrix X, with which we are principally concerned, not the matrix of sums of squares and cross products XX. A further advantage of this approach is that algorithms for generating the singular-value decomposition are more stable numerically than those for eigensystem analysis, although in practice this is not likely to be a severe handicap if one prefers the eigensystem approach.
The covariance matrix of b^ is

( ) Var b^ =  2 (XX)-1 =  2TL-1T

and the variance of the jth regression coefficient is the jth diagonal element of this matrix, or

( )   Var ^ j

= 2

p

t

2 ji

= 2

p

t

2 ji

2
i=1 i

i=1 i

Note also that apart from 2, the jth diagonal element of T-1T is the jth VIF, so

  VIFj

=

p i=1

t

2 ji

i2

=

p i=1

t

2 ji

i

Clearly, one or more small singular values (or small eigenvalues) can dramatically inflate the variance of ^ j. Belsley, Kuh, and Welsch suggest using variance decomposition proportions, defined as

 ij

=

t

2 ji

i2

VIFj

,

j = 1, 2, ... , p

as measures of multicollinearity. If we array the ij in a p × p matrix , then the elements of each column of  are just the proportions of the variance of each ^ j (or each VIF) contributed by the ith singular value (or eigenvalue). If a high proportion
of the variance for two or more regression coefficients is associated with one small singular value, multicollinearity is indicated. For example, if 32 aud 34 are large, the third singular value is associated with a multicollinearity that is inflating the variances of ^2 and ^4 Condition indices greater than 30 and variance decomposition proportions greater than 0.5 are recommended guidelines.

MULTICOLLINEARITY DIAGNOSTICS

301

TABLE 9.7 Variance Decomposition Proportions for the Webster, Gunst, and Mason [1974] Data

Condition Number Eigenvalue Indices

Variance Decomposition Proportions

X1

X2

X3

X4

X5

X6

A. Regressors Centered

1

2.42879

1.00000

2

1.54615

1.25334

3

0.92208

1.62297

4

0.79398

1.74900

5

0.30789

2.80864

6

0.00111 46.86052

0.0003 0.0004 0.0028 0.0000 0.0011 0.9953

0.0005 0.0000 0.0033 0.0000 0.0024 0.9937

0.0004 0.0005 0.0001 0.0002 0.0025 0.9964

0.0000 0.0012 0.0001 0.0003 0.0000 0.9984

0.0531 0.0032 0.0006 0.2083 0.7175 0.0172

0.0350 0.0559 0.0018 004845 004199 0.0029

B. Regressors Not Centered

1

2.63287

1.00000 0.0001 0.0003 0.0003 0.0001 0.0001 0.0217 0.0043

2

1.82065

1.20255 0.0000 0.0001 0.0002 0.0005 0.0000 0.0523 0.0949

3

1.03335 159622 0.0000 0.0002 0.0000 0.0002 0.0013 0.0356 0.1010

4

0.65826

1.99994 0.0000 0.0005 0.0000 0.0005 0.0003 0.1906 0.3958

5

0.60573

2.08485 0.0000 0.0025 0.0035 0.0001 0.0001 0.0011 0.0002

6

0.24884

3.25280 0.0000 0.0012 0.0023 0.0028 0.0000 0.6909 0.4003

7

0.00031 92.25341 0.9999 0.9953 0.9936 0.9959 0.9983 0.0178 0.0034

Table 9.7 displays the condition indices of X (j) and the variance-decomposition proportions (the ij) for the Webster, Gunst, and Mason data. In panel A of this table we have centered the regressors so that these variables are (xij - xj), j = 1, 2, . . . ,6. In Section 9.4.2 we observed that the VIFs in a polynomial model are affected by centering the linear terms in the model before generating the higher order polynomial terms. Centering will also affect the variance decomposition proportions (and also the eigenvalues and eigenvectors). Essentially, centering removes any nonessential ill-conditioning resulting from the intercept.
Notice that there is only one large condition index (6 = 46.86 > 30), so there is one dependence in the columns of X. Furthermore, the variance decomposition proportions 61, 62, 63, and 64 all exceed 0.5, indicating that the first four regressors are involved in a multicollinear relationship. This is essentially the same information derived previously from examining the eigenvalues.
Belsley, Kuh, and Welsch [1980] suggest that the regressors should be scaled to unit length but not centered when computing the variance decomposition proportions so that the role of the intercept in near-linear dependences can be diagnosed. This option is displayed in panel B of Table 9.7. Note that the effect of this is to increase the spread in the eigenvalues and make the condition indices larger.
There is some controversy about whether regression data should be centered when diagnosing multicollinearity using either the eigensystem analysis or the variance decomposition proportion approach. Centering makes the intercept orthogonal to the other regressors, so we can view centering as an operation that removes ill-conditioning that is due to the model's constant term. If the intercept has no physical interpretation (as is the case in many applications of regression in engineering and the physical sciences), then ill-conditioning caused by the constant term is truly "nonessential," and thus centering the regressors is entirely appropriate.

302 MULTICOLLINEARITY
However, if the intercept has interpretative value, then centering is not the best approach. Clearly the answer to this question is problem specific. For excellent discussions of this point, see Brown [1977] and Myers [1990].
9.4.4 Other Diagnostics
There are several other techniques that are occasionally useful in diagnosing multicollinearity. The determinant of XX can be used as an index of multicollinearity. Since the XX matrix is in correlation form, the possible range of values of the determinant is 0  |XX|  1. If |XX| = 1, the regressors are orthogonal, while if |XX| = 0, there is an exact linear dependence among the regressors. The degree of multicollinearity becomes more severe as |XX| approaches zero. While this measure of multicollinearity is easy to apply, it does not provide any information on the source of the multicollinearity.
Willan and Watts [1978] suggest another interpretation of this diagnostic. The joint 100(1 - ) percent confidence region for  based on the observed data is
( ) ( ) b - b^  XX b - b^  p^ 2F,p,n- p-1
while the corresponding confidence region for b^ based on the orthogonal reference design described earlier is
( ) ( ) b - b^  b - b^  p^ 2F,p,n- p-1
The orthogonal reference design produces the smallest joint confidence region for fixed sample size and rms values and a given . The ratio of the volumes of the two confidence regions is |XX|1/2, so that |XX|1/2 measures the loss of estimation power due to multicollinearity. Put another way, 100(|XX|1/2 - 1) reflects the percentage increase in the volume of the joint confidence region for  because of the near-linear dependences in X. For example, if |XX| = 0.25, then the volume of the joint confidence region is 100[(0.25)-1/2 - 1] = 100% larger than it would be if an orthogonal design had been used.
The F statistic for significance of regression and the individual t (or partial F) statistics can sometimes indicate the presence of multicollinearity. Specifically, if the overall F statistic is significant but the individual t statistics are all nonsignificant, multicollinearity is present. Unfortunately, many data sets that have significant multicollinearity will not exhibit this behavior, and so the usefulness of this measure of multicollinearity is questionable.
The signs and magnitudes of the regression coefficients will sometimes provide an indication that multicollinearity is present. In particular, if adding or removing a regressor produces large changes in the estimates of the regression coefficients, multicollinearity is indicated. If the deletion of one or more data points results in large changes in the regression coefficients, there may be multicollinearity present. Finally, if the signs or magnitudes of the regression coefficients in the regression model are contrary to prior expectation, we should be alert to possible multicollinearity. For example, the least-squares model for the acetylene data has large standardized regression coefficients for the x1x3 interaction and for the squared terms x12 and x32. It is somewhat unusual for quadratic models to display large regres-

METHODS FOR DEALING WITH MULTICOLLINEARITY

303

sion coefficients for the higher order terms, and so this may be an indication of multicollinearity. However, one should be cautious in using the signs and magnitudes of the regression coefficients as indications of multicollinearity, as many seriously ill-conditioned data sets do not exhibit behavior that is obviously unusual in this respect.
We believe that the VIFs and the procedures based on the eigenvalues of XX are the best currently available multicollinearity diagnostics. They are easy to compute, straightforward to interpret, and useful in investigating the specific nature of the multicollinearity. For additional information on these and other methods of detecting multicollinearity, see Belsley, Kuh, and Welsch [1980], Farrar and Glauber [1997], and Willan and Watts [1978].

9.4.5 SAS and R Code for Generating Multicollinearity Diagnostics
The appropriate SAS code for generating the multicollinearity diagnostics for the acetylene data is
proc reg; model conv = t h c t2 h2·c2 th tc hc / corrb vif collin;
The corrb option prints the variance­covariance matrix of the estimated coefficients in correlation form. The vif option prints the VIFs. The collin option prints the singular-value analysis including the condition numbers and the variance decomposition proportions. SAS uses the singular values to compute the condition numbers. Some other software packages use the eigenvalues, which are the squares of the singular values. The collin option includes the effect of the intercept on the diagnostics. The option collinoint performs the singular-value analysis excluding the intercept.
The collinearity diagnostics in R require the packages "perturb" and "car". The R code to generate the collinearity diagnostics for the delivery data is:
deliver.model <- lm(timecases+dist, data=deliver) print(vif(deliver.model)) print(colldiag(deliver.model))

9.5 METHODS FOR DEALING WITH MULTICOLLINEARITY
Several techniques have been proposed for dealing with the problems caused by multicollinearity. The general approaches include collecting additional data, model respecification, and the use of estimation methods other than least squares that are specifically designed to combat the problems induced by multicollinearity.
9.5.1 Collecting Additional Data
Collecting additional data has been suggested as the best method of combating multicollinearity (e.g., see Farrar and Glauber [1967] and Silvey [1969]). The additional data should be collected in a manner designed to break up the multicollinearity in the existing data. For example, consider the delivery time data first introduced

304 MULTICOLLINEARITY
Example 3.1. A plot of the regressor cases (x1) versus distance (x2) is shown in the matrix of scatterplots, Figure 3.4. We have remarked previously that most of these data lie along a line from low values of cases and distance to high values of cases and distance, and consequently there may be some problem with multicollinearity. This could be avoided by collecting some additional data at points designed to break up any potential multicollinearity, that is, at points where cases are small and distance is large and points where cases are large and distance is small.
Unfortunately, collecting additional data is not always possible because of economic constraints or because the process being studied is no longer available for sampling. Even when additional data are available it may be inappropriate to use if the new data extend the range of the regressor variables far beyond the analysts region of interest. Furthermore, if the new data points are unusual or atypical of the process being studied, their presence in the sample could be highly influential on the fitted model. Finally, note that collecting additional data is not a viable solution to the multicollinearity problem when the multicollinearity is due to constraints on the model or in the population. For example, consider the factors family income (x1) and house size (x2) plotted in Figure 9.1. Collection of additional data would be of little value here, since the relationship between family income and house size is a structural characteristic of the population. Virtually all the data in the population will exhibit this behavior.
9.5.2 Model Respecification
Multicollinearity is often caused by the choice of model, such as when two highly correlated regressors are used in the regression equation. In these situations some respecification of the regression equation may lessen the impact of multicollinearity. One approach to model respecification is to redefine the regressors. For example, if x1, x2, and x3 are nearly linearly dependent, it may be possible to find some function such as x = (x1 + x2)/x3 or x = x1x2x3 that preserves the information content in the original regressors but reduces the ill-conditioning.
Another widely used approach to model respecification is variable elimination. That is, if x1, x2 and x3 are nearly linearly dependent, eliminating one regressor (say x3) may be helpful in combating multicollinearity. Variable elimination is often a highly effective technique. However, it may not provide a satisfactory solution if the regressors dropped from the model have significant explanatory power relative to the response y. That is, eliminating regressors to reduce multicollinearity may damage the predictive power of the model. Care must be exercised in variable selection because many of the selection procedures are seriously distorted by multicollinearity, and there is no assurance that the final model will exhibit any lesser degree of multicollinearity than was present in the original data. We discuss appropriate variable elimination techniques in Chapter 10.
9.5.3 Ridge Regression
When the method of least squares is applied to nonorthogonal data, very poor estimates of the regression coefficients can be obtained. We saw in Section 9.3 that the variance of the least-squares estimates of the regression coefficients may be considerably inflated, and the length of the vector of least-squares parameter

> > > > > >

METHODS FOR DEALING WITH MULTICOLLINEARITY

305

E () =  (unbiased) V () large

E ( *)   (biased) V ( *) small





 E ( *)

*

(a)

(b)

Figure 9.4 Sampling distribution of (a) nnbiased and (b) biased estimators of . (Adapted from Marquardt and Snee [1975], with permission of the publisher.)

estimates is too long on the average. This implies that the absolute value of the least-squares estimates are too large and that they are very unstable, that is, their magnitudes and signs may change considerably given a different sample.
The problem with the method of least squares is the requirement that b^ be an unbiased estimator of . The Gauss-Markov property referred to in Section 3.2.3 assures us that the least-squares estimator has minimum variance in the class of unbiased linear estimators, but there is no guarantee that this variance will be small. The situation is illustrated in Figure 9.4a, where the sampling distribution of b^ , the unbiased estimator of , is Shown. The variance of b^ is large, implying that confidence intervals on  would be wide and the point estimate b^ is very unstable.
One way to alleviate this problem is to drop the requirement that the estimator of  be unbiased. Suppose that we can find a biased estimator of , say b^ *, that has a smaller variance than the unbiased estimator b^ . The mean square error of the estimator b^* is defined as
( ) ( ) ( ) ( ) MSE b^* = E b^* - b 2 = Var b^* + E b^* - b 2
or
( ) ( ) ( ) MSE b^* = Var b^* + bias in b^* 2
Note that the MSE is just the expected squared distance from b^ * to  [see Eq. (9.4)]. By allowing a small amount of bias in b^ *, the variance of b^ * can be made small such that the MSE of b^* is less than the variance of the unbiased estimator . Figure 9.4b illustrates a situation where the variance of the biased estimator is considerably smaller than the variance of the unbiased estimator (Figure 9.4a). Consequently, confidence intervals on  would be much narrower using the biased estimator. The small variance for the biased estimator also implies that b^ * is a more stable estimator of  than is the unbiased estimator b^ .
A number of procedures have been developed for obtaining biased estimators of regression coefficients. One of these procedures is ridge regression, originally proposed by Hoerl and Kennard [1970a, b]. The ridge estimator is found by solving a slightly modified version of the normal equations. Specifically we define the ridge estimator b^R as the solution to
(XX + kI) b^R = Xy

306 MULTICOLLINEARITY

or
b^R = (XX + kI)-1 Xy

where k  0 is a constant selected by the analyst. The procedure is called ridge regression because the underlying mathematics are similar to the method of ridge analysis used earlier by Hoerl [1959] for describing the behavior of second-order response surfaces. Note that when k = 0, the ridge estimator is the least-squares estimator.
The ridge estimator is a linear transformation of the least-squares estimator since
b^R = (XX + kI)-1 Xy = (XX + kI)-1(XX) b^ = Zk b^
( ) ( ) Therefore, since E b^R = E Zk b^ = Zk b, b^R is a biased estimator of . We usually
refer to the constant k as the biasing parameter. The covariance matrix of b^R is
( ) Var b^R =  2 (XX + kI)-1 XX (XX + kI)-1

The mean square error of the ridge estimator is

( ) ( ) ( ) MSE b^R = Var b^R + bias in b^R 2

=  2Tr (XX + kI)-1 XX (XX + kI)-1  + k2b  (XX + kI)-2 b

 =  2

p j=1

j
(j + k)2

+ k2b  (XX + kI)-2 b

where 1, 2, . . . , p are the eigenvalues of XX. The first term on the right-hand side of this equation is the sum of variances of the parameters in b^R and the second term is the square of the bias. If k > 0, note that the bias in b^R increases with k. However, the variance decreases as k increases.
In using ridge regression we would like to choose a value of k such that the reduction in the variance term is greater than the increase in the squared bias. If this can be done, the mean square error of the ridge estimator b^R will be less than the variance of the least-squares estimator b^ . Hoerl and Kennard proved that there exists a nonzero value of k for which the MSE of b^R is less than the variance of the leastsquares estimator b^, provided that  is bounded. The residual sum of squares is

( ) ( ) SSRes = y - Xb^R  y - Xb^R

( ) ( ) ( ) ( ) = y - Xb^  y - Xb^ + b^R - b^  XX b^R - b^

(9.7)

Since the first term on the right-hand side of Eq. (9.7) is the residual sum of squares for the least-squares estimates b^, we see that as k increases, the residual sum of squares increases. Consequently, because the total sum of squares is fixed, R2 decreases as k increases. Therefore, the ridge estimate will not necessarily provide the best "fit" to the data, but this should not overly concern us, since we are more interested in obtaining a stable set of parameter estimates. The ridge estimates may

METHODS FOR DEALING WITH MULTICOLLINEARITY

307

result in an equation that does a better job of predicting future observations than would least squares (although there is no conclusive proof that this will happen).
Hoed and Kennard have suggested that an appropriate value of k may be determined by inspection of the ridge trace. The ridge trace is a plot of the elements of b^R versus k for values of k usually in the interval 0­1. Marquardt and Snee [1975] suggest using up to about 25 values of k, spaced approximately logarithmically over the interval [0, 1]. If multicollinearity is severe, the instability in the regression coefficients will be obvious from the ridge trace. As k is increased, some of the ridge estimates will vary dramatically. At some value of k, the ridge estimates b^R will stabilize. The objective is to select a reasonably small value of k at which the ridge estimates b^R are stable. Hopefully this will produce a set of estimates with smaller MSE than the least-squares estimates.

Example 9.2 The Acetylene Data
To obtain the ridge solution for the acetylene data, we must solve the equations
(XX + kI) b^R = Xy for several values 0  k  1, with XX and Xy in correlation
form. The ridge trace is shown in Figure 9.5, and the ridge coefficients for several values of k are listed in Table 9.8. This table also shows the residual mean square and R2 for each ridge model. Notice that as k increases, MSRes increases and R2 decreases. The ridge trace illustrates the instability of the least-squares solution, as there are large changes in the regression coefficients for small values of k. However, the coefficients stabilize rapidly as k increases.
Judgment is required to interpret the ridge trace and select an appropriate value of k. We want to choose k large enough to provide stable coefficients, but not unnecessarily large ones, as this introduces additional bias and increases the residual mean square. From Figure 9.5 we see that reasonable coefficient stability is achieved in the region 0.008 < k < 0.064 without a severe increase in the residual mean square (or loss in R2). If we choose k = 0.032, the ridge regression model is

>

>

0.7 0.6 R1 0.5

0.4

0.3

>> >

0.2

R2 R11

>

R 0.1 R33 R23

0

-0.1 -0.2

>

>

0.05 0.1 R13R22
R12

>

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5 k

-0.3

-0.4

>

R3

-0.5

Figure 9.5 Ridge trace for acetylene data using nine regressors.

308

TABLE 9.8 Coefficients at Various Values of k

k

.000

.001

.002

.004

R.1 R.2 R.3  R.12  R.13 R.23  R.11 R.22  R.33
MSRes R2

.3377 .2337 -.6749 -.4799 -2.0344 -.2675 -.8346 -.0904 -1.0015 .00038 .998

.6770 .2242 -.2129 -.4479 -.2774 -.2173 .0643 -.0732 -.2451 .00047 .997

.6653 .2222 -.2284 -.4258 -.1887 -.1920 .1035 -.0682 -.1853 .00049 .997

.6362 .2199 -.2671 -.3913 -.1350 -.1535 .1214 -.0621 -.1313 .00054 .997

.008
.6003 .2173 -.3134 -.3437 -.1017 -.1019 .1262 -.0558 -.0825 .00062 .996

.016
.5672 .2148 -.3515 -.2879 -.0809 -.0433 .1254 -.0509 -.0455 .00074 .996

.032
.5392 .2117 -.3735 -.2329 -.0675 .0123 .1249 -.0481 -.0267 .00094 .994

.064
.5122 .2066 -.3800 -.1862 -.0570 .0562 .1258 -.0464 -.0251 .00127 .992

.128
.4806 .1971 -.3724 -.1508 -.0454 .0849 .1230 -.0444 -.0339 .00206 .988

.256
.4379 .1807 -.3500 -.1249 -.0299 .0985 .1097 -.0406 -.0464 .00425 .975

.512
.3784 .1554 -.3108 -.1044 -.0092 .0991 .0827 -.0341 -.0586 .01002 .940

METHODS FOR DEALING WITH MULTICOLLINEARITY

309

y^ = 0.5392x1 + 0.2117x2 - 0.3735x3 - 0.2329x1x2 - 0.0675x1x3 + 0.0123x2 x3 + 0.1249x12 - 0.0481x22 - 0.0267x32

Note that in this model the estimates of 13, 11, and 23 are considerably smaller than the least-squares estimates and the original negative estimates of 23 and 11 are now positive. The ridge model expressed in terms of the original regressors is

P^ = 0.7598 + 0.1392T + 0.0547H - 0.0965C - 0.0680TH - 0.0194TC + 0.0039CH + 0.0407T 2 - 0.0112H 2 - 0.0067C2

Figure 9.6 shows the performance of the ridge model in prediction for both interpolation (points A, B, E, F, I, and J) and extrapolation (points C, D, G, and H). Comparing Figures 9.6 and 9.3, we note that the ridge model predicts as well as the nine-term least-squares model at the boundary of the region covered by the data. However, the ridge model gives much more realistic predictions when extrapolating than does least squares. We conclude that ridge regression has produced a model that is superior to the original least squares fit.
The ridge regression estimates may be computed by using an ordinary leastsquares computer program and augmenting the standardized data as follows:

X

XA =  

k

I

p

 

,

yA

=

y 0p 

where kIp is a p × p diagonal matrix with diagonal elements equal to the square root of the biasing parameter and 0p is a p × 1 vector of zeros. The ridge estimates
are then computed from

b^R = (XA XA )-1 XA yA = (XX + kIp )-1 Xy

Table 9.9 shows the augmented matrix XA and vector yA required to produce the

ridge solution for the acetylene data with k = 0.032.



Some Other Properties of Ridge Regression Figure 9.7 illustrates the geometry of ridge regression for a two-regressor problem. The point b^ at the center of the ellipses corresponds to the least-squares solution, where the residual sum of
squares takes on its minimum value. The small ellipse represents the locus of points in the 1, 2 plane where the residual sum of squares is constant at some value greater than the minimum. The ridge estimate b^R is the shortest vector from the origin that produces a residual sum of squares equal to the value represented by the small ellipse. That is, the ridge estimate b^R produces the vector of regression coefficients with the smallest norm consistent with a specified increase in the
residual sum of squares. We note that the ridge estimator shrinks the least-squares

310 MULTICOLLINEARITY

0.10

A

p = 17.0

>

p = 16.17

p = 15.0 B 0.08 p = 14.89

>

D

H

>

>

p = 29.91

p = 33.81

Contact time (seconds)

0.06

p = 38.5

E

>

0.04

p = 37.78

p = 35.0

>

p = 36.48 F

0.02

p = 47.5

>

C p = 30.58

>

G p = 34.79

>

I p = 47.39 p = 50.5
J

>

p = 49.09

0

1100

1200

1300

Reactor temperature (°C)

Figure 9.6 Performance of the ridge model with k = 0.032 in prediction and extrapolation for the acetylene data. (Adapted from Marquardt and Snee [1975], with permission of the publisher.)

estimator toward the origin. Consequently, ridge estimators (and other biased estimators generally) are sometimes called shrinkage estimators. Hocking [1976] has observed that the ridge estimator shrinks the least-squares estimator with respect to the contours of XX. That is, b^R is the solution to
( ) ( ) Minimize b - b^  XX b - b^ 
subject to b b  d2
where the radius d depends on k. Many of the properties of the ridge estimator assume that the value of k is fixed.
In practice, since k is estimated from the data by inspection of the ridge trace, k is stochastic. It is of interest to ask if the optimality properties cited by Hoerl and Kennard hold if k is stochastic. Several authors have shown through simulations that ridge regression generally offers improvement in mean square error over least squares when k is estimated from the data. Theobald [1974] has generalized the

TABLE 9.9 Augmented Matrix XA and Vector yA for Generating the Ridge Solution for the Acetylene Data with k = 0.032

.280224 .280224 .280224 .280224 .280224 .280224 -.04003 -.04003 -.04003 -.04003 -.04003 -.04003 xA = -.36029 -.36029 -.36029 -.36029 .17888 0 0 0 0 0 0 0 0

-.22544 -.15704 -.06584 .048167 .207774 .481385 -.32577 -.22544 -.06584 .048167 .207774 .481385 -.32577 -.22544 -.06584 .207774 0 .17888 0 0 0 0 0 0 0

-.23106 -.23106 -.23514 -.2229 -.21882 -.23106 -.00255 -.01887 -.06784 -.1168 -.05152
.005609 .356528 .470781 .421815 .37285 0 0 .17888 0 0 0 0 0 0

-.33766 -.25371 -.14179 -.00189
.193976 .529744 -.00413 -.02171 -.0497 -.06968 -.09766 -.14563 .452517 .294227 .042401 -.3893 0 0 0 .17888 0 0 0 0 0

-.02085 -.02085 -.02579 -.01098 -.00605 -.02085
.258949 .261769 -.270231 .278693 .267411 .257539 -.29615 -.47384 -.39769 -.32153 0 0 0 0 .17888 0 0 0 0

309525 .236588 .140577 .0196 -.14065 -.44415 .073001 .088842 .089856 .043276 .019961 .0832021 -.46678 -.42042 -.05859 .427375 0 0 0 0 0 .17888 0 0 0

.078278 .078278 .078278 .078278 .078278 .078278 -.29746 -.29746 -.29746 -.29746 -.29746 -.29746 .328768 .328768 .328768 .328768 0 0 0 0 0 0 .17888 0 0

-.04116 -.1327 -.20378 -.2107 -.06745
.593235 .152387 -.04116 -.20378 -.2107 -.06745 .593235 .152387 -.04116 -.20378 -.06745 0 0 0 0 0 0 0 .17888 0

-.03452 -.03452 -.02735 -.04847 -.05526 -.03452 -.23548 -.23418 -.21822 -.18419 -.22554 -.23538
.243742 .599999 .435271 .288613 0 0 0 0 0 0 0 0 .17888

.27979 .305829 .312339 .26894 .24724 .182141 -.1759 -.09995 -.03486 -.02401 .041094 .051944 yA = -.0458 -.04146 -.33865 -.14335 0 0 0 0 0 0 0 0 0

311

312 MULTICOLLINEARITY

2 > >

Contours of constant residual sum of squares
R 

Figure 9.7 A geometrical interpretation of ridge regression.

conditions under which ridge regression leads to smaller MSE than least squares. The expected improvement depends on the orientation of the  vector relative to the eigenvectors of XX. The expected improvement is greatest when  coincides with the eigenvector associated with the largest eigenvalue of XX. Other interesting results appear in Lowerre [1974] and Mayer and Willke [1973].
Obenchain [1977] has shown that nonstochastically shrunken ridge estimators yield the same t and F statistics for testing hypotheses as does least squares. Thus, although ridge regression leads to biased point estimates, it does not generally require a new distribution theory. However, distributional properties are still unknown for stochastic choices of k. One would assume that when k is small, the usual normal-theory inference would be approximately applicable.
Relationship to Other Estimators Ridge regression is closely related to Bayesian Estimation. Generally, if prior information about  can be described by a pvariate normal distribution with mean vector 0 and covariance matrix V0, then the Bayes estimator of  is

b^ B

=



1 2

X



X

+

V0-1



-1



1 2

Xy +

V0-1b0 

The use of Bayesian methods in regression is discussed in Leamer [1973, 1978] and

Zellner [1971]. Two major drawbacks of this approach are that the data analyst must

make an explicit statement about the form of the prior distribution and the statisti-

cal theory is not widely understood. However, if we choose the prior mean 0 = 0

and

V0

=



2 0

I,

then

we

obtain

METHODS FOR DEALING WITH MULTICOLLINEARITY

313

b^B + (XX + kI)-1 Xy  b^R,

2k =  2



2 0

the usual ridge estimator. In effect, the method of least squares can be viewed as a Bayes estimator using an unbounded uniform prior distribution for . The ridge estimator results from a prior distribution that places weak boundedness conditions on . Also see Lindley and Smith [1972].

Methods for Choosing k Much of the controversy concerning ridge regression centers around the choice of the biasing parameter k. Choosing k by inspection of the ridge trace is a subjective procedure requiring judgment on the part of the analyst. Several authors have proposed procedures for choosing k that are more analytical. Hoerl, Kennard, and Baldwin [1975] have suggested that an appropriate choice for k is

k

=

p^ 2 b^ b^

(9.8)

where b^ and ^ 2 are found from the least-squares solution. They showed via simulation that the resulting ridge estimator had significant improvement in MSE over least squares. In a subsequent paper, Hoerl and Kennard [1976] proposed an iterative estimation procedure based on Eq. (11.8). McDonald and Galarneau [1975] suggest choosing k so that

 b^ R b^ R

=

b^ b^ -  2

p j=1

1   j

 

A drawback to this procedure is that k may be negative, Mallows [1973] suggested a graphical procedure for selecting k based on a modification of his Cp statistic. Another approach chooses k to minimize a modification of the PRESS statistic. Wahba, Golub, and Health [1979] suggest choosing k to minimize a cross-validation statistic.
There are many other possibilities for choosing k. For example, Marquardt [1970] has proposed using a value of k such that the maximum VIP is between 1 and 10, preferably closer to 1. Other methods of choosing k have been suggested by Dempster, Schatzoff, and Wermuth [1971], Goldstein and Smith [1974], Lawless and Wang [1976], Lindley and Smith [1972], and Obenchain [1975]. Hoerl and Kennard [1970a] proposed an extension of standard ridge regression that allows separate k's for each regression. This is called generalized ridge regression. There is no guarantee that these methods are superior to straightforward inspection of the ridge trace.

9.5.4 Principal-Component Regression
Biased estimators of regression coefficients can also be obtained by using a procedure known as principal-component regression. Consider the canonical form of the model,

314 MULTICOLLINEARITY

y = Za + e

where

Z = XT, a = Tb, TXXT = ZZ = L

Recall that  = diag(1, 2, . . . , p) is a p × p diagonal matrix of the eigenvalues of XX and T is a p × p orthogonal matrix whose columns are the eigenvectors associated with 1, 2, . . . , p. The columns of Z, which define a new set of orthogonal regressors, such as

Z = [Z1, Z2, ... , Zp ]

are referred to as principal components. The least-squares estimator of a^ is

a^ = (ZZ)-1 Zy = L-1Zy

and the covariance matrix of a^ is

Var (a^ ) =  2 (ZZ)-1 =  2 L-1

Thus, a small eigenvalue of XX means that the variance of the corresponding orthogonal regression coefficient will be large. Since

pp

  ZZ =

ZiZj = L

i=1 j=1

we often refer to the eigenvalue j as the variance of the jth principal component. If all the j are equal to unity, the original regressors are orthogonal, while if a j is exactly equal to zero, this implies a perfect linear relationship between the original regressors. One or more of the j near zero implies that multicollinearity is present. Note also that the covariance matrix of the standardized regression coefficients b^ is

( ) Var b^ = Var (Ta^ ) = TL-1T 2

( ) This implies that the variance of ^ j is ^ 2



p j=1

t

2 ji

i

. Therefore, the variance of ^ j

is a linear combination of the reciprocals of the eigenvalues. This demonstrates how

one or more small eigenvalues can destroy the precision of the least-squares estimate ^ j.
We have observed previously how the eigenvalues and eigenvectors of XX provide

specific information on the nature of the multicollinearity. Since Z = XT, we have

p
 Zi = tjiX j j=1

(9.9)

where Xj is the jth column of the X matrix and tji are the elements of the ith column of T (the ith eigenvector of XX). If the variance of the ith principal

METHODS FOR DEALING WITH MULTICOLLINEARITY

315

component (j) is small, this implies that Zi is nearly constant, and Eq. (9.9) indicates that there is a linear combination of the original regressors that is nearly constant. This is the definition of multicollinearity, that is, the tji are the constants in Eq. (9.1). Therefore, Eg. (9.9) explains why the elements of the eigenvector associated with a smaIl eigenvalue of XX identify the regressors involved in the multicollinearity.
The principal-component regression approach combats multicollinearity by using less than the full set of principal components in the model. To obtain the principalcomponent estimator, assume that the regressors are arranged in order of decreasing eigenvalues, 1  2  ···  p > 0. Suppose that the last s of these eigenvalues are approximately equal to zero. In principal-component regression the principal components corresponding to near-zero eigenvalues are removed from the analysis and least squares applied to the remaining components. That is,

a^ PC = Ba^
where bl = b2 = ··· = bp-s = 1 and bp-s+1 = bp-s+2 = ··· = bp = 0. Thus, the principalcomponent estimator is

 a^ 1 

 

a^ 2

 



a^ PC

=

a^

p-

s

 

p

-

s

components

 0  s components



0





 0 

or in terms of the standardized regressors

p-s
 b^PC = Ta^ PC = lj-1tj Xyt j j=1

(9.10)

A simulation study by Gunst and Mason [1977] showed that principal-component regression offers considerable improvement over least squares when the data are ill-conditioned. They also point out that another advantage of principal components is that exact distribution theory and variable selection procedures are available (see Mansfield, Webster, and Gunst [1977]). Some computer packages will perform principal-component regression.

Example 9.3 Principal-Component Regression for the Acetylene Data

We illustrate the use of principal-component regression for the acetylene data. We begin with the linear transformation Z = XT that transforms the original standardized regressors into an orthogonal set of variables (the principal components). The

316 MULTICOLLINEARITY

TABLE 9.10 Matrix T of Eigenvectors and Eigenvalnes j for the Acetylene Data

Eigenvectors

Eigenvalues j

.3387 .1324 -.4137 -.2191 .4493 .2524 -.4056 .0258 -.4667

.1057 .3391 -.0978 .5403 .0860 --.5172 -.0742 .5316 -.0969

.6495 -.0068 -.4696
.0897 -.2863 -.0570
.4404 -.2240
.1421

.0073 -.7243 -.0718
.3612 .1912 -.3447 -.2230 -.3417 -.1337

.1428 -5843 -.0182 -.1661 -.0943
.2007 .1443 .7342 -.0350

-.2488 .0205 .0160 .3733 .0333 .3232 .5393
-.0705 -.6299

-.2077 -.0102 -.1468 -.5885
.0575 -.6209
.3233 -.0057 -.3089

-.5436 -.0295 -.7172
.0909 .1543 .1280 .0565 .0761 .3631

.1768 -.0035
.2390 .0003 .7969 .0061 .4087 .0050 .3309

4.20480 2.16261 1.13839 1.04130 0.38453 0.04951 0.01363 0.00513 0.00010

eigenvalues j and the T matrix for the acetylene data are shown in Table 9.10. This matrix indicates that the relationship between z1 (for example) and the standardized regressors is
z1 = 0.3387x1 + 0.1324x2 - 0.4137x3 - 0.2191x1x2 + 0.4493x1x3 + 0.2524x2 x3 - 0.4056x12 + 0.0258x22 - 0.4667x32
The relationships between the remaining principal components z2, z3, . . . , z9 and the standardized regressors are determined similarly. Table 9.11 shows the elements of the Z matrix (sometimes called the principal-component scores).
The principal-component estimator reduces the effects of multicollinearity by using a subset of the principal components in the model. Since there are four small eigenvalues for the acetylene data, this implies that there are four principal components that should be deleted. We will exclude z6, z7, z8, and z9 and consider regressions involving only the first five principal components.
Suppose we consider a regression model involving only the first principal component, as in
y = 1z1 + 
The fitted model is
y^ = -0.35225z1
or a^ P C = [-0.35225, 0, 0, 0, 0, 0, 0, 0, 0]. The coefficients in terms of the standardized
regressors are found from b^PC = Ta^ PC. Panel A of Table 9.11 shows the resulting standardized regression coefficients as well as the regression coefficients in terms of the original centered regressors. Note that even though only one principal component is included, the model produces estimates for all nine standardized regression coefficients.
The results of adding the other principal components z2, z3, z4, and z5 to the model one at a time are displayed in panels B, C, D, and E, respectively, of Table 9.12. We see that using different numbers of principal components in the model produces

317

TABLE 9.11 Matrix Z = XT for the Acetylene Data

Observation

Z1

Z2

Z3

Z4 (= Zx1x2 )

1

.5415 -1.0347 1.0487

-.1880

2

.4846

-.8830 1.1638

-.0468

3

.4046

-.6129

.2914

.0676

4

.3388

-.1513 3.3176

.1315

5

.2353

.6905 1.2785

-.0089

6

.0310

2.7455

.9535

-.7783

7

.5940

-.0165 -1.0885

1.1554

8

.6385

-.2399 -.9170

1.0916

9

.7139

-.3558 -.7151

.8354

10

.7436

-.2228 -.6170

.5668

11

.7668

.1034 -.8626

-.0706

12

.8726

1.1054 -1.5272 -1.8442

13

-1.7109

.8164 -.3702

1.2052

14

-2.1618

.1860 -.1026

.5619

15

-1.6050

-.6784 -.2117

-.3325

16

-.8875 -1.4521 -.6417

-2.3461

Z5 (= Zx1x3 )
1.7389 .8909 -.0025 -.7526 -1.0842 .2235 1.5790 .3634 -.9374 -1.4297 -1.3472 .8129 .8885 -.1290 -.7456 -.0690

Z6 (= Zx2x3 )
-.6593 -.3874 -.1631
.3579 .6884 .2093 .1926 .4238 .3207 -.4038 -.3706 -.9285 1.9123 -2.5588 -.0658 1.4324

( ) Z7 = Zx12
.6492 .5067 .2187 .1269 -.4181 -1.1200 -1.3363 -1.2453 -.6525 .5657 1.5958 .8411 2.0708 -.3380 -.8259 -.6387

( ) Z8 = Zx22
.7822 .2045 -.0898 -1.2150 -1.2768 1.3128 -.4626 -.7138 .5144 2.5203 -.8815 -.8981 .2251 -.1080 -.4662 .5524

( ) Z9 = Zx32
.2402 -.1939 -1.6609
.9250 1.6754 -1.1453
.5964 -.3611 -.7716 1.4085 -1.3485
.7053 -.1036
.8652 -1.0012
.1699

318

TABLE 9.12 Principal Components Regression for the Acetylene Data

Principal Components in Model

A

B

C

D

E

Parameter

z1
Standarized Original Estimate Estimate

z1, z2

z1, z2, z3

Standardized Original Standardized Original Estimate Estimate Estimate Estimate

z1, z2, z3, z4
Standardized Original Estimate Estimate

z1, z2, z3, z4, z5
Standardized Original Estimate Estimate

0 1 2 3 12 13 23 11 22 33 R2
MSRes

.0000 .1193 .0466 -.1457 -.0772 .1583 .0889 -.1429 .0091 -.1644 .5217 .079713

42.1943 1.4194 .5530 -1.7327 -1.0369 2.0968 1.2627 -2.1429 .0968 -1.9033

.0000 .1188 .0450 -.1453 -.0798 .1578 .0914 -.1425 .0065 -.1639 .5218 .079705

42.2219 1.4141 .5346 -1.7281 -1.0738 2.0922 1.2950 -2.1383 .0691 -1.8986

.0000 .5087 .0409 -.4272 -.0260 -.0143 .0572 .1219 -.1280 -.0786 .9320 .011333

36.6275 6.0508 .4885 -5.0830 -.3502 -.1843 .8111 1.8295 -1.3779 -.9125

.0000 .5070 .2139 -.4100 -.1123 -.0597 .1396 .1751 -.0460 -.0467 .9914 .001427

34.6688 6.0324 2.5438 -4.8803 -1.5115 -.7926 1.9816 2.6268 -.4977 -.5392

.0000 .5056 .2195 -.4099 -.1107 -.0588 .1377 .1738 -.0533 -.0463 .9915 .00142

34.7517 6.0139 2.6129 4.8757 -1.4885 -.7788 1.9493 2.6083 -.5760 -.5346

METHODS FOR DEALING WITH MULTICOLLINEARITY

319

substantially different estimates of the regression coefficients. Furthermore, the

principal-component estimates differ considerably from the least-squares estimates

(e.g., see Table 9.8). However, the principal-component procedure with either four

or five components included results in coefficient estimates that do not differ dra-

matically from those produced by the other biased estimation metbods (refer to the

ordinary ridge regression estimates in Table 9.9. Principal-component analysis

shrinks the large least-squares estimates of 13 and 33 and changes the sign of the
original negative least-squares estimate of 11. The five-component model does not substantially degrade the fit to the original data as there has been little loss in R2

from the least-squares model. Thus, we conclude that the relationship based on the

first five principal components provides a more plausible model for the acetylene

data than was obtained via ordinary least squares.

Marquardt [1970] suggested a generalization of principal-component regression.

He felt that the assumption of an integral rank for the X matrix is too restrictive

and proposed a "fractional rank" estimator that allows the rank to be a piecewise

continuous function.

Hawkins [1973] and Webster et al. [1974] developed latent root procedures fol-

lowing the same philosophy as principal components. Gunst, Webster, and Mason

[1976] and Gunst and Masou [1977] indicate that latent root regression may provide

considerable improvement in mean square error over least squares. Gunst [1979]

points out that latent root regression can produce regression coefficients that are

very sinillar to those found by principal components, particularly when there are

only one or two strong multicollinearities in X.A number of large-sample properties

of latent root regression are in White and Gunst [1979].



9.5.5 Comparison and Evaluation of Biased Estimators
A number of Monte Carlo simulation studies have been conducted to examine the effectiveness of biased estimators and to attempt to determine which procedures perform best. For example, see McDonald and Galarneau [1975], Hoerl and Kennard [1976], Hoerl, Kennard, and Baldwin [1975] (who compare least squares and ridge), Gunst et al. [1976] (latent root versus least squares), Lawless [1978], Hemmerle and Brantle [1978] (ridge, generalized ridge, and least squares), Lawless and Wang [1976] (least squares, ridge, and principal components), Wichern and Churchill [1978], Gibbons [1979] (various forms of ridge), Gunst and Mason [1977] (ridge, principal components, latent root, and others), and Dempster et al. [1977]. The Dempster et al. [1977] study compared 57 different estimators for 160 different model configurations. While no single procedure emerges from these studies as best overall, there is considerable evidence indicating the superiority of biased estimation to least squares if multicollinearity is present. Our own preference in practice is for ordinary ridge regression with k selected by inspection of the ridge trace. The procedure is straightforward and easy to implement on a standard least-squares computer program, and the analyst can learn to interpret the ridge trace very quickly. It is also occasionally useful to find the "optimum" value of k suggested by Hoerl, Kennard, and Baldwin [1975] and the iteratively estimated "optimum" k of Hoed and Kennard [1976] and compare the resulting models with the one obtained via the ridge trace.

320 MULTICOLLINEARITY
The use of biased estimators in regression is not without controversy. Several authors have been critical of ridge regression and other related biased estimation techniques. Conniffe and Stone [1973, 1975] have criticized the use of the ridge trace to select the biasing parameter, since b^R will change slowly and eventually stabilize as k increases even for orthogonal regressors. They also claim that if the data are not adequate to support a least-squares analysis, then it is unlikely that ridge regression will be of any substantive help, since the parameter estimates will be nonsensical. Marquardt and Snee [1975] and Smith and Goldstein [1975] do not accept these conclusions and feel that biased estimators are a valuable tool for the data analyst confronted by ill-conditioned data. Several authors have noted that while we can prove that there exists a k such that the mean square error of the ridge estimator is always less than the mean square error of the least-squares estimator, there is no assurance that the ridge trace (or any other method that selects the biasing parameter stochastically by analysis of the data) produces the optimal k.
Draper and Van Nostrand [1977a, b, 1979] are also critical of biased estimators. They find fault with a number of the technical details of the simulation studies used as the basis of claims of improvement in MSE for biased estimation, suggesting that the simulations have been designed to favor the biased estimators. They note that ridge regression is really only appropriate in situations where external information is added to a least-squares problem. This may take the form of either the Bayesian formulation and interpretation of the procedure or a constrained least-squares problem in which the constraints on  are chosen to reflect the analyst's knowledge of the regression coefficients to "improve the conditioning" of the data.
Smith and Campbell [1980] suggest using explicit Bayesian analysis or mixed estimation to resolve multicollinearity· problems. They reject ridge methods as weak and imprecise because they only loosely incorporate prior beliefs and information into the analysis. When explicit prior information is known, then Bayesian or mixed estimation should certainly be used. However, often the prior information is not easily reduced to a specific prior distribution, and ridge regression methods offer a method to incorporate, at least approximately, this knowledge.
There has also been some controversy surrounding whether the regressors and the response should be centered and scaled so that XX and Xy are in correlation form. This results in an artificial removal of the intercept from the model. Effectively the intercept in the ridge model is estimated by y. Hoerl and Kennard [1970a, b] use this approach, as do Marquardt and Snee [1975], who note that centering tends to minimize any nonessential ill-conditioning when fitting polynomials. On the other hand, Brown [1977] feels that the variables should not be centered, as centering affects only the intercept estimate and not the slopes. Belsley, Kuh, and Welsch [1980] suggest not centering the regressors so that the role of the intercept in any near-linear dependences may be diagnosed. Centering and scaling allow the analyst to think of the parameter estimates as standardized regression coefficients, which is often intuitively appealing. Furthermore, centering the regressors can remove nonessential ill-conditioning, thereby reducing variance inflation in the parameter estimates. Consequently, we recommend both centering and scaling the data.
Despite the objections noted, we believe that biased estimation methods are useful techniques that the analyst should consider when dealing with multicollinearity. Biased estimation methods certainly compare very favorably to other methods

USING SAS TO PERFORM RIDGE AND PRINCIPAL-COMPONENT REGRESSION

321

for handling multicollinearity, such as variable elimination. As Marquardt and Snee [1975] note, it is often better to use some of the information in all of the regressors, as ridge regression does, than to use all of the information in some regressors and none of the information in others, as variable elimination does. Furthermore, variable elimination can be thought of as a form of biased estimation because subset regression models often produce biased estimates of the regression coefficients. In effect, variable elimination often shrinks the vector of parameter estimates, as does ridge regression. We do not recommend the mechanical or automatic use of ridge regression without thoughtful study of the data and careful analysis of the adequacy of the final model. Properly used, biased estimation methods are a valuable tool in the data analyst's kit.

9.6 USING SAS TO PERFORM RIDGE AND PRINCIPAL-COMPONENT REGRESSION
Table 9.14 gives the SAS code to perform ridge regression for the acetylene data. The lines immediately prior to the cards statement center and scale the linear terms. The other statements create the interaction and pure quadratic terms. The option
ridge = 0.006 to 0.04 by .002
on the first proc reg statement creates the series of k's to be used for the ridge trace. Typically, we would start the range of values for k at 0, which would yield the ordinary least-squares (OLS) estimates. Unfortunately, for the acetylene data the OLS estimates greatly distort the ridge trace plot to the point that it is very difficult to select a good choice for k. The statement
plot / ridgeplot nomodel;
creates the actual ridge trace. The option
ridge = .032
on the second proc reg statement fixes the value of k to 0.032. Table 9.15 gives the additional SAS code to perform principal-component regres-
sion. The statement
proc princomp data=acetylene out=pc_acetylene std,
sets up the principal-component analysis and creates an output data data set called
pc_acetylene.
The std option standardizes the principal-component scores to unit variance. The statement

322 MULTICOLLINEARITY
TABLE 9.14 SAS Code to Perform Ridge Regression for Acetylene Data
data acetylene; input conv t h c; t =(t - 1212.5) / 80.623; h =(h - 12.44) / 5.662; c =(c - 0.0403) / 0.03164; th = t*h; tc = t*c; hc = h*c ; t2 = t*t; h2 = h*h; c2 = c*c; cards; 49.0 1300 7.5 0.0120 50.2 1300 9.0 0.0120 50.5 1300 11.0 0.0115 48.5 1300 13.5 0.0130 47.5 1300 17.0 0.0135 44.5 1300 23.0 0.0120 28.0 1200 5.3 0.0400 31.5 1200 7.5 0.0380 34.5 1200 11.0 0.0320 35.0 1200 13.5 0.0260 38.0 1200 17.0 0.0340 38.5 1200 23.0 0.0410 15.0 1100 5.3 0.0840 17.0 1100 7.5 0.0980 20.5 1100 11.0 0.0920 29.5 1100 17.0 0.0860 proc reg outest = b ridge = 0.006 to 0.04 by .002; model conv = t h c t2 h2 c2 th tc hc / noprint; plot / ridgeplot nomodel; run; proc reg outest = b2 data = acetylene ridge =.032; model conv = t h c t2 h2 c2 th tc hc; run;proc print data = b2i run;
var t h c th tc hc t2 h2 c2;
specifies the specific variables from which to create the principal components. In this case, the variables are all of the regressors. The statement
ods select eigenvectors eigenvalues;
creates the eigenvectors and eigenvalues. The other two ods statements set up the output. This procedure creates the principal component, names them prinl, prin2, and so on, and stores them in the output data set, which in this example is

PROBLEMS 323
TABLE 9.15 SAS Code to Perform Principal-Component Regression for Acetylene Data proc princomp data = acetylene out = pc_acetylene std; var t h c th tc he t2 h2 c2; ods select eigenvectors eigenvalues; ods trace on; ods show; run; proc reg data = pc_acetylene; model conv = prinl prin2 prin3 prin4 prin5 prin6 prin7 prin8 prin9 / vif; run; proc reg data = pc_acetylene; model conv = prinl; run; proc reg data = pc_acetylene; model conv = prinl prin2; run;
pc_acetylene
The remainder of the code illustrates how to use proc reg with the principal components as the regressors. SAS does not automatically convert the resulting regression equation in the principal components back to the original variables. The analyst must perform this calculation using the appropriate eigenvectors.
PROBLEMS
9.1 Consider the soft drink delivery time data in Example 3.1. a. Find the simple correlation between cases (x1) an distance (x2). b. Find the variance inflation factors. c. Find the condition number of XX. Is there evidence of multicollinearity in these data?
9.2 Consider the Hald cement data in Table B.21. a. From the matrix of correlations between the regressors, would you suspect that multicollinearity is present? b. Calculate the variance inflation factors. c. Find the eigenvalues of XX. d. Find the condition number of XX.
9.3 Using the Hald cement data (Example 10.1), find the eigenvector associated with the smallest eigenvalue of XX. Interpret the elements of this vector. What can you say about the source of multicollinearity in these data?
9.4 Find the condition indices and the variance decomposition proportions for the Hald cement data (Table B.21), assuming centered regressors. What can you say about multicollinearity in these data?

324 MULTICOLLINEARITY
9.5 Repeat Problem 9.4 without centering the regressors and compare the results. Which approach do you think is better?
9.6 Use the regressors x2 (passing yardage), x7 (percentage of rushing plays), and x8 (opponents' yards rushing) for the National Football League data in Table B.1. a. Does the correlation matrix give any indication of multicollinearity? b. Calculate the variance inflation factors and the condition number of XX. Is there any evidence of multicollinearity?
9.7 Consider the gasoline mileage data in Table B.3. a. Does the correlation matrix give any indication of multicollinearity? b. Calculate the variance inflation factors and the condition number of XX. Is there any evidence of multicollinearity?
9.8 Using the gasoline mileage data in Table B.3 find the eigenvectors associated with the smallest eigenvalues of XX. Interpret the elements of these vectors. What can you say about the source of multicollinearity in these data?
9.9 Use the gasoline mileage data in Table B.3 and compute the condition indices and variance-decomposition proportions, with the regressors centered. What statements can you make about multicollinearity in these data?
9.10 Analyze the housing price data in Table B.4 for multicollinearity. Use the variance inflation factors and the condition number of XX.
9.11 Analyze the chemical process data in Table B.5 for evidence of multicollinearity. Use the variance inflation factors and the condition number of XX.
9.12 Analyze the patient satisfaction data in Table B.17 for multicollinearity.
9.13 Analyze the fuel consumption data in Table B.18 for multicollinearity.
9.14 Analyze the wine quality of young red wines data in Table B.19 for multicollinearity.
9.15 Analyze the methanol oxidation data in Table B.20 for multicollinearity.
9.16 The table below shows the condition indices and variance decomposition proportions for the acetylene data using centered regressors. Use this information to diagnose multicollinearity in the data and draw appropriate conclusions.

Condition

Number Eigenvalue Indices

T

Variance Decomposition Proportions

H

C TH TC HC T2 H2 C2

1

4.204797

1.00000o 0.0001 0.0024 0.0001 0.0004 0.0000 0.0004 0.0000 0.0001 0.0000

2

2.162611

1.394387 0.0000 0.0305 0.0000 0.0044 0.0000 0.0035 0.0000 0.0412 0.0000

3

1.138392

1.921882 0.0010 0.0000 0.0003 0.0002 0.0000 0.0001 0.0001 0.0139 0.0000

4

1.041305

2.009480 0.0000 0.2888 0.0000 0.0040 0.0000 0.0032 0.0000 0.0354 0.0000

5

0.384532

3.306788 0.0001 0.5090 0.0000 0.0023 0.0000 0.0029 0.0000 0.4425 0.0000

6

0.049510

9.215620 0.0034 0.0049 0.0000 0.0874 0.0000 0.0565 0.0033 0.0319 0.0071

7

0.013633

17.562062 0.0096 0.0051 0.0031 0.8218 0.0000 0.7922 0.0042 0.0001 0.0053

8

0.0051232 28.648589 0.1514 0.0936 0.1461 0.0773 0.0007 0.1210 0.0002 03526 0.0229

9

0.0000969 208.285 0.8343 0.0657 0.8504 0.0022 0.9993 0.0201 0.9920 0.0822 0.9646

PROBLEMS 325
9.17 Apply ridge regression to the Hald cement data in Table B.21. a. Use the ridge trace to select an appropriate value of k. Is the final model a good one? b. How much inflation in the residual sum of squares has resulted from the use of ridge regression? c. Compare the ridge· regression model with the two-regressor model involving x1 and x2 developed by all possible regressions in Example 9.1.
9.18 Use ridge regression on the Hald cement data (Table B.21) using the value of k in Eq. (9.8). Compare this value of k value selected by the ridge trace in Problem 9.17. Does the final model differ greatly from the one in Problem 9.17?
9.19 Estimate the parameters in a model for the gasoline mileage data in Table B.3 using ridge regression. a. Use the ridge trace to select an appropriate value of k. Is the resulting model adequate? b. How much inflation in the residual sum of squares has resulted from the use of ridge regression? c. How much reduction in R2 has resulted from the use of ridge regression?
9.20 Estimate the parameters in a model for the gasoline mileage data in Table B.3 using ridge regression with the value of k determined by Eq. (9.8). Does this model differ dramatically from the one developed in Problem 9.19?
9.21 Estimate model parameters for the Hald cement data (Table B.21) using principal-component regression. a. What is the loss in R2 for this model compared to least squares? b. How much shrinkage in the coefficient vector has resulted? c. Compare the principal-component model with the ordinary ridge model developed in Problem 9.17. Comment on any apparent differences in the models.
9.22 Estimate the model parameters for the gasoline mileage data using principalcomponent regression. a. How much has the residual sum of squares increased compared to least squares? b. How much shrinkage in the coefficient vector has resulted? c. Compare the principal-component and ordinary ridge models (Problem 9.19). Which model do you prefer?
9.23 Consider the air pollution and mortality data given in Table B.15. a. Is there a problem with collinearity? Discuss how you arrived at this decision. b. Perform a ridge trace on these data. c. Select a k based upon the ridge trace from part b. Which estimates of the coefficients do you prefer for these data, ridge or OLS? Justify your answer.

326 MULTICOLLINEARITY
d. Use principal-component regression to analyze these data. Discuss the principal-component regression results with the ridge regression and OLS results.
9.24 Show that the ridge estimator is the solution to the problem
( ) ( ) Minimize b - b^  XX b - b^ b subject to b b  d2

9.25 Pure Shrinkage Estimators (Stein [1960]). The pure shrinkage estimator is defined as b^s = cb^, were 0  c  1 is a constant chosen by the analyst. Describe the kind of shrinkage that this estimator introduces, and compare it with the shrinkage that results from ridge regression. Intuitively, which estimator seems preferable?
9.26 Show that the pure shrinkage estimator (Problem 9.25) is the solution to
( ) ( ) Minimize b - b^  b - b^ b
subject to b b  d2

9.27 The mean square error criterion for ridge regression is

( )   ( ) ( ) E L21

p
=
j=1

j j + k

2

+

p j=1



2 j

k

2

j + k 2

Try to find the value of k that minimizes E (L21 ). What difficulties are
encountered?

9.28 Consider the mean square error criterion for generalized ridge regression. Show that the mean square error is minimized by choosing kj =  2  2j , j = 1, 2, . . . , p.

9.29 Show that if XX is in correlation form,  is the diagonal matrix of eigenvalues
of XX, and T is the corresponding matrix of eigenvectors, then the variance inflation factors are the main diagonal elements of T-1T.

CHAPTER 10
VARIABLE SELECTION AND MODEL BUILDING
10.1 INTRODUCTION 10.1.1 Model-Building Problem In the preceding chapters we have assumed that the regressor variables included in the model are known to be important. Our focus was on techniques to ensure that the functional form of the model was correct and that the underlying assumptions were not violated. In some applications theoretical considerations or prior experience can be helpful in selecting the regressors to be used in the model.
In previous chapters, we have employed the classical approach to regression model selection, which assumes that we have a very good idea of the basic form of the model and that we know all (or nearly all) of the regressors that should be used. Our basic strategy is as follows:
1. Fit the full model (the model with all of the regressors under consideration). 2. Perform a thorough analysis of this model, including a full residual analysis.
Often, we should perform a thorough analysis to investigate possible collinearity. 3. Determine if transformations of the response or of some of the regressors are necessary. 4. Use the t tests on the individual regressors to edit the model. 5. Perform a thorough analysis of the edited model, especially a residual analysis, to determine the model's adequacy.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
327

328

VARIABLE SELECTION AND MODEL BUILDING

In most practical problems, especially those involving historical data, the analyst has a rather large pool of possible candidate regressors, of which only a few are likely to be important. Finding an appropriate subset of regressors for the model is often called the variable selection problem.
Good variable selection methods are very important in the presence of multicollinearity. Frankly, the most common corrective technique for multicollinearity is variable selection. Variable selection does not guarantee elimination of multicollinearity. There are cases where two or more regressors are highly related; yet, some subset of them really does belong in the model. Our variable selection methods help to justify the presence of these highly related regressors in the final model.
Multicollinearity is not the only reason to pursue variable selection techniques. Even mild relationships that our multicollinearity diagnostics do not flag as problematic can have an impact on model selection. The use of good model selection techniques increases our confidence in the final model or models recommended.
Building a regression model that includes only a subset of the available regressors involves two conflicting objectives. (1) We would like the model to include as many regressors as possible so that the information content in these factors can influence the predicted value of y. (2) We want the model to include as few regressors as possible because the variance of the prediction y^ increases as the number of regressors increases. Also the more regressors there are in a model, the greater the costs of data collection and model maintenance. The process of finding a model that is a compromise between these two objectives is called selecting the "best" regression equation. Unfortunately, as we will see in this chapter, there is no unique definition of "best." Furthermore, there are several algorithms that can be used for variable selection, and these procedures frequently specify different subsets of the candidate regressors as best.
The variable selection problem is often discussed in an idealized setting. It is usually assumed that the correct functional specification of the regressors is known (e.g., 1/x1, ln x2) and that no outliers or influential observations are present. In practice, these assumptions are rarely met. Residual analysis, such as described in Chapter 4, is useful in revealing functional forms for regressors that might be investigated, in pointing out new candidate regressors, and for identifying defects in the data such as outliers. The effect of influential or high-leverage observations should also be determined. Investigation of model adequacy is linked to the variable selection problem. Although ideally these problems should be solved simultaneously, an iterative approach is often employed, in which (1) a particular variable selection strategy is employed and then (2) the resulting subset model is checked for correct functional specification, outliers, and influential observations. This may indicate that step 1 must be repeated. Several iterations may be required to produce an adequate model.
None of the variable selection procedures described in this chapter are guaranteed to produce the best regression equation for a given data set. In fact, there usually is not a single best equation but rather several equally good ones. Because variable selection algorithms are heavily computer dependent, the analyst is sometimes tempted to place too much reliance on the results of a particular procedure. Such temptation is to be avoided. Experience, professional judgment in the subjectmatter field, and subjective considerations all enter into the variable selection problem. Variable selection procedures should be used by the analyst as methods

INTRODUCTION 329
to explore the structure of the data. Good general discussions of variable selection in regression include Cox and Snell [1974], Hocking [1972, 1976], Hocking and LaMotte [1973], Myers [1990], and Thompson [1978a, b].

10.1.2 Consequences of Model Misspecification
To provide motivation for variable selection we will briefly review the consequences of incorrect model specification. Assume that there are K candidate regressors x1, x2, . . . , xK and n  K + 1 observations on these regressors and the response y. The full model, containing all K regressors, is

or equivalently

K
 yi = 0 + j xij + i, i = 1, 2, ... , n j =1

(10.1a)

y = Xb + e

(10.1b)

We assume that the list of candidate regressors contains all the important variables. Note that Eq. (10.1) contains an intercept term 0. While 0 could also be a candidate for selection, it is typically forced into the model. We assume that all equations include an intercept term. Let r be the number of regressors that are deleted from Eq. (10.1). Then the number of variables that are retained is p = K + 1 - r. Since the intercept is included, the subset model contains p - 1 = K - r of the original regressors.
The model (10.1) may be written as

y = Xpbp + Xrbr + e

(10.2)

where the X matrix has been partitioned into Xp, an n × p matrix whose columns represent the intercept and the p - 1 regressors to be retained in the subset model, and Xr, an n × r matrix whose columns represent the regressors to be deleted from the full model. Let  be partitioned conformably into p and r. For the full model the least-squares estimate of  is

b^* = (XX)-1 Xy

(10.3)

and an estimate of the residual variance 2 is

^

2 *

=

yy - n-

b^ *Xy K -1

=

y

I

-

X (XX)-1
n- K -1

X

y

(10.4)

The

components

of

b^ *

are

denoted

by

b^

* p

and

b^ r*,

and

y^ i*

denotes

the

fitted

values.

For the subset model

y = Xpbp + e

(10.5)

330

VARIABLE SELECTION AND MODEL BUILDING

the least-squares estimate of p is
b^ p = (XpX p )-1 Xpy
the estimate of the residual variance is

(10.6)

^ 2 = yy - b^ p Xpy = y I - X p (Xp ) X p -1 Xp  y

n- p

n- p

(10.7)

and the fitted values are y^i. The properties of the estimates

b^ p

and

^ 2

from

the

subset

model

have

been

investigated by several authors, including Hocking [1974, 1976], Narula and Ramberg

[1972], Rao [1971], Rosenberg and Levy [1972], and Walls and Weeks [1969].

The results can be summarized as follows:

1. The expected value of b^ p is
( ) E b^ p = bp + (XpX p )-1 XpXrbr = bp + Abr

where A = (XpX p )-1 XpXr (A is sometimes called the alias matrix). Thus, b^ p is

a biased estimate of p unless the regression coefficients corresponding to the

deleted variables (r) are zero or the retained variables are orthogonal to the

deleted variables (XpXr = 0).
( ) ( ) 2. The variances of b^ p and b^ * are Var b^ p =  2 (XpX p )-1 and Var b^ * =  2 (XX)-1,

( ) ( ) respectively. Also the matrix Var

b^

* p

- Var

b^ p

is positive semidefinite, that is,

the variances of the least-squares estimates of the parameters in the full model

are greater than or equal to the variances of the corresponding parameters in

the subset model. Consequently, deleting variables never increases the vari-

ances of the estimates of the remaining parameters.

3.

Since

b^ p

is

a

biased

estimate

of

p

and

b^

* p

is

not,

it

is

more

reasonable

to

compare the precision of the parameter estimates from the full and subset

models in terms of mean square error. Recall that if ^ is an estimate of the

parameter , the mean square error of ^ is

MSE(^ ) = Var(^ ) + E (^ ) -  2

The mean square error of b^ p is

( ) MSE b^ p =  2 (XpX p )-1 + AbrbrA

( ) If the matrix Var b^r* - brbr is positive semidefinite, the matrix

( ) ( ) Var

b^

* p

- MSE b^ p

is positive semidefinite. This means that the least-squares

estimates of the parameters in the subset model have smaller mean square

error than the corresponding parameter estimates from the full model when

the deleted variables have regression coefficients that are smaller than the

standard errors of their estimates in the full model.

INTRODUCTION 331

4.

The

parameter

^

2 *

from

the

full

model

is

an

unbiased

estimate

of

 2. However,

for the subset model

( ) E ^ 2 =  2 + brXr I - X p (XpX p )-1 Xp  Xrbr
n- p

That is, ^ 2 is generally biased upward as an estimate of  2.

5.

Suppose we wish to predict the response the full model, the predicted value is y^* =

xatb^t*h, ewpitohinmt exan=x[xp,axnrd].pIrfewdiectuiosne

variance

Var (y^*) =  2 1 + x (XX)-1 x
However, if the subset model is used, y^ = xpb^ p with mean

E ( y^ ) = xpbp + xpAbr

and prediction mean square error
MSE( y^ ) =  2 1 + xp (XpX p )-1 x p  + (xpAbr - xrbr )2
Note that y^ is a biased estimate of y unless xpAbr = 0, which is only true in general if XpXrbr = 0. Furthermore, the variance of y^* from the full model is not less than the variance of y^ from the subset model. In terms of mean square error we can show that

Var (y^*)  MSE(y^ )
( ) provided that the matrix Var b^r* - brbr is positive semidefinite.

Our motivation for variable selection can be summarized as follows. By deleting variables from the model, we may improve the precision of the parameter estimates of the retained variables even though some of the deleted variables are not negligible. This is also true for the variance of a predicted response. Deleting variables potentially introduces bias into the estimates of the coefficients of retained variables and the response. However, if the deleted variables have small effects, the MSE of the biased estimates will be less than the variance of the unbiased estimates. That is, the amount of bias introduced is less than the reduction in the variance. There is danger in retaining negligible variables, that is, variables with zero coefficients or coefficients less than their corresponding standard errors from the full model. This danger is that the variances of the estimates of the parameters and the predicted response are increased.
Finally, remember from Section 1.2 that regression models are frequently built using retrospective data, that is, data that have been extracted from historical records. These data are often saturated with defects, including outliers, "wild" points, and inconsistencies resulting from changes in the organization's data collection and information-processing system over time. These data defects can have great impact

332

VARIABLE SELECTION AND MODEL BUILDING

on the variable selection process and lead to model misspecification.A very common problem in historical data is to find that some candidate regressors have been controlled so that they vary over a very limited range. These are often the most influential variables, and so they were tightly controlled to keep the response within acceptable limits. Yet because of the limited range of the data, the regressor may seem unimportant in the least-squares fit. This is a serious model misspecification that only the model builder's nonstatistical knowledge of the problem environment may prevent. When the range of variables thought to be important is tightly controlled, the analyst may have to collect new data specifically for the model-building effort. Designed experiments are helpful in this regard.

10.1.3 Criteria for Evaluating Subset Regression Models
Two key aspects of the variable selection problem are generating the subset models and deciding if one subset is better than another. In this section we discuss criteria for evaluating and comparing subset regression models. Section 10.2 will present computational methods for variable selection.

Coefficient of Multiple Determination A measure of the adequacy of a regres-
sion model that has been widely used is the coefficient of multiple determination, R2. Let Rp2 denote the coefficient of multiple determination for a subset regression model with p terms, that is, p - 1 regressors and an intercept term 0. Computationally,

Rp2

=

SSR ( p)
SST

=

1-

SSRes (
SST

p)

(10.8)

where SSR(p) and SSRes(p) denote the regression sum of squares and the residual

sum

of

squares,

respectively,

for

a

p-term

subset

model.

Note

that

there

are

 

K p - 1

values of Rp2 for each value of p, one for each possible subset model of size p. Now

Rp2 increases as p increases and is a maximum when p = K + 1. Therefore, the analyst

uses this criterion by adding regressors to the model up to the point where an addi-

tional variable is not useful in that it provides only a small increase in Rp2. The

general approach is illustrated in Figure 10.1, which presents a hypothetical plot of

the maximum value of Rp2 for each subset of size p against p. Typically one examines

a display such as this and then specifies the number of regressors for the final model

as the point at which the "knee" in the curve becomes apparent. Clearly this requires

judgment on the part of the analyst.

Since we cannot find an "optimum" value of R2 for a subset regression

model, we must look for a "satisfactory" value. Aitkin [1974] has proposed one

solution to this problem by providing a test by which all subset regression models

that have an R2 not significantly different from the R2 for the full model can be

identified. Let

( ) R02 = 1 - 1 - RK2 +1 (1 + d,n,K )

(10.9)

1.0

R

2 p

INTRODUCTION 333

0.0

p

K+1

Figure 10.1 Plot of Rp2 versus p.

where

d ,n,K

=

KF ,K,n-K-1 n- K -1

and RK2 +1 is the value of R2 for the full model. Aitkin calls any subset of regressor variables producing an R2 greater than R20 an R2-adequate () subset.
Generally, it is not straightforward to use R2 as a criterion for choosing

the number of regressors to include in the model. However, for a fixed number of

variables

p,

Rp2

can

be

used

to

compare

the

K  p - 1

subset

models

so

generated.

Models having large values of Rp2 are preferred.

Adjusted R 2 To avoid the difficulties of interpreting R2, some analysts prefer to use the adjusted R2 statistic, defined for a p-term equation as

( ) R2 Adj, p

=

1

-

 

n- n-

1 p

1 - Rp2

(10.10)

The

R2 Adj, p

statistic

does

not

necessarily

increase

as

additional

regressors

are

intro-

duced into the model. In fact, it can be shown (Edwards [1969], Haitovski [1969],

and

Seber

[1977])

that

if

s

regressors

are

added

to

the

model,

R2 Adj, p+ s

will

exceed

RA2 dj,p if and only if the partial F statistic for testing the significance of the s additional

regressors exceeds 1. Consequently, one criterion for selection of an optimum subset

model is to choose the model that has a maximum RA2 dj,p. However, this is equivalent

to another criterion that we now present.

Residual Mean Square The residual mean square for a subset regression model, for example,

MSRes (

p)

=

SSRes ( p)
n- p

(10.11)

may be used as a model evaluation criterion. The general behavior of MSRes(p) as p increases is illustrated in Figure 10.2. Because SSRes(p) always decreases

334

VARIABLE SELECTION AND MODEL BUILDING

MSRes(p)

p
Figure 10.2 Plot of MSRes(p) versus p.

as p increases, MSRes(p) initially decreases, then stabilizes, and eventually may increase. The eventual increase in MSRes(p) occurs when the reduction in SSRes(p) from adding a regressor to the model is not sufficient to compensate for the loss of one degree of freedom in the denominator of Eq. (10.11). That is, adding a regressor to a p-term model will cause MSRes(p + 1) to be greater than MSRes(p) if the decrease in the residual sum of squares is less than MSRes(p). Advocates of the MSRes(p) criterion will plot MSRes(p) versus p and base the choice of p on the following:

1. The minimum MSRes(p) 2. The value of p such that MSRes(p) is approximately equal to MSRes for the full
model
3. A value of p near the point where the smallest MSRes(p) turns upward

The subset regression model that minimizes MSRes(p) will also maximize RA2 dj,p. To see this, note that

R2 Adj, p

=

1-

n- n-

1 (1
p

-

) Rp2

=

1

-

n- n-

1 p

SSRes ( p)
SST

=

1

-

MSRes ( p) SST (n - 1)

Thus, the criteria minimum MSRes(p) and maximum adjusted R2 are equivalent.

Mallows's Cp Statistic Mallows [1964, 1966, 1973, 1995] has proposed a criterion that is related to the mean square error of a fitted value, that is,

E[y^i - E (yi )]2 = [E (yi ) - E (y^i )]2 + Var (y^i )

(10.12)

Note that E(yi) is the expected response from the true regression equation and E (y^i ) is the expected response from the p-term subset model. Thus, E (yi ) - E (y^i ) is the
bias at the ith data point. Consequently, the two terms on the right-hand side of Eq.
(10.12) are the squared bias and variance components, respectively, of the mean
square error. Let the total squared bias for a p-term equation be

n
 SSB( p) = [E (yi ) - E (y^i )]2 i=1

INTRODUCTION 335

and define the standardized total mean square error as

  p

=

1 2

  

n i=1

[E (yi ) - E (y^i )]2

+

n i=1

Var (y^i )


 =

SSB (
2

p)

+

1 2

n
Var (y^i )
i=1

It can be shown that

(10.13)

n
 Var (y^i ) = p 2
i=1
and that the expected value of the residual sum of squares from a p-term equation is

E[SSRes ( p)] = SSB( p) + (n - p) 2

Substituting for in=1 Var (y^i ) and SSB(p) in Eq. (10.13) gives

p

=

1 2

{E[SSRes (

p)] -

(n

-

p) 2

+

p

2}

=

E[SSRes (
2

p)]

-

n

+

2p

(10.14)

Suppose that ^ 2 is a good estimate of  2. Then replacing E[SSRes(p)] by the observed value SSRes(p) produces an estimate of p, say

Cp

=

SSRes (
^ 2

p)

-

n

+

2p

(10.15)

If the p-term model has negligible bias, then SSB(p) = 0. Consequently, E[SSRes(p)] = (n - p)2, and

E [C p

Bias

=

0]

=

(n

- p)
2

2

-

n

+

2

p

=

p

When using the Cp criterion, it can be helpful to visualize the plot of Cp as a function of p for each regression equation, such as shown in Figure 10.3. Regression equations with little bias will have values of Cp that fall near the line Cp = p (point A in Figure 10.3) while those equations with substantial bias will fall above this line
(point B in Figure 10.3). Generally, small values of Cp are desirable. For example, although point C in Figure 10.3 is above the line Cp = p, it is below point A and thus represents a model with lower total error. It may be preferable to accept some bias
in the equation to reduce the average error of prediction. To calculate Cp, we need an unbiased estimate of  2. Frequently we use the
residual mean square for the full equation for this purpose. However, this forces Cp = p = K + 1 for the full equation. Using MSRes(K + 1) from the full model as an estimate of  2 assumes that the full model has negligible bias. If the full model
has several regressors that do not contribute significantly to the model (zero regression coefficients), then MSRes(K + 1) will often overestimate  2, and consequently

336

VARIABLE SELECTION AND MODEL BUILDING

Cp = p B
A

Cp

C

012345678 p
Figure 10.3 A Cp plot.

the values of Cp will be small. If the Cp statistic is to work properly, a good estimate of  2 must be used. As an alternative to MSRes(K + 1), we could base our estimate of  2 on pairs of points that are "near neighbors" in x space, as illustrated in Section 4.5.2.
The Akaike Information Criterion and Bayesian Analogues (BICs) Akaike proposed an information criterion, AIC, based on maximizing the expected entropy of the model. Entropy is simply a measure of the expected information, in this case the Kullback-Leibler information measure. Essentially, the AIC is a penalized log-likelihood measure. Let L be the likelihood function for a specific model. The AIC is
AIC = -2 ln (L) + 2 p,
where p is the number of parameters in the model. In the case of ordinary least squares regression,

AIC

=

n

ln



SSRes n



+

2

p.

The key insight to the AIC is similar to RA2 dj and Mallows Cp. As we add regressors to the model, SSRes, cannot increase. The issue becomes whether the decrease in SSRes justifies the inclusion of the extra terms.
There are several Bayesian extensions of the AIC. Schwartz (1978) and Sawa (1978) are two of the more popular ones. Both are called BIC for Bayesian information criterion. As a result, it is important to check the fine print on the statistical software that one uses! The Schwartz criterion (BICSch) is

BICSch = -2 ln (L) + pln (n).

This criterion places a greater penalty on adding regressors as the sample size increases. For ordinary least squares regression, this criterion is

INTRODUCTION 337

BICSch

=

n ln 

SSRes n



+

p ln ( n).

R uses this criterion as its BIC. SAS uses the Sawa criterion, which involves a more complicated penalty term. This penalty term involves  2 and  4, which SAS estimates by MSRes from the full model.
The AIC and BIC criteria are gaining popularity. They are much more commonly used in the model selection procedures involving more complicated modeling situations than ordinary least squares, for example, the mixed model situation outlined in Section 5.6. These criteria are very commonly used with generalized linear models (Chapter 13).

Uses of Regression and Model Evaluation Criteria As we have seen, there are several criteria that can be used to evaluate subset regression models. The criterion that we use for model selection should certainly be related to the intended use of the model. There are several possible uses of regression, including (1) data description, (2) prediction and estimation, (3) parameter estimation, and (4) control.
If the objective is to obtain a good description of a given process or to model a complex system, a search for regression equations with small residual sums of squares is indicated. Since SSRes is minimized by using all K candidate regressors, we usually prefer to eliminate some variables if only a small increase in SSRes results. In general, we would like to describe the system with as few regressors as possible while simultaneously explaining the substantial portion of the variability in y.
Frequently, regression equations are used for prediction of future observations or estimation of the mean response. In general, we would like to select the regressors such that the mean square error of prediction is minimized. This usually implies that regressors with small effects should be deleted from the model. One could also use the PRESS statistic introduced in Chapter 4 to evaluate candidate equations produced by a subset generation procedure. Recall that for a p-term regression model

[ ]  n
PRESSp =
i=1

yi - y^(i)

2

=

n i=1



1

ei - hii



2

(10.16)

One then selects the subset regression model based on a small value of PRESSp. While PRESSp has intuitive appeal, particularly for the prediction problem, it is not a simple function of the residual sum of squares, and developing an algorithm for variable selection based on this criterion is not straightforward. This statistic is, however, potentially useful for discriminating between alternative models, as we will illustrate.
If we are interested in parameter estimation, then clearly we should consider both the bias that results from deleting variables and the variances of the estimated coefficients. When the regressors are highly multicollinear, the least-squares estimates of the individual regression coefficients may be extremely poor, as we saw in Chapter 9.
When a regression model is used for control, accurate estimates of the parameters are important.This implies that the standard errors of the regression coefficients should be small. Furthermore, since the adjustments made on the x's to control y

338

VARIABLE SELECTION AND MODEL BUILDING

will be proportional to the ^'s, the regression coefficients should closely represent the effects of the regressors. If the regressors are highly multicollinear, the ^'s may
be very poor estimates of the effects of individual regressors.

10.2 COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION
We have seen that it is desirable to consider regression models that employ a subset of the candidate regressor variables. To find the subset of variables to use in the final equation, it is natural to consider fitting models with various combinations of the candidate regressors. In this section we will discuss several computational techniques for generating subset regression models and illustrate criteria for evaluation of these models.
10.2.1 All Possible Regressions
This procedure requires that the analyst fit all the regression equations involving one candidate regressor, two candidate regressors, and so on. These equations are evaluated according to some suitable criterion and the "best" regression model selected. If we assume that the intercept term 0 is included in all equations, then if there are K candidate regressors, there are 2K total equations to be estimated and examined. For example, if K = 4, then there are 24 = 16 possible equations, while if K = 10, there are 210 = 1024 possible regression equations. Clearly the number of equations to be examined increases rapidly as the number of candidate regressors increases. Prior to the development of efficient computer codes, generating all possible regressions was impractical for problems involving more than a few regressors. The availability of high-speed computers has motivated the development of several very efficient algorithms for all possible regressions. We illustrate Minitab and SAS in this chapter. The R function leaps() in the leaps directory performs an all possible regressions methodology.
Example 10.1 The Hald Cement Data
Hald [1952] presents data concerning the heat evolved in calories per gram of cement (y) as a function of the amount of each of four ingredients in the mix: tricalcium aluminate (x1), tricalcium silicate (x2), tetracalcium alumino ferrite (x3), and dicalcium silicate (x4). The data are shown in Appendix Table B.21. These reflect quite serious problems with multicollinearity. The VIFs are:
x1: 38.496 x2: 254.423 x3: 46.868 x4: 282.513
We will use these data to illustrate the all-possible-regressions approach to variable selection.

These are "classical" data for illustrating the problems inherent in variable selection. For other analysis, see Daniel and Wood [1980], Draper and Smith [1998], and Seber [1977].

COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION

339

TABLE 10.1 Summary of All Possible Regressions for the Hald Cement Data

Number of

Regressors

Regressors

in Model

p in Model

SSRes(p)

Rp2

R2 Adj, p

MSRes(p)

None 1 1 1 1 2 2 2 2 2 2 3 3 3 3 4

1

None

2715.7635 0

0

226.3136

2

x1

1265.6867 0.53395 0.49158 115.0624

2

x2

906.3363 0.66627 0.63593 82.3942

2

x3

1939.4005 0.28587 0.22095 176.3092

2

x4

883.8669 0.67459 0.64495 80.3515

3

x1x2

57.9045 0.97868 0.97441

5.7904

3

x1x3

1227.0721 0.54817 0.45780 122.7073

3

x1x4

74.7621 0.97247 0.96697

7.4762

3

x2x3

415.4427 0.84703 0.81644 41.5443

3

x2x4

868.8801 0.68006 0.61607 86.8880

3

x3x4

175.7380 0.93529 0.92235 17.5738

4

x1x2x3

48.1106 0.98228 0.97638

5.3456

4

x1x2x4

47.9727 0.98234 0.97645

5.3303

4

x1x3x4

50.8361 0.98128 0.97504

5.6485

4

x2x3x4

73.8145 0.97282 0.96376

8.2017

5

x1x2x3x4

47.8636 0.98238 0.97356

5.9829

Cp
442.92 202.55 142.49 315.16 138.73
2.68 198.10
5.50 62.44 138.23 22.37 3.04 3.02 3.50 7.34 5.00

TABLE 10.2 Least-Squares Estimates for All Possible Regressions (Hald Cement Data)

Variables in Model

^ 0

^1

^ 2

^ 3

^ 4

x1 x2 x3 x4 x1x2 x1x3 x1x4 x2x3 x2x4 x3x4 x1x2x3 x1x2x4 x2x3x4 x1x3x4 x1x2x3x4

81.479 57.424 110.203 117.568 52.577 72.349 103.097 72.075 94.160 131.282 48.194 71.648 203.642 111.684 62.405

1.869
1.468 2.312 1.440
1.696 1.452 1.052 1.551

0.789
0.662
0.731 0.311 0.657 0.416 -0.923 0.510

-1.256
0.494
-1.008
-1.200 0.250
-1.448 -0.410 0.102

-0.738
-0.614
-0.457 -0.724
-0.237 -1.557 -0.643 -0.144

Since there are K = 4 candidate regressors, there are 24 = 16 possible regression equations if we always include the intercept 0. The results of fitting these 16 equations are displayed in Table 10.1. The Rp2, RA2 dj,p, MSRes(p), and Cp statistics are also given in this table.
Table 10.2 displays the least-squares estimates of the regression coefficients. The
partial nature of regression coefficients is readily apparent from examination of this

340

VARIABLE SELECTION AND MODEL BUILDING

1.00
0.95 R2p
0.90

xx11xx24 x3x4

x1x2x3 x1x2x4 x1x3x4
x2x3x4

x1x2x3x4

0.85

x2x3

0.80

x4 0.6745 x2x4 0.6801

x2 x1

0.6663 0.5339

x1x3

0.5482

x3 0.2859

1

2

3

4

5

p

Figure 10.4 Plot of Rp2 versus p, Example 10.1.

table. For example, consider x2. When the model contains only x2, the least-squares estimate of the x2 effect is 0.789. If x4 is added to the model, the x2 effect is 0.311, a reduction of over 50%. Further addition of x3 changes the x2 effect to -0.923. Clearly the least-squares estimate of an individual regression coefficient depends heavily
on the other regressors in the model. The large changes in the regression coefficients
observed in the Hald cement data are consistent with a serious problem with
multicollinearity. Consider evaluating the subset models by the Rp2 criterion. A plot of Rp2 versus p
is shown in Figure 10.4. From examining this display it is clear that after two regressors are in the model, there is little to be gained in terms of R2 by introducing
additional variables. Both of the two-regressor models (x1, x2) and (x1, x4) have essentially the same R2 values, and in terms of this criterion, it would make little
difference which model is selected as the final regression equation. It may be prefer-
able to use (x1, x4) because x4 provides the best one-regressor model. From Eq. (10.9) we find that if we take  = 0.05,

( ) R20 = 1 -

1 - R52

 1

+

4F0.05,4,8 8



=

1

-

0.01762

1

+

4 ( 3.84)
8

 

=

0.94855

Therefore, any subset regression model for which Rp2 > R20 = 0.94855 is R2 adequate (0.05); that is, its R2 is not significantly different from RK2 +1. Clearly, several models

COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION

341

TABLE 10.3 Matrix of Simple Correlations for Hald's Data in Example 10.1

x1

x2

x3

x4

y

x1

1.0

x2

0.229

1.0

x3

-0.824

-0.139

1.0

x4

-0.245

-0.973

0.030

1.0

y

0.731

0.816

-0.535

-0.821

1.0

in Table 10.1 satisfy this criterion, and so the choice of the final model is still not clear.
It is instructive to examine the pairwise correlations between xi and xj and between xi and y. These simple correlations are shown in Table 10.3. Note that the pairs of regressors (xl, x3) and (x2, x4) are highly correlated, since

r13 = -0.824 and r24 = -0.973

Consequently, adding further regressors when x1 and x2 or when x1 and x4 are already in the model will be of little use since the information content in the excluded
regressors is essentially present in the regressors that are in the model. This correla-
tive structure is partially responsible for the large changes in the regression coeffi-
cients noted in Table 10.2.
A plot of MSRes(p) versus p is shown in Figure 10.5. The minimum residual mean square model is (x1, x2, x4), with MSRes(4) = 5.3303. Note that, as expected, the model that minimizes MSRes(p) also maximizes the adjusted R2. However, two of the other three-regressor models [(x1, x2, x3) and (x1, x3, x4)] and the two-regressor models [(x1, x2) and (x1, x4)] have comparable values of the residual mean square. If either (x1, x2) or (x1, x4) is in the model, there is little reduction in residual mean square by adding further regressors. The subset model (x1, x2) may be more appropriate than (x1, x4) because it has a smaller value of the residual mean square.
A Cp plot is shown in Figure 10.6. To illustrate the calculations, suppose we take ^ = 5.9829 (MSRes from the full model) and calculate C3 for the model (x1, x4). From Eq. (10.15) we find that

C3

=

SSRes (3)
^ 2

-

n

+

2p

=

74.7621 5.9829

- 13

+

2 (3)

=

5.50

From examination of this plot we find that there are four models that could be acceptable: (x1, x2), (x1, x2, x3), (x1, x2, x4), and (x1, x3, x4). Without considering additional factors such as technical information about the regressors or the costs of data collection, it may be appropriate to choose the simplest model (x1, x2) as the final model because it has the smallest Cp.
This example has illustrated the computational procedure associated with model building with all possible regressions. Note that there is no clear-cut choice of the best regression equation. Very often we find that different criteria suggest different equations. For example, the minimum Cp equation is (x1, x2) and the minimum MSRes

342

VARIABLE SELECTION AND MODEL BUILDING

226.3136

xx31

176.3092 115.0624

x4 80.3515

x2 82.3941

x1x3122.7073 x2x4 86.8880

x2x3 41.5443

20 x3x4
15

MSRes (p)

10

x2x3x4

x1x4

5

x1x2

x1x3x4 x1x2x3 x1x2x4

x1x2x3x4

0

0

1

2

3

4

5

p

Figure 10.5 Plot of MSRes(p) versus p, Example 10.1.

equation is (x1, x2, x4). All "final" candidate models should be subjected to the usual

tests for adequacy, including investigation of leverage points, influence, and multicol-

linearity. As an illustration, Table 10.4 examines the two models (x1, x2) and (x1, x2,

x4) with respect to PRESS and their variance inflation factors (VIFs). Both models

have very similar values of PRESS (roughly twice the residual sum of squares for

the minimum MSRes equation), and the R2 for prediction computed from PRESS is

similar for both models. However, x2 and x4 are highly multicollinear, as evidenced

by the larger variance inflation factors in (x1, x2, x4). Since both models have equiva-

lent PRESS statistics, we would recommend the model with (x1, x2) based on the

lack of multicollinearity in this model.



Efficient Generation of All Possible Regressions There are several algorithms potentially useful for generating all possible regressions. For example, see Furnival [1971], Furnival and Wilson [1974], Gartside [1965, 1971], Morgan and Tatar [1972], and Schatzoff, Tsao, and Fienberg [1968]. The basic idea underlying all these algorithms is to perform the calculations for the 2K possible subset models in such a way that sequential subset models differ by only one variable. This allows very efficient

COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION

343

6
5 Cp
4
3

442.92

None
315.16 202.55 142.49 138.73

x3 198.10 x1 138.73 x2 62.44 x4 22.37

x1x3 x2x4 x2x3 x3x4
7.34

x2x3x4

x1x4

x1x2

x1x3x4 xx11xx22xx34

x1x2x3x4

2

1

0

0

1

2

3

4

5

p

Figure 10.6 The Cp plot for Example 10.1.

numerical methods to be used in performing the calculations. These methods are

usually based on either Gauss­Jordan reduction or the sweep operator (see Beaton

[1964] or Seber [1977]). Some of these algorithms are available commercially. For

example, the Furnival and Wilson [1974] algorithm is an option in the MlNlTAB

and SAS computer programs.

A sample computer output for Minitab applied to the Hald cement data is

shown in Figure 10.7. This program allows the user to select the best subset regre-

ssion model of each size for 1  p  K + 1 and displays the Cp, Rp2, and MSRes(p)

criteria.

It

also

displays

the

values

of

the

Cp,

Rp2 ,

R2 Adj, p

and

S

=

MSRes ( p) statistics

for several (but not all) models for each value of p. The program has the capability

of identifying the m best (for m  5) subset regression models.

Current all-possible-regression procedures will very efficiently process up to

about 30 candidate regressors with computing times that are comparable to the

usual stepwise-type regression algorithms discussed in Section 10.2.2. Our experi-

ence indicates that problems with 30 or less candidate regressors can usually be

solved relatively easily with an all-possible-regressions approach.

344

VARIABLE SELECTION AND MODEL BUILDING

TABLE 10.4 Comparisons of Two Models for Hald's Cement Data

Observation i

y^ = 52.58 + 1.468x1 + 0.662x2a

ei

hii

[ei/(1 - hii)]2

1

-1.5740

0.25119

4.4184

2

-1.0491

0.26189

2.0202

3

-1.5147

0.11890

2.9553

4

-1.6585

0.24225

4.7905

5

-1.3925

0.08362

2.3091

6

4.0475

0.11512

20.9221

7

-1.3031

0.36180

4.1627

8

-2.0754

0.24119

7.4806

9

1.8245

0.17195

4.9404

10

1.3625

0.55002

9.1683

11

3.2643

0.18402

16.0037

12

0.8628

0.19666

1.1535

13

-2.8934

0.21420

13.5579

PRESS x1, x2 = 93.8827

a

R2 Prediction

=

0.9654,

VIF1

=

1.05,

VIF 2

=

1.06.

R b 2 Prediction

=

0.9684, VIF1

=

1.07,

VIF 2

=

18.78, VIF4

=

18.94.

y^ = 71.65 + 1.452x1 + 0.416x2 - 0.237x4b

ei

hii

[ei/(1 - hii)]2

0.0617 1.4327 -1.8910 -1.8016 0.2562 3.8982 -1.4287 -3.0919 1.2818 0.3539 2.0977 1.0556 -2.2247

0.52058

0.0166

0.27670

3.9235

0.13315

4.7588

0.24431

5.6837

0.35733

0.1589

0.11737

19.5061

0.36341

5.0369

0.34522

22.2977

0.20881

2.6247

0.65244

1.0368

0.32105

9.5458

0.20040

1.7428

0.25923

9.0194

PRESS x1, x2, x4 = 85.3516

Figure 10.7 Computer output (Minitab) for Furnival and Wilson all-possible-regression algorithm.
10.2.2 Stepwise Regression Methods Because evaluating all possible regressions can be burdensome computationally, various methods have been developed for evaluating only a small number of subset regression models by either adding or deleting regressors one at a time. These

COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION

345

methods are generally referred to as stepwise-type procedures. They can be classified into three broad categories: (1) forward selection, (2) backward elimination, and (3) stepwise regression, which is a popular combination of procedures 1 and 2. We now briefly describe and illustrate these procedures.

Forward Selection This procedure begins with the assumption that there are no regressors in the model other than the intercept.An effort is made to find an optimal subset by inserting regressors into the model one at a time. The first regressor selected for entry into the equation is the one that has the largest simple correlation with the response variable y. Suppose that this regressor is x1. This is also the regressor that will produce the largest value of the F statistic for testing significance of regression. This regressor is entered if the F statistic exceeds a preselected F value, say FIN (or F-to-enter). The second regressor chosen for entry is the one that now has the largest correlation with y after adjusting for the effect of the first regressor entered (x1) on y. We refer to these correlations as partial correlations. They are the simple correlations between the residuals from the regression y^ = ^0 + ^1x1 and the residuals from the regressions of the other candidate regressors on x1, say x^ j = ^ 0 j + ^ 1j x1, j = 2, 3, . . . , K.
Suppose that at step 2 the regressor with the highest partial correlation with y is x2. This implies that the largest partial F statistic is

F

=

SSR ( x2 x1 ) MSRes ( x1, x2 )

If this F value exceeds FIN, then x2 is added to the model. In general, at each step the regressor having the highest partial correlation with y (or equivalently the largest partial F statistic given the other regressors already in the model) is added to the model if its partial F statistic exceeds the preselected entry level FIN. The procedure terminates either when the partial F statistic at a particular step does not exceed FIN or when the last candidate regressor is added to the model.
Some computer packages report t statistics for entering or removing variables. This is a perfectly acceptable variation of the procedures we have described, because t2 2, = F ,1,.
We illustrate the stepwise procedure in Minitab. SAS and the R function step () in the mass directory also perform this procedure.

Example 10.2 Forward Selection--Hald Cement Data
We will apply the forward selection procedure to the Hald cement data given in Example 10.1. Figure 10.8 shows the results obtained when a particular computer program, the Minitab forward selection algorithm, was applied to these data. In this program the user specifies the cutoff value for entering variables by choosing a type I error rate . Furthermore, Minitab uses the t statistics for decision making regarding variable selection, so the variable with the largest partial correlation with y is added to the model if |t| > t /2, . In this example we will use  = 0.25, the default value in Minitab.

346

VARIABLE SELECTION AND MODEL BUILDING

Figure 10.8 Forward selection results from Minitab for the Hald cement data.

From Table 10.3, we see that the regressor most highly correlated with y is x4(r4y = -0.821), and since the t statistic associated with the model using x4 is t = 4.77 and t0.25/2,11 = 1.21, x4 is added to the equation. At step 2 the regressor having the largest partial correlation with y (or the largest t statistic given that x4 is in the model) is x1, and since the partial F statistic for this regressor is t = 10.40, which exceeds t0.25/2,10 = 1.22, x1 is added to the model. In the third step, x2 exhibits the highest partial correlation with y. The t statistic associated with this variable is 2.24, which is larger than t0.25/2,9 = 1.23, and so x2 is added to the model. At this point the only remaining candidate regressor is x3, for which the t statistic does not exceed the cutoff value t0.25/2,8 = 1.24, so the forward selection procedure terminates with

y^ = 71.6483 + 1.4519x1 + 0.4161x2 - 0.2365x4

as the final model.



Backward Elimination Forward selection begins with no regressors in the model and attempts to insert variables until a suitable model is obtained. Backward elimination attempts to find a good model by working in the opposite direction. That is, we begin with a model that includes all K candidate regressors. Then the partial F statistic (or equivalently, a t statistic) is computed for each regressor as if it were the last variable to enter the model. The smallest of these partial F (or t) statistics

COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION

347

is compared with a preselected value, FOUT (or tOUT), for example, and if the smallest partial F (or t), value is less than FOUT (or tOUT), that regressor is removed from the model. Now a regression model with K - 1 regressors is fit, the partial F (or t) statistics for this new model calculated, and the procedure repeated. The backward elimination algorithm terminates when the smallest partial F (or t) value is not less than the preselected cutoff value FOUT (or tOUT).
Backward elimination is often a very good variable selection procedure. It is particularly favored by analysts who like to see the effect of including all the candidate regressors, just so that nothing "obvious" will be missed.

Example 10.3 Backward Elimination--Hald Cement Data
We will illustrate backward elimination using the Hald cement data from Example 10.1. Figure 10.9 presents the results of using the Minitab version of backward elimination on those data. In this run we have selected the cutoff value by using  = 0.10, the default in Minitab. Minitab uses the t statistic for removing variables; thus, a regressor is dropped if the absolute value of its t statistic is less than tOUT = t0.1/2,n-p. Step 1 shows the results of fitting the full model. The smallest t value is 0.14, and it is associated with x3. Thus, since t = 0.14 < tOUT = t0.10/2,8 = 1.86, x3 is removed from the model. At step 2 in Figure 10.9, we see the results of fitting the

Figure 10.9 Backward selection results from Minitab for the Hald cement data.

348

VARIABLE SELECTION AND MODEL BUILDING

three-variable model involving (x1, x2, x4). The smallest tstatistic in this model, t = -1.37, is associated with x4. Since |t| = 1.37 < tOUT = t0.20/2,9 = 1.83, x4 is removed from the model. At step 3, we see the results of fitting the two-variable model involving (x1, x2). The smallest t statistic in this model is 12.41, associated with x1, and since this exceeds tOUT = t0.10/2,10 = 1.81, no further regressors can be removed from the model. Therefore, backward elimination terminates, yielding the final model

y^ = 52.5773 + 1.4683x1 + 0.6623x2

Note that this is a different model from that found by forward selection. Further-

more, it is the model tentatively identified as best by the all-possible-regressions

procedure.



Stepwise Regression The two procedures described above suggest a number of possible combinations. One of the most popular is the stepwise regression algorithm of Efroymson [1960]. Stepwise regression is a modification of forward selection in which at each step all regressors entered into the model previously are reassessed via their partial F (or t) statistics. A regressor added at an earlier step may now be redundant because of the relationships between it and regressors now in the equation. If the partial F (or t) statistic for a variable is less than FOUT (or tOUT), that variable is dropped from the model.
Stepwise regression requires two cutoff values, one for entering variables and one for removing them. Some analysts prefer to choose FIN (or tIN) = FOUT (or tOUT), although this is not necessary. Frequently we choose FIN (or tIN) > FOUT (or tOUT), making it relatively more difficult to add a regressor than to delete one.

Example 10.4 Stepwise Regression--Hald Cement Data

Figure 10.10 presents the results of using the Minitab stepwise regression algorithm on the Hald cement data. We have specified the  level for either adding or
removing a regressor as 0.15. At step 1, the procedure begins with no variables in the model and tries to add x4. Since the t statistic at this step exceeds tIN = t0.15/2,11 = 1.55, x4 is added to the model. At step 2, x1 is added to the model. If the t statistic value for x4 is less than tOUT = t0.15/2,10 = 1.56, x4 would be deleted. However, the t value for x4 at step 2 is -12.62, so x4 is retained. In step 3, the stepwise regression algorithm adds x2 to the model. Then the t statistics for x1 and x4 are compared to tOUT = t0.15/2,9 = 1.57. Since for x4 we find a t value of -1.37, and since |t| = 1.37 is less than tOUT = 1.57, x4 is deleted. Step 4 shows the results of removing x4 from the model. At this point the only remaining candidate regressor is x3, which cannot be added because its t value does not exceed tIN. Therefore, stepwise regression terminates with the model

y^ = 52.5773 + 1.4683x1 + 0.6623x2

This is the same equation identified by the all-possible-regressions and backward

elimination procedures.



COMPUTATIONAL TECHNIQUES FOR VARIABLE SELECTION

349

Figure 10.10 Stepwise selection results from Minitab for the Hald cement data.
General Comments on Stepwise-Type Procedures The stepwise regression algorithms described above have been criticized on various grounds, the most common being that none of the procedures generally guarantees that the best subset regression model of any size will be identified. Furthermore, since all the stepwisetype procedures terminate with one final equation, inexperienced analysts may conclude that they have found a model that is in some sense optimal. Part of the problem is that it is likely, not that there is one best subset model, but that there are several equally good ones.
The analyst should also keep in mind that the order in which the regressors enter or leave the model does not necessarily imply an order of importance to the regressors. It is not unusual to find that a regressor inserted into the model early in the procedure becomes negligible at a subsequent step. This is evident in the Hald cement data, for which forward selection chooses x4 as the first regressor to enter. However, when x2 is added at a subsequent step, x4 is no longer required because of the high intercorrelation between x2 and x4. This is in fact a general problem with the forward selection procedure. Once a regressor has been added, it cannot be removed at a later step.
Note that forward selection, backward elimination, and stepwise regression do not necessarily lead to the same choice of final model. The intercorrelation between the regressors affects the order of entry and removal. For example, using the Hald cement data, we found that the regressors selected by each procedure were as follows:

350

VARIABLE SELECTION AND MODEL BUILDING

Forward selection

x1 x2 x4

Backward elimination x1 x2

Stepwise regression x1 x2

Some users have recommended that all the procedures be applied in the hopes of either seeing some agreement or learning something about the structure of the data that might be overlooked by using only one selection procedure. Furthermore, there is not necessarily any agreement between any of the stepwise-type procedures and all possible regressions. However, Berk [1978] has noted that forward selection tends to agree with all possible regressions for small subset sizes but not for large ones, while backward elimination tends to agree with all possible regressions for large subset sizes but not for small ones.
For these reasons stepwise-type variable selection procedures should be used with caution. Our own preference is for the stepwise regression algorithm followed by backward elimination. The backward elimination algorithm is often less adversely affected by the correlative structure of the regressors than is forward selection (see Mantel [1970]).
Stopping Rules for Stepwise Procedures Choosing the cutoff values FIN (or tIN) and/or FOUT (or tOUT) in stepwise-type procedures can be thought of as specifying a stopping rule for these algorithms. Some computer programs allow the analyst to specify these numbers directly, while others require the choice of a type 1 error rate  to generate the cutoff values. However, because the partial F (or t) value examined at each stage is the maximum of several correlated partial F (or t) variables, thinking of  as a level of significance or type 1 error rate is misleading. Several authors (e.g., Draper, Guttman, and Kanemasa [1971] and Pope and Webster [1972]) have investigated this problem, and little progress has been made toward either finding conditions under which the "advertised" level of significance on the t or F statistic is meaningful or developing the exact distribution of the F (or t)-to-enter and F (or t)-to-remove statistics.
Some users prefer to choose relatively small values of FIN and FOUT (or the equivalent t statistics) so that several additional regressors that would ordinarily be rejected by more conservative F values may be investigated. In the extreme we may choose FIN and FOUT so that all regressors are entered by forward selection or removed by backward elimination revealing one subset model of each size for p = 2, 3, . . . , K + 1. These subset models may then be evaluated by criteria such as Cp or MSRes to determine the final model. We do not recommend this extreme strategy because the analyst may think that the subsets so determined are in some sense optimal when it is likely that the best subset model was overlooked. A very popular procedure is to set FIN = FOUT = 4, as this corresponds roughly to the upper 5% point of the F distribution. Still another possibility is to make several runs using different values for the cutoffs and observe the effect of the choice of criteria on the subsets obtained.
There have been several studies directed toward providing practical guidelines in the choice of stopping rules. Bendel and Afifi [1974] recommend  = 0.25 for forward selection. These are the defaults in Minitab. This would typically result in a numerical value of FIN of between 1.3 and 2. Kennedy and Bancroft [1971] also suggest  = 0.25 for forward selection and recommend  = 0.10 for backward elimi-

STRATEGY FOR VARIABLE SELECTION AND MODEL BUILDING

351

Fit the full model

Perform residual analysis

Do we

need a

Yes

transformation?

Transform data

Perform all possible regressions
No Select models for further analysis
Make recommendations Figure 10.11 Flowchart of the model-building process.
nation. The choice of values for the cutoffs is largely a matter of the personal preference of the analyst, and considerable latitude is often taken in this area.
10.3 STRATEGY FOR VARIABLE SELECTION AND MODEL BUILDING
Figure 10.11 summarizes a basic approach for variable selection and model building. The basic steps are as follows:
1. Fit the largest model possible to the data. 2. Perform a thorough analysis of this model. 3. Determine if a transformation of the response or of some of the regressors is
necessary. 4. Determine if all possible regressions are feasible.
· If all possible regressions are feasible, perform all possible regressions using such criteria as Mallow's Cp adjusted R2, and the PRESS statistic to rank the best subset models.

352

VARIABLE SELECTION AND MODEL BUILDING

· If all possible regressions are not feasible, use stepwise selection techniques to generate the largest model such that all possible regressions are feasible. Perform all possible regressions as outlined above.
5. Compare and contrast the best models recommended by each criterion. 6. Perform a thorough analysis of the "best" models (usually three to five models). 7. Explore the need for further transformations. 8. Discuss with the subject-matter experts the relative advantages and disadvan-
tages of the final set of models.

By now, we believe that the reader has a good idea of how to perform a thorough analysis of the full model. The primary reason for analyzing the full model is to get some idea of the "big picture." Important questions include the following:

· What regressors seem important? · Are there possible outliers? · Is there a need to transform the response? · Do any of the regressors need transformations?

It is crucial for the analyst to recognize that there are two basic reasons why one may need a transformation of the response:

· The analyst is using the wrong "scale" for the purpose. A prime example of this situation is gas mileage data. Most people find it easier to interpret the response as "miles per gallon"; however, the data are actually measured as "gallons per mile." For many engineering data, the proper scale involves a log transformation.
· There are significant outliers in the data, especially with regard to the fit by the full model. Outliers represent failures by the model to explain some of the responses. In some cases, the responses themselves are the problem, for example, when they are mismeasured at the time of data collection. In other cases, it is the model itself that creates the outlier. In these cases, dropping one of the unimportant regressors can actually clear up this problem.

We recommend the use of all possible regressions to identify subset models whenever it is feasible. With current computing power, all possible regressions is typically feasible for 20­30 candidate regressors, depending on the total size of the data set. It is important to keep in mind that all possible regressions suggests the best models purely in terms of whatever criteria the analyst chooses to use. Fortunately, there are several good criteria available, especially Mallow's Cp, adjusted R2, and the PRESS statistic. In general, the PRESS statistic tends to recommend smaller models than Mallow's Cp, which in turn tends to recommend smaller models than the adjusted R2. The analyst needs to reflect on the differences in the models in light of each criterion used. All possible regressions inherently leads to the recommendation of several candidate models, which better allows the subject-matter expert to bring his or her knowledge to bear on the problem. Unfortunately, not all statistical software packages support the all-possible-regressions approach.

STRATEGY FOR VARIABLE SELECTION AND MODEL BUILDING

353

The stepwise methods are fast, easy to implement, and readily available in many software packages. Unfortunately, these methods do not recommend subset models that are necessarily best with respect to any standard criterion. Furthermore, these methods, by their very nature, recommend a single, final equation that the unsophisticated user may incorrectly conclude is in some sense optimal.
We recommend a two-stage strategy when the number of candidate regressors is too large to employ the all-possible-regressions approach initially. The first stage uses stepwise methods to "screen" the candidate regressors, eliminating those that clearly have negligible effects. We then recommend using the all-possible-regressions approach to the reduced set of candidate regressors. The analyst should always use knowledge of the problem environment and common sense in evaluating candidate regressors. When confronted with a large list of candidate regressors, it is usually profitable to invest in some serious thinking before resorting to the computer. Often, we find that we can eliminate some regressors on the basis of logic or engineering sense.
A proper application of the all-possible-regressions approach should produce several (three to five) final candidate models. At this point, it is critical to perform thorough residual and other diagnostic analyses of each of these final models. In making the final evaluation of these models, we strongly suggest that the analyst ask the following questions:

1. Are the usual diagnostic checks for model adequacy satisfactory? For example, do the residual plots indicate unexplained structure or outliers or are there one or more high leverage points that may be controlling the fit? Do these plots suggest other possible transformation of the response or of some of the regressors?
2. Which equations appear most reasonable? Do the regressors in the best model make sense in light of the problem environment? Which models make the most sense from the subject-matter theory?
3. Which models are most usable for the intended purpose? For example, a model intended for prediction that contains a regressor that is unobservable at the time the prediction needs to be made is unusable. Another example is a model that includes a regressor whose cost of collecting is prohibitive.
4. Are the regression coefficients reasonable? In particular, are the signs and magnitudes of the coefficients realistic and the standard errors relatively small?
5. Is there still a problem with multicollinearity?

If these four questions are taken seriously and the answers strictly applied, in some (perhaps many) instances there will be no final satisfactory regression equation. For example, variable selection methods do not guarantee correcting all problems with multicollinearity and influence. Often, they do; however, there are situations where highly related regressors still make significant contributions to the model even though they are related. There are certain data points that always seem to be problematic.

354

VARIABLE SELECTION AND MODEL BUILDING

The analyst needs to evaluate all the trade-offs in making recommendations about the final model. Clearly, judgment and experience in the model's intended operation environment must guide the analyst as he/she makes decisions about the final recommended model.
Finally, some models that fit the data upon which they were developed very well may not predict new observations well. We recommend that the analyst assess the predictive ability of models by observing their performance on new data not used to build the model. If new data are not readily available, then the analyst should set aside some of the originally collected data (if practical) for this purpose. We discuss these issues in more detail in Chapter 11.

10.4 CASE STUDY: GORMAN AND TOMAN ASPHALT DATA USING SAS
Gorman and Toman (1966) present data concerning the rut depth of 31 asphalt pavements prepared under different conditions specified by five regressors. A sixth regressor is used as an indicator variable to separate the data into two sets of runs. The variables are as follows: y is the rut depth per million wheel passes, x1 is the viscosity of the asphalt, x2 is the percentage of asphalt in the surface course, x3 is the percentage of asphalt in the base course, x4 is the run, x5 is the percentage of fines in the surface course, and x6 is the percentage of voids in the surface course. It was decided to use the log of the viscosity as the regressor, instead of the actual viscosity, based upon consultation with a civil engineer familiar with this material. Viscosity is an example of a measurement that is usually more nearly linear when expressed on a log scale.
The run regressor is actually an indicator variable. In regression model building, indicator variables can often present unique challenges. In many cases the relationships between the response and the other regressors change depending on the specific level of the indicator. Readers familiar with experimental design will recognize the concept of interaction between the indicator variable and at least some of the other regressors. This interaction complicates the model-building process, the interpretation of the model, and the prediction of new (future) observations. In some cases, the variance of the response is very different at the different levels of the indicator variable, which further complicates model building and prediction.
An example helps us to see the possible complications brought about by an indicator variable. Consider a multinational wine-making firm that makes Cabernet Sauvignon in Australia, California, and France. This company wishes to model the quality of the wine as measured by its professional tasting staff according to the standard 100-point scale. Clearly, local soil and microclimate as well as the processing variables impact the taste of the wine. Some potential regressors, such as the age of the oak barrels used to age the wine, may behave similarly from region to region. Other possible regressors, such as the yeast used in the fermentation process, may behave radically differently across the regions. Consequently, there may be considerable variability in the ratings for the wines made from the three regions, and it may be quite difficult to find a single regression model that describes wine quality incorporating the indicator variables to model the three regions. This model would

CASE STUDY: GORMAN AND TOMAN ASPHALT DATA USING SAS

355

TABLE 10.5 Gorman and Toman Asphalt Data

Observation, i

yi

xi1

xi2

xi3

xi4

xi5

xi6

1

6.75

2.80

4.68

4.87

0

8.4

4.916

2

13.00

1.40

5.19

4.50

0

6.5

4.563

3

14.75

1.40

4.82

4.73

0

7.9

5.321

4

12.60

3.30

4.85

4.76

0

8.3

4.865

5

8.25

1.70

4.86

4.95

0

8.4

3.776

6

10.67

2.90

5.16

4.45

0

7.4

4.397

7

7.28

3.70

4.82

5.05

0

6.8

4.867

8

12.67

1.70

4.86

4.70

0

8.6

4.828

9

12.58

0.92

4.78

4.84

0

6.7

4.865

10

20.60

0.68

5.16

4.76

0

7.7

4.034

11

3.58

6.00

4.57

4.82

0

7.4

5.450

12

7.00

4.30

4.61

4.65

0

6.7

4.853

13

26.20

0.60

5.07

5.10

0

7.5

4.257

14

11.67

1.80

4.66

5.09

0

8.2

5.144

15

7.67

6.00

5.42

4.41

0

5.8

3.718

16

12.25

4.40

5.01

4.74

0

7.1

4.715

17

0.76

88.00

4.97

4.66

1

6.5

4.625

18

1.35

62.00

4.01

4.72

1

8.0

4.977

19

1.44

50.00

4.96

4.90

1

6.8

4.322

20

1.60

58.00

5.20

4.70

1

8.2

5.087

21

1.10

90.00

4.80

4.60

1

6.6

5.971

22

0.85

66.00

4.98

4.69

1

6.4

4.647

23

1.20

140.00

5.35

4.76

1

7.3

5.115

24

0.56

240.00

5.04

4.80

1

7.8

5.939

25

0.72

420.00

4.80

4.80

1

7.4

5.916

26

0.47

500.00

4.83

4.60

1

6.7

5.471

27

0.33

180.00

4.66

4.72

1

7.2

4.602

28

0.26

270.00

4.67

4.50

1

6.3

5.043

29

0.76

170.00

4.72

4.70

1

6.8

5.075

30

0.80

98.00

5.00

5.07

1

7.2

4.334

31

2.00

35.00

4.70

4.80

1

7.7

5.705

also be of minimal value in predicting wine quality for a Cabernet Sauvignon produced from grapes grown in Oregon. In some cases, the best thing to do is to build separate models for each level of the indicator variable.
Table 10.5 gives the asphalt data. Table 10.6 gives the appropriate SAS code to perform the initial analysis of the data. Table 10.7 gives the resulting SAS output. Figures 10.12­10.19 give the residual plots from Minitab.
We note that the overall F test indicates that at least one regressor is important. The R2 is 0.8060, which is good. The t tests on the individual coefficients indicate that only the log of the viscosity is important, which we will see later is misleading. The variance inflation factors indicate problems with log-visc and run. Figure 10.13 is the plot of the residuals versus the predicted values and indicates a major problem. This plot is consistant with the need for a log transformation of the response. We see a similar problem with Figure 10.14, which is the plot of the residuals versus the log of the viscosity. This plot is also interesting because it suggests that there may

356

VARIABLE SELECTION AND MODEL BUILDING

TABLE 10.6 Initial SAS Code for Untransformed Response

data asphalt;

input rut_depth viscosity surface base run fines voids; log_visc = log(viscosity);

cards;

6.75

2.80 4.68 4.87 0 8.4 4.916

13.00

1.40 5.19 4.50 0 6.5 4.563

14.75

1.40 4.82 4.73 0 7.9 5.321

12.60

3.30 4.85 4.76 0 8.3 4.865

8.25

1.70 4.86 4.95 0 8.4 3.776

10.67

2.90 5.16 4.45 0 7.4 4.397

7.28

3.70 4.82 5.05 0 6.8 4.867

12.67

1.70 4.86 4.70 0 8.6 4.828

12.58

0.92 4.78 4.84 0 6.7 4.865

20.60

0.68 5.16 4.76 0 7.7 4.034

3.58

6.00 4.57 4.82 0 7.4 5.450

7.00

4.30 4.61 4.65 0 6.7 4.853

26.20

0.60 5.07 5.10 0 7.5 4.257

11.67

1.80 4.66 5.09 0 8.2 5.144

7.67

6.00 5.42 4.41 0 5.8 3.718

12.25

4.40 5.01 4.74 0 7.1 4.715

0.76 88.00 4.97 4.66 1 6.5 4.625

1.35 62.00 4.01 4.72 1 8.0 4.977

1.44 50.00 4.96 4.90 1 6.8 4.322

1.60 58.00 5.20 4.70 1 8.2 5.087

1.10 90.00 4.80 4.60 1 6.6 5.971

0.85 66.00 4.98 4.69 1 6.4 4.647

1.20 140.00 5.35 4.76 1 7.3 5.115

0.56 240.00 5.04 4.80 1 7.8 5.939

0.72 420.00 4.80 4.80 1 7.4 5.916

0.47 500.00 4.83 4.60 1 6.7 5.471

0.33 180.00 4.66 4.72 1 7.2 4.602

0.26 270.00 4.67 4.50 1 6.3 5.043

0.76 170.00 4.72 4.70 1 6.8 5.075

0.80 98.00 5.00 5.07 1 7.2 4.334

2.00 35.00 4.70 4.80 1 7.7 5.705

proc reg; model rut_depth = log_visc surface base run fines voids / vif;

plot rstudent.*(predicted. log_visc surface base run fines voids);

plot npp.*rstudent.;

run;

be two distinct models: one for the low viscosity and another for the high viscosity. This point is reemphasized in Figure 10.17, which is the residuals versus run plot. It looks like the first run (run 0) involved all the low-viscosity material while the second run (run 1) involved the high-viscosity material.
The plot of residuals versus run reveals a distinct difference in the variability between these two runs. We leave the exploration of this issue as an exercise. The residual plots also indicate one possible outlier.

CASE STUDY: GORMAN AND TOMAN ASPHALT DATA USING SAS

357

TABLE 10.7 SAS Output for Initial Analysis of Asphalt Data

The REG Procedure

Model: MODEL1 Dependent Variable: rut_depth

Number of Observations Read

31

Number of Observations Used

31

Analysis of Variance

Source

Sum of DF Squares

Mean Square F Value Pr > F

Model

6 1101.41861 183.56977 16.62

Error

24 265.09983 11.04583

Corrected Total 30 1366.51844

<.0001

Root MSE Dependent Mean Coeff Var

3.32353 R-Square 0.8060 6.50710 Adj R-Sq 0.7575 51.07541
Parameter Estimates

Variable
Intercept log_visc surface base run fines voids

Parameter Standard

Variance

DF Estimate

Error t Value Pr > |t| Inflation

1 -14.95916 25.28809 -0.59

1 -3.15151 0.91945 -3.43

1

3.97057 2.49665 1.59

1

1.26314 3.97029 0.32

1

1.96548 3.64720 0.54

1

0.11644 1.01239 0.12

1

0.58926 1.32439 0.44

0.5597 0.0022 0.1248 0.7531 0.5949 0.9094 0.6604

0 10.86965
1.23253 1.33308 9.32334 1.47906 1.59128

Percent Deleted residual

99

95 90
80 70 60 50 40 30 20
10
5

1

­2 ­1

0

1

2

3

4

Deleted residual

Figure 10.12 Normal probability plot of the residuals for the asphalt data.

5 4

3

2

1

0

­1

­2

5

­5

0

5

10

15

Fitted value

Figure 10.13 Residuals versus the fitted values for the asphalt data.

358

VARIABLE SELECTION AND MODEL BUILDING

5

4

Deleted residual

3

2

1

0

­1

­2

0

1

2

3

4

5

6

7

Log_visc

Figure 10.14 Residuals versus the log of the viscosity for the asphalt data.

Deleted residual

5 4 3 2 1
0 ­1 ­2
4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 Surface
Figure 10.15 Residuals versus surface for the asphalt data.

Deleted residual

5 4 3 2 1
0 ­1 ­2
4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 Base
Figure 10.16 Residuals versus base for the asphalt data.

5

4

Deleted residual

3

2

1

0 ­1

­2

0.0

0.2

0.4

0.6

0.8

1.0

Run

Figure 10.17 Residuals versus run for the asphalt data.

5

4

Deleted residual

3

2

1

0 ­1

­2

6.0

6.5 7.0

7.5

Fines

8.0 8.5

Figure 10.18 Residuals versus fines for the asphalt data.

5

4

Deleted residual

3

2

1

0 ­1

­2

4.0

4.5

5.0

5.5

6.0

Voids

Figure 10.19 Residuals versus voids for the asphalt data.

CASE STUDY: GORMAN AND TOMAN ASPHALT DATA USING SAS

359

TABLE 10.8 SAS Code for Analyzing Transformed Response Using Full Model

data asphalt;

input rut_depth viscosity surface base run fines voids; log_rut = log(rut_depth); log_visc = log(viscosity);

cards;

6.75

2.80 4.68 4.87 0 8.4 4.916

13.00

1.40 5.19 4.50 0 6.5 4.563

14.75

1.40 4.82 4.73 0 7.9 5.321

12.60

3.30 4.85 4.76 0 8.3 4.865

8.25

1.70 4.86 4.95 0 8.4 3.776

10.67

2.90 5.16 4.45 0 7.4 4.397

7.28

3.70 4.82 5.05 0 6.8 4.867

12.67

1.70 4.86 4.70 0 8.6 4.828

12.58

0.92 4.78 4.84 0 6.7 4.865

20.60

0.68 5.16 4.76 0 7.7 4.034

3.58

6.00 4.57 4.82 0 7.4 5.450

7.00

4.30 4.61 4.65 0 6.7 4.853

26.20

0.60 5.07 5.10 0 7.5 4.257

11.67

1.80 4.66 5.09 0 8.2 5.144

7.67

6.00 5.42 4.41 0 5.8 3.718

12.25

4.40 5.01 4.74 0 7.1 4.715

0.76 88.00 4.97 4.66 1 6.5 4.625

1.35 62.00 4.01 4.72 1 8.0 4.977

1.44 50.00 4.96 4.90 1 6.8 4.322

1.60 58.00 5.20 4.70 1 8.2 5.087

1.10 90.00 4.80 4.60 1 6.6 5.971

0.85 66.00 4.98 4.69 1 6.4 4.647

1.20 140.00 5.35 4.76 1 7.3 5.115

0.56 240.00 5.04 4.80 1 7.8 5.939

0.72 420.00 4.80 4.80 1 7.4 5.916

0.47 500.00 4.83 4.60 1 6.7 5.471

0.33 180.00 4.66 4.72 1 7.2 4.602

0.26 270.00 4.67 4.50 1 6.3 5.043

0.76 170.00 4.72 4.70 1 6.8 5.075

0.80 98.00 5.00 5.07 1 7.2 4.334

2.00 35.00 4.70 4.80 1 7.7 5.705

proc reg; model log_rut = log_visc surface base run fines voids / vif;

plot rstudent.*(predicted. log_visc surface base run fines

voids);

plot npp.*rstudent.;

run;

Table 10.8 gives the SAS code to generate the analysis on the log of the rut depth data. Table 10.9 gives the resulting SAS output.
Once again, the overall F test indicates that at least one regressor is important. The R2 is very good. It is important to note that we cannot directly compare the R2 from the untransformed response to the R2 of the transformed response. However, the observed improvement in this case does support the use of the transformation. The t tests on the individual coefficients continue to suggest that the log of the

360

VARIABLE SELECTION AND MODEL BUILDING

TABLE 10.9 SAS Output for Transformed Response and Full Model

The REG Procedure

Model: MODEL1 Dependent Variable: log_rut

Number of Observations Read

31

Number of Observations Used

31

Analysis of Variance

Source

Sum of DF Squares

Model

6 56.34362

Error

24 2.28876

Corrected Total 30 58.63238

Mean Square
9.39060 0.09537

F Value Pr > F 98.47 <.0001

Root MSE Dependent Mean Coeff Var

0.30881 1.12251 27.51101

R-Square Adj R-Sq

0.9610 0.9512

Variable
Intercept log_visc surface base run fines voids

Parameter Estimates

Parameter Standard

Variance

DF Estimate Error t Value Pr > |t| Inflation

1 -1.23294 1 -0.55769 1 0.58358 1 -0.10337 1 -0.34005 1 0.09775
1 0.19885

2.34970 0.8543 0.23198 0.36891 0.33889 0.09407 0.12306

-0.52 -6.53
2.52 -0.28 -1.00 1.04
1.62

0.6046 <.0001 0.0190 0.7817 0.3257 0.3091 0.1192

10.86965 1.23253 1.33308 9.32334 1.47906 1.59128

Percent Deleted residual

99

95 90
80 70 60 50 40 30 20
10
5

1

­3 ­2

­1

0

1

2

3

Deleted residual

Figure 10.20 Normal probability plot of the residuals for the asphalt data after the log transformation.

3

2

1

0

­1

­2

­1

0

1

2

3

Fitted value

Figure 10.21 Residuals versus the fitted values for the asphalt data after the log transformation.

CASE STUDY: GORMAN AND TOMAN ASPHALT DATA USING SAS

361

3

3

Deleted residual

Deleted residual

2

2

1

1

0

0

­1

­1

­2

­2

0

1

2

34

5

6

7

Log_visc

4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 Surface

Figure 10.22 Residuals versus the log of the viscosity for the asphalt data after the log transformation.

Figure 10.23 Residuals versus surface for the asphalt data after the log transformation.

3

3

Deleted residual

Deleted residual

2

2

1

1

0

0

­1

­1

­2 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 Base
Figure 10.24 Residuals versus base for the asphalt data after the log transformation.

­2

0.0

0.2

0.4

0.6

0.8

1.0

Run

Figure 10.25 Residuals versus run for the asphalt data after the log transformation.

3

Deleted residual

2

1

0

­1

­2

6.0

6.5 7.0

7.5

8.0 8.5

Fines

Figure 10.26 Residuals versus fines for the asphalt data after the log transformation.

3

2

Deleted residual

1

0

­1

­2

4.0

4.5

5.0

5.5

6.0

Voids

Figure 10.27 Residuals versus voids for the asphalt data after the log transformation.

viscosity is important. In addition, surface also looks important. The regressor voids appear marginal. There are no changes in the variance inflation factors because we only transformed the response, the variance inflation factors depend only on the relationships among the regressors.
Figures 10.20­10.27 give the residual plots. The plots of residual versus predicted value and residual versus individual regressor look much better, again supporting the value of the transformation. Interestingly, the normal probability plot of the

362

VARIABLE SELECTION AND MODEL BUILDING

TABLE 10.10 SAS Code for All Possible Regressions of Asphalt Data
proc reg;
model log_rut = log_visc surface base run fines voids / selection= cp best = 10;
run; proc reg; model log_rut = log_visc surface base run
fines voids / selection= adjrsq best = 10; run; proc reg; model log_rut =log_visc surface base
run fines voids / selection= forward; run; proc reg; model log_rut = log_visc surface base
run fines voids / selection= backward; run; proc reg; model log_rut = log_visc surface base
run fines voids / selection= stepwise; run;

residuals actually looks a little worse. On the whole, we should feel comfortable using the log of the rut depth as the response. We shall restrict all further analysis to the transformed response.
Table 10.10 gives the SAS source code for the all-possible-regressions approach. Table 10.11 gives the annotated output.
Both the stepwise and backward selection techniques suggested the variables log of viscosity, surface, and voids. The forward selection techniques suggested the variables log of viscosity, surface, voids, run, and fines. Both of these models were in the top five models in terms of the Cp statistic.
We can obtain the PRESS statistic for a specific model by the following SAS model statement:
model log_rut=log_visc surface voids/p clm cli;
Table 10.12 summarizes the Cp, adjusted R2, and PRESS information for the best five models in terms of the Cp statistics. This table represents one of the very rare situations where a single model seems to dominate.
Table 10.13 gives the SAS code for analyzing the model that regresses log of rut depth against log of viscosity, surface, and voids. Table 10.14 gives the resulting SAS output. The overall F test is very strong. The R2 is 0.9579, which is quite high. All three of the regressors are important. We see no problems with multicollinearity as evidenced by the variance inflation factors. The residual plots, which we do not show, all look good. Observation 18 has the largest hat diagonal, R-student, and DFFITS value, which indicates that it is influential. The DFBETAS suggest that this observation impacts the intercept and the surface regressor. On the whole, we should feel comfortable recommending this model.

CASE STUDY: GORMAN AND TOMAN ASPHALT DATA USING SAS

363

TABLE 10.11 Annotated SAS Output for All Possible Regressions of Asphalt Data

The REG Procedure Model: MODEL1
Dependent Variable: log_rut

C(p) Selection Method

Number of Observations Read

31

Number of Observations Used

31

Number in Model

C(p)

R-Square Variables in Model

3

2.9066 0.9579 log_visc surface voids

4

4.0849 0.9592 log_visc surface run voids

4

4.2564 0.9589 log_visc surface fines voids

4

4.8783 0.9579 log_visc surface base voids

5

5.0785 0.9608 log_visc surface run fines voids

2

5.2093 0.9509 log_visc surface

3

5.6161 0.9535 log_visc surface fines

4

5.7381 0.9565 log_visc surface run fines

3

5.8902 0.9530 log_visc surface run

5

6.0069 0.9593 log_visc surface base fines voids

The REG Procedure Model: MODEL1
Dependent Variable: log_rut

Adjusted R-Square Selection Method

Number of Observations Read

31

Number of Observations Used

31

Number in Adjusted Model R-Square R-Square Variables in Model

3

0.9532 0.9579 log_visc surface voids

5

0.9530 0.9608 log_visc surface run fines voids

4

0.9529 0.9592 log_visc surface run voids

4

0.9526 0.9589 log_visc surface fines voids

4

0.9514 0.9579 log_visc surface base voids

6

0.9512 0.9610 log_visc surface base run fines

voids

5

0.9512 0.9593 log_visc surface base fines voids

5

0.9510 0.9592 log_visc surface base run voids

4

0.9498 0.9565 log_visc surface run fines

3

0.9483 0.9535 log_visc surface fines

364

VARIABLE SELECTION AND MODEL BUILDING

TABLE 10.12 Summary of the Best Models for Asphalt Data

Variables in Model

log visc

surface

voids

run

fines

Cp

Adjusted R2

X

X

X

2.9

0.9532

X

X

X

X

4.1

0.9529

X

X

X

X

4.3

0.9526

X

X

X

X

X

4.9

0.9530

X

X

5.1

0.9474

PRESS
3.75 4.16 3.91 4.24 3.66

TABLE 10.13 SAS Code for Recommended Model for Asphalt Data
data asphalt; input rut_depth viscosity surface base run fines voids; log_rut = log(rut_depth); log_visc = log(viscosity); cards;
6.75 2.80 4.68 4.87 0 8.4 4.916 13.00 1.40 5.19 4.50 0 6.5 4.563 14.75 1.40 4.82 4.73 0 7.9 5.321 12.60 3.30 4.85 4.76 0 8.3 4.865
8.25 1.70 4.86 4.95 0 8.4 3.776 10.67 2.90 5.16 4.45 0 7.4 4.397
7.28 3.70 4.82 5.05 0 6.8 4.867 12.67 1.70 4.86 4.70 0 8.6 4.828 12.58 0.92 4.78 4.84 0 6.7 4.865 20.60 0.68 5.16 4.76 0 7.7 4.034
3.58 6.00 4.57 4.82 0 7.4 5.450 7.00 4.30 4.61 4.65 0 6.7 4.853 26.20 0.60 5.07 5.10 0 7.5 4.257 11.67 1.80 4.66 5.09 0 8.2 5.144 7.67 6.00 5.42 4.41 0 5.8 3.718 12.25 4.40 5.01 4.74 0 7.1 4.715 0.76 88.00 4.97 4.66 1 6.5 4.625 1.35 62.00 4.01 4.72 1 8.0 4.977 1.44 50.00 4.96 4.90 1 6.8 4.322 1.60 58.00 5.20 4.70 1 8.2 5.087 1.10 90.00 4.80 4.60 1 6.6 5.971 0.85 66.00 4.98 4.69 1 6.4 4.647 1.20 140.00 5.35 4.76 1 7.3 5.115 0.56 240.00 5.04 4.80 1 7.8 5.939 0.72 420.00 4.80 4.80 1 7.4 5.916 0.47 500.00 4.83 4.60 1 6.7 5.471 0.33 180.00 4.66 4.72 1 7.2 4.602 0.26 270.00 4.67 4.50 1 6.3 5.043 0.76 170.00 4.72 4.70 1 6.8 5.075 0.80 98.00 5.00 5.07 1 7.2 4.334 2.00 35.00 4.70 4.80 1 7.7 5.705 proc reg; model log_rut = log_visc surface voids/influence vif; plot rstudent.*(predicted. log_visc surface voids); plot npp.*rstudent.; run;

TABLE 10.14 SAS Output for Analysis of Recommended Model The REG Procedure

Source Model Error Corrected Total Root MSE Dependent Mean Coeff Var
Variable Intercept log_visc surface voids

Obs Residual

1

-0.2070

2

-0.1966

3

-0.0503

Model: MODEL1 Dependent Variable: log_rut

Number of Observations Read

31

Number of Observations Used

31

Analysis of Variance

Sum of

DF

Squares

Mean Square

F Value

3

56.16180

18.72060

204.59

27

2.47059

0.09150

30

58.63238

Pr > F <.0001

0.30250 1.12251 26.94821

R-Square Adj R-Sq

0.9579 0.9532

Parameter

DF

Estimate

1

-1.02079

1

-0.64649

1

0.55547

1

0.24479

Parameter Estimates

Standard

Error

t Value

1.36430 0.02879

-0.75 -22.46

0.22044

2.52

0.11560

2.12

Pr > |t| 0.4608 <.0001 0.0180 0.0436

Variance Inflation
1.28617 1.15993 1.46344

Dependent Variable: log_rut

Output Statistics Hat Diag Cov

DFBETAS

RStudent -0.7058 -0.6826 -0.1764

H 0.0772 0.1114 0.1424

Ratio 1.1681 1.2189 1.3495

DFFITS -0.2041 -0.2417 -0.0719

Intercept -0.0769
0.1182 0.0200

log_visc 0.1167 0.1332 0.0574

surface
0.0960 -0.1428 -0.0056

voids -0.0261 -0.0446 -0.0489

(Continued)

365

366

TABLE 10.14 (Continued)

4

0.4414

5

-0.1499

1.5366 -0.5391

6

0.1340

0.4559

7

-0.0170

-0.0567

8

0.0216

0.0729

9

-0.3471

10

-0.0570

11

-0.4181

-1.2241 -0.1993 -1.5063

12

0.1610

0.5484

13

0.0980

0.3392

14

0.0101

0.0348

15

0.2957

1.1257

16

0.5471

17

-0.2519

1.9489 -0.8659

18

0.5433

2.6569

19

0.1014

20

-0.0179

21

-0.1027

22

-0.3369

0.3489
-0.0614 -0.3638 -1.1632

23

0.1740

24

-0.2692

0.6355 -1.0019

25

0.4828

1.8173

26

0.2613

27

-0.4457

28

-0.5355

0.9190 -1.6629 -1.9830

29

0.2025

30

-0.0765

0.6930 -0.2671

31

0.005194

0.0178

0.0528 0.1778 0.0839
0.0510 0.0723 0.1053 0.1394 0.1185 0.0827 0.1177 0.1125 0.2385 0.0496 0.0835 0.4405 0.1071 0.1100 0.1568 0.0711 0.1992 0.2108 0.1628 0.1216 0.1636 0.1166 0.0851 0.1348 0.1036

0.8671 1.3529 1.2297
1.2248 1.2526 1.0388 1.3431 0.9440 1.2107 1.2950 1.3101 1.2625 0.7093 1.1325 0.7952 1.2783 1.3060 1.3515 1.0220 1.3654 1.2664 0.8610 1.1651 0.9281 0.7494 1.1814 1.3295 1.2973

0.3626 -0.2507
0.1380
-0.0131 0.0203
-0.4199 -0.0802 -0.5522
0.1646
0.1239
0.0124
0.6300
0.4453 -0.2614
2.3574
0.1209 -0.0216 -0.1569 -0.3218
0.3169 -0.5178
0.8015
0.3420 -0.7354 -0.7204
0.2114 -0.1054
0.0060

Sum of Residuals Sum of Squared Residuals Predicted Residual SS (PRESS)

0.0080 -0.1569 -0.0421
-0.0016 -0.0003 -0.0402
0.0044 -0.0904
0.1004 -0.0066
0.0020 -0.1032 -0.1210 -0.0360
2.1151
0.0421
0.0157
0.0739 -0.0272 -0.2377
0.3663 -0.2915 -0.0501 -0.4870 -0.3287
0.0768 -0.0273 -0.0013

-0.2230 0.0084
-0.0402
0.0075 -0.0150
0.3364
0.0341
0.2813 -0.0596 -0.0744 -0.0089
0.1201 -0.1883 -0.1884
0.5423
0.0768 -0.0051
0.0072 -0.2132
0.1073 -0.0799
0.2740
0.2267 -0.5330 -0.5361
0.1467 -0.0770 -0.0011
0 2.47059 3.75152

-0.0230 0.0893
0.0710
0.0021 -0.0007
0.0760 -0.0232
0.2296 -0.1102
0.0264 -0.0041
0.3139
0.1606 -0.0208 -2.2282 -0.0081 -0.0166 -0.0317 -0.0420
0.2581 -0.2768
0.1280
0.0250
0.3814
0.3067 -0.0740 -0.0015 -0.0004

0.0828
0.1975 -0.0196
-0.0024 0.0053
-0.1167 0.0251
-0.2858 -0.0145 -0.0177
0.0051 -0.3428
0.0472
0.1372 -0.8813 -0.0906 -0.0066 -0.1260
0.1546
0.0801 -0.3662
0.4568
0.0588
0.4930
0.2484 -0.0525
0.0746
0.0044

PROBLEMS 367
PROBLEMS
10.1 Consider the National Football League data in Table B.1. a. Use the forward selection algorithm to select a subset regression model. b. Use the backward elimination algorithm to select a subset regression model. c. Use stepwise regression to select a subset regression model. d. Comment on the final model chosen by these three procedures.
10.2 Consider the National Football League data in Table B.1. Restricting your attention to regressors x1 (rushing yards), x2 (passing yards), x4 (field goal percentage), x7 (percent rushing), x8 (opponents' rushing yards), and x9 (opponents' passing yards), apply the all-possible-regressions procedure. Evaluate Rp2, Cp, and MSRes for each model. Which subset of regressors do you recommend?
10.3 In stepwise regression, we specify that FIN  FOUT (or tIN  tOUT). Justify this choice of cutoff values.
10.4 Consider the solar thermal energy test data in Table B.2. a. Use forward selection to specify a subset regression model. b. Use backward elimination to specify a subset regression model. c. Use stepwise regression to specify a subset regression model. d. Apply all possible regressions to the data. Evaluate Rp2, Cp, and MSRes for each model. Which subset model do you recommend? e. Compare and contrast the models produced by the variable selection strategies in parts a­d.
10.5 Consider the gasoline mileage performance data in Table B.3. a. Use the all-possible-regressions approach to find an appropriate regression model. b. Use stepwise regression to specify a subset regression model. Does this lead to the same model found in part a?
10.6 Consider the property valuation data found in Table B.4. a. Use the all-possible-regressions method to find the "best" set of regressors. b. Use stepwise regression to select a subset regression model. Does this model agree with the one found in part a?
10.7 Use stepwise regression with FIN = FOUT = 4.0 to find the "best" set of regressor variables for the Belle Ayr liquefaction data in Table B.5. Repeat the analysis with FIN = FOUT = 2.0. Are there any substantial differences in the models obtained?
10.8 Use the all-possible-regressions method to select a subset regression model for the Belle Ayr liquefaction data in Table B.5. Evaluate the subset models using the Cp criterion. Justify your choice of final model using the standard checks for model adequacy.

368

VARIABLE SELECTION AND MODEL BUILDING

10.9 Analyze the tube-flow reactor data in Table B.6 using all possible regressions. Evaluate the subset models using the Rp2, Cp, and MSRes criteria. Justify your choice of final model using the standard checks for model adequacy.

10.10

Analyze the air pollution and mortality data in Table B.15 using all possible regressions. Evaluate the subset models using the Rp2, Cp, and MSRes criteria. Justify your choice of the final model using the standard checks for model adequacy.
a. Use the all-possible-regressions approach to find the best subset model for rut depth. Use Cp as the criterion.
b. Repeat part a using MSRes as the criterion. Did you find the same model?
c. Use stepwise regression to find the best subset model. Did you find the same equation that you found in either part a or b above?

10.11 Consider the all-possible-regressions analysis of Hald's cement data in Example 10.1. If the objective is to develop a model to predict new observations, which equation would you recommend and why?

10.12 Consider the all-possible-regressions analysis of the National Football
League data in Problem 10.2. Identify the subset regression models that are R2 adequate (0.05).

10.13

Suppose that the full model is yi = 0 + 1xi1 + 2xi2 + i, i = 1, 2, . . . , n, where xi1 and xi2 have been coded so that S11 = S22 = 1. We will also consider fitting a subset model, say yi = 0 + 1xi1 + i. a. Let ^1* be the least-squares estimate of 1 from the full model. Show that
( ) Var ^1* =  2 (1 - r122 ), where r12 is the correlation between x1 and x2.
b. Let ^1 be the least-squares estimate of 1 from the subset model. Show
( ) that Var ^1 =  2. Is 1 estimated more precisely from the subset model
or from the full model?
( ) c. Show that E ^1 = 1 + r122. Under what circumstances is ^1 an unbiased
estimator of 1?
( ) d. Find the mean square error for the subset estimator ^1. Compare MSE ^1 ( ) with Var ^1* . Under what circumstances is ^1 a preferable estimator, with
respect to MSE?
You may find it helpful to reread Section 10.1.2.

10.14

Table B.11 presents data on the quality of Pinot Noir wine.
a. Build an appropriate regression model for quality y using the allpossible-regressions approach. Use Cp as the model selection criterion, and incorporate the region information by using indicator variables.
b. For the best two models in terms of Cp, investigate model adequacy by using residual plots. Is there any practical basis for selecting between these models?
c. Is there any difference between the two models in part b in terms of the PRESS statistic?

PROBLEMS 369

10.15 Use the wine quality data in Table B.11 to construct a regression model for quality using the stepwise regression approach. Compare this model to the one you found in Problem 10.4, part a.

10.16

Rework Problem 10.14, part a, but exclude the region information.
a. Comment on the difference in the models you have found. Is there indication that the region information substantially improves the model?
b. Calculate confidence intervals as mean quality for all points in the data set using the models from part a of this problem and Problem 10.14, part a. Based on this analysis, which model would you prefer?

10.17

Table B.12 presents data on a heat treating process used to carburize gears. The thickness of the carburized layer is a critical factor in overall reliability of this component. The response variable y = PITCH is the result of a carbon analysis on the gear pitch for a cross-sectioned part. Use all possible regressions and the Cp criterion to find an appropriate regression model for these data. Investigate model adequacy using residual plots.

10.18 Reconsider the heat treating data from Table B.12. Fit a model to the PITCH response using the variables

x1 = SOAKTIME × SOAKPCT and x2 = DIFFTIME × DIFFPCT

as regressors. How does this model compare to the one you found by the all-possible-regressions approach of Problem 10.17?

10.19 Repeat Problem 10.17 using the two cross-product variables defined in Problem 10.18 as additional candidate regressors. Comment on the model that you find.

10.20

Compare the models that you have found in Problems 10.17, 10.18, and 10.19 by calculating the confidence intervals on the mean of the response PITCH for all points in the original data set. Based on a comparison of these confidence intervals, which model would you prefer? Now calculate the PRESS statistic for these models. Which model would PRESS indicate is likely to be the best for predicting new observations on PITCH?

10.21

Table B.13 presents data on the thrust of a jet turbine engine and six candidate regressors. Use all possible regressions and the Cp criterion to find an appropriate regression model for these data. Investigate model adequacy using residual plots.

10.22

Reconsider the jet turbine engine thrust data in Table B.13. Use stepwise regression to find an appropriate regression model for these data. Investigate model adequacy using residual plots. Compare this model with the one found by the all-possible-regressions approach in Problem 10.21.

10.23 Compare the two best models that you have found in Problem 10.21 in terms of Cp by calculating the confidence intervals on the mean of the thrust response for all points in the original data set. Based on a comparison

370

VARIABLE SELECTION AND MODEL BUILDING

of these confidence intervals, which model would you prefer? Now calculate the PRESS statistic for these models. Which model would PRESS indicate is likely to be the best for predicting new observations on thrust?

10.24

Table B.14 presents data on the transient points of an electronic inverter. Use all possible regressions and the Cp criterion to find an appropriate regression model for these data. Investigate model adequacy using residual plots.

10.25

Reconsider the electronic inverter data in Table B.14. Use stepwise regression to find an appropriate regression model for these data. Investigate model adequacy using residual plots. Compare this model with the one found by the all-possible-regressions approach in Problem 10.24.

10.26

Compare the two best models that you have found in Problem 10.24 in terms of Cp by calculating the confidence intervals on the mean of the response for all points in the original data set. Based on a comparison of these confidence intervals, which model would you prefer? Now calculate the PRESS statistic for these models. Which model would PRESS indicate is likely to be the best for predicting new response observations?

10.27

Reconsider the electronic inverter data in Table B.14. In Problems 10.24 and 10.25, you built regression models for the data using different variable selection algorithms. Suppose that you now learn that the second observation was incorrectly recorded and should be ignored.
a. Fit a model to the modified data using all possible regressions, using Cp as the criterion. Compare this model to the model you found in Problem 10.24.
b. Use stepwise regression to find an appropriate model for the modified data. Compare this model to the one you found in Problem 10.25.
c. Calculate the confidence intervals as the mean response for all points in the modified data set. Compare these results with the confidence intervals from Problem 10.26. Discuss your findings.

10.28

Consider the electronic inverter data in Table B.14. Delete observation 2 from the original data. Electrical engineering theory suggests that we should define new variables as follows: y* = ln y, x1* = 1 x1 , x2* = x2 , x3* = 1 x3 , and x4* = x4 .
a. Find an appropriate subset regression model for these data using all possible regressions and the Cp criterion.
b. Plot the residuals versus y^* for this model. Comment on the plots.
c. Discuss how you could compare this model to the ones built using the original response and regressors in Problem 10.27.

10.29

Consider the Gorman and Toman asphalt data analyzed in Section 10.4. Recall that run is an indicator variable. a. Perform separate analyses of those data for run = 0 and run = 1. b. Compare and contrast the results of the two analyses from part a.

PROBLEMS 371

c. Compare and contrast the results of the two analyses from part a with the results of the analysis from Section 10.4.

10.30

Table B.15 presents data on air pollution and mortality. Use the all-possibleregressions selection on the air pollution data to find appropriate models for these data. Perform a thorough analysis of the best candidate models. Compare your results with stepwise regression. Thoroughly discuss your recommendations.

10.31

Use the all-possible-regressions selection on the patient satisfaction data in Table B.17. Perform a thorough analysis of the best candidate models. Compare your results with stepwise regression. Thoroughly discuss your recommendations.

10.32

Use the all-possible-regressions selection on the fuel consumption data in Table B.18. Perform a thorough analysis of the best candidate models. Compare your results with stepwise regression. Thoroughly discuss your recommendations.

10.33

Use the all-possible-regressions selection on the wine quality of young red wines data in Table B.19. Perform a thorough analysis of the best candidate models. Compare your results with stepwise regression. Thoroughly discuss your recommendations.

10.34

Use the all-possible-regressions selection on the methanol oxidation data in Table B.20. Perform a thorough analysis of the best candidate models. Compare your results with stepwise regression. Thoroughly discuss your recommendations.

CHAPTER 11
VALIDATION OF REGRESSION MODELS
11.1 INTRODUCTION
Regression models are used extensively for prediction or estimation, data description, parameter estimation, and control. Frequently the user of the regression model is a different individual from the model developer. Before the model is released to the user, some assessment of its validity should be made. We distinguish between model adequacy checking and model validation. Model adequacy checking includes residual analysis, testing for lack of fit, searching for high-leverage or overly influential observations, and other internal analyses that investigate the fit of the regression model to the available data. Model validation, however, is directed toward determining if the model will function successfully in its intended operating environment.
Since the fit of the model to the available data forms the basis for many of the techniques used in the model development process (such as variable selection), it is tempting to conclude that a model that fits the data well will also be successful in the final application. This is not necessarily so. For example, a model may have been developed primarily for predicting new observations. There is no assurance that the equation that provides the best fit to existing data will be a successful predictor. Influential factors that were unknown during the model-building stage may significantly affect the new observations, rendering the predictions almost useless. Furthermore, the correlative structure between the regressors may differ in the model-building and prediction data. This may result in poor predictive performance for the model. Proper validation of a model developed to predict new observations should involve testing the model in that environment before it is released to the user.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc. 372

VALIDATION TECHNIQUES

373

Another critical reason for validation is that the model developer often has little or no control over the model's final use. For example, a model may have been developed as an interpolation equation, but when the user discovers that it is successful in that respect, he or she will also extrapolate with it if the need arises, despite any warnings or cautions from the developer. Furthermore, if this extrapolation performs poorly, it is almost always the model developer and not the model user who is blamed for the failure. Regression model users will also frequently draw conclusions about the process being studied from the signs and magnitudes of the coefficients in their model, even though they have been cautioned about the hazards of interpreting partial regression coefficients. Model validation provides a measure of protection for both model developer and user.
Proper validation of a regression model should include a study of the coefficients to determine if their signs and magnitudes are reasonable. That is, can ^ j be reasonably interpreted as an estimate of the effect of xj? We should also investigate the stability of the regression coefficients. That is, are the ^ j obtained from a new sample likely to be similar to the current coefficients? Finally, validation requires that the model's prediction performance be investigated. Both interpolation and extrapolation modes should be considered.
This chapter will discuss and illustrate several techniques useful in validating regression models. Several references on the general subject of validation are Brown, Durbin, and Evans [1975], Geisser [1975], McCarthy [1976], Snee [1977], and Stone [1974]. Snee's paper is particularly recommended.

11.2 VALIDATION TECHNIQUES
Three types of procedures are useful for validating a regression model:
1. Analysis of the model coefficients and predicted values including comparisons with prior experience, physical theory, and other analytical models or simulation results
2. Collection of new (or fresh) data with which to investigate the model's predictive performance
3. Data splitting, that is, setting aside some of the original data and using these observations to investigate the model's predictive performance
The final intended use of the model often indicates the appropriate validation methodology. Thus, validation of a model intended for use as a predictive equation should concentrate on determining the model's prediction accuracy. However, because the developer often does not control the use of the model, we recommend that, whenever possible, all the validation techniques above be used. We will now discuss and illustrate these techniques. For some additional examples, see Snee [1977].
11.2.1 Analysis of Model Coefficients and Predicted Values
The coefficients in the final regression model should be studied to determine if they are stable and if their signs and magnitudes are reasonable. Previous experience, theoretical considerations, or an analytical model can often provide information

374

VALIDATION OF REGRESSION MODELS

concerning the direction and relative size of the effects of the regressors. The coefficients in the estimated model should be compared with this information. Coefficients with unexpected signs or that are too large in absolute value often indicate either an inappropriate model (missing or misspecified regressors) or poor estimates of the effects of the individual regressors. The variance inflation factors and the other multicollinearity diagnostics in Chapter 19 also are an important guide to the validity of the model. If any VIF exceeds 5 or 10, that particular coefficient is poorly estimated or unstable because of near-linear dependences among the regressors. When the data are collected across time, we can examine the stability of the coefficients by fitting the model on shorter time spans. For example, if we had several years of monthly data, we could build a model for each year. Hopefully, the coefficients for each year would be similar.
The predicted response values y^ can also provide a measure of model validity. Unrealistic predicted values such as negative predictions of a positive quantity or predictions that fall outside the anticipated range of the response, indicate poorly estimated coefficients or an incorrect model form. Predicted values inside and on the boundary of the regressor variable bull provide a measure of the model's interpolation performance. Predicted values outside this region are a measure of extrapolation performance.

Example 11.1 The Bald Cement Data

Consider the Hald cement data introduced in Example 10.1. We used all possible regressions to develop two possible models for these data, model 1,

and model 2,

y^ = 52.58 + 1.468x1 + 0.662x2 y^ = 71.65 + 1.452x1 + 0.416x2 - 0.237x4

Note that the regression coefficient for x1 is very similar in both models, although

the intercepts are very different and the coefficients of x2 are moderately different.

In

Table

10.5

we

calculated

the

values

of

the

PRESS

statistic,

R , 2 Prediction

and

the

VIFs

for both models. For model 1 both VIFs are very small, indicating no potential

problems with multicollinearity. However, for model 2, the VIFs associated with x2

and x4 exceed 10, indicating that moderate problems with multicollinearity are

present. Because multicollinearity often impacts the predictive performance of a

regression model, a reasonable initial validation effort would be to examine the

predicted values to see if anything unusual is apparent. Table 11.1 presents

the predicted values corresponding to each individual observation for both models.

The predicted values are virtually identical for both models, so there is little reason

to believe that either model is inappropriate based on this test of prediction perfor-

mance. However, this is only a relative simple test of model prediction performance,

not a study of how either model would perform if moderate extrapolation were

required. Based in this simple analysis of coefficients and predicted values, there

is little reason to doubt the validity of either model, but as noted in Example 10.1,

we would probably prefer model 1 because it has fewer parameters and smaller

VIFs.



VALIDATION TECHNIQUES

375

TABLE 11.1 Prediction Values for Two Models for Hald Cement Data

y

x1

x2

x3

x4

Model 1

78.5

7

26

6

60

80.074

74.3

1

29

15

52

73.251

104.3

11

56

8

20

105.815

87.6

11

31

8

47

89.258

95.9

7

52

6

33

97.293

109.2

11

55

9

22

105.152

102.7

3

71

17

6

104.002

72.5

1

31

22

44

74.575

93.1

2

54

18

22

91.275

115.9

21

47

4

26

114.538

83.8

1

40

23

34

80.536

113.3

11

66

9

12

112.437

109.4

10

68

8

12

112.293

Model 2
78.438 72.867 106.191 89.402 95.644 105.302 104.129 75.592 91.818 115.546 81.702 112.244 111.625

11.2.2 Collecting Fresh Data--Confirmation Runs
The most effective method of validating a regression model with respect to its prediction performance is to collect new data and directly compare the model predictions against them. If the model gives accurate predictions of new data, the user will have greater confidence in both the model and the model-building process. Sometimes these new observations are called confirmation runs. At least 15­20 new observations are desirable to give a reliable assessment of the model's prediction performance. In situations where two or more alternative regression models have been developed from the data, comparing the prediction performance of these models on new data may provide a basis for final model selection.
Example 11.2 The Delivery Time Data
Consider the delivery time data introduced in Example 3.1. We have previously developed a least-squares fit for these data. The objective of fitting the regression model is to predict new observations. We will investigate the validity of the leastsquares model by predicting the delivery time for fresh data.
Recall that the original 25 observations came from four cities: Austin, San Diego, Boston, and Minneapolis. Fifteen new observations from Austin, Boston, San Diego, and a fifth city, Louisville, are shown in Table 11.2, along with the corresponding predicted delivery times and prediction errors from the least-squares fit y^ = 2.3412 + 1.6159x1 + 0.0144x2 (columns 5 and 6). Note that this prediction data set consists of 11 observations from cities used in the original data collection process and 4 observations from a new city. This mix of old and new cities may provide some information on how well the two models predict at sites where the original data were collected and at new sites.
Column 6 of Table 11.2 shows the prediction errors for the least-squares model. The average prediction error is 0.4060, which is nearly zero, so that model seems to produce approximately unbiased predictions. There is only one relatively large prediction error, associated with the last observation from Louisville. Checking the original data reveals that this observation is an extrapolation point. Furthermore,

376

VALIDATION OF REGRESSION MODELS

TABLE 11.2 Prediction Data Set for the Delivery Time Example

(1)

(2)

(3)

(4)

Observation
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40

City
San Diego San Diego Boston Boston Boston Boston Boston Boston Austin Austin Austin Louisville Louisville Louisville Louisville

Cases, x1
22 7 15 5 6 6 10 4 1 3 12 10 7 8 32

Distance, x2
905 520 290 500 1000 225 775 212 144 126 655 420 150 360 1530

Observed Time, y
51.00 16.80 26.16 19.90 24.00 18.55 31.93 16.95
7.00 14.00 37.03 18.62 16.10 24.38 64.75

(5)

(6)

Least-Squares Fit

y^

y - y^

50.9230 21.1405 30.7557 17.6207 26.4366 15.2766 29.6602 11.8576
6.0307 9.0033 31.1640 24.5482 15.8125 20.4524 76.0820

0.0770 -4.3405 -4.5957
2.2793 -2.4366
3.2734 2.2698 5.0924 0.9693 4.9967 5.8660 -5.9282 0.2875 3.9276 -11.3320

this point is quite similar to point 9, which we know to be influential. From an overall perspective, these prediction errors increase our confidence in the usefulness of the model. Note that the prediction errors are generally larger than the residuals from the least-squares fit. This is easily seen by comparing the residual mean square

MSRes = 10.6239

from the fitted model and the average squared prediction error

40

 (yi - y^i )2

i = 26

= 332.2809 = 22.1521

15

15

from the new prediction data. Since MSRes (which may be thought of as the average variance of the residuals from the fit) is smaller than the average squared prediction error, the least-squares regression model does not predict new data as well as it fits the existing data. However, the degradation of performance is not severe, and so we conclude that the least-squares model is likely to be successful as a predictor. Note also that apart from the one extrapolation point the prediction errors from Louisville are not remarkably different from those experienced in the cities where the original data were collected. While the sample is small, this is an indication that the model may be portable. More extensive data collection at other sites would be helpful in verifying this conclusion.
It is also instructive to compare R2 from the least-squares fit (0.9596) to the percentage of variability in the new data explained by the model, say

VALIDATION TECHNIQUES

377

40

 (yi - y^i )2

 R2 Prediction

= 1-

i = 26 40
( yi

- y^i )2

= 1 - 332.2809 3206.2338

= 0.8964

i = 26

Once again, we see that the least-squares model does not predict new obser-

vations as well as it fits the original data. However, the "loss" in R2 for prediction

is slight.



Collecting new data has indicated that the least-squares fit for the delivery time data results in a reasonably good prediction equation. The interpolation parlor-mance of the model is likely to be better than when the model is used for extrapolation.

11.2.3 Data Splitting
In many situations, collecting new data for validation purposes is not possible. The data collection budget may already have been spent, the plant may have been converted to the production of other products or other equipment and resources needed for data collection may be unavailable. When these situations occur, a reasonable procedure is to split the available data into two parts, which Snee [1977] calls the estimation data and the prediction data. The estimation data are used to build the regression model, and the prediction data are then used to study the predictive ability of the model. Sometimes data splitting is called cross validation (see Mosteller and Tukey [1968] and Stone [1974]).
Data splitting may be done in several ways. For example, the PRESS statistic

[ ]  n
PRESS =
i=1

yi - y^(i)

2

=

n i=1

 

ei 1 - hii

 

2

(11.1)

is a form of data splitting. Recall that PRESS can be used in computing the R2-like statistic

R2 Prediction

=

1-

PRESS SST

that measures in an approximate sense how much of the variability in new observations the model might be expected to explain. To illustrate, recall that in Chapter 4 (Example 4.6) we calculated PRESS for the model fit to the original 25 observations on delivery time and found that PRESS = 457.4000. Therefore,

R2 Prediction

=

1-

PRESS SST

=

1-

457.4000 5784.5426

=

0.9209

Now for the least-squares fit R2 = 0.9596, so PRESS would indicate that the model is likely to be a very good predictor of new observations. Note that the R2 for

378

VALIDATION OF REGRESSION MODELS

prediction based on PRESS is very similar to the actual prediction performance observed for this model with new data in Example 11.2.
If the data are collected in a time sequence, then time may be used as the basis of data splitting. That is, a particular time period is identified, and all observations collected before this time period are used to form the estimation data set, while observations collected later than this time period form the prediction data set. Fitting the model to the estimation data and examining its prediction accuracy for the prediction data would be a reasonable validation procedure to determine how the model is likely to perform in the future. This type of validation procedure is relatively common practice in time series analysis for investigating the potential performance of a forecasting model (for some examples, see Montgomery, Johnson, and Gardiner [1990]). For examples involving regression models, see Cady and Allen [1972] and Draper and Smith [1998].
In addition to time, other characteristics of the data can often be used for data splitting. For example, consider the delivery time data from Example 3.1 and assume that we had the additional 15 observations in Table 11.2 also available. Since there are five cities represented in the sample, we could use the observations from San Diego, Boston, and Minneapolis (for example) as the estimation data and the observations from Austin and Louisville as the prediction data. This would give 29 observations for estimation and 11 observations for validation. In other problem situations, we may find that operators, batches of raw materials, units of test equipment, laboratories, and so forth, can be used to form the estimation and prediction data sets. In cases where no logical basis of data splitting exists, one could randomly assign observations to the estimation and prediction data sets. If random allocations are used, one could repeat the process several times so that different subsets of observations are used for model fitting.
A potential disadvantage to these somewhat arbitrary methods of data splitting is that there is often no assurance that the prediction data set "stresses" the model severely enough. For example, a random division of the data would not necessarily ensure that some of the points in the prediction data set are extrapolation points, and the validation effort would provide no information on how well the model is likely to extrapolate. Using several different randomly selected estimation-- prediction data sets would help solve this potential problem. In the absence of an obvious basis for data splitting, in some situations it might be helpful to have a formal procedure for choosing the estimation and prediction data sets.
Snee [1977] describes the DUPLEX algorithm for data splitting. He credits the development of the procedure to R. W. Kennard and notes that it is similar to the CADEX algorithm that Kennard and Stone [1969) proposed for design construction. The procedure utilizes the distance between all pairs of observations in the data set. The algorithm begins with a list of the n observations where the k regressors are standardized to unit length, that is,

zij

=

xij - xj S1jj 2

,

i = 1, 2, ... n,

j = 1, 2, ... , k

where Sjj = ( in=1 xij - xj )2 is the corrected sum of squares of the jth regressor. The
standardized regressors are then orthonormalized. This can be done by factoring the ZZ matrix as

VALIDATION TECHNIQUES

379

ZZ = TT

(11.2)

where T is a unique k × k upper triangular matrix. The elements of T can be found using the square root or Cholesky method (see Graybill [1976, pp. 231­236]). Then make the transformation

W = ZT-1

(11.3)

resulting in a new set of variables (the w's) that are orthogonal and have unit vari-

ance. This transformation makes the factor space more spherical.

Using

the

orthonormalized

points,

the

Euclidean

distance

between

all

(

n 2

)

pairs

of points is calculated. The pair of points that are the farthest apart is assigned to

the estimation data set. This pair of points is removed from the list of points and

the pair of remaining points that are the farthest apart is assigned to the prediction

data set. Then this pair of points is removed from the list and the remaining point

that is farthest from the pair of points in the estimation data set is included in the

estimation data set. At the next step, the remaining unassigned point that is farthest

from the two points in the prediction data set is added to the prediction data. The

algorithm then continues to alternatively place the remaining points in either the

estimation or prediction data sets until all n observations have been assigned.

Snee [1977] suggests measuring the statistical properties of the estimation and

prediction data sets by comparing the pth root of the determinants of the XX

matrices for these two data sets, where p is the number of parameters in the model.

The determinant of XX is related to the volume of the region covered by the points.

Thus, if XE and XP denote the X matrices for points in the estimation and prediction

data sets, respectively, then

 XE XE  1 p  XP XP 

is a measure of the relative volumes of the regions spanned by the two data sets. Ideally this ratio should be close to unity. It may also be useful to examine the variance inflation factors for the two data sets and the eigenvalue spectra of XE XE and XP XP to measure the relative correlation between the regressors.
In using any data-splitting procedure (including the DUPLEX algorithm), several points should be kept in mind:

1. Some data sets may be too small to effectively use data splitting. Snee [1977] suggests that at least n  2p + 25 observations are required if the estimation and prediction data sets are of equal size, where p is the largest number of parameters likely to be required in the model. This sample size requirement ensures that there are a reasonable number of error degrees of freedom for the model.
2. Although the estimation and prediction data sets are often of equal size, one can split the data in any desired ratio. Typically the estimation data set would be larger than the prediction data set. Such splits are found by using the datasplitting procedure until the prediction data set contains the required number

380

VALIDATION OF REGRESSION MODELS

of points and then placing the remaining unassigned points in the estimation data set. Remember that the prediction data set should contain at least 15 points in order to obtain a reasonable assessment of model performance.
3. Replicates or points that are near neighbors in x space should be eliminated before splitting the data. Unless these replicates are eliminated, the estimation and prediction data sets may be very similar, and this would not necessarily test the model severely enough. In an extreme case where every point is replicated twice, the DUPLEX algorithm would form the estimation data set with one replicate and the prediction data set with the other replicate. The nearneighbor algorithm described in Section 4.5.2 may also be helpful. Once a set of near neighbors is identified, the average of the x coordinates of these points should be used in the data-splitting procedure.
4. A potential disadvantage of data splitting is that it reduces the precision with which regression coefficients are estimated. That is, the standard errors of the regression coefficients obtained from the estimation data set will be larger than they would have been if all the data had been used to estimate the coefficients. In large data sets, the standard errors may be small enough that this loss in precision is unimportant. However, the percentage increase in the standard errors can be large. If the model developed from the estimation data set is a satisfactory, predictor, one way to improve the precision of estimation is to reestimate the coefficients using the entire data set. The estimates of the coefficients in the two analyses should be very similar if the model is an adequate predictor of the prediction data set.
5. Double-cross validation may be useful in some problems. This is a procedure in which the data are first split into estimation and prediction data sets, a model developed from the estimation data, and its performance investigated using the prediction data. Then the roles of the two data sets are reversed; a model is developed using the original prediction data, and it is used to predict the original estimation data. The advantage of this procedure is that it provides two evaluations of model performance. The disadvantage is that there are now three models to choose from, the two developed via data splitting and the model fitted to all the data. If the model is a good predictor, it will make little difference which one is used, except that the standard errors of the coefficients in the model fitted to the total data set will be smaller. If there are major differences in predictive performance, coefficient estimates, or functional form for these models, then further analysis is necessary to discover the reasons for these differences.

Example 11.3 The Delivery Time Data
All 40 observations for the delivery time data in Examples 3.1 and 11.2 are shown in Table 11.3. We will assume that these 40 points were collected at one time and use the data set to illustrate data splitting with the DUPLEX algorithm. Since the model will have two regressors, an equal split of the data will give 17 error degrees of freedom for the estimation data. This is adequate, so DUPLEX can be used to generate the estimation and prediction data sets. An x1 - x2 plot is shown in Figure 11.1. Examination of the data reveals that there are two pairs of points that are near

TABLE 11.3 Delivery Time Data

Observation, i

Cases, xl

Distance, x2

1

7

560

2

3

220

3

3

340

4

4

80

5

6

150

6

7

330

7

2

110

8

7

210

9

30

1460

10

5

605

11

16

688

12

10

215

13

4

255

14

6

462

15

9

448

16

10

776

17

6

200

18

7

132

19

3

36

20

17

770

21

10

140

22

26

810

23

9

450

24

8

635

25

4

150

26

22

905

27

7

520

28

15

290

29

5

500

30

6

1000

31

6

225

32

10

775

33

4

212

34

1

144

35

3

126

36

12

655

37

10

420

38

7

150

39

8

360

40

32

1530

VALIDATION TECHNIQUES

381

Delivery Time, y
16.68 11.50 12.03 14.88 13.75 18.11 8.00 17.83 79.24 21.50 40.33 21.00 13.50 19.75 24.00 29.00 15.35 19.00 9.50 35.10 17.90 52.32 18.75 19.83 10.75 51.00 16.80 26.16 19.90 24.00 18.55 31.93 16.95 7.00 14.00 37.03 18.62 16.10 24.38 64.75

Estimation (E) or Prediction (P) Data Set
P P P E E E E E E E P P E P E P P E P E E E E E E P E P E E E P P P P P P P P P

neighbors in the x space, observations 15 and 23 and observations 16 and 32. These two clusters of points are circled in Figure 11.1. The x1 - x2 coordinates of these clusters of points are averaged and the list of points for use in the DUPLEX algorithm is shown in columns 1 and 2 of Table 11.4.
The standardized and orthonormalized data are shown in columns 3 and 4 of Table 11.4 and plotted in Figure 11.2. Notice that the region of coverage is more

382

VALIDATION OF REGRESSION MODELS

Distance, x2

1500 1400 1300 1200 1100 1000
900 800 700 600 500 400 300 200 100
0 0 5 10 15 20 25 30 35 Cases, x1
Figure 11.1 Scatterplot of delivery volume x1 versus distance x2, Example 11.3.

spherical than in Figure 11.1. Figure 11.2 and Table 11.3 and 11.4 also show how DUPLEX splits the original points into estimation and prediction data. The convex hulls of the two data sets are shown in Figure 11.2. This indicates that the prediction data set contains both interpolation and extrapolation points. For these two data sets we find that XE XE = 0.44696 and XP XP = 0.22441. Thus,

 

XE XE XP XP

1 3 

=

 00..4242649461 1 3

=

1.26

indicating that the volumes of the two regions are very similar. The VIFs for the estimation and prediction data are 2.22 and 4.43, respectively, so there is no strong evidence of multicollinearity and both data sets have similar correlative structure.
Panel A of Table 11.5 summarizes a least-squares fit to the estimation data. The parameter estimates in this model exhibit reasonable signs and magnitudes, and the VIFs are acceptably small. Analysis of the residuals (not shown) reveals no severe model inadequacies, except that the normal probability plot indicates that the error distribution has heavier tails than the normal. Checking Table 11.3, we see that point 9, which has previously been shown to be influential, is in the estimation data set. Apart from our concern about the normality assumption and the influence of point 9, we conclude that the least-squares fit to the estimation data is not unreasonable.

VALIDATION TECHNIQUES

383

TABLE 11.4 Delivery Time Data with Near-Neighborhood Points Averaged

(1)

(2)

(3)

(4)

Observation, i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15, 23 16, 32 17 18 19 20 21 22 24 25 26 27 28 29 30 31 33 34 35 36 37 38 39 40

Original Variables

Cases, x1 Distance, x2

7

560

3

220

3

340

4

80

6

150

7

330

2

110

7

210

30

1460

5

605

16

688

10

215

4

255

6

462

9

449

10

775.5

6

200

7

132

3

36

17

770

10

140

26

810

8

635

4

150

22

905

7

520

15

290

5

500

6

1000

6

225

4

212

1

144

3

126

12

655

10

420

7

150

8

360

32

1530

Standardized Orthonormalized Data

w1
-.047671 -.136037
.136037 -.113945 -.069762 -.047671 -.158128 -.047671
.460432 -.091854
.151152 .018603 -.113945 -.069762 -.003488 .018603 -.069762 -.047671 -.136037 .173243 .018603 .372066 -.025580 -.113945 .283700 -.047671 .129060 -.091854 -.069762 -.069762 -.113945 - .180219 -.136037 .062786 .018603 -.047671 -.025580 .504614

w2
.158431 .013739 .108082 -.126981 -.133254 -.022393 -.042089 -.116736 .160977 .255116 -.016816 -.204765 .010603 .112038 .009857 .235895 -.093945 -.178059 -.130920 .016998 -.263729 -.227434 .186742 -.071948 -.030133 .126983 -.299067 .172566 .535009 -.074290 -.023204 .015295 -.060163 .079853 -.043596 -.163907 -.029461 .154704

Estimation (E) or Prediction (P) Data Set
P P P E E E E E E E P P E P E P P E P E E E E E P E P E E E P P P P P P P P

Columns 2 and 3 of Table 11.6 show the results of predicting the observations in the prediction data set using the least-squares model developed from the estimation data. We see that the predicted values generally correspond closely to the observed values. The only unusually large prediction error is for point 40, which has the largest observed time in the prediction data. This point also has the largest values of x1 (32

384

VALIDATION OF REGRESSION MODELS

0.5

0.4

Estimation data

0.3
·
0.2 Prediction data
0.1

2

0

-0.1

-0.2

-0.3 -0.2 -0.1 0

0.1 0.2 0.3 0.4 0.5

1 Figure 11.2 Estimation data (×) and prediction data (·) using orthonormalized regressors.

TABLE 11.5 Summary of Least-Squares Fit to the Delivery Time Data

A Analysis Using Estimation Data

B. Analysis Using All Data

Coefficient Standard

Coefficient Standard

Variable Estimate

Error

t0 Variable Estimate

Error

Intercept

2.4123

1.4165

x1

1.6392

0.1769

x2

0.0136

0.0036

MSRes = 13.9145, R2 = 0.952

1.70 Intercept 3.9840

0.9861

9.27 x1

1.4877

0.1376

3.78 x2

0.0134

0.0028

MSRes = 13.6841, R2 = 0.944

t0
4.04 10.81 4.72

cases) and x2 (1530 ft) in the entire data set. It is very similar to point 9 in the esti-
mation data (x1 = 30, x2 = 1460) but represents an extrapolation for the model fit to the estimation data. The sum of squares of the prediction errors is  ei2 = 322.4452, and the approximate R2 for prediction is

 R2 Prediction

= 1-

ei2 = 1 - 322.4452 = 0.922

SST

4113.5442

DATA FROM PLANNED EXPERIMENTS

385

TABLE 11.6 Prediction Performance for the Model Developed from the Estimation Data

(1)

(2)

(3)

Least-Squares Fit

Observation, i
1 2 3 11 12 14 16 17 19 26 28 32 33 34 35 36 37 38 39 40

Observed, yi
16.68 11.50 12.03 40.33 21.00 19.75 29.00 15.35 9.50 51.00 26.16 31.93 16.95 7.00 14.00 37.03 18.62 16.10 24.38 64.75

Predicted, y^i
21.4976 10.3199 11.9508 37.9901 21.7264 18.5265 29.3509 14.9657 7.8192 50.7746 30.9417 29.3373 11.8504 6.0086 9.0424 30.9848 24.5125 15.9254 20.4187 75.6609

Prediction Error, ei = yi - y^i
-4.8176 1.1801 0.0792 2.3399
-0.7264 1.2235
-0.3509 0.3843 1.6808 0.2254
-4.7817 2.5927 5.0996 0.9914 4.9576 6.0452
-5.8925 0.1746 3.9613
-10.9109

where SST = 4113.5442 is the corrected sum of squares of the responses in the

prediction data set. Thus, we might expect this model to "explain" about 92.2% of

the variability in new data, as compared to the 95.2% of the variability explained

by the least-squares fit to the estimation data. This loss in R2 is small, so there

is reasonably strong evidence that the least-squares model will be a satisfactory

predictor.



11.3 DATA FROM PLANNED EXPERIMENTS
Most of the validation techniques discussed in this chapter assume that the model has been developed from unplanned data. While the techniques could also be applied in situations where a designed experiment has been used to collect the data, usually validation of a model developed from such data is somewhat easier. Many experimental designs result in regression coefficients that are nearly uncorrelated, so multicollinearity is not usually a problem. An important aspect of experimental design is selection of the factors to be studied and identification of the ranges they are to be varied over. If done properly, this helps ensure that all the important regressors are included in the data and that an appropriate range of values has been obtained for each regressor. Furthermore, in designed experiments considerable

386

VALIDATION OF REGRESSION MODELS

effort is usually devoted to the data collection process itself. This helps to minimize problems with "wild" or dubious observations and yields data with relatively small measurement errors.
When planned experiments are used to collect data, it is usually desirable to perform additional trials for use in testing the predictive performance of the model. In the experimental design literature, these extra trials are called confirmation runs. A widely used approach is to include the points that would allow fitting a model one degree higher than presently employed. Thus, if we are contemplating fitting a first-order model, the design should include enough points to fit at least some of the terms in a second-order model.

PROBLEMS
11.1 Consider the regression model developed for the National Football League data in Problem 3.1. a. Calculate the PRESS statistic for this model. What comments can you make about the likely predictive performance of this model? b. Delete half the observations (chosen at random), and refit the regression model. Have the regression coefficients changed dramatically? How well does this model predict the number of games won for the deleted observations? c. Delete the observation for Dallas, Los Angeles, Houston, San Francisco, Chicago, and Atlanta and refit the model. How well does this model predict the number of games won by these teams?
11.2 Split the National Football League data used in Problem 3.1 into estimation and prediction data sets. Evaluate the statistical properties of these two data sets. Develop a model from the estimation data and evaluate its performance on the prediction data. Discuss the predictive performance of this model.
11.3 Calculate the PRESS statistic for the model developed from the estimation data in Problem 11.2. How well is the model likely to predict? Compare this indication of predictive performance with the actual performance observed in Problem 11.2.
11.4 Consider the delivery time data discussed in Example 11.3. Find the PRESS statistic for the model developed from the estimation data. How well is the model likely to perform as a predictor? Compare this with the observed performance in prediction.
11.5 Consider the delivery time data discussed in Example 11.3. a. Develop a regression model using the prediction data set. b. How do the estimates of the parameters in this model compare with those from the model developed from the estimation data? What does this imply about model validity? c. Use the model developed in part a to predict the delivery times for the observations in the original estimation data. Are your results consistent with those obtained in Example 11.3?

PROBLEMS 387

11.6 In Problem 3.5 a regression model was developed for the gasoline mileage data using the regressor engine displacement x1 and number of carburetor barrels x6. Calculate the PRESS statistic for this model. What conclusions can you draw about the model's likely predictive performance?

11.7 In Problem 3.6 a regression model was developed for the gasoline mileage data using the regressor vehicle length x8 and vehicle weight x10. Calculate the PRESS statistic for this model. What conclusions can you draw about the potential performance of this model as a predictor?

11.8 PRESS statistics for two different models for the gasoline mileage data were calculated in Problems 11.6 and 11.7. On the basis of the PRESS statistics, which model do you think is the best predictor?

11.9 Consider the gasoline mileage data in Table B.3. Delete eight observations (chosen at random) from the data and develop an appropriate regression model. Use this model to predict the eight withheld observations. What assessment would you make of this model's predictive performance?

11.10

Consider the gasoline mileage .data in Table B.3. Split the data into estimation and prediction sets.
a. Evaluate the statistical properties of these data sets.
b. Fit a model involving x1 and x6 to the estimation data. Do the coefficients and fitted values from this model seem reasonable?
c. Use this model to predict the observations in the prediction data set. What is your evaluation of this model's predictive performance?

11.11

Refer to Problem 11.2. What are the standard errors of the regression coefficients for the model developed from the estimation data? How do they compare with the standard errors for the model in Problem 3.5 developed using all the data?

11.12

Refer to Problem 11.2. Develop a model for the National Football League data using the prediction data set.
a. How do the coefficients and estimated values compare with those quantities for the models developed from the estimation data?
b. How well does this model predict the observations in the original estimation data set?

11.13

What difficulties do you think would be encountered in developing a computer program to implement the DUPLEX algorithm? For example, how efficient is the procedure likely to be for large sample sizes? What modifications in the procedure would you suggest to overcome those difficulties?

11.14 11.15

If Z is the n × k matrix of standardized regressors and T is the k × k upper triangular matrix in Eq. (11.3), show that the transformed regressors W = ZT-1 are orthogonal and have unit variance.
Show that the least-squares estimate of  (say b^(i)) with the ith observation deleted can be written in terms of the estimate based on all n points as

b^(i)

=

b^

-

ei 1 - hii

(XX)-1 xi

388

VALIDATION OF REGRESSION MODELS

11.16

Consider the heat treating data in Table B.12. Split the data into prediction and estimation data sets.
a. Fit a model to the estimation data set using all possible regressions. Select the minimum Cp model.
b. Use the model in part a to predict the responses for each observation in the prediction data set. Calculate R2 for prediction. Comment on model adequacy.

11.17

Consider the jet turbine engine thrust data in Table B.13. Split the data into prediction and estimation data sets.
a. Fit a model to the estimation data using all possible regressions. Select the minimum Cp model.
b. Use the model in part a to predict each observation in the prediction data set. Calculate R2 for prediction. Comment on model adequacy.

11.18

Consider the electronic inverter data in Table B.14. Delete the second observation in the data set. Split the remaining observations into prediction and estimation data sets.
a. Find the minimum Cp equation for the estimation data set.
b. Use the model in part a to predict each observation in the prediction data set. Calculate R2 for prediction and comment on model adequacy.

11.19

Table B.11 presents 38 observations on wine quality.
a. Select four observations at random from this data set, then delete these observations and fit a model involving only the regressor flavor and the indicator variables for the region information to the remaining observations. Use this model to predict the deleted observations and calculate R2 for prediction.
b. Repeat part a 100 times and compute the average R2 for prediction for all 100 repetitions.
c. Fit the model to all 38 observations and calculate the R2 for prediction based on PRESS.
d. Comment on all three approaches from parts a­c above as measures of model validity.

11.20

Consider all 40 observations on the delivery time data. Delete 10% (4) of
the observations at random. Fit a model to the remaining 36 observations, predict the four deleted values, and calculate R2 for prediction. Repeat these calculations 100 times. Calculate the average R2 for prediction. What infor-
mation does this convey about the predictive capability of the model? How does the average of the 100 R2 for prediction values compare to R2 for pre-
diction based on PRESS for all 40 observations?

CHAPTER 12

INTRODUCTION TO NONLINEAR REGRESSION

Linear regression models provide a rich and flexible framework that suits the needs of many analysts. However, linear regression models are not appropriate for all situations. There are many problems in engineering and the sciences where the response variable and the predictor variables are related through a known nonlinear function. This leads to a nonlinear regression model. When the method of least squares is applied to such models, the resulting normal equations are nonlinear and, in general, difficult to solve. The usual approach is to directly minimize the residual sum of squares by an iterative procedure. In this chapter we describe estimating the parameters in a nonlinear regression model and show how to make appropriate inferences on the model parameters. We also illustrate computer software for noulinear regression.

12.1 LINEAR AND NONLINEAR REGRESSION MODELS

12.1.1 Linear Regression Models In previous chapters we have concentrated on the linear regression model

y = 0 + 1x1 + 2 x2 + + k xk + 

(12.1)

These models include not only the first-order relationships, such as Eq. (12.1), but also polynomial models and other more complex relationships. In fact, we could write the linear regression model as

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
389

390

INTRODUCTION TO NONLINEAR REGRESSION

y = 0 + 1z1 + 2z2 + + rzr + 

(12.2)

where zi represents any function of the original regressors x1, x2, . . . , xk, including transformations such as exp(xi), xi , and sin(xi). These models are called linear regression models because they are linear in the unknown parameters, the j, j = 1, 2, . . . , k.
We may write the linear regression model (12.1) in a general form as

y = xb + 
= f (x, b ) + 

(12.3)

where x = [1, x1, x2, . . . , xk]. Since the expected value of the model errors is zero, the expected value of the response variable is

E ( y) = E[ f (x, b ) +  ] = f (x, b )

We usually refer to f(x, ) as the expectation function for the model. Obviously, the expectation function here is just a linear function of the unknown parameters.

12.1.2 Nonlinear Regression Models
There are many situations where a linear regression model may not be appropriate. For example, the engineer or scientist may have direct knowledge of the form of the relationship between the response variable and the regressors, perhaps from the theory underlying the phenomena. The true relationship between the response and the regressors may be a differential equation or the solution to a differential equation. Often, this will lead to a model of nonlinear form.
Any model that is not linear in the unknown parameters is a nonlinear regression model. For example, the model

y = 1e2x + 

(12.4)

is not linear in the unknown parameters 1 and 2. We will use the symbol  to represent a parameter in a nonlinear model to emphasize the difference between the linear and the nonlinear case.
In general, we will write the nonlinear regression model as

y = f (x, q ) + 

(12.5)

where  is a p × 1 vector of unknown parameters and  is an uncorrelated randomerror term with E() = 0 and Var() = 2. We also typically assume that the errors
are normally distributed, as in linear regression. Since

E ( y) = E[ f (x, q ) +  ] = f (x, q )

(12.6)

ORIGINS OF NONLINEAR MODELS

391

we call f(x, ) the expectation function for the nonlinear regression model. This is very similar to the linear regression case, except that now the expectation function is a nonlinear function of the parameters.
In a nonlinear regression model, at least one of the derivatives of the expectation function with respect to the parameters depends on at least one of the parameters. In linear regression, these derivatives are not functions of the unknown parameters. To illustrate these points, consider a linear regression model

y = 0 + 1x1 + 2 x2 + + k xk + 

with expectation function f (x, b ) = 0 + kj=1 j xj Now

f

( x,
 j

b

)

=

xj

,

j = 0, 1, ... , k

where x0  1. Notice that in the linear case the derivatives are not functions of the 's
Now consider the nonlinear model

y = f ( x, q ) + 
= 1e2x + 

The derivatives of the expectation function with respect to 1 and 2 are

f ( x, q ) = e2x
1

and

f

( x, q
2

)

=

1xe2x

Since the derivatives are a function of the unknown parameters 1 and 2, the model is nonlinear.

12.2 ORIGINS OF NONLINEAR MODELS
Nonlinear regression models often strike people as being very ad hoc because these models typically involve mathematical functions that are nonintuitive to people outside of the specific application area. Too often, people fail to appreciate the scientific theory underlying these nonlinear regression models.The scientific method uses mathematical models to describe physical phenomena. In many cases, the theory describing the physical relationships involves the solution of a set of differential equations, especially whenever rates of change are the basis for the mathematical model. This section outlines how the differential equations that form the heart of the theory describing physical behavior lead to nonlinear models. We discuss two examples. The first example deals with reaction rates and is more straightforward. The second example gives more details about the underlying theory to illustrate why nonlinear regression models have their specific forms. Our key point is that nonlinear regression models are almost always deeply rooted in the appropriate science.

392

INTRODUCTION TO NONLINEAR REGRESSION

Example 12.1

We first consider formally incorporating the effect of temperature into a secondorder reaction kinetics model. For example, the hydrolysis of ethyl acetate is well modeled by a second-order kinetics model. Let At be the amount of ethyl acetate at time t. The second-order model is

dAt dt

= -kAt2

where k is the rate constant. Rate constants depend on temperature, which we will incorporate into our model later. Let A0 be the amount of ethyl acetate at time zero. The solution to the rate equation is

1 = 1 + kt At A0

With some algebra, we obtain

At

=

A0 1 + A0tk

We next consider the impact of temperature on the rate constant. The Arrhenius equation states

k

=

C1

exp 

-

Ea RT



where Ea is the activation energy and C1 is a constant. Substituting the Arrhenius equation into the rate equation yields

At

=

1+

A0tC1

A0
exp ( -Ea

/ RT

)

Thus, an appropriate nonlinear regression model is

At

=

1
1 + 2t exp(-3

/T

)

+ t

where 1 = A0, 2 = C1A0, and 3 = Ea/R.

(12.7)


Example 12.2
We next consider the Clausius­Clapeyron equation, which is an important result in physical chemistry and chemical engineering. This equation describes the relationship of vapor pressure and temperature.

ORIGINS OF NONLINEAR MODELS

393

Vapor pressure is the physical property which explains why puddles of water evaporate away. Stable liquids at a given temperature are those that have achieved an equilibrium with their vapor phase. The vapor pressure is the partial pressure of the vapor phase at this equilibrium. If the vapor pressure equals the ambient pressure, then the liquid boils. Puddles evaporate when the partial pressure of the water vapor in the ambient atmosphere is less than the vapor pressure of water at that temperature. The nonequilibrium condition presented by this difference between the actual partial pressure and the vapor pressure causes the puddle's water to evaporate over time.
The chemical theory that describes the behavior at the vapor­liquid interface notes that at equilibrium the Gibbs free energies of both the vapor and liquid phases must be equal. The Gibbs free energy G is given by

G = U + PV -TS = H -TS

where U is the "internal energy," P is the pressure, V is the volume, T is the "absolute" temperature, S is the entropy, and H = U + PV is the enthalpy. Typically, in thermodynamics, we are more interested in the change in Gibbs free energy than its absolute value. As a result, the actual value of U is often of limited interest. The derivation of the Clausius­Clapeyron equation also makes use of the ideal gas law,

PV = RT
where R is the ideal gas constant. Consider the impact of a slight change in the temperature when holding the
volume fixed. From the ideal gas law, we observe that an increase in the temperature necessitates an increase in the pressure. Let dG be the resulting differential in the Gibbs free energy. We note that

dG

=



G P

T

dP

+



G T

P

dT

= VdP - SdT

Let the subscript 1 denote the liquid phase and the subscript v denote the vapor phase. Thus, G1 and Gv are the Gibbs free energies of the liquid and vapor phases, respectively. If we maintain the vapor­liquid equilibrium as we change the temperature and pressure, then

dG1 = dGv V1dP - S1dT = VvdP - SvdT

Rearranging, we obtain

dP = Sv - S1 dT Vv - V1

(12.8)

394

INTRODUCTION TO NONLINEAR REGRESSION

We observe that the volume occupied by the vapor is much larger than the volume occupied by the liquid. Effectively, the difference is so large that we can treat V1 as zero. Next, we observe that entropy is defined by

dS = dQ T
where Q is the heat exchanged reversibly between the system and its surroundings. For our vapor­liquid equilibrium situation, the net heat exchanged is Hvap, which is the heat of vaporization at temperature T. Thus,

We then can rewrite (12.8) as

Sv

-

S1

=

H vap T

From the ideal gas law, We then may rewrite (12.8) as

dP = Hvap dT VT V = RT
P

dP = PHvap dT RT 2

Rearranging, we obtain, Integrating, we obtain

dP = HvapdT P RT 2

ln

(

P

)

=

C

-

C1

1 T

where C is an integration constant and

(12.9)

We can reexpress (12.9) as

C1

=

H vap R

P

=

C0

+

C

exp 

-

C1 T



(12.10)

NONLINEAR LEAST SQUARES

395

where C0 is another integration constant. Equation (12.9) suggests a simple linear regression model of the form

ln(P )i

=

0

+

1

1 Ti

+ i

(12.11)

Equation (12.10) on the other hand, suggests a nonlinear regression model of the form

Pi

= 1

exp

 

2 Ti

 

+

i

(12.12)

It is important to note that there are subtle, yet profound differences between these

two possible models. We discuss some of the possible differences between linear and

nonlinear models in Section 12.4.



12.3 NONLINEAR LEAST SQUARES

Suppose that we have a sample of n observations.on the response and the regressors, say yi, xi1, xi2, . . . , xik, for i = 1, 2, . . . , n. We have observed previously that the method of least squares in linear regression involves minimizing the least-squares function

  n  

k

2

S(b) =

i=1

 yi 

-  0 +

j=1

 j xij 

Because this is a linear regression model, when we differentiate S() with respect to the unknown parameters and equate the derivatives to zero, the resulting normal equations are linear equations, and consequently, they are easy to solve.
Now consider the nonlinear regression situation. The model is

yi = f (xi , q ) + i , i = 1, 2, ... , n

where now xi = [1, xi1, xi2, ... , xik ] for i = 1, 2, . . . , n. The least-squares function is

n
 S (q ) = [yi - f (xi, q )]2 i=1

(12.13)

To find the least-squares estimates we must differentiate Eq. (12.13) with respect to each element of . This will provide a set of p normal equations for the nonlinear regression situation. The normal equations are

n
i=1

[ yi

-

f

(

xi

,

q

)]

 

f

(xi, q )
 j q =q^

=

0

for j = 1, 2, ... , p

(12.14)

In a nonlinear regression model the derivatives in the large square brackets will be functions of the unknown parameters. Furthermore, the expectation function is also a nonlinear function, so the normal equations can be very difficult to solve.

396

INTRODUCTION TO NONLINEAR REGRESSION

Example 12.3 Normal Equations for a Nonlinear Model

Consider the nonlinear regression model in Eq. (12.4):

y = 1e2x +  The least-squares normal equations for this model are

n
  yi -^1e^2xi  e^2xi = 0
i=1 n
  yi -^1e^2xi ^1xie^2xi = 0
i=1

(12.15)

After simplification, the normal equations are

n

n

  yie^2xi -^1 e2^2xi = 0

i=1

i=1

n

n

  yi xie^2xi -^1 xie2^2xi = 0

i=1

i=1

(12.16)

These equations are not linear in ^1 and ^2, and no simple closed-form solution exists. In general, iterative methods must be used to find the values of ^1 and ^2. To

further complicate the problem, sometimes there are multiple solutions to the

normal equations. That is, there are multiple stationary values for the residual sum

of squares function S().



Geometry of Linear and Nonlinear Least Squares Examining the geometry of the least-squares problem is helpful in understanding the complexities introduced by a nonlinear model. For a given sample, the residual-sum-of-squares function S() depends only on the model parameters . Thus, in the parameter space (the space defined by the 1, 2, . . . , p), we can represent the function S() with a contour plot, where each contour on the surface is a line of constant residual sum of squares.
Suppose the regression model is linear; that is, the parameters are  = , and the residual-sum-of-squares function is S() Figure 12.1a shows the contour plot for this situation. If the model is linear in the unknown parameters, the contours are ellipsoidal and have a unique global minimum at the least-squares estimator b^ .
When the model is nonlinear, the contours will often appear as in Figure 12.1b. Notice that these contours are not elliptical and are in fact quite elongated and irregular in shape. A "banana-shape" appearance is very typical. The specific shape and orientation of the residual sum of squares contours depend on the form of the nonlinear model and the sample of data that have been obtained. Often the surface will be very elongated near the optimum, so many solutions for  will produce a residual sum of squares that is close to the global minimum. This results in a problem that is ill-conditioned, and in such problems it is often difficult to find the global minimum for . In some situations, the contours may be so irregular that there are several local minima and perhaps more than one global minimum. Figure 12.1c shows a situation where there is one local minimum and a global minimum.

TRANFORMATION TO A LINEAR MODEL

397

> > >

2

 = (Xi'X )-1 X'y 2

30 50 70 90

(a)

1

2 

3050 70 90

(b)

1

Local minimum
40 30 40 50
60 40 70
 (global minimum)
1 (c)

Figure 12.1 Contours of the residual-sum-of-squares function: (a) linear model; (b) nonlinear model; (c) nonlinear model with local and global minima.

Maximum-Likelihood Estimation We have concentrated on least squares in the nonlinear case. If the error terms in the model are normally and independently distributed with constant variance, application of the method of maximum likelihood to the estimation problem will lead to least squares. For example, consider the model in Eq. (12.4):

yi = 1e2xi + i , i = 1, 2, ... , n

(12.17)

If the errors are normally and independently distributed with mean zero and variance 2, then the likelihood function is

[ ] ( ) ( ) L ,  2

=

1 2 2

n

2

exp

- 

1 2

2

n i=1

yi - 1e2xi

2 



(12.18)

Clearly, maximizing this likelihood function is equivalent to minimizing the residual sum of squares. Therefore, in the normal-theory case, least-squares estimates are the same as maximum-likelihood estimates.

12.4 TRANFORMATION TO A LINEAR MODEL

It is sometimes useful to consider a transformation that induces linearity in the model expectation function. For example, consider the model

y = f ( x, q ) + 
= 1e2x + 

(12.19)

The Clausius­Clapeyron equation (12.12) is an example of this model. Now since
E ( y) = f ( x, q ) = 1e2x, we can linearize the expectation function by taking
logarithms,

ln E ( y) = ln1 +2x

which we saw in Eq. (12.11) in our derivation of the Clausius­Clapeyron equation. Therefore, it is tempting to consider rewriting the model as

398

INTRODUCTION TO NONLINEAR REGRESSION

ln y = ln1 + 2x +  = 0 + 1x + 

(12.20)

and using simple linear regression to estimate 0 and 1. However, the linear leastsquares estimates of the parameters in Eq. (12.20) will not in general be equivalent to the nonlinear parameter estimates in the original model (12.19). The reason is that in the original nonlinear model least squares implies minimization of the sum of squared residuals on y, whereas in the transformed model (12.20) we are minimizing the sum of squared residuals on ln y.
Note that in Eq. (12.19) the error structure is additive, so taking logarithms cannot produce the model in Eq. (12.20). If the error structure is multiplicative, say

y = 1e2x

(12.21)

then taking logarithms will be appropriate, since

ln y = ln1 +2x + ln = 0 + 1x +  *

(12.22)

and if * follows a normal distribution, all the standard linear regression model properties and associated inference will apply.
A nonlinear model that can be transformed to an equivalent linear form is said to be intrinsically linear. However, the issue often revolves around the error structure, namely, do the standard assumptions on the errors apply to the original nonlinear model or to the linearized one? This is sometimes not an easy question to answer.

Example 12.4 The Puromycin Data

Bates and Watts [1988] use the Michaelis­Menten model for chemical kinetics to relate the initial velocity of an enzymatic reaction to the substrate concentration x. The model is

y = 1x +  x +2

(12.23)

The data for the initial rate of a reaction for an enzyme treated with puromycin are shown in Table 12.1 and plotted in Figure 12.2.
We note that the expectation function can be linearized easily, since

1 = x +2 = 1 + 2 1
f ( x, q ) 1x 1 1 x
= 0 + 1x

so we are tempted to fit the linear model

y* = 0 + 1u + 

TRANFORMATION TO A LINEAR MODEL

399

TABLE 12.1 Reaction Velocity and Substrate Concentration for Puromycin Experiment

Substrate Concentration (ppm)

Velocity [(counts/min)/min]

0.02

47

76

0.06

97

107

0.11

123

139

0.22

152

159

0.56

191

201

1.10

200

207

Velocity ((counts/min)/min)

200
150
100
50
0 0.0 0.2 0.4 0.6 0.8 1.0 Concentration (ppm)
Figure 12.2 Plot of reaction velocity versus substrate concentration for the puromycin experiment. (Adapted from Bates and Watts [1988], with permission of the publisher.)

where y* = 1/y and u = 1/x. The resulting least-squares fit is

y^ * = 0.005107 + 0.0002472u

Figure 12.3a shows a scatterplot of the transformed data y* and u with the straightline fit superimposed. As there are replicates in the data, it is easy to see from Figure 12.2 that the variance of the original data is approximately constant, while Figure 12.3a indicates that in the transformed scale the constant-variance assumption is unreasonable.
Now since

0

=

1 1

and

1

=

2 1

400

INTRODUCTION TO NONLINEAR REGRESSION

Velocity 0 50 100 150 200

0.020

Velocity-1 0.010 0.015

0.005

0 10 20 30 40 50

0.0 0.2 0.4 0.6 0.8 1.0 1.2

Concentration-1

Concentration

(a)

(b)

Figure 12.3 (a) Plot of inverse velocity versus inverse concentration for the puromycin data. (b) Fitted curve in the original scale. (Adapted from Bates and Watts [1988], with permission of the publisher.)

we have

0.005107

=

1 ^1

and

0.0002472

=

^2 ^1

and so we can estimate 1 and 2 in the original model as

^1 = 195.81 and ^2 = 0.04841

Figure 12.3b shows the fitted curve in the original scale along with the data. Observe

from the figure that the fitted asymptote is too small. The variance at the replicated

points has been distorted by the transformation, so runs with low concentration

(high reciprocal concentration) dominate the least-squares fit, and as a result the

model does not fit the data well at high concentrations.



12.5 PARAMETER ESTIMATION IN A NONLINEAR SYSTEM

12.5.1 Linearization
A method widely used in computer algorithms for nonlinear regression is linearization of the nonlinear function followed by the Gauss­Newton iteration method of parameter estimation. Linearization is accomplished by a Taylor series expansion
of f(xi, ) about the point q0 = [10, 20, ... , p0 ] with only the linear terms retained.
This yields

 f

(xi, q ) =

f (xi, q0 ) +

p j=1

 

f

(xi


,
j

q

)

 q

=q0

(

j

-j0 )

(12.24)

PARAMETER ESTIMATION IN A NONLINEAR SYSTEM

401

If we set

fi0 = f (xi , q0 )



0 j

=j

-j0

Zi0j

=

 f 

(xi, q0 )

 j

 q

=q0

we note that the nonlinear regression model can be written as

p
 yi - fi0 =  j0Zi0j + i , i = 1, 2, ... , n j=1

(12.25)

That is, we now have a linear regression model. We usually call 0 the starting values for the parameters.
We may write Eq. (12.25) as

y0 = Z0b0 + e so the estimate of 0 is
b^0 = (Z0Z0 )-1 Z0y0 = (Z0Z0 )-1 Z0 (y - f0 )

(12.26) (12.27)

Now since 0 =  - 0, we could define

q^1 = b^0 + q0

(12.28)

as revised estimates of . Sometimes b^0 is called the vector of increments. We may now place the revised estimates q^1 in Eq. (12.24) (in the same roles played by the initial estimates 0) and then produce another set of revised estimates, say q^2, and so forth.
In general, we have at the kth iteration

q^k+1 = q^k + b^k = q^k + (ZkZk )-1 Zk (y - fk )

(12.29)

where
[ ] Zk = Zikj [ ] fk = f1k, f2k, ... , fnk 
q^k = [1k , 2k , ... ,  pk ]
This iterative process continues until convergence, that is, until
( )  ^j,k+1 -^jk /^jk  <  , j = 1, 2, ... , p
where  is some small number, say 1.0 × 10-6. At each iteration the residual sum of
( ) squares S q^k should be evaluated to ensure that a reduction in its value has been
obtained.

402

INTRODUCTION TO NONLINEAR REGRESSION

Example 12.5 The Puromycin Data

Bates and Watts [1988] use the Gauss­Newton method to fit the Michaelis­Menten model to the puromycin data in Table 12.1 using the starting values 10 = 205 and 20 = 0.08. Later we will discuss how these starting values were obtained. At this starting point, the residual sum of squares S(0) = 3155. The data, fitted values, residuals, and derivatives evaluated at each observation are shown in Table 12.2. To
illustrate how the required quantities are calculated, note that

f ( x, 1, 2 ) = x

1

2 + x

and

f

( x, 1,
2

2

)

=

-1x
(2 + x)2

and since the first observation on x is x1 = 0.02, we have

Z101

=

x1 2 +

x

2 =0.08

=

0.02 0.08 + 0.02

=

0.2000

Z102

=

-1x1
(2 + x1 )2

1=205, 2 =0.08

=

( -205) ( 0.02 ) (0.08 + 0.02)2

=

-410.00

The derivatives Zi0j are now collected into the matrix Z0 and the vector of increments calculated from Eq. (12.27) as

b^0

=

 8.03  -0.017

TABLE 12.2 Data, Fitted Values, Residuals, and Derivatives for the Puromycin Data
at q^0 = [205, 0.08]

i

xi

yi

fi0

1

0.02

76

41.00

2

0.02

47

41.00

3

0.06

97

87.86

4

0.06

107

87.86

5

0.11

123

118.68

6

0.11

139

118.68

7

0.22

159

150.33

8

0.22

152

150.33

9

0.56

191

179.38

10

0.56

201

179.38

11

1.10

207

191.10

12

1.10

200

191.10

yi - fi0
35.00 6.00 9.14 19.14 4.32 20.32 8.67 1.67 11.62 21.62 15.90 8.90

Zi01
0.2000 0.2000 0.4286 0.4286 0.5789 0.5789 0.7333 0.7333 0.8750 0.8750 0.9322 0.9322

Zi02
-410.00 -410.00 -627.55 -627.55 -624.65 -624.65 -501.11 -501.11 -280.27 -280.27 -161.95 -161.95

PARAMETER ESTIMATION IN A NONLINEAR SYSTEM

403

The revised estimate q^1 from Eq. (12.28) is

q^1 = b^0 + q0

=

 8.03  -0.017

+

205.00  0.08 

=

213.03  0.063 

( ) The residual sum of squares at this point is S q^1 = 1206, which is considerably
smaller than S(0). Therefore, q^1 is adopted as the revised estimate of , and another iteration would be performed.
( ) The Gauss­Newton algorithm converged at q^ = [212.7, 0.0641] with S q^ = 1195.
Therefore, the fitted model obtained by linearization is

y^ =

q^1x x + q^2

=

212.7x x + 0.0641

Figure 12.4 shows the fitted model. Notice that the nonlinear model provides a much better fit to the data than did the transformation followed by linear regression in Example 12.4 (compare Figures 12.4 and 12.3b).
Residuals can be obtained from a fitted nonlinear regression model in the usual way, that is,

ei = yi - y^i , i = 1, 2, ... , n

Velocity, y

240
200
160
120
80
40
0 0 0.2 0.4 0.6 0.8 1 1.2 Concentration, x
Figure 12.4 Plot of fitted nonlinear regression model, Example 12.5.

404

INTRODUCTION TO NONLINEAR REGRESSION

27

17

7

ei >

-3

-13 0 40 80 120 160 200 240 yi
Figure 12.5 Plot of residuals versus predicted values, Example 12.5.

In this example the residuals are computed from

ei

=

yi

-

^1xi xi + ^2

=

yi

-

212.7x , xi + 0.0641

i = 1, 2, ... ,10

The residuals are plotted versus the predicted values in Figure 12.5. A normal

probability plot of the residuals is shown in Figure 12.6. There is one moderately

large residual; however, the overall fit is satisfactory, and the model seems to be a

substantial improvement over that obtained by the transformation approach in

Example 12.4.



Computer Programs Several PC statistics packages have the capability to fit nonlinear regression models. Both JMP and Minitab (version 16 and higher) have this capability. Table 12.3 is the output from JMP that results from fitting the Michaelis­Menten model to the puromycin data in Table 12.1. JMP required 13 iterations to converge to the final parameter estimates. The output provides the estimates of the model parameters, approximate standard errors of the parameter estimates, the error or residual sum of squares, and the correlation matrix of the parameter estimates. We make use of some of these quantities in later sections.

Estimation of  2 When the estimation procedure converges to a final vector of parameter estimates q^ , we can obtain an estimate of the error variance 2 from the residual mean square

  ( ) ( ) n ( yi - y^i )2

n yi - f xi , q^ 2 S q^

^ 2 = MSRes = i=1 n - p

= i=1

n- p

= n- p

(12..30)

PARAMETER ESTIMATION IN A NONLINEAR SYSTEM

405

99.9 99 95

Cumulative normal probability × 100

80

50

20

5 1

0.1

-13

-3

7

17

27

ei

Figure 12.6 Normal probability plot of residuals, Example 12.5.

TABLE 12.3 JMP Output for Fitting the Michaelis­Menten Model to the Puromycin Data

Nonlinear Fit Response: Velocity, Predictor: Michaelis Menten Model (2P)

Criterion Iteration Obj Change Relative Gradient Gradient

Current 13
2.001932e-12 3.5267226e-7 0.0001344207

Stop Limit 60
1e-15 0.000001 0.000001

Parameter

theta1

theta2

SSE 1195.4488144

N

12

Current Value 212.68374295 0.0641212814

Solution

SSE

DFE

MSE

RMSE

1195.4488144

10

119.54488

10.933658

Parameter theta1 theta2

Estimate 212.68374295 0.0641212814

ApproxStdErr 6.94715515 0.00828095

Solved By: Analytic NR

Correlation of Estimates
theta1 theta2

theta1 1.0000 0.7651

theta2 0.7651 1.0000

406

INTRODUCTION TO NONLINEAR REGRESSION

where p is the number of parameters in the nonlinear regression model. For the
puromycin data in Example 12.5, we found that the residual sum of squares at the
( ) final iteration was S q^ = 1195 (also see the JMP output in Table 12.3), so the esti-
mate of  2 is

( ) S q^
^ 2 =

= 1195 = 119.5

n - p 12 - 2

We may also estimate the asymptotic (large-sample) covariance matrix of the parameter vector q^ by

( ) Var q^ =  2 (ZZ)-1

(12.31)

where Z is the matrix of partial derivatives defined previously, evaluated at the final-iteration least-squares estimate q^.
The covariance matrix of the q^ vector for the Michaelis­Menten model in
Example 12.5 is

( ) Var q^

=

^

2

( ZZ )-1

=

119.5

 

0.4037 36.82 × 10-5

36.82 × 10-5  57.36 × 10-8 

The main diagonal elements of this matrix are approximate variances of the estimates of the regression coefficients. Therefore, approximate standard errors on the coefficients are

( ) ( ) se ^1 = Var ^1 = 119.5(0.4037) = 6.95
and

( ) ( ) se ^2 = Var ^2 = 119.5(57.36 × 10-8 ) = 8.28 × 10-3

and the correlation between ^1 and ^2 is about

36.82 × 10-5

= 0.77

0.4037(57.36 × 10-8 )

These values agree closely with those reported in the JMP output, Table 12.3.
Graphical Perspective on Linearization We have observed that the residualsum-of-squares function S() for a nonlinear regression model is usually an irregular "banana-shaped" function, as shown in panels b and c of Figure 12.1. On the other hand, the residual-sum-of-squares function for linear least squares is very well behaved; in fact, it is elliptical and has the global minimum at the bottom of the "bowl." Refer to Figure 12.1a. The linearization technique converts the nonlinear regression problem into a sequence of linear ones, starting at the point 0.
The first iteration of linearization replaces the irregular contours with a set of elliptical contours. The irregular contours of S() pass exactly through the starting

PARAMETER ESTIMATION IN A NONLINEAR SYSTEM

407

>

2
S() contour passing through 0

> >
>

2  (global minimum)
2
Residual sum of sequences contour in the linearized problem
1 (linearized solution first iteration)
0 (starting value)

1 (a)

> > >

3 2
1 0
(b)

>



>

4

5

1

Figure 12.7 A geometric view of linearization: (a) the first iteration; (b) evolution of successive linearization iterations.

point 0, as shown in Figure 12.7a. When we solve the linearized problem, we are moving to the global minimum on the set of elliptical contours. This is done by ordinary linear least squares. Then the next iteration just repeats the process, starting at the new solution q^1. The eventual evolution of linearization is a sequence of linear problems for which the solutions "close in" on the global minimum of the nonlinear function. This is illustrated in Figure 12.7b. Provided that the nonlinear problem is not too ill-conditioned, either because of a poorly specified model or inadequate data, the linearization procedure should converge to a good estimate of the global minimum in a few iterations.
Linearization is facilitated by a good starting value 0, that is, one that is reasonably close to the global minimum. When 0 is close to q^, the actual residual-sum-ofsquares contours of the nonlinear problem are usually well-approximated by the contours of the linearized problem. We will discuss obtaining starting values in Section 12.5.3.
12.5.2 Other Parameter Estimation Methods
The basic linearization method described in Section 12.5.1 may converge very slowly in some problems. In other problems, it may generate a move in the wrong direction,
( ) with the residual-sum-of-squares function S q^k actually increasing at the kth itera-
tion. In extreme cases, it may fail to converge at all. Consequently, several other techniques for solving the nonlinear regression problem have been developed. Some of them are modifications and refinements of the linearization scheme. In this section we give a brief description of some of these procedures.
Method of Steepest Descent The method of steepest descent attempts to find the global minimum on the residual-sum-of-squares function by direct minimization. The objective is to move from an initial starting point 0 in a vector direction with components given by the derivatives of the residual-sum-of-squares function with respect to the elements of . Usually these derivatives are estimated by fitting a first-order or planar approximation around the point 0,The regression coefficients in the first-order model are taken as approximations to the first derivatives.

408

INTRODUCTION TO NONLINEAR REGRESSION

The method of steepest descent is widely used in response surface methodology to move from an initial estimate of the optimum conditions for a process to a region more likely to contain the optimum. The major disadvantage of this method in solving the nonlinear regression problem is that it may converge very slowly. Steepest descent usually works best when the starting point is a long way from the optimum. However, as the current solution gets closer to the optimum, the procedure will produce shorter and shorter moves and a "zig-zag" behavior. This is the convergence problem mentioned previously.

Fractional Increments A standard modification to the linearization technique is the use of fractional increments. To describe this method, let b^k be the standard increment vector in Eq. (12.29) at the kth iteration, but continue to the next itera-
( ) ( ) ( ) ( ) tion only if S q^k+1 < S q^k . If S q^k+1 > S q^k , use b^k / 2 as the vector of increments.
This halving could be used several times during an iteration, if necessary. If after a
( ) specified number of trials a reduction in S q^k+1 is not obtained, the procedure is
terminated. The general idea behind this method is to keep the linearization proce-
dure from making a step at any iteration that is too big. The fractional increments
technique is helpful when convergence problems are encountered in the basic lin-
earization procedure.

Marquardt's Compromise Another popular modification to the basic linearization algorithm was developed by Marquardt [1963]. He proposed computing the vector of increments at the kth iteration from

(ZkZk + Ip ) b^k = Zk (y - fk )

(12.32)

where  > 0. Note the similarity to the ridge regression estimator in Chapter 11. Since the regressor variables are derivatives of the same function, the linearized
function invites multicollinearity. Thus, the ridgelike procedure in Eq. (12.32) is
intuitively reasonable. Marquardt [1963] used a search procedure to find a value of  that would reduce the residual sum of squares at each stage.
Different computer programs select  in different ways. For example, PROC NLIN in SAS begins with  = 10-8. A series of trial-and-error computations are done at each iteration with  repeatedly multiplied by 10 until

( ) ( ) S q^k+1 < S q^k

(12.33)

The procedure also involves reducing  by a factor of 10 at each iteration as long as Eq. (12.33) is satisfied. The strategy is to keep  as small as possible while ensuring that the residual sum of squares is reduced at each iteration. This general procedure is often called Marquardt's compromise, because the resulting vector of increments produced by his method usually lies between the Gauss­Newton vector in the linearization vector and the direction of steepest descent.

12.5.3 Starting Values
Fitting a nonlinear regression model requires starting values 0 of the model parameters. Good starting values, that is, values of 0 that are close to the true parameter

STATISTICAL INFERENCE IN NONLINEAR REGRESSION

409

values, will minimize convergence difficulties. Modifications to the linearization procedure such as Marquardt's compromise have made the procedure less sensitive to the choice of starting values, but it is always a good idea to select 0 carefully. A poor choice could cause convergence to a local minimum on the function, and we might be completely unaware that a suboptimal solution has been obtained.
In nonlinear regression models the parameters often have some physical meaning, and this can be very helpful in obtaining starting values. It may also be helpful to plot the expectation function for several values of the parameters to become familiar with the behavior of the model and how changes in the parameter values affect this behavior.
For example, in the Michaelis­Menten function used for the puromycin data, the parameter 1 is the asymptotic velocity of the reaction, that is, the maximum value of f as x  . Similarly, 2 represents the half concentration, or the value of x such that when the concentration reaches that value, the velocity is one-half the maximum value. Examining the scatter diagram in Figure 12.2 would suggest that 1 = 205 and 2 = 0.08 would be reasonable starting values. These values were used in Example 12.5.
In some cases we may transform the expectation function to obtain starting values. For example, the Michaelis­Menten model can be "linearized" by taking the reciprocal of the expectation function. Linear least squares can be used on the reciprocal data, as we did in Example 12.4, resulting in estimates of the linear parameters. These estimates can then be used to obtain the necessary starting values 0. Graphical transformation can also be very effective. A nice example of this is given in Bates and Watts [1988, p. 47].

12.6 STATISTICAL INFERENCE IN NONLINEAR REGRESSION
In a linear regression model when the errors are normally and independently distributed, exact statistical tests and confidence intervals based on the t and F distributions are available, and the parameter estimates have useful and attractive statistical properties. However, this is not the case in nonlinear regression, even when the errors are normally and independently distributed. That is, in nonlinear regression the least-squares (or maximum-likelihood) estimates of the model parameters do not enjoy any of the attractive properties that their counterparts do in linear regression, such as unbiasedness, minimum variance, or normal sampling distributions. Statistical inference in nonlinear regression depends on large-sample or asymptotic results. The large-sample theory generally applies for both normally and nonnormally distributed errors.
The key asymptotic results may be briefly summarized as follows. In general, when the sample size n is large, the expected value of q^ is approximately equal to , the true vector of parameter estimates, and the covariance matrix of q^ is approximately 2(ZZ)-1, where Z is the matrix of partial derivatives evaluated at the final-iteration least-squares estimate q^. Furthermore, the sampling distribution of q^ is approximately normal. Consequently, statistical inference for nonlinear regression when the sample size is large is carried out exactly as it is for linear regression. The statistical tests and confidence intervals are only approximate procedures.

410

INTRODUCTION TO NONLINEAR REGRESSION

Example 12.6 The Puromycin Data

Reconsider the Michaelis­Menten model for the puromycin data from Example
12.5. The JMP output for the model is shown in Table 12.3. To test for significance of regression (that is, H0: 1 = 2 = 0) we could use an ANOVA-like procedure. We can compute the total sum of squares of the y's as SST = 271,909.0. So the model or regression sum of squares is:

SSmodel = SST - SSRes = 271, 410 - 1195.4 = 270, 214.6

Therefore, the test for significance of regression is

F0

=

SSmodel / 2 MSError

=

270, 241.6 / 2 119.5

= 1130.61

and compute an approximate P value from the F2,10 distribution. This P value is considerably less than 0.0001, so we are safe in rejecting the null hypothesis and concluding that at least one of the model parameters is nonzero. To test hypotheses on the individual model parameters, H0: 1 = 0 and H0: 2 = 0, we could compute approximate t statistics as

( ) t0

=

^1 se ^1

= 212.7 = 30.62 6.9471

and

( ) t0

=

^2 se ^2

= 0.0641 = 7.74 0.00828

The approximate P values for these two test statistics are both less than 0.01. Therefore, we would conclude that both parameters are nonzero.
Approximate 95% confidence intervals on 1 and 2 are found as follows:
( ) ( ) ^1 - t0.025,10se ^1  1  ^1 + t0.025,10se ^1
212.7 - 2.228(6.9471)  1  212.7 + 2.228(6.9471)
197.2  1  228.2
and
( ) ( ) ^2 - t0.025,10se ^2  2  ^2 + t0.025,10se ^2
0.0641 - 2.228(0.00828)  2  0.0641 + 2.228(0.00828)
0.0457  2  0.0825

EXAMPLES OF NONLINEAR REGRESSION MODELS

411

respectively. In constructing these intervals, we have used the results from the com-

puter output in Table 12.3. Other approximate confidence intervals and prediction

intervals would be constructed by inserting the appropriate nonlinear regression

quantities into the corresponding equations from linear regression.



Validity of Approximate Inference Since the tests, procedures, and confidence intervals in nonlinear regression are based on large-sample theory and typically the sample size in a noulinear regression problem may not be all that large, it is logical to inquire about the validity of the procedures. It would be desirable to have a guideline or "rule of thumb" that would tell us when the sample size is large enough so that the asymptotic results are valid. Unfortunately, no such general guideline is available. However, there are some indicators that the results may be valid in a particular application.

1. If the nonlinear regression estimation algorithm converges in only a few iterations, then this indicates that the linear approximation used in solving the problem was very satisfactory, and it is likely that the asymptotic results will apply nicely. Convergence requiring many iterations is a symptom that the asymptotic results may not apply, and other adequacy checks should be considered.
2. Several measures of model curvature and nonlinearity have been developed. This is discussed by Bates and Watts [1988]. These measures describe quantitatively the adequacy of the linear approximation. Once again, an inadequate linear approximation would indicate that the asymptotic inference results are questionable.
3. In Chapter 15 will illustrate a resampling technique called the bootstrap that can be used to study the sampling distribution of estimators, to compute approximate standard errors, and to find approximate confidence intervals. We could compute bootstrap estimates of these quantities and compare them to the approximate standard errors and confidence intervals produced by the asymptotic results. Good agreement with the bootstrap estimates is an indication that the large-sample inference results are valid.

When there is some indication that the asymptotic inference results are not valid, the model-builder has few choices. One possibility is to consider an alternate form of the model, if one exists, or perhaps a different nonlinear regression model. Sometimes, graphs of the data and graphs of different nonlinear model expectation functions may be helpful in this regard. Alternatively, one may use the inference results from resampling or the bootstrap. However, if the model is wrong or poorly specified, there is little reason to believe that resampling results will be any more valid than the results from large-sample inference.

12.7 EXAMPLES OF NONLINEAR REGRESSION MODELS
Ideally a nonlinear regression model is chosen based on theoretical considerations from the subject-matter field.That is, specific chemical, physical, or biological knowledge leads to a mechanistic model for the expectation function rather than an

412

INTRODUCTION TO NONLINEAR REGRESSION

empirical one. Many nonlinear regression models fall into categories designed for specific situations or environments. In this section we discuss a few of these models.
Perhaps the best known category of nonlinear models are growth models. These models are used to describe how something grows with changes in a regressor variable. Often the regressor variable is time. Typical applications are in biology, where plants and organisms grow with time, but there are also many applications in economics and engineering. For example, the reliability growth in a complex system over time may often be described with a nonlinear regression model.
The logistic growth model is

y=

1

+

1 + 2 exp(-3x)

(12.34)

The parameters in this model have a simple physical interpretation. For x = 0, y = 1/ (1 + 2) is the level of y at time (or level) zero. The parameter 1 is the limit to growth as x  . The values of 2 and 3 must be positive. Also, the term -3x in the denominator exponent of Eq. (12.34) could be replaced by a more general structure in several regressors. The logistic growth model is essentially the model given by Eq. (12.7) derived in Example (12.1).
The Gompertz model given by

y = 1 exp(-2e-3x ) + 

(12.35)

is another widely used growth model. At x = 0 we have y = 1e-3 and 1 is the limit to growth as x  .
The Weibull growth model is

y = 1 -2 exp(-3x4 ) + 

(12.36)

When x = 0, we have y = 1 - 2, while the limiting growth is 1 as x  . In some applications the expected response is given by the solution to a set of
linear differential equations. These models are often called compartment models, and since chemical reactions can frequently be described by linear systems of firstorder differential equations, they have frequent application in chemistry, chemical engineering, and pharmacokinetics. Other situations specify the expectation function as the solution to a nonlinear differential equation or an integral equation that has no analytic solution. There are special techniques for the modeling and solution of these problems. The interested reader is referred to Bates and Watts [1988].

12.8 USING SAS AND R
SAS developed PROC NLIN to perform nonlinear regression analysis. Table 12.4 gives the source code to analyze the puromycin data introduced in Example 12.4. The statement PROC NLIN tells the software that we wish to perform a nonlinear regression analysis. By default, SAS uses the Gauss­Newton method to find the parameter estimates. If the Gauss­Newton method has problems converging to final estimates, we suggest using Marquardt's compromise. The appropriate SAS command to request the Marquardt compromise is

USING SAS AND R 413
TABLE 12.4 SAS Code for Puromycin Data Set
data puromycin; input x y; cards; 0.02 76 0.02 47 0.06 97 0.06 107 0.11 123 0.11 139 0.22 159 0.22 152 0.56 191 0.56 201 1.10 207 1.10 200 proc nlin; parms tl = 195.81
t2 = 0.04841; model y = tl*x/ (t2 + x); der.tl = x/ (t2 + x); der.t2 = -tl*x/ ((t2 + x) * (t2 + x)); output out = puro2 student = rs p = yp; run; goptions device = win hsize = 6 vsize = 6; symbol value = star; proc gplot data = puro2; plot rs*yp rs*x; plot y*x= " * " yp*x= " + " /overlay; run; proc capability data = puro2; var rS; qqplot rs; run;
proc nlin method = marquardt;
The parms statement specifies the names for the unknown parameters and gives the starting values for the parameter estimates. We highly recommend the use of specific starting values for the estimation procedure, especially if we can linearize the expectation function. In this particular example, we have used the solutions for the estimated parameters found in Example 12.2 when we linearized the model. SAS allows a grid search as an alternative. Please see the SAS help menu for more details. The following statement illustrates how to initiate a grid search in SAS for the puromycin data:
parms tl = 190 to 200 by 1 t2 = 0.04 to 0.05 by .01;

414

INTRODUCTION TO NONLINEAR REGRESSION

The model statement gives the specific model. Often, our nonlinear models are sufficiently complicated that it is useful to define new variables to simplify the model expression. The Michaelis­Menten model is simple enough that we do not require new variables. However, the following statements illustrate how we could define these variables. These statements must come between the parms and model statements.

denom = X + t2; model y = t1*x/denom;

The two statements that begin with der. are the derivatives of the expectation function with regard to the unknown parameters. der.t1 is the derivative with respect to 1, and der.t2 is the derivative with respect to 2. We can specify these derivatives using any variables that we had defined in order to simplify the expression of the model. We highly recommend specifying these derivatives because the efficiency of the estimation algorithm often depends heavily upon this information. SAS does not require the derivative information; however, we strongly recommend it.
The output statement tells SAS what information we wish to add to the original puromycin data set. In this example, we add residual information so that we can create "nice" residual plots. The portion out = puro2 names the resulting data set puro2. The portion student = rs tells SAS to add the studentized residuals to puro2 and call them rs. Similarly, the portion p = yp tells SAS to add the predicted values and call them yp. See both Appendix D.4 and the SAS help menu for more background on the output statement.
The remainder of the code is very similar to the code used to generate nice residual plots for linear regression that we illustrated in Section 4.2.3. This section of code produces the residual-versus-predicted-value plot, the residual-versusregressor plot, an overlay of the original data and the predicted values, and a normal probability plot of the residuals. These plots are not shown. An annotated version of the resulting SAS output file is given in Table 12.5. We only want the normal probability plot of the residuals from the PROC CAPABILITY analysis.
We now outline the appropriate R code to analyze the puromycin data. This analysis assumes that the data are in a file named "puromycin.txt." The R code to read the data into the package is:

puro <- read.table("puromycin.txt",header=TRUE, sep="")

The object puro is the R data set. The commands

puro.model<-nls(yt1*x/(t2+x),start=list(t1=205,t2=.08),data =puro)
summary(puro.model)

tell R to estimate the model and to print the estimated coefficients and their tests. The commands

yhat <- fitted(puro.model) e <- residuals(trans.model)

USING SAS AND R 415

TABLE 12.5 SAS Output for Purtomycin Data

The NLIN Procedure Dependent Variable y Method: Gauss­Newton

Iter 0 1 2 3 4 5

Iterative Phase

Sum of

t1

t2

Squares

195.8

0.0484

1920.0

210.9

0.0614

1207.0

212.5

0.0638

1195.6

212.7

0.0641

1195.5

212.7

0.0641

1195.4

212.7

0.0641

1195.4

NOTE: Convergence criterion met.

Estimation Summary

Method

Gauss­Newton

Iterations

5

R

9.867E-6

PPC(t2)

4.03E-6

RPC(t2)

0.000042

Object

1.149E-8

Objective

1195.449

Observations Read

12

Observations Used

12

Observations Missing

0

NOTE: An intercept was not specified for this model.

Source

Sum of

Mean

Approx

DF

Squares

Square

F Value

Pr > F

Model

2

Error

10

Uncorrected

Total

12

270214 1195.4
271409

135107 119.5

1130.18

<.0001

Parameter
t1 t2

Approx Estimate Std Error Approximate 95% Confidence Limits

212.7 0.0641

6.9471 0.00828

197.2 0.0457

The SAS System 2

The NLIN Procedure

Approximate Correlation Matrix

t1

t2

t1 1.0000000 0.7650834

t2 0.7650834 1.0000000

228.2 0.0826

416

INTRODUCTION TO NONLINEAR REGRESSION

qqnorm(e) plot(yhat,e) plot(puro$x,t)

set up and then create the appropriate residual plots. The commands

puro2 <- cbind(puro,yhat,e) write.table(puro2, "puromycin_output.txt")

create a file "puromycin_output.txt" which the user than can import into his/her favorite package for doing graphics.

PROBLEMS

12.1 Consider the Michaelis­Menten model introduced in Eq. (12.23). Graph the expectation function for this model for 1 = 200 and 2 = 0.04, 0.06, 0.08, 0.10. Overlay these curves on the same set of x­y axes. What effect does the parameter 2 have on the behavior of the expectation function?
12.2 Consider the Michaelis­Menten model introduced in Eq. (12.23). Graph the expectation function for 1 = 100, 150, 200, 250 for 2 = 0.06. Overlay these curves on the same set of x­y axes. What effect does the parameter 1 have on the behavior of the expectation function?

12.3 Graph the expectation function for the logistic growth model (12.34) for 1 = 10, 2 = 2, and values of 3 = 0.25, 1, 2, 3, respectively. Overlay these plots on the same set of x­y axes. What effect does the parameter 3 have on the expectation function?

12.4 Sketch the expectation function for the logistic growth model (12.34) for 1 = 1, 3 = 1, and values of 2 = 1, 4, 8, respectively. Overlay these plots on the same x­y axes. Discuss the effect of 2 on the shape of the function.

12.5 Consider the Gompertz model in Eq. (12.35). Graph the expectation func-

tion

for

1

=

1,

3

=

1,

and

2

=

1 8

,

1,

8,

64

over

the

range

0



x



10.

a. Discuss the behavior of the model as a function of 2.

b. Discuss the behavior of the model as x  .

c. What is E(y) when x = 0?

12.6 For the models shown below, determine whether it is a linear model, an

intrinsically linear model, or a nonlinear model. If the model is intrinsically

linear, show how it can be linearized by a suitable transformation.

a.

y

=

 e2 +3x 1

+



b. y = 1 + 2 x1 + 2 x23 + 

c. y = 1 + 2/1x + 
d. 1 ( x1 )2 ( x2 )3 + 

e. 1 + 2e3x + 

PROBLEMS 417
12.7 Reconsider the regression models in Problem 12.6, parts a­e. Suppose the error terms in these models were multiplicative, not additive. Rework the problem under this new assumption regarding the error structure.
12.8 Consider the following observations:

x

y

0.5

0.68

1.58

1

0.45

2.66

2

2.50

2.04

4

6.19

7.85

8

56.1

54.2

9

89.8

90.2

10

147.7

146.3

a. Fit the nonlinear regression model

y = 1e2x + 
to these data. Discuss how you obtained the starting values. b. Test for significance of regression. c. Estimate the error variance 2. d. Test the hypotheses H0: 1 = 0 and H0: 2 = 0. Are both model parameters
different from zero? If not, refit an appropriate model. e. Analyze the residuals from this model. Discuss model adequacy.

12.9 Reconsider the data in the previous problem. The response measurements in the two columns were collected on two different days. Fit a new model

y = 3x2 + 1e2x1 + 
to these data, where x1 is the original regressor from Problem 12.8 and x2 is an indicator variable with x2 = 0 if the observation was made on day 1 and x2 = 1 if the observation was made on day 2. Is there any indication that there is a difference between the two days (use 30 = 0 as the starting value).

12.10 Consider the model

y = 1 -2e-3x + 

This is called the Mitcherlich equation, and it is often used in chemical engineering. For example, y may be yield and x may be reaction time.
a. Is this a nonlinear regression model?
b. Discuss how you would obtain reasonable starting values of the parameters 1, 2, and 3.
c. Graph the expectation function for the parameter values 1 = 0.5, 2 = -0.10, and 3 = 0.10. Discuss the shape of the function.
d. Graph the expectation function for the parameter values 1 = 0.5, 2 = 0.10, and 3 = 0.10. Compare the shape with the one obtained in part c.

418

INTRODUCTION TO NONLINEAR REGRESSION

12.11 The data below represent the fraction of active chlorine in a chemical product as a function of time after manufacturing.

Available Chlorine, yi
0.49, 0.49 0.48, 0.47, 0.48, 0.47 0.46, 0.46, 0.45, 0.43 0.45, 0.43, 0.43 0.44, 0.43, 0.43 0.46, 0.45 0.42, 0.42, 0.43 0.41, 0.41, 0.40 0.42, 0.40, 0.40 0.41, 0.40, 0.41 0.41, 0.40 0.40, 0.40, 0.38 0.41, 0.40 0.40 0.41, 0.38 0.40, 0.40 0.39 0.39

Time, xi
8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42

a. Construct a scatterplot of the data.
b. Fit the Mitcherlich law (see Problem 12.10) to these data. Discuss how you obtained the starting values.
c. Test for significance of regression. d. Find approximate 95% confidence intervals on the parameters 1, 2, and
3. Is there evidence to support the claim that all three parameters are different from zero?
e. Analyze the residuals and comment on model adequacy.

12.12 Consider the data below.

x2

50

75

100

4.70

5.52

3.98

2

2.68

3.75

4.22

6.35

5.88

6.28

x1 4

6.10

7.69

7.12

7.85

9.00

11.43

6

9.25

9.78

9.62

These data were collected in an experiment where x1 = reaction time in minutes and x2 = temperature in degrees Celsius. The response variable y is concentration (grams per liter). The engineer is considering the model
y = 1 ( x1 )2 ( x2 )3 + 

PROBLEMS 419

a. Note that we can linearize the expectation function by taking logarithms. Fit the resulting linear regression model to the data.
b. Test for significance of regression. Does it appear that both variables x1 and x2 have important effects?
c. Analyze the residuals and comment on model adequacy.

12.13

Continuation of Problem 12.12
a. Fit the nonlinear model given in Problem 12.12 using the solution you obtained by linearizing the expectation function as the starting values.
b. Test for significance of regression. Does it appear that both variables x1 and x2 have important effects?
c. Analyze the residuals and comment on model adequacy.
d. Which model do you prefer, the nonlinear model or the linear model from Problem 12.12?

12.14 Continuation of Problem 12.12. The two observations in each cell of the data table in Problem 12.12 are two replicates of the experiment. Each replicate was run from a unique batch of raw material. Fit the model

y = 4 x3 + 1 ( x1 )2 ( x2 )3 + 

where x3 = 0 if the observation comes from replicate 1 and x3 = 1 if the observation comes from replicate 2. Is there an indication of a difference between the two batches of raw material?
12.15 The following table gives the vapor pressure of water for various temperatures, previously reported in Exercise 5.2.

Temperature (°K)
273 283 293 303 313 323 333 343 353 363 373

Vapor Pressure (mm Hg)
4.6 9.2 17.5 31.8 55.3 92.5 149.4 233.7 355.1 525.8 760.0

a. Plot a scatter diagram. Does it seem likely that a straight-line model will be adequate?
b. Fit the straight-line model. Compute the summary statistics and the residual plots. What are your conclusions regarding model adequacy?
c. From physical chemistry the Clausius­Clapeyron equation states that

ln

(

pv

)



-

1 T

420

INTRODUCTION TO NONLINEAR REGRESSION

Repeat part b using the appropriate transformation based on this information. d. Fit the appropriate non-linear model. e. Discuss the differences in these models. Discuss which model you prefer.
12.16 The following data were collected on specific gravity and spectrophotometer analysis for 26 mixtures of NG (nitroglycerine), TA (triacetin), and 2 NDPA (2-nitrodiphenylamine).

Mixture
1 2 3 4 5
6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

xl (% NG)
79.98 80.06 80.10 77.61 77.60
77.63 77.34 75.02 75.03 74.99 74.98 72.50 72.50 72.50 72.49 69.98 69.98 69.99 69.99 67.51 67.50 67.48 67.49 64.98 64.98 64.99

x2 (% TA)
19.85 18.91 16.87 22.36 21.38
20.35 19.65 24.96 23.95 22.99 22.00 27.47 26.48 25.48 24.49 29.99 29.00 27.99 26.99 32.47 31.47 30.50 29.49 34.00 33.00 31.99

x3 (% 2 NDPA)
0 1.00 3.00 0 1.00
2.00 2.99 0 1.00 2.00 3.00 0 1.00 2.00 3.00 0 1.00 2.00 3.00 0 1.00 2.00 3.00 1.00 2.00 3.00

y (Specific Gravity)
1.4774 1.4807 1.4829 1.4664 1.4677
1.4686 1.4684 1.4524 1.4537 1.4549 1.4565 1.4410 1.4414 1.4426 1.4438 1.4279 1.4287 1.4291 1.4301 1.4157 1.4172 1.4183 1.4188 1.4042 1.4060 1.4068

Source: Raymond H. Myers, Technometrics, vol. 6, no. 4 (November 1964): 343­356.

There is a need to estimate activity coefficients from the model

y=

1

=

1x1 + 1x2 + 3x3

The quantity parameters 1, 2, and 3 are ratios of activity coefficients to the individual specific gravity of the NG, TA, and 2 NDPA, respectively. a. Determine starting values for the model parameters. b. Use nonlinear regression to fit the model. c. Investigate the adequacy of the nonlinear model.

CHAPTER 13
GENERALIZED LINEAR MODELS
13.1 INTRODUCTION
In Chapter 5, we developed and illustrated data transformation as an approach to fitting regression models when the assumptions of a normally distributed response variable with constant variance are not appropriate. Transformation of the response variable is often a very effective way to deal with both response nonnormality and inequality of variance. Weighted least squares is also a potentially useful way to handle the non-constant variance problem. In this chapter, we present an alternative approach to data transformation when the "usual" assumptions of normality and constant variance are not satisfied. This approach is based on the generalized linear model (GLM).
The GLM is a unification of both linear and nonlinear regression models that also allows the incorporation of nonnormal response distributions. In a GLM, the response variable distribution must only be a member of the exponential family, which includes the normal, Poisson, binomial, exponential, and gamma distributions as members. Furthermore, the normal-error linear model is just a special case of the GLM, so in many ways, the GLM can be thought of as a unifying approach to many aspects of empirical modeling and data analysis.
We begin our presentation of these models by considering the case of logistic regression. This is a situation where the response variable has only two possible outcomes, generically called success and failure and denoted by 0 and 1. Notice that the response is essentially qualitative, since the designation success or failure is entirely arbitrary. Then we consider the situation where the response variable is a count, such as the number of defects in a unit of product or the number of relatively
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
421

422 GENERALIZED LINEAR MODELS
rare events such as the number of Atlantic hurricanes that make landfall on the United States in a year. Finally, we discuss how all these situations are unified by the GLM. For more details of the GLM, refer to Myers, Montgomery, Vining, and Robinson [2010].

13.2 LOGISTIC REGRESSION MODELS

13.2.1 Models with a Binary Response Variable
Consider the situation where the response variable in a regression problem takes on only two possible values, 0 and 1. These could be arbitrary assignments resulting from observing a qualitative response. For example, the response could be the outcome of a functional electrical test on a semiconductor device for which the results are either a success, which means the device works properly, or a failure, which could be due to a short, an open, or some other functional problem.
Suppose that the model has the form

yi = xib + i

(13.1)

where xi = [1, xi1, xi2, ... , xik ],  = [0, 1, 2, . . . , k], and the response variable yj,
takes on the value either 0 or 1. We will assume that the response variable yi is a Bernoulli random variable with probability distribution as follows:

yi

Probability

1

P(yi = 1) = i

0

P(yi = 0) = 1 - i

Now since E(i) = 0, the expected value of the response variable is

This implies that

E ( yi ) = 1(i ) + 0(1 - i ) = i E ( yi ) = xib = i

This means that the expected response given by the response function E ( yi ) = xib
is just the probability that the response variable takes on the value 1. There are some very basic problems with the regression model in Eq. (13.1). First,
note that if the response is binary, then the error terms i can only take on two values, namely,
i = 1 - xib when yi = 1
i = -xib when yi = 0

LOGISTIC REGRESSION MODELS

423

Consequently, the errors in this model cannot possibly be normal. Second, the error variance is not constant, since

2 yi

= E{yi

- E ( yi )}2

= (1 - i )2 i

+ (0 - i )2 (1 - i ) = i (1 - i )

Notice that this last expression is just

2 yi

=

E ( yi )[1 - E ( yi )]

since E ( yi ) = xib = i. This indicates that the variance of the observations (which is
the same as the variance of the errors because i = yi - i, and i is a constant) is a
function of the mean. Finally, there is a constraint on the response function, because

0  E ( yi ) = i  1

This restriction can cause serious problems with the choice of a linear response function, as we have initially assumed in Eq. (13.1) It would be possible to fit a model to the data for which the predicted values of the response lie outside the 0, 1 interval.
Generally, when the response variable is binary, there is considerable empirical evidence indicating that the shape of the response function should be nonlinear. A monotonically increasing (or decreasing) S-shaped (or reverse S-shaped) function, such as shown in Figure 13.1, is usually employed. This function is called the logistic response function and has the form

E

(

y)

=

1

exp(xb ) + exp(xb

)

=

1

+

1
exp (

-xb

)

(13.2)

The logistic response function can be easily linearized. One approach defines the structural portion of the model in terms of a function of the response function mean. Let

 = xb

(13.3)

be the linear predictor where  is defined by the transformation

 = ln  1 -

(13.4)

This transformation is often called the logit transformation of the probability , and the ratio /(1 - ) in the transformation is called the odds. Sometimes the logit transformation is called the log-odds.

13.2.2 Estimating the Parameters in a Logistic Regression Model The general form of the logistic regression model is

424 GENERALIZED LINEAR MODELS

1.2 1
0.8 E(y) 0.6
0.4

1.2 1
0.8 E(y) 0.6
0.4

0.2

0.2

00

2

4

6

8 10 12 14 00

2

4

6

8 10 12 14

x

x

(a)

(b)

1 0.8 E (y) 0.6 0.4 0.2
00

1

0.8

E (y) 0.6 0.4

2

46 x1

8

10

0

2

4

6 8 10 x2

0.2 00

2

46 x1

8

10

0

2

4

6 8 10 x2

(c)

(d)

Figure 13.1 Examples of the logistic response function: (a) E(y) = 1/(1 + e-6.0 + 1.0x); (b) E(y) = 1/
( ) ( ) (1 + e-6.0 + 1.0x); (c) E ( y) = 1/ 1 + e-5.0 + 0.65x1 + 0.4x2 ; (d) E ( y) = 1/ 1 + e-5.0 + 0.65x1 + 0.4x2 + 0.15x1x2 .

yi = E ( yi ) + i

(13.5)

where the observations yi are independent Bernoulli random variables with expected values

E ( yi

)

=

i

=

exp(xib ) 1 + exp(xib

)

(13.6)

We use the method of maximum likelihood to estimate the parameters in the linear predictor xib.
Each sample observation follows the Bernoulli distribution, so the probability
distribution of each sample observation is

fi

(

yi

)

=



yi i

(1

-

i

)1- yi

,

i = 1, 2, ... , n

and of course each observation yi takes on the value 0 or 1. Since the observations are independent, the likelihood function is just

n

n

  L( y1, y2, ... , yn, b ) =

fi ( yi ) =

 yi i

(1

-

i

)1- yi

i=1

i=1

(13.7)

LOGISTIC REGRESSION MODELS

425

It is more convenient to work with the log-likelihood

n
 ln L( y1, y2, ... , yn, b ) = ln fi ( yi )

i=1

  =

n i=1

 

yi

ln

 

1

 -

i

 

+

n i=1

ln(1 - i )

Now since 1 - i = [1 + exp(xib )]-1 and i = ln[i / (1 - i )] = xib, the log-likelihood
can be written as

n

n

  ln L(y, b ) = yixib - ln[1 + exp(xib )]

i=1

i=1

(13.8)

Often in logistic regression models we have repeated observations or trials at each level of the x variables. This happens frequently in designed experiments. Let yi represent the number of 1's observed for the ith observation and ni be the number of trials at each observation. Then the log-likelihood becomes

n

n

n

   ln L(y, b ) = yi ln(i ) + ni ln(1 - i ) - yi ln(1 - i )

i=1

i=1

i=1

n

n

  = yi ln(i ) + (ni - yi )ln(1 - i )

i=1

i=1

(13.9)

Numerical search methods could be used to compute the maximum-likelihood estimates (or MLEs) b^ . However, it turns out that we can use iteratively reweighted least squares (IRLS) to actually find the MLEs. For details of this procedure, refer to Appendix C.14. There are several excellent computer programs that implement maximum-likelihood estimation for the logistic regression model, such as SAS PROC GENMOD, JMP and Minitab.
Let b^ be the final estimate of the model parameters that the above algorithm produces. If the model assumptions are correct, then we can show that asymptotically

( ) ( ) E b^ = b and Var b^ = (XVX)-1

(13.10)

where the matrix V is an n × n diagonal matrix containing the estimated variance of each observation on the main diagonal; that is, the ith diagonal element of V is
Vii = ni^i (1 - ^i )
The estimated value of the linear predictor is ^i = xib^, and the fitted value of the logistic regression model is written as

( ( ) ) ( ) y^i

=

^ i

=

exp (^i ) 1 + exp (^i )

=

exp xi b^ 1 + exp xi b^

=

1

+

1 exp

-xi b^

(13.11)

426 GENERALIZED LINEAR MODELS

Example 13.1 The Pneumoconiosis Data

A 1959 article in the journal Biometrics presents data concerning the proportion of coal miners who exhibit symptoms of severe pneumoconiosis and the number of years of exposure. The data are shown in Table 13.1. The response variable of interest, y, is the proportion of miners who have severe symptoms. A graph of the response variable versus the number of years of exposure is shown in Figure 13.2. A reasonable probability model for the number of severe cases is the binomial, so we will fit a logistic regression model to the data.
Table 13.2 contains some of the output from Minitab. In subsequent sections, we will discuss in more detail the information contained in this output. The section of the output entitled Logistic Regression Table presents the estimates of the regression coefficients in the linear predictor.
The fitted logistic regression model is

y^

= ^

=

1 1 + e+4.7965 - 0.0935x

where x is the number of years of exposure. Figure 13.3 presents a plot of the fitted values from this model superimposed on the scatter diagram of the sample data. The logistic regression model seems to provide a reasonable fit to the sample data. If we let CASES be the number of severe cases and MINERS be the number of miners the appropriate SAS code to analyze these data is

proc genmod; model CASES = MINERS / dist = binomial type1 type3;

Minitab will also calculate and display the covariance matrix of the model parameters. For the model of the pneumoconiosis data, the covariance matrix is

( ) Var

b^

=

 0.323283 -0.0083480

-0.0083480 0.0002380 

The standard errors of the model parameter estimates reported in Table 13.2 are

the square roots of the main diagonal elements of this matrix.



TABLE 13.1 The Pneumoconiosis Data

Number of Years of Exposure

Number of Severe Cases

5.8

0

15.0

1

21.5

3

27.5

8

33.5

9

39.5

8

46.0

10

51.5

5

Total Number of Miners
98 54 43 48 51 38 28 11

Proportion of Severe Cases, y
0 0.0185 0.0698 0.1667 0.1765 0.2105 0.3571 0.4545

0.5 0.45
0.4 0.35
0.3 y 0.25
0.2 0.15
0.1 0.05
0

20

40

60

Years of exposure, x

Figure 13.2 A scatter diagram of the pneumoconiosis data from Table 13.1.

LOGISTIC REGRESSION MODELS

427

0.6

0.5

=y

0.4

=y

>

y 0.3

0.2

0.1

0

0

20

40

60

Years of exposure, x

Figure 13.3 The fitted logistic regression model for pneumoconiosis data from Table 13.1.

TABLE 13.2 Binary Logistic Regression: Severe Cases, Number of Miners versus Years Link Function Logit Response Information

Variable Severe cases
Number of miners

Value Success Failure Total

Logistic Regression Table

Count 44 327 371

Predictor Constant Years

Coef

SE Coef

-4.79648 0.568580

0.0934629 0.0154258

Odds 95% CI

Z

P Ratio Lower Upper

-8.44

6.06 0.000 1.10 1.07 1.13

Log- Likelihood = -109.664 Test that all slopes are zero: G = 50.852, DF = 1, P-Value = 0.000

Goodness-of-Fit Tests

Method

Chi-Square

DF

Pearson

5.02854

6

Deviance

6.05077

6

Hosmer­Lemeshow 5.00360

5

P 0.540 0.418 0.415

Table of Observed and Expected Frequencies: (See Hosmer­Lemeshow Test for the Pearson Chi-Square Statistic)

Value Success
Obs Exp Failure Obs Exp Total

1
0 1.4
98 96.6
98

2
1 1.8
53 52.2
54

Group

3

4

3

8

2.5

4.7

40 40.5
43

40 43.3
48

5
9 8.1
42 42.9
51

6
8 9.5
30 28.5
38

7
15 16.1
24 22.9
39

Total 44
327 371

428 GENERALIZED LINEAR MODELS

13.2.3 Interpretation of the Parameters in a Logistic Regression Model It is relatively easy to interpret the parameters in a logistic regression model. Consider first the case where the linear predictor has only a single regressor, so that the fitted value of the linear predictor at a particular value of x, say xi, is
^ ( xi ) = ^0 + ^1xi
The fitted value at xi + 1 is
^ ( xi + 1) = ^0 + ^1 ( xi + 1)
and the difference in the two predicted values is
^ ( xi + 1) -^ ( xi ) = ^1
Now ^ ( xi ) is just the log-odds when the regressor variable is equal to xi, and ^ ( xi + 1)
is just the log-odds when the regressor is equal to xi + 1. Therefore, the difference in the two fitted values is

^ ( xi + 1) -^ ( xi ) = ln (oddsxi +1 ) - ln (oddsxi )

=

ln

 

oddsxi +1 oddsxi

 

=

^1

If we take antilogs, we obtain the odds ratio

O^ R

=

oddsxi +1 oddsxi

=

e ^1

(13.12)

The odds ratio can be interpreted as the estimated increase in the probability of success associated with a one-unit change in the value of the predictor variable. In general, the estimated increase in the odds ratio associated with a change of d units
( ) in the predictor variable is exp d^1 .

Example 13.2 The Pneumoconiosis Data In Example 13.1 we fit the logistic regression model

y^

=

1+

1 e +4.7965

- 0.0935x

to the pneumoconiosis data of Table 13.1. Since the linear predictor contains only one regressor variable and ^1 = 0.0935, we can compute the odds ratio from Eq. (13.12) as
O^ R = e^1 = e0.0935 = 1.10

LOGISTIC REGRESSION MODELS

429

This implies that every additional year of exposure increases the odds of contract-

ing a severe case of pneumoconiosis by 10%. If the exposure time increases by 10

( ) years, then the odds ratio becomes exp d^1 = exp[10(0.0935)] = 2.55. This indicates

that the odds more than double with a 10-year exposure.



There is a close connection between the odds ratio in logistic regression and the 2 × 2 contingency table that is widely used in the analysis of categorical data. Consider Table 13.3 which presents a 2 × 2 contingency table where the categorical response variable represents the outcome (infected, not infected) for a group of patients treated with either an active drug or a placebo. The nij are the numbers of patients in each cell. The odds ratio in the 2 × 2 contingency table is defined as

Proportion infected | active drug = n11 / n01 = n11  n00 Proportional infected | placebo n10 / n00 n10  n01

Consider a logistic regression model for these data. The linear predictor is

ln 

 1-



=

0

+

1x1

When x1 = 0, we have

0

=

ln

P(y P(y

= =

1| 0|

x1 x1

= =

0) 0)

Now let x1 = 1:

ln 

 1-



=

0

+

1x1

ln

P(y P(y

= =

1| 0|

x1 x1

= =

1) 1)

=

ln

P(y P(y

= =

1| 0|

x1 x1

= =

0) 0)

+

1

Solving for 1 yields

1

=

ln

P(y P(y

= =

1 | x1 0 | x1

= 1)  P ( y = 0 | = 1)  P ( y = 1 |

x1 x1

= =

0) 0)

=

ln

n11 n01

 n00  n10

TABLE 13.3 A 2 × 2 Contingency Table

Response

xl = 0, Active Drug

y = 0, not infected

n00

y = 1, infected

n10

xl = 1, Placebo
n01 n11

430 GENERALIZED LINEAR MODELS
so exp(1) is equivalent to the odds ratio in the 2 × 2 contingency table. However, the odds ratio from logistic regression is much more general than the traditional 2 × 2 contingency table odds ratio. Logistic regression can incorporate other predictor variables, and the presence of these variables can impact the odds ratio. For example, suppose that another variable, x2 = age, is available for each patient in the drug study depicted in Table 13.3. Now the linear predictor for the logistic regression model for the data would be

ln 

 1-



=

0

+

1x1

+

2 x2

This model allows the predictor variable age to impact the estimate of the odds ratio for the drug variable. The drug odds ratio is still exp(1), but the estimate of 1 is potentially affected by the inclusion of x2 = age in the model. It would also be possible to include an interaction term between drug and age in the model, say

ln 

 1-



=

0

+

1x1

+

2 x2

+

12 x1x2

In this model the odds ratio for drug depends on the level of age and would be computed as exp(1 + 12x2).
The interpretation of the regression coefficients in the multiple logistic regression model is similar to that for the case where the linear predictor contains only one
( ) regressor. That is, the quantity exp ^ is the odds ratio for regressor xj, assuming
that all other predictor variables are constant.

13.2.4 Statistical Inference on Model Parameters
Statistical inference in logistic regression is based on certain properties of maximumlikelihood estimators and on likelihood ratio tests. These are large-sample or asymptotic results. This section discusses and illustrates these procedures using the logistic regression model fit to the pneumoconiosis data from Example 13.1.
Likelihood Ratio Tests A likelihood ratio test can be used to compare a "full" model with a "reduced" model that is of interest. This is analogous to the "extra-sumof-squares" technique that we have used previously to compare full and reduced models. The likelihood ratio test procedure compares twice the logarithm of the value of the likelihood function for the full model (FM) to twice the logarithm of the value of the likelihood function of the reduced model (RM) to obtain a test statistic, say

LR

=

2

ln

L(FM ) L(RM )

=

2[ln

L(

FM

)

-

ln

L

(

RM

)]

(13.13)

For large samples, when the reduced model is correct, the test statistic LR follows a chi-square distribution with degrees of freedom equal to the difference in the

LOGISTIC REGRESSION MODELS

431

number of parameters between the full and reduced models. Therefore, if the test statistic LR exceeds the upper  percentage point of this chi-square distribution, we would reject the claim that the reduced model is appropriate.
The likelihood ratio approach can be used to provide a test for significance of regression in logistic regression. This test uses the current model that had been fit to the data as the full model and compares it to a reduced model that has constant probability of success. This constant-probability-of-success model is

E ( y)

=

=

e0 1 + e0

that is, a logistic regression model with no regressor variables. The maximumlikelihood estimate of the constant probability of success is just y/n, where y is the total number of successes that have been observed and n is the number of observations. Substituting this into the log-likelihood function in Equation (13.9) gives the maximum value of the log-likelihood function for the reduced model as

ln L(RM ) = y ln( y) + (n - y)ln(n - y) - n ln(n)

Therefore the likelihood ratio test statistic for testing significance of regression is

  LR

=

2

 

n

yiln^i +

n

(ni - yi ) ln (1 - ^i )

 i=1

i=1

-[y ln( y) + (n - y)ln(n - y) - n ln(n)]


(13.14)

A large value of this test statistic would indicate that at least one of the regressor variables in the logistic regression model is important because it has a nonzero regression coefficient.
Minitab computes the likelihood ratio test for significance of regression in logistic regression. In the Minitab output in Table 13.2 the test statistic in Eq. (13.14) is reported as G = 50.852 with one degree of freedom (because the full model has only one predictor). The reported P value is 0.000 (the default reported by Minitab when the calculated P value is less than 0.001).
Testing Goodness of Fit The goodness of fit of the logistic regression model can also be assessed using a likelihood ratio test procedure. This test compares the current model to a saturated model, where each observation (or group of observations when ni > 1) is allowed to have its own parameter (that is, a success probability). These parameters or success probabilities are yi/ni, where yi is the number of successes and ni is the number of observations. The deviance is defined as twice the difference in log-likelihoods between this saturated model and the full model (which is the current model) that has been fit to the data with estimated
( ) ( ) success probability ^i = exp xib^ / 1 + exp xib^ . The deviance is defined as

432 GENERALIZED LINEAR MODELS

 D

=

2 ln

L(saturated model) L(FM )

=

2

n i=1

 

yi

ln

 

yi ni i

 

+

( ni

-

yi

)

ln

 

ni - yi
ni (1 - ^i

)

 

(13.15)

In calculating the deviance, note that yln( y/n^ ) = 0 if y = 0, and if y = n we have (n - y)ln[(n - y) /n(1 - ^ )] = 0. When the logistic regression model is an adequate fit
to the data and the sample size is large, the deviance has a chi-square distribution with n - p degrees of freedom, where p is the number of parameters in the model. Small values of the deviance (or a large P value) imply that the model provides a satisfactory fit to the data, while large values of the deviance imply that the current model is not adequate. A good rule of thumb is to divide the deviance by its number of degrees of freedom. If the ratio D/(n - p) is much greater than unity, the current model is not an adequate fit to the data.
Minitab calculates the deviance goodness-of-fit statistic. In the Minitab output in Table 13.2, the deviance is reported under Goodness-of-Fit Tests. The value reported is D = 6.05077 with n - p = 8 - 2 = 6 degrees of freedom. The P value iis 0.418 and the ratio D/(n - p) is approximately unity, so there is no apparent reason to doubt the adequacy of the fit.
The deviance has an analog in ordinary normal-theory linear regression. In the linear regression model D = SSRes/2.This quantity has a chi-square distribution with n - p degrees of freedom if the observations are normally and independently distributed. However, the deviance in normal-theory linear regression contains the unknown nuisance parameter 2, so we cannot compute it directly. However, despite this small difference, the deviance and the residual sum of squares are essentially equivalent.
Goodness of fit can also be assessed with a Pearson chi-square statistic that compares the observed and expected probabilities of success and failure at each group of observations. The expected number of successes is ni^i and the expected number
of failures is ni (1 - ^i ), i = 1, 2, . . . , n. The Pearson chi-square statistic is

  2

=

n i=1

 

(

yi



- ni^i )2
ni^ i

+ [(ni

- yi ni

) - ni (1 - (1 - ni^i )

^

i

)]2

  

=

n i=1

( yi - ni^i ) ni^i (1 - ^i )

(13.16)

The Pearson chi-square goodness-of-fit statistic can be compared to a chi-square distribution with n - p degrees of freedom. Small values of the statistic (or a large P value) imply that the model provides a satisfactory fit to the data. The Pearson chi-square statistic can also be divided by the number of degrees of freedom n - p and the ratio compared to unity. If the ratio greatly exceeds unity, the goodness of fit of the model is questionable.
The Minitab output in Table 13.2 reports the Pearson chi-square statistic under Goodness-of-Fit Tests. The value reported is 2 = 6.02854 with n - p = 8 - 2 = 6 degrees of freedom. The P value is 0.540 and the ratio D/(n - p) does not exceed unity, so there is no apparent reason to doubt the adequacy of the fit.
When there are no replicates on the regressor variables, the observations can be grouped to perform a goodness-of-fit test called the Hosmer-Lemeshow test. In this procedure the observations are classified into g groups based on the estimated prob-

LOGISTIC REGRESSION MODELS

433

abilities of success. Generally, about 10 groups are used (when g = 10 the groups are
called the deciles of risk) and the observed number of successes Oj and failures Nj - Oj are compared with the expected frequencies in each group, N j j and
N j (1 -  j ), where Nj is the number of observations in the jth group and the average
estimated success probability in the jth group is  j = igroup j ^i /N j. The Hosmer­
Lemeshow statistic is really just a Pearson chi-square goodness-of-fit statistic com-
paring observed and expected frequencies:

 HL =

n j=1

(Oj - N j j )2 Nj j (1 -  j )

(13.17)

If the fitted logistic regression model is correct, the HL statistic follows a chisquare distribution with g - 2 degrees of freedom when the sample size is large. Large values of the HL statistic imply that the model is not an adequate fit to the data. It is also useful to compute the ratio of the Hosmer­Lemeshow statistic to the number of degrees of freedom g - p with values close to unity implying an adequate fit.
MINlTAB computes the Hosmer­Lemeshow statistic. For the pneumoconiosis data the HL statistic is reported in Table 13.2 under Goodness-of-Fit Tests. This computer package has combined the data into g = 7 groups. The grouping and calculation of observed and expected frequencies for success and failure are reported at the bottom of the MINlTAB output. The value of the test statistic is HL = 5.00360 with g - p = 7 - 2 = 5 degrees of freedom. The P value is 0.415 and the ratio HL/df is very close to unity, so there is no apparent reason to doubt the adequacy of the fit.

Testing Hypotheses on Subsets of Parameters Using Deviance We can also use the deviance to test hypotheses on subsets of the model parameters, just as we used the difference in regression (or error) sums of squares to test similar hypotheses in the normal-error linear regression model case. Recall that the model can be written as

h = Xb = X1b1 + X2b2

(13.18)

where the full model has p parameters, 1 contains p - r of these parameters, 2 contains r of these parameters, and the columns of the matrices X1 and X2 contain the variables associated with these parameters.
The deviance of the full model will be denoted by D(). Suppose that we wish to test the hypotheses

H0 : b2 = 0, H1 : b2  0

(13.19)

Therefore, the reduced model is

h = X1b1

(13.20)

434 GENERALIZED LINEAR MODELS
Now fit the reduced model, and let D(1) be the deviance for the reduced model. The deviance for the reduced model will always be larger than the deviance for the full model, because the reduced model contains fewer parameters. However, if the deviance for the reduced model is not much larger than the deviance for the full model, it indicates that the reduced model is about as good a fit as the full model, so it is likely that the parameters in 2 are equal to zero. That is, we cannot reject the null hypothesis above. However, if the difference in deviance is large, at least one of the parameters in 2 is likely not zero, and we should reject the null hypothesis. Formally, the difference in deviance is

D(b2 | b1 ) = D(b1 ) - D(b )

(13.21)

and this quantity has n - (p - r) - (n - p) = r degrees of freedom. If the null hypothesis is true and if n is large, the difference in deviance in Eq. (13.21) has a chi-square distribution with r degrees of freedom. Therefore, the test statistic and decision criteria are

if D(b2 | b1 )  2,r reject the null hypothesis if D(b2 | b1 ) < 2,r do not reject the null hypothesis

(13.22)

Sometimes the difference in deviance D(2|1) is called the partial deviance.

Example 13.3 The Pneumoconiosis Data
Once again, reconsider the pneumoconiosis data of Table 13.1. The model we initially fit to the data is

y^

= ^

=

1 1 + e+4.7965 - 0.0935x

Suppose that we wish to determine whether adding a quadratic term in the linear predictor would improve the model. Therefore, we will consider the full model to be

y

=

1+

e-(0

1
+ 1x

+

) 11x2

Table 13.4 contains the output from Minitab for this model. Now the linear predictor for the full model can be written as

 = Xb = X1b1 + X2b2 = 0 + 1x + 11x2

435

TABLE 13.4 Binary Logistic Regression: Severe Cases, Number of Miners versus Years

Link Function Logit

Response Information Variable Severe cases
Number of miners

Value Success Failure Total

Count 44
327 371

Logistic Regression Table

Predictor Constant Years Years*Years

Coef -6.71079
0.227607 -0.0020789

SE Coef 1.53523 0.0927560 0.0013612

Z -4.37
2.45 -1.53

P 0.000 0.014 0.127

Odds Ratio
1.26 1.00

Log-Likelihood=-108.279 Test that all slopes are zero: G = 53.621, DF = 2, P-value = 0.000

Goodness-of-Fit Tests

Method Pearson Deviance Hosmer­Lemeshow

Chi-Square 2.94482 3.28164 2.80267

DF

P

5

0.708

5

0.657

5

0.730

95% Lower
1.05 1.00

Table of Observed and Expected Frequencies: (See Hosmer­Lemeshow Test for the Pearson Chi-Square Statistic)

Value Success
Obs Exp Failure Obs Exp Total

1
0 0.4
98 97.6
98

2
1 1.2
53 52.8
54

3
3 2.5
40 40.5
43

Group 4
8 5.6
40 42.4
48

5
9 9.9
42 41.1
51

6
8 10.5
30 27.5
38

7
15 13.8
24 25.2
39

CI Upper 1.51 1.00
Total 44 327 371

436 GENERALIZED LINEAR MODELS From Table 13.4, we find that the deviance for the full model is

D(b ) = 3.28164

with n - p = 8 - 3 = 5 degrees of freedom. Now the reduced model has Xl1 = 0 + 1x, so X22 = 11x2 with r = 1 degree of freedom. The reduced model was originally fit in Example 13.1, and Table 13.2 shows the deviance for the reduced
model to be

D(b1 ) = 6.05077

with p - r = 3 - 1 = 2 degrees of freedom. Therefore, the difference in deviance between the full and reduced models is computed using Eq. (13.21) as

D(b2 | b1 ) = D(b1 ) - D(b )
= 6.05077 - 3.28164 = 2.76913

which would be referred to a chi-square distribution with r = 1 degree of freedom.

Since the P value associated with the difference in deviance is 0.0961, we might

conclude that there is some marginal value in including the quadratic term in the

regressor variable x = years of exposure in the linear predictor for the logistic

regression model.



Tests on Individual Model Coefficients Tests on individual model coefficients, such as

H0 :  j = 0, H1 :  j  0

(13.22)

can be conducted by using the difference-in-deviance method as illustrated in Example 13.3.There is another approach, also based on the theory of maximum likelihood estimators. For large samples, the distribution of a maximum-likelihood estimator is approximately normal with little or no bias. Furthermore, the variances and covariances of a set of maximum-likelihood estimators can be found from the second partial derivatives of the log-likelihood function with respect to the model parameters, evaluated at the maximum-likelihood estimates. Then a t-like statistic can be constructed to test the above hypotheses. This is sometimes referred to as Wald inference.
Let G denote the p × p matrix of second partial derivatives of the log-likelihood function, that is,

Gij

=

2L(b ) ,
i j

i, j = 0, 1, ... , k

LOGISTIC REGRESSION MODELS

437

G is called the Hessian matrix. If the elements of the Hessian are evaluated at the maximum-likelihood estimators b = b^ , the large-sample approximate covariance matrix of the regression coefficients is

( ) ( ) Var b^ = -G b^ -1 = (XVX)-1

(13.23)

Notice that this is just the covariance matrix of b^ given earlier. The square roots of the diagonal elements of this covariance matrix are the large-sample standard errors of the regression coefficients, so the test statistic for the null hypothesis in

H0 :  j = 0, H1 :  j  0

is

( ) Z0

=

^ j se ^j

(13.24)

The reference distribution for this statistic is the standard normal distribution. Some computer packages square the Z0 statistic and compare it to a chi-square distribution with one degree of freedom.

Example 13.4 The Pneumoconiosis Data

Table 13.3 contains output from MlNITAB for the pneumoconiosis data, originally given in Table 13.1. The fitted model is

y^

=

1+

e +6.7108

1
- 0.2276 x

+

0.0021x2

The Minitab output gives the standard errors of each model coefficient and the Z0 test statistic in Eq. (13.24). Notice that the P value for 1 is P = 0.014, implying that

years of exposure is an important regressor. However, notice that the P value for

1 is P = 0.127, suggesting that the squared term in years of exposure does not con-

tribute significantly to the fit.

Recall from the previous example that when we tested for the significance of 11

using the partial deviance method we obtained a different P value. Now in linear

regression, the t test on a single regressor is equivalent to the partial F test on a

single variable (recall that the square of the t statistic is equal to the partial F sta-

tistic). However, this equivalence is only true for linear models, and the GLM is a

nonlinear model.



Confidence Intervals It is straightforward to use Wald inference to construct confidence intervals in logistic regression. Consider first finding confidence intervals on individual regression coefficients in the linear predictor. An approximate 100(1 - ) percent confidence interval on the jth model coefficient is

( ) ( ) ^j - Z /2se ^j   j  ^j + Z /2se ^j

(13.25)

438 GENERALIZED LINEAR MODELS

Example 13.5 The Pneumoconiosis Data

Using the Minitab output in Table 13.3, we can find an approximate 95% confidence interval on 11 from Eq. (13.25) as follows:

( ) ( ) ^11 - Z0.025se ^11  11  ^11 + Z0.025se ^11

-0.0021 - 1.96(0.00136)  11  -0.0021 + 1.96(0.00136)

-0.0048  11  0.0006



Notice that the confidence interval includes zero, so at the 5% significance level, we
would not reject the hypothesis that this model coefficient is zero. The regression coefficient j is also the logarithm of the odds ratio. Because we know how to find a confidence interval (CI) for j, it is easy to find a CI for the odds ratio. The point
( ) estimate of the odds ratio is O^ R = exp ^j and the 100(1 - ) percent CI for the
odds ratio is

( ) ( ) exp ^j - Z /2se ^j   OR  exp ^j + Z /2se ^j 

(13.26)

The CI for the odds ratio is generally not symmetric around the point estimate.
( ) Furthermore, the point estimate O^ R = exp ^j actually estimates the median of the
sampling distribution of O^ R.

Example 13.6 The Pneumoconiosis Data

Reconsider the original logistic regression model that we fit to the pneumoconiosis

data in Example 13.1. From the Minitab output for this data shown in Table 13.2

( ) we find that the estimate of 1 is ( ) Because the standard error of

^1 ^1

= is

0.0934629 and the odds se ^1 = 0.0154258, we

ratio O^ R can find

= a

exp 95%

^ j CI

= 1.10. on the

odds ratio as follows:

exp[0.0934629 - 1.96(0.0154258)]  OR  exp[0.0934629 - 1.96(0.0154258)] exp(0.063228)  OR  exp(0.123697)
1.07  OR  1.13

This agrees with the 95% CI reported by Minitab in Table 13.2.



It is possible to find a CI on the linear predictor at any set of values of the pre-
dictor variables that is of interest. Let x0 = [1, x01, x02, ... , x0k ] be the values of the
regressor variables that are of interest. The linear predictor evaluated at x0 is x0b^. The variance of the linear predictor at this point is
( ) ( ) Var x0b^ = x0Var b^ x0 = x0 (XVX)-1 x0
so the 100(1 - ) percent CI on the linear predictor is

LOGISTIC REGRESSION MODELS

439

x0b^ - Z /2 x0 (XVX)-1 x0  x0b  x0b^ + Z /2 x0 (XVX )-1 x0

(13.27)

The CI on the linear predictor given in Eq. (13.27) enables us to find a CI on the
estimated probability of success 0 at the point of interest x0 = [1, x01, x02, ... , x0k ]. Let

L(x0 ) = x0b^ - Z /2 x0 (XVX)-1 x0

and

U (x0 ) = x0b^ + Z /2 x0 (XVX)-1 x0

be the lower and upper 100(1 - ) percent confidence bounds on the linear predictor at the point x0 from Eq. (13.27). Then the point estimate of the probability of
( ) ( ) success at this point is ^0 = exp x0b^ / 1 + exp x0b^  and the 100(1 - ) percent CI
on the probability of success at x0 is

exp[L(x0 )] 1 + exp[L(x0 )]



0



exp[U (x0 )] 1 + exp[U (x0 )]

(13.28)

Example 13.7 The Pneumoconiosis Data
Suppose that we want to find a 95% CI on the probability of miners with x = 40 years of exposure contracting pneumoconiosis. From the fitted logistic regression model in Example 13.1, we can calculate a point estimate of the probability at 40 years of exposure as

^ 0

=

e-4.7965 + 0.0935(40) 1 + e-4.7965 + 0.0935(40)

=

e -1.0565 1 + e-1.0565

= 0.2580

To find the CI, we need to calculate the variance of the linear predictor at this point. The variance is

( ) Var x0b^ = x0 (XVX)-1 x0

= [1

40]

 0.32383 -0.0083480

-0.0083480 0.0002380 

1 40

=

0.036243

Now

L(x0 ) = -1.0565 - 1.96 0.036343 = -1.4296
and

U (x0 ) = -1.0565 + 1.96 0.036343 = -0.6834

440 GENERALIZED LINEAR MODELS

Therefore the 95% CI on the estimated probability of contracting pneumoconiosis for miners that have 40 years of exposure is

exp[L(x0 )] 1 + exp[L(x0 )]



0



exp[U (x0 )] 1 + exp[U (x0 )]

exp ( -1.4296 ) 1 + exp(-1.4296)



0



exp ( -0.6834 ) 1 + exp(-0.6834)

0.1932  0  0.3355



13.2.5 Diagnostic Checking in Logistic Regression
Residuals can be used for diagnostic checking and investigating model adequacy in logistic regression. The ordinary residuals are defined as usual,

ei = yi - y^i = yi - ni^i , i = 1, 2, ... , n

(13.29)

In linear regression the ordinary residuals are components of the residual sum of squares; that is, if the residuals are squared and summed, the residual sum of squares results. In logistic regression, the quantity analogous to the residual sum of squares is the deviance. This leads to a deviance residual, defined as

di

=

±

2 

 

yi

ln

 

yi ni^ i

 

+

(

ni

-

yi

)

ln

 

ni - yi
ni (1 - ^i

  1
) 

2
,

i = 1, 2, ... , n

(13.30)

The sign of the deviance residual is the same as the sign of the corresponding
ordinary residual. Also, when yi = 0, di = - -2nln (1 - ^i ), and when yi = ni,
di = -2nln^i . Similarly, we can define a Pearson residual

ri =

yi - ni^i ,
ni^i (1 - ^i )

i = 1, 2, ... , n

(13.31)

It is also possible to define a hat matrix analog for logistic regression,

H = V1 2X (XVX)-1 XV1 2

(13.32)

where V is the diagonal matrix defined earlier that has the variances of each obser-
vation on the main diagonal, Vii = ni^i (1 - ^i ), and these variances are calculated
using the estimated probabilities that result from the fitted logistic regression model.
The diagonal elements of H, hii, can be used to calculate a standardized Pearson residual

sri =

ri = 1 - hii

yi - ni^i

,

(1 - hii ) ni^i (1 - ^i )

i = 1, 2, ... , n

(13.33)

The deviance and Pearson residuals are the most appropriate for conducting model adequacy checks. Plots of these residuals versus the estimated probability and a

LOGISTIC REGRESSION MODELS

441

TABLE 13.5 Residuals for the Pneumoconiosis Data

Observation
1 2 3 4 5 6 7 8

Observed Probability
0.000000 0.018519 0.069767 0.166667 0.176471 0.210526 0.357143 0.454545

Estimated Probability
0.014003 0.032467 0.058029 0.097418 0.159029 0.248861 0.378202 0.504215

Deviance Residuals
-1.66251 -0.62795
0.31961 1.48516 0.33579 -0.55678 -0.23067 -0.32966

Pearson Residuals
-1.17973 -0.57831
0.32923 1.61797 0.34060 -0.54657 -0.22979 -0.32948

hii
0.317226 0.214379 0.174668 0.186103 0.211509 0.249028 0.387026 0.260001

Standardized Pearson Residuals
-1.42772 -0.65246
0.36239 1.79344 0.38358 -0.63072 -0.29350 -0.38301

Percent di

99

95 90
80 70 60 50 40 30 20
10 5
1 ­2

Mean ­0.1584

StDev 0.9142

N

8

AD

0.279

P Value 0.544

­1

0

1

2

di

Figure 13.4 Normal probability plot of the deviance residuals.

1.5

1.0

0.5

0.0

­0.0

­1.0

­1.5

­2.0

0.0

0.1

0.2

0.3

^i

0.4

0.5

Figure 13.5 Plot of deviance residuals versus estimated probabilities.

normal probability plot of the deviance residuals are useful in checking the fit of the model at individual data points and in checking for possible outliers.
Table 13.5 displays the deviance residuals, Pearson residuals, hat matrix diagonals, and the standardized Pearson residuals for the pneumoconiosis data. To illustrate the calculations, consider the deviance residual for the third observation. From Eq. (13.30)

d3

=

2 

 

y3

ln

 

y3 n3^ 3

 

+ (n3

-

y3

)

ln

 

n3 - y3
n3 (1 - ^3

)

  1   

2

=

+

2 

 

3

ln

 

43

(

3 0.058029

)

 

+

(

43

-

3

)

ln

 

43

(1

43 - 3 - 0.058029

)

 

 

1  

2

= 0.3196

which closely matches the value reported by Minitab in Table 13.5. The sign of the deviance residual d3 is positive because the ordinary residual e3 = y3 - n3^3 is positive.
Figure 13.4 is the normal probability plot of the deviance residuals and Figure 13.5 plots the deviance residuals versus the estimated probability of success. Both

442 GENERALIZED LINEAR MODELS
plots indicate that there may be some problems with the model fit. The plot of deviance residuals versus the estimated probability indicates that the problems may be at low estimated probabilities. However, the number of distinct observations is small (n = 8), so we should not attempt to read too much into these plots.

13.2.6 Other Models for Binary Response Data
In our discussion of logistic regression we have focused on using the logit, defined as ln[/(1 - )], to force the estimated probabilities to lie between zero and unity. This leads to the logistic regression model



=

exp(xb ) 1 + exp(xb

)

However, this is not the only way to model a binary response. Another possibility is to make use of the cumulative normal distribution, say -1(). The function -1() is called the Probit. A linear predictor can be related to the probit, x = -1(),
resulting in a regression model

 =  (xb )

(13.34)

Another possible model is provided by the complimentary log-log relationship log[-log(1 - ) = x. This leads to the regression model

 = 1 - exp[- exp(xb )]

(13.35)

A comparison of all three possible models for the linear predictor x = 1 + 5x is

shown in Figure 13.6. The logit and probit functions are very similar, except when

the estimated probabilities are very close to either 0 or 1. Both of these functions

have

estimated

probability



=

1 2

when

x

=

-0/1

and

exhibit

symmetric

behavior

around this value. The complimentary log-log function is not symmetric. In general,

it is very difficult to see meaningful differences between these three models when

sample sizes are small.

13.2.7 More Than Two Categorical Outcomes
Logistic regression considers the situation where the response variable is categorical, with only two outcomes. We can extend the classical logistic regression model to cases involving more than two categorical outcomes. First consider a case where there are m + 1 possible categorical outcomes but the outcomes are nominal. By this we mean that there is no natural ordering of the response categories. Let the outcomes be represented by 0, 1, 2, . . . , m. The probabilities that the responses on observation i take on one of the m + 1 possible outcomes can be modeled as

LOGISTIC REGRESSION MODELS

443

1.0

0.8 0.6

^

0.4

0.2

Variable Logit

Probit

Comp log-log

0.0

­1.0

­0.5

0.0

0.5

1.0

x

Figure 13.6 Logit, probit, and complimentary log-log functions for the linear predictor x = 1 + 5x.

P ( yi = 0) =

1
m

 1 + expxib ( j) 

j=i

P ( yi = 1) =

expxib (1) 
m

 1 + expxib ( j) 

j=i

P ( yi = m) =

expxib (m) 
m

 1 + expxib ( j) 

j=i

(13.36)

Notice that there are m parameter vectors. Comparing each response category to a "baseline" category produces logits

ln

P ( yi P ( yi

= =

1) 0)

=

xi b (1)

ln

P ( yi P ( yi

= =

2) 0)

=

xi b (2)

ln

P ( yi P ( yi

= m) = 0)

=

xi b (m)

(13.37)

where our choice of zero as the baseline category is arbitrary. Maximum-likelihood estimation of the parameters in these models is fairly straightforward and can be performed by several software packages.

444 GENERALIZED LINEAR MODELS
A second case involving multilevel categorical response is an ordinal response. For example, customer satisfaction may be measured on a scale as not satisfied, indifferent, somewhat satisfied, and very satisfied. These outcomes would be coded as 0, 1, 2, and 3, respectively. The usual approach for modeling this type of response data is to use logits of cumulative probabilities:

ln

1

P ( yi  k - P ( yi 

)
k

)

=

k

+

xi b ,

k = 0, 1, ... , m

The cumulative probabilities are

P(

yi



k)

=

exp(k + xib ) 1 + exp(k + xib

)

,

k = 0, 1, ... , m

This model basically allows each response level to have its own unique intercept. The intercepts increase with the ordinal rank of the category. Several software packages can also fit this variation of the logistic regression model.

13.3 POISSON REGRESSION

We now consider another regression modeling scenario where the response variable of interest is not normally distributed. In this situation the response variable represents a count of some relatively rare event, such as defects in a unit of manufactured product, errors or "bugs" in software, or a count of particulate matter or other pollutants in the environment. The analyst is interested in modeling the relationship between the observed counts and potentially useful regressor or predictor variables. For example, an engineer could be interested in modeling the relationship between the observed number of defects in a unit of product and production conditions when the unit was actually manufactured.
We assume that the response variable yi is a count, such that the observation yi = 0, 1, . . . . A reasonable probability model for count data is often the Poisson distribution

f ( y) = e-  y , y = 0, 1, ...
y!

(13.38)

where the parameter  > 0. The Poisson is another example of a probability distribution where the mean and variance are related. In fact, for the Poisson distribution it is straightforward to show that

E ( y) =  and Var ( y) = 

That is, both the mean and variance of the Poisson distribution are equal to the parameter .

POISSON REGRESSION

445

The Poisson regression model can be written as

yi = E ( yi ) + i , i = 1, 2, ... , n

(13.39)

We assume that the expected value of the observed response can be written as

E ( yi ) = i

and that there is a function g that relates the mean of the response to a linear predictor, say

g ( i ) = i = 0 + 1x1 + + k xk = xib

(13.40)

The function g is usually called the link function. The relationship between the mean and the linear predictor is

i = g-1 (i ) = g-1 (xib )

(13.41)

There are several link functions that are commonly used with the Poisson distribution. One of these is the identity link

g ( i ) = i = xib

(13.42)

When this link is used, E ( yi ) = i = xib since i = g-1 (xib ) = xib. Another popular
link function for the Poisson distribution is the log link

g ( i ) = ln ( i ) = xib

(13.43)

For the log link in Eq. (13.43), the relationship between the mean of the response variable and the linear predictor is

i = g-1 (xi b ) = exib

(13.44)

The log link is particularly attractive for Poisson regression because it ensures that all of the predicted values of the response variable will be nonnegative.
The method of maximum likelihood is used to estimate the parameters in Poisson regression. The development follows closely the approach used for logistic regression. If we have a random sample of n observations on the response y and the predictors x, then the likelihood function is

  L(y,  ) =

n i=1

fi ( yi ) =

n i=1

e

-



 yi i

yi !

  n

 yi i

 exp 

-

n

i

 

= i=1

 i=1 
n

 yi !

i=1

(13.45)

where i = g-1 (xib ). Once the link function is selected, we maximize the log-
likelihood

446 GENERALIZED LINEAR MODELS

n

n

n

   ln L(y, b ) = yi ln(i ) - i - ln( yi !)

i=1

i=1

i=1

(13.46)

Iteratively reweighted least squares can be used to find the maximum-likelihood
estimates of the parameters in Poisson regression, following an approach similar to that used for logistic regression. Once the parameter estimates b^ are obtained, the fitted Poisson regression model is

( ) y^i = g-1 xib^

(13.47)

For example, if the identity link is used, the prediction equation becomes
( ) y^i = g-1 xib^ = xib^

and if the log link is selected, then

( ) ( ) y^i = g-1 xib^ = exp xib^

Inference on the model and its parameters follows exactly the same approach as used for logistic regression. That is, model deviance and the Pearson chisquare statistic are overall measures of goodness of fit, and tests on subsets of model parameters can be performed using the difference in deviance between the full and reduced models. These are likelihood ratio tests. Wald inference, based on large-sample properties of maximum-likelihood estimators, can be used to test hypotheses and construct confidence intervals on individual model parameters.

Example 13.8 The Aircraft Damage Data
During the Vietnam War, the United States Navy operated several types of attack (a bomber in USN parlance) aircraft, often for low-altitude strike missions against bridges, roads, and other transportation facilities. Two of these included the McDonnell Douglas A-4 Skyhawk and the Grumman A-6 Intruder. The A-4 is a singleengine, single-place light-attack aircraft used mainly in daylight. It was also flown by the Blue Angels, the Navy's flight demonstration team, for many years. The A-6 is a twin-engine, dual-place, all-weather medium-attack aircraft with excellent day/night capabilities. However, the Intruder could not be operated from the smaller Essex-class aircraft carriers, many of which were still in service during the conflict.
Considerable resources were deployed against the A-4 and A-6, including small arms, AAA or antiaircraft artillery, and surface-to-air missiles. Table 13.6 contains data from 30 strike missions involving these two types of aircraft. The regressor x1 is an indicator variable (A-4 = 0 and A-6 = 1), and the other regressors

POISSON REGRESSION

447

TABLE 13.6 Aircraft Damage Data

Observation

y

xl

x2

1

0

0

4

2

1

0

4

3

0

0

4

4

0

0

5

5

0

0

5

6

0

0

5

7

1

0

6

8

0

0

6

9

0

0

6

10

2

0

7

11

1

0

7

12

1

0

7

13

1

0

8

14

1

0

8

15

2

0

8

16

3

1

7

17

1

1

7

18

1

1

7

19

1

1

10

20

2

1

10

21

0

1

10

22

1

1

12

23

1

1

12

24

2

1

12

25

5

1

8

26

1

1

8

27

1

1

8

28

5

1

14

29

5

1

14

30

7

1

14

x3
91.5 84.0 76.5 69.0 61.5 80.0 72.5 65.0 57.5 50.0 103.0 95.5 88.0 80.5 73.0 116.1 100.6 85.0 69.4 53.9 112.3 96.7 81.1 65.6 50.0 120.0 104.4 88.9 73.7 57.8

x2 and x3 are bomb load (in tons) and total months of aircrew experience. The response variable is the number of locations where damage was inflicted on the aircraft.
We will model the damage response as a function of the three regressors. Since the response is a count, we will use a Poisson regression model with the log link. Table 13.7 presents some of the output from SAS PROC GENMOD a widely used software package for fitting generalized linear models, which include Poisson regression. The SAS code for this example is
proc genmod; model y = xl x2 x3 / dist = poisson type1 type3;
The Type 1 analysis is similar to the Type 1 sum of squares analysis, also known as the sequential sum of squares analysis. The test on any given term is conditional

448 GENERALIZED LINEAR MODELS

TABLE 13.7 SAS PROC GENMOD Output for Aircraft Damage Data in Example 13.8

The GENMOD Procedure

Model Information

Description Data Set Distribution Link Function Dependent Variable Observations Used

Value WORK.PLANE POISSON LOG Y 30

Criteria for Assessing Goodness of Fit

Criterion

DF

Value

Deviance

26

28.4906

Scaled Deviance

26

28.4906

Pearson Chi-Square

26

25.4279

Scaled Pearson X2 Log Likelihood

26

25.4279

-11.3455

Va1ue/DF 1.0958 1.0958 0.9780 0.9780

Analysis of Parameter Estimates

Parameter DF Estimate Std Err Chi Square

INTERCEPT

1

-0.3824

0.8630

0.1964

Xl

1

0.8805

0.5010

3.0892

X2

1

0.1352

0.0653

4.2842

X3

1

-0.0127

0.0080

2.5283

SCALE

0

1.0000

0.0000

Note: The scale parameter was held fixed.

Pr > Chi 0.6577 0.0788 0.0385 0.1118

Source INTERCEPT Xl X2 X3

LR Statistics for Type 1 Analysis

Deviance

DF

Chi Square

57.5983

0

38.3497

1

19.2486

31.0223

1

7.3274

28.4906

1

2.5316

Pr > Chi
0.0001 0.0068 0.1116

Source Xl X2 X3

LR Statistics for Type 3 Analysis

DF

Chi Square

1

3.1155

1

4.3911

1

2.5316

Pr > Chi 0.0775 0.0361 0.1116

The GENMOD Procedure Model Information

Description Data Set Distribution Link Function Dependent Variable Observations Used

Value WORK.PLANE POISSON LOG Y 30

POISSON REGRESSION

449

TABLE 13.7 (Continued)

Criteria for Assessing Goodness of Fit

Criterion Deviance Scaled Deviance Pearson Chi-Square Scaled Pearson X2 Log Likelihood

DF

Value

Va1ue/DF

28

33.0137

1.1791

28

33.0137

1.1791

28

33.4108

1.1932

28

33.4108

-13.6071

1.1932

Analysis of Parameter Estimates

Parameter

DF Estimate Std Err

INTERCEPT

1

-1.6491

0.4996

X2

1

0.2282

0.0462

SCALE

0

1.0000

0.0000

Note: The scale parameter was held fixed.

Chi Square 10.8980 24.3904

Pr > Chi 0.0010 0.0001

Source INTERCEPT X2

LR Statistics for Type 1 Analysis

Deviance

DF

Chi Square

57.5983

0

33.0137

1

24.5846

Pr > Chi 0.0001

Source X2

LR Statistics for Type 3 Analysis

DF

Chi Square

1

24.5846

Pr > Chi 0.0001

based on all previous terms in the analysis being included in the model. The

intercept is always assumed in the model, which is why the Type 1 analysis begins

with the term x1, which is the first term specified in the model statement. The Type

3 analysis is similar to the individual t-tests in that it is a test of the contribution of

the specific term given all the other terms in the model. The model in the first page

of the table uses all three regressors. The model adequacy checks based on deviance

and the Pearson chi-square statistics are satisfactory, but we notice that x3 = crew

experience is not significant, using both the Wald test and the type 3 partial deviance

( ) (notice

that

the

Wald

statistic

reported

is

^ / se

^



2
,

which

is

referred

to

a

chi-square distribution with a single degree of freedom). This is a reasonable indica-

tion that x3 can be removed from the model. When x3 is removed, however, it turns out that now x1 = type of aircraft is no longer significant (you can easily verify that

the type 3 partial deviance for x1 in this model has a P value of 0.1582). A moment

of reflection on the data in Table 13.6 will reveal that there is a lot of multicollinear-

ity in the data. Essentially, the A-6 is a larger aircraft so it will carry a heavier bomb

load, and because it has a two-man crew, it may tend to have more total months of

crew experience. Therefore, as x1 increases, there is a tendency for both of the other

regressors to also increase.

To investigate the potential usefulness of various subset models, we fit all three

two-variable models and all three one-variable models to the data in Table 13.6. A

brief summary of the results obtained is as follows:

450 GENERALIZED LINEAR MODELS

Model
x1x2x3 x1x2 x1x3 x2x3 x1 x2 x3

Deviance
28.4906 31.0223 32.8817 31.6062 38.3497 33.0137 54.9653

Difference in Deviance Compared
to Full Model
2.5316 4.3911 3.1155 9.8591 4.5251 26.4747

P Value
0.1116 0.0361 0.0775 0.0072 0.1041 <0.0001

From examining the difference in deviance between each of the subset models and the full model, we notice that deleting either x1 or x2 results in a two-variable model that is significantly worse than the full model. Removing x3 results in a model that is not significantly different than the full model, but as we have already noted, x1 is not significant in this model. This leads us to consider the one-variable models. Only one of these models, the one containing x2, is not significantly different from the full model. The SAS PROC GENMOD output for this model is shown in the second page of Table 13.7. The Poisson regression model for predicting damage is

y^ = e-1.6491+ 0.2282 x2

The deviance for this model is D() = 33.0137 with 28 degrees of freedom,

and the P value is 0.2352, so we conclude that the model is an adequate fit to the

data.



13.4 THE GENERALIZED LINEAR MODEL

All of the regression models that we have considered in the two previous sections of this chapter belong to a family of regression models called the generalized linear model (GLM). The GLM is actually a unifying approach to regression and experimental design models, uniting the usual normal-theory linear regression models and nonlinear models such as logistic and Poisson regression.
A key assumption in the GLM is that the response variable distribution is a member of the exponential family of distributions, which includes (among others) the normal, binomial, Poisson, inverse normal, exponential, and gamma distributions. Distributions that are members of the exponential family have the general form

f ( yi , i ,  ) = exp{[yii - b(i )]/ a( ) + h( yi ,  )}

(13.48)

where  is a scale parameter and i is called the natural location parameter. For members of the exponential family,

 = E ( y) = db(i )
di

Var

(

y)

=

d 2 b ( i
di2

)

a(

)

=

d di

a(

)

(13.49)

THE GENERALIZED LINEAR MODEL

451

Let

Var

(

)

=

Var ( y) a( )

=

d di

(13.50)

where Var() denotes the dependence of the variance of the response on its mean. This is a characteristic of all distributions that are a member of the exponential family, except for the normal distribution. As a result of Eq. (13.50), we have

di = 1
d Var ( )

(13.51)

In Appendix C.14 we show that the normal, binomial, and Poisson distributions are members of the exponential family.

13.4.1 Link Functions and Linear Predictors
The basic idea of a GLM is to develop a linear model for an appropriate function of the expected value of the response variable. Let i be the linear predictor defined by

i = g[E ( yi )] = g ( i ) = xib

(13.52)

Note that the expected response is just

E ( yi ) = g-1 (i ) = g-1 (xib )

(13.53)

We call the function g the link function. Recall that we introduced the concept of a link function in our description of Poisson regression. There are many possible choices of the link function, but if we choose

i = i

(13.54)

we say that i is the canonical link. Table 13.8 shows the canonical links for the most common choices of distributions employed with the GLM.

TABLE 13.8 Canonical Links for the Generalized Linear Model

Distribution

Canonical Link

Normal Binomial Poisson Exponential
Gamma

i = i (identity link)

i

=

ln

 

1

 -

i


i

 

(logistic

link)

i = ln() (log link)

i

=1 i

(reciprocal

link)

i

=1 i

(reciprocal

link)

452 GENERALIZED LINEAR MODELS

There are other link functions that could be used with a GLM, including:

1. The probit link,

i = -1 [E ( yi )]

where  represents the cumulative standard normal distribution function. 2. The complementary log-log link,

i = ln{ln[1 - E ( yi )]}

3. The power family link,

i

=

Eln([Eyi

) , ( yi

)],

0  =0

A very fundamental idea is that there are two components to a GLM: the response distribution and the link function. We can view the selection of the link function in a vein similar to the choice of a transformation on the response. However, unlike a transformation, the link function takes advantage of the natural distribution of the response. Just as not using an appropriate transformation can result in problems with a fitted linear model, improper choices of the link function can also result in significant problems with a GLM.

13.4.2 Parameter Estimation and Inference in the GLM
The method of maximum likelihood is the theoretical basis for parameter estimation in the GLM. However, the actual implementation of maximum likelihood results in an algorithm based on IRLS. This is exactly what we saw previously for the special cases of logistic and Poisson regression. We present the details of the procedure in Appendix C.14. In this chapter, we rely on SAS PROC GENMOD for model fitting and inference.
If b^ is the final value of the regression coefficients that the IRLS algorithm produces and if the model assumptions, including the choice of the link function, are correct, then we can show that asymptotically

( ) ( ) E b^ = b and Var b^ = a( )(XVX)-1

(13.55)

where the matrix V is a diagonal matrix formed from the variances of the estimated parameters in the linear predictor, apart from a().
Some important observations about the GLM are as follows:
1. Typically, when experimenters and data analysts use a transformation, they use OLS to actually fit the model in the transformed scale.
2. In a GLM, we recognize that the variance of the response is not constant, and we use weighted least squares as the basis of parameter estimation.

THE GENERALIZED LINEAR MODEL

453

3. This suggests that a GLM should outperform standard analyses using transformations when a problem remains with constant variance after taking the transformation.
4. All of the inference we described previously on logistic regression carries over directly to the GLM. That is, model deviance can be used to test for overall model fit, and the difference in deviance between a full and a reduced model can be used to test hypotheses about subsets of parameters in the model. Wald inference can be applied to test hypotheses and construct confidence intervals about individual model parameters.

Example 13.9 The Worsted Yarn Experiment

Table 13.9 contains data from an experiment conducted to investigate the three factors x1 = length, x2 = amplitude, and x3 = load on the cycles to failure y of worsted yarn. The regressor variables are coded, and readers who have familiarity with designed experiments will recognize that the experimenters here used a 33 factorial design. The data also appear in Box and Draper [1987] and Myers, Montgomery, and AndersonCook [2009]. These authors use the data to illustrate the utility of variance-stabilizing

TABLE 13.9 Data from the Worsted Yarn Experiment

x1

x2

x3

y

-1

-1

-1

674

0

-1

-1

1414

1

-1

-1

3636

-1

0

-1

338

0

0

-1

1022

1

0

-1

1568

-1

1

-1

170

0

1

-1

442

1

1

-1

1140

-1

-1

0

370

0

-1

0

1198

1

-1

0

3184

-1

0

0

266

0

0

0

620

1

0

0

1070

-1

1

0

118

0

1

0

332

1

1

0

884

-1

-1

1

292

0

-1

1

634

1

-1

1

2000

-1

0

1

210

0

0

1

438

1

0

1

566

-1

1

1

90

0

1

1

220

1

1

1

360

454 GENERALIZED LINEAR MODELS

transformations. Both Box and Draper [1987] and Myers, Montgomery, and AndersonCook [2009] show that the log transformation is very effective in stabilizing the variance of the cycles-to-failure response. The least-squares model is

y^ = exp(6.33 + 0.83x1 - 0.63x2 - 0.39x3 )

The response variable in this experiment is an example of a nonnegative response that would be expected to have an asymmetric distribution with a long right tail. Failure data are frequently modeled with exponential, Weibull, lognormal, or gamma distributions both because they possess the anticipated shape and because sometimes there is theoretical or empirical justification for a particular distribution.
We will model the cycles-to-failure data with a GLM using the gamma distribution and the log link. From Table 13.8 we observe that the canonical link here is the inverse link; however, the log link is often a very effective choice with the gamma distribution.
Table 13.10 presents some summary output information from SAS PROC GENMOD for the worsted yarn data. The appropriate SAS code is

proc genmod; model y = x1 x2 x3 / dist = gamma link = log type1 type3;
Notice that the fitted model is

y^ = exp(6.35 + 0.84x1 - 0.63x2 - 0.39x3 )

which is virtually identical to the model obtained via data transformation. Actually,

since the log transformation works very well here, it is not too surprising that

the GLM produces an almost identical model. Recall that we observed that the

GLM is most likely to be an effective alternative to a data transformation when the

transformation fails to produce the desired properties of constant variance and

approximate normality in the response variable.

For the gamma response case, it is appropriate to use the scaled deviance in the

SAS output as a measure of the overall fit of the model. This quantity would be

compared to the chi-square distribution with n - p degrees of freedom, as usual.

From Table 13.10 we find that the scaled deviance is 27.1276, and referring this to

a chi-square distribution with 23 degrees of freedom gives a P value of approxi-

mately 0.25, so there is no indication of model inadequacy from the deviance crite-

rion. Notice that the scaled deviance divided by its degrees of freedom is also close

to unity. Table 13.10 also gives the Wald tests and the partial deviance statistics (both

type 1 or "effects added in order" and type 3 or "effects added last" analyses) for

each regressor in the model. These test statistics indicate that all three regressors

are important predictors and should be included in the model.



13.4.3 Prediction and Estimation with the GLM

For any generalized linear model, the estimate of the mean response at some point of interest, say x0, is

( ) y^0 = ^0 = g-1 x0b^

(13.56)

THE GENERALIZED LINEAR MODEL

455

TABLE 13.10 SAS PROC GENMOD Output for the Worsted Yarn Experiment The GENMOD Procedure Model Information

Description Data Set Distribution Link Function Dependent Variable Observations Used

Value WORK.WOOL GAMMA LOG CYCLES 27

Criteria for Assessing Goodness of Fit

Criterion

DF

Value

Deviance

23

0.7694

Scaled Deviance

23

27.1276

Pearson Chi [Square

23

0.7274

Scaled Pearson X2 Log Likelihood

23

25.6456

-161.3784

Value/DF 0.0335 1.1795 0.0316 1.1150

Analysis of Parameter Estimates

.Parameter DF Estimate Std Err Chi Square Pr > Chi

INTERCEPT

1

6.3489

0.0324

38373.0419

0.0001

A

1

0.8425

0.0402

438.3606

0.0001

B

1

-0.6313

0.0396

253.7576

0.0001

C

1

-0.3851

0.0402

91.8566

0.0001

SCALE

1

35.2585

9.5511

Note: The scale parameter was estimated by maximum likelihood.

.Source INTERCEPT A B C

LR Statistics for Type 1 Analysis

Deviance

DF

Chi Square

22.8861

0

10.2104

1

23.6755

3.3459

1

31.2171

0.7694

1

40.1106

Pr > Chi
0.0001 0.0001 0.0001

Source A B C

LR Statistics for Type 3 Analysis

DF

Chi Square

1

77.2935

1

63.4324

1

40.1106

Pr > Chi 0.0001 0.0001 0.0001

where g is the link function and it is understood that x0 may be expanded to model form if necessary to accommodate terms such as interactions that may have been included in the linear predictor. An approximate confidence interval on the mean response at this point can be computed as follows. Let  be the asymptotic variance­ covariance matrix for b^ ; thus,
S = a( )(XVX)-1

456 GENERALIZED LINEAR MODELS

The asymptotic variance of the estimated linear predictor at x0 is
( ) Var (^0 ) = Var x0b^ = x0 Sx0

Thus, an estimate covariance matrix

of this variance of b^ . The 100(1 -

is )

x0 S^ x0, where S^ is percent confidence

the estimated interval on the

variance­ true mean

response at the point x0 is

L   (x0 ) U

(13.57)

where

( ) ( ) L = g-1 x0 b^ - Z / 2x0 S^ x0 and U = g-1 x0b^ + Z / 2x0 S^ x0

(13.58)

This method is used to compute the confidence intervals on the mean response reported in SAS PROC GENMOD.This method for finding the confidence intervals usually works well in practice, because b^ is a maximum-likelihood estimate, and therefore any function of b^ is also a maximum-likelihood estimate. The above procedure simply constructs a confidence interval in the space defined by the linear predictor and then transforms that interval back to the original metric.
It is also possible to use Wald inference to derive other expressions for approximate confidence intervals on the mean response. Refer to Myers, Montgomery, and Anderson-Cook [2009] for the details.

Example 13.10 The Worsted Yarn Experiment

Table 13.11 presents three sets of confidence intervals on the mean response for the

worsted yarn experiment originally described in Example 13.10. In this table, we

have shown 95% confidence intervals on the mean response for all 27 points in the

original experimental data for three models: the least-squares model in the log scale,

the untransformed response from this least-squares model, and the GLM (gamma

response distribution and log link). The GLM confidence intervals were computed

from Eq. (13.58). The last two columns of Table 13.11 compare the lengths of the

normal-theory least-squares confidence intervals from the untransformed response

to those from the GLM. Notice that the lengths of the GLM intervals are uniformly

shorter that those from the least-squares analysis based on transformations. So even

though the prediction equations produced by these two techniques are very similar

(as we noted in Example 13.9), there is some evidence to indicate that the predic-

tions obtained from the GLM are more precise in the sense that the confidence

intervals will be shorter.



13.4.4 Residual Analysis in the GLM
Just as in any model-fitting procedure, analysis of residuals is important in fitting the GLM. Residuals can provide guidance concerning the overall adequacy of the model, assist in verifying assumptions, and give an indication concerning the appropriateness of the selected link function.

457

TABLE 13.11 Comparison of 95% Confidence Intervals on the Mean Response for the Worsted Yarn Data

Using Least-Squares Methods with Log Data Transformation

Transformed

Untransformed

Using the Generalized Linear Model

Predicted

Obs.

Value

95% Confidence Interval

Predicted Value

95% Confidence Interval

Predicted Value

95% Confidence Interval

1

2.83

2

2.66

3

2.49

4

2.56

5

2.39

6

2.22

7

2.29

8

2.12

9

1.94

10

3.20

11

3.02

12

2.85

13

2.92

14

2.75

15

2.58

16

2.65

17

2.48

18

2.31

19

3.56

20

3.39

21

3.22

22

3.28

23

3.11

24

2.94

25

3.01

26

2.84

27

2.67

(2.76, 2.91) (2.60, 2.73) (2.42, 2.57) (2.50, 2.62) (2.34, 2.44) (2.15, 2.28) (2.21, 2.36) (2.05, 2.18) (1.87, 2.02) (3.13, 3.26) (2.97, 3.08) (2.79, 2.92) (2.87, 2.97) (2.72, 2.78) (2.63, 2.63) (2.58, 2.71) (2.43, 2.53) (2.24, 2.37) (3.48, 3.63) (3.32, 3.45) (3.14, 3.29) (3.22, 3.35) (3.06, 3.16) (2.88, 3.01) (2.93, 3.08) (2.77, 2.90) (2.59, 2.74)

682.50 460.26 310.38 363.25 244.96 165.20 193.33 130.38 87.92 1569.28 1058.28 713.67 835.41 563.25 379.84 444.63 299.85 202.16 3609.11 2433.88 1641.35 1920.88 1295.39 873.57 1022.35 689.45 464.94

(573.85, 811.52) (397.01, 533.46) (260.98, 369.06) (313.33, 421.11) (217.92, 275.30) (142.50, 191.47) (162.55, 229.93) (112.46, 151.15) (73.93, 104.54) (1353.94, 1819.28) (941.67, 1189.60) (615.60, 827.37) (743.19, 938.86) (523.24, 606.46) (337.99, 426.97) (383.53, 515.35) (266.75, 336.98) (174.42, 234.37) (3034.59, 4292.40) (2099.42, 2821.63) (1380.07, 1951.64) (1656.91, 2226.90) (1152.66, 1455.79) (753.53, 1012.74) (859.81, 1215.91) (594.70, 799.28) (390.93, 552.97)

680.52 463.00 315.01 361.96 246.26 167.55 192.52 130.98 89.12 1580.00 1075.00 731.50 840.54 571.87 389.08 447.07 304.17 206.95 3670.00 2497.00 1699.00 1952.00 1328.00 903.51 1038.00 706.34 480.57

(583.83, 793.22) (407.05, 526.64) (271.49, 365.49) (317.75, 412.33) (222.55, 272.51) (147.67, 190.10) (165.69, 223.70) (115.43, 148.64) (76.87, 103.32) (1390.00, 1797.00) (972.52, 1189.00) (644.35, 830.44) (759.65, 930.04) (536.67, 609.38) (351.64, 430.51) (393.81, 507.54) (275.13, 336.28) (182.03, 235.27) (3165.00, 4254.00) (2200.00, 2833.00) (1462.00, 1974.00) (1720.00, 2215.00) (1200.00, 1470.00) (793.15, 1029.00) (894.79, 1205.00) (620.99, 803.43) (412.29, 560.15)

Length of the 95% Confidence Interval

Least Squares

GLM

237.67 136.45 108.09 107.79 57.37 48.97 67.38 38.69 30.62 465.34 247.92 211.77 195.67 83.22 88.99 131.82 70.23 59.95 1257.81 722.21 571.57 569.98 303.14 259.22 356.10 204.58 162.04

209.39 119.50 94.00 94.58 49.96 42.42 58.01 33.22 26.45 407.00 216.48 186.09 170.39 72.70 78.87 113.74 61.15 53.23 1089.00 633.00 512.00 495.00 270.00 235.85 310.21 182.44 147.86

458 GENERALIZED LINEAR MODELS

The ordinary or raw residuals from the GLM are just the differences between the observations and the fitted values,

ei = yi - y^i = yi - ^i

(13.59)

It is generally recommended that residual analysis in the GLM be performed using deviance residuals. Recall that the ith deviance residual is defined as the square root of the contribution of the ith observation to the deviance multiplied by the sign of the ordinary residual. Equation (13.30) gave the deviance residual for logistic regression. For Poisson regression with a log link, the deviance residuals are

( ) di

=

±

 

yi

ln

yi e xi b^

 -

yi - exib^

 

1

2
,

i = 1, 2, ... , n

where the sign is the sign of the ordinary residual. Notice that as the observed value of the response yi and the predicted value y^i = exib^ become closer to each other, the deviance residuals approach zero.
Generally, deviance residuals behave much like ordinary residuals do in a standard normal-theory linear regression model. Thus, plotting the deviance residuals on a normal probability scale and versus fitted values is a logical diagnostic. When plotting deviance residuals versus fitted values, it is customary to transform the fitted values to a constant information scale. Thus,

1. For normal responses, use y^i. 2. For binomial responses, use 2sin-1 ^i . 3. For Poisson responses, use 2 y^i .
4. For gamma responses, use 2ln( y^i ).

Example 13.11 The Worsted Yarn Experiment

Table 13.12 presents the actual observations from the worsted yarn experiment in

Example 13.9, along with the predicted values from the GLM (gamma response

with log link) that was fit to the data, the raw residuals, and the deviance residuals.

These quantities were computed using SAS PROC GENMOD. Figure 13.7a is a

normal probability plot of the deviance residuals and Figure 13.7b is a plot of the

deviance residuals versus the "constant information" fitted values, 2ln( y^i ). The

normal probability plot of the deviance residuals is generally satisfactory, while the

plot of the deviance residuals versus the fitted values indicates that one of the

observations may be a very mild outlier. Neither plot gives any significant indication

of model inadequacy, however, so we conclude that the GLM with gamma response

variable distribution and a log link is a very satisfactory model for the cycles-to-

failure response.



13.4.5 Using R to Perform GLM Analysis
The workhorse routine within R for analyzing a GLM is "glm." The basic form of this statement is:

THE GENERALIZED LINEAR MODEL

459

TABLE 13.12 Predicted Values and Residuals from the Worsted Yarn Experiment

Response

Predicted

Linear Predictor

yi

y^

x

ei

di

674 370 292 338 266 210 170 118 90 1414 1198 634 1022 620 438 442 332 220 3636 3184 2000 1568 1070 566 1140 884 360

680.5198 462.9981 315.0052 361.9609 246.2636 167.5478 192.5230 130.9849
89.1168 1580.2950 1075.1687 731.5013 840.5414 571.8704 389.0774 447.0747 304.1715 206.9460 3669.7424 2496.7442 1698.6836 1951.8954 1327.9906 903.5111 1038.1916 706.3435 480.5675

6.5229 6.1377 5.7526 5.8915 5.5064 5.1213 5.2602 4.8751 4.4899 7.3654 6.9802 6.5951 6.7340 6.3489 5.9638 6.1027 5.7176 5.3325 8.2079 7.8227 7.4376 7.5766 7.1914 6.8063 6.9452 6.5601 6.1750

-6.5198 -92.9981 -23.0052 -23.9609
19.7364 42.4522 -22.5230 -12.9849
0.8832 -166.2950
122.8313 -97.5013 181.4586
48.1296 48.9226 -5.0747 27.8285 13.0540 -33.7424 687.2558 301.3164 -383.8954 -257.9906 -337.5111 101.8084 177.6565 -120.5675

-0.009611 -0.2161 -0.0749 -0.0677
0.0781 0.2347 -0.1219 -0.1026 0.009878 -0.1092 0.1102 -0.1397 0.2021 0.0819 0.1208 -0.0114 0.0888 0.0618 -0.009223 0.2534 0.1679 -0.2113 -0.2085 -0.4339 0.0950 0.2331 -0.2756

Normal probability × 100

99

0.3

95

90

0.2

80

0.1

70

60

0.0

50 40 30

rdi -0.1

20

-0.2

10 5

1

-0.5

0.0

-0.3

-0.4

-0.5

0.5

9 10 11 12 13 14 15 16 17

di

2 ln(yi)

(a)

(b)

Figure 13.7 Plots of the deviance residuals from the GLM for the worsted yarn data.
(a) Normal probability plot of deviance results. (b) Plot of the deviance residuals versus
2ln( y^i )

460 GENERALIZED LINEAR MODELS
glm(formula, family, data)
The formula specification is exactly the same as for a standard linear model. For example, the formaula for the model  = 0 + 1x1 + 2x2 is
y  x1+x2
The choices for family and the links available are:
· binomial (logit, probit, log, complementary loglog), · gaussian (identity, log, inverse), · Gamma (identity, inverse, log) · inverse.gaussian (1/2, identity, inverse, log) · poisson (identity, log, square root), and · quasi (logit, probit, complementary loglog, identity, inverse, log, 1/2, square
root).
R is case-sensitive, so the family is Gamma, not gamma. By default, R uses the canonical link. To specify the probit link for the binomial family, the appropriate family phrase is binomial (link = probit).
R can produce two different predicted values. The "fit" is the vector of predicted values on the original scale. The "linear.predictor" is the vector of the predicted values for the linear predictor. R can produce the raw, the Pearson, and the deviance residuals. R also can produce the "influence measures," which are the individual observation deleted statistics. The easiest way to put all this together is through examples.
We first consider the pneumoconiosis data from Example 13.1. The data set is small, so we do not need a separate data file. The R code is:
> years <- c(5.8, 15.0, 21.5, 27.5, 33.5, 39.5, 46.0, 51.5) > cases <- c(0, 1, 3, 8, 9, 8, 10, 5) > miners <- c(98, 54, 43, 48, 51, 38, 28, 11) > ymat <- cbind(cases, miners-cases) > ashford <- data.frame(ymat, years) > anal <- glm(ymat  years, family=binomial, data=ashford) summary(anal) pred_prob <- anal$fit eta_hat <- anal$linear.predictor dev_res <- residuals(anal, c="deviance") influence.measures(anal) df <- dfbetas(anal) df_int <- df[,1] df_years <- df[,2] hat <- hatvalues(anal) qqnorm(dev_res) plot(pred_prob,dev_res) plot(eta_hat,dev_res)

THE GENERALIZED LINEAR MODEL

461

plot(years,dev_res) plot(hat,dev_res) plot(pred_prob,df_years) plot(hat,df_years) ashford2 <- cbind(ashford,pred_prob,eta_hat,dev_res,df_int, df_years,hat) write.table(ashford2, ashford_output.txt)

We next consider the Aircraft Damage example from Example 13.8. The data are in the file aircraft_damage_data.txt. The appropriate R code is
air <- read.table(aircraft_damage_data.txt,header=TRUE, sep=) air.model <- glm(yx1+x2+x3, dist=poisson, data=air) summary(air.model) print(influence.measures(air.model)) yhat <- air.model$fit dev_res <- residuals(air.model, c=deviance) qqnorm(dev_res) plot(yhat,dev_res) plot(air$x1,dev_res) plot(air$x2,dev_res) plot(air$x3,dev_res) air2 <- cbind(air,yhat,dev_res) write.table(air2, aircraft damage_output.txt)

Finally, consider the Worsted Yarn example from Example 13.9. The data are in the file worsted_data.txt. The appropriate R code is
yarn <- read.table(worsted_data.txt,header=TRUE, sep=) yarn.model <- glm(yx1+x2+x3, dist=Gamma(link=log), data=air) summary(yarn.model) print(influence.measures(yarn.model)) yhat <- air.model$fit dev_res <- residuals(yarn.model, c=deviance) qqnorm(dev_res) plot(yhat,dev_res) plot(yarn$x1,dev_res) plot(yarn$x2,dev_res) plot(yarn$x3,dev_res) yarn2 <- cbind(yarn,yhat,dev_res) write.table(yarn2, yarn_output.txt)

13.4.6 Overdispersion
Overdispersion is a phenomenon that sometimes occurs when we are modeling response data with either a binomial or Poisson distribution. Basically, it means that the variance of the response is greater than one would anticipate for that choice of

462 GENERALIZED LINEAR MODELS
response distribution. An overdispersion condition is often diagnosed by evaluating the value of model deviance divided by degrees of freedom. If this value greatly exceeds unity, then overdispersion is a possible source of concern.
The most direct way to model this situation is to allow the variance function of the binomial or Poisson distributions to have a multiplicative dispersion factor , so that
Var ( y) =  (1 -  ) binomial distribution
Var ( y) =  Poisson distribution
The models are fit in the usual manner, and the values of the model parameters are not affected by the value of . The parameter  can be specified directly if its value is known or it can be estimated if there is replication of some data points. Alternatively, it can be directly estimated. A logical estimate for  is the deviance divided by its degrees of freedom. The covariance matrix of model coefficients is multiplied by  and the scaled deviance and log-likelihoods used in hypothesis testing are divided by .
The function obtained by dividing a log-likelihood by  for the binomial or Poisson error distribution case is no longer a proper log-likelihood function. It is an example of a quasi-likelihood function. Fortunately, most of the asymptotic theory for log-likelihoods applies to quasi-likelihoods, so we can justify computing approximate standard errors and deviance statistics just as we have done previously.

PROBLEMS
13.1 The table below presents the test-firing results for 25 surface-to-air antiaircraft missiles at targets of varying speed. The result of each test is either a hit (y = 1) or a miss (y = 0).

Test

Target Speed, x (knots) y

Test Target Speed, x (knots) y

1

400

0

14

330

1

2

220

1

15

280

1

3

490

0

16

210

1

4

210

1

17

300

1

5

500

0

18

470

1

6

270

0

19

230

0

7

200

1

20

430

0

8

470

0

21

460

0

9

480

0

22

220

1

10

310

1

23

250

1

11

240

1

24

200

1

12

490

0

25

390

0

13

420

0

a. Fit a logistic regression model to the response variable y. Use a simple linear regression model as the structure for the linear predictor.
b. Does the model deviance indicate that the logistic regression model from part a is adequate?

PROBLEMS 463
c. Provide an interpretation of the parameter 1 in this model. d. Expand the linear predictor to include a quadratic term in target speed.
Is there any evidence that this quadratic term is required in the model?
13.2 A study was conducted attempting to relate home ownership to family income. Twenty households were selected and family income was estimated, along with information concerning home ownership (y = 1 indicates yes and y = 0 indicates no). The data are shown below.

Household
1 2 3 4 5 6 7 8 9 10

Income
38,000 51,200 39,600 43,400 47,700 53,000 41,500 40,800 45,400 52,400

Home Ownership
Status
0 1 0 1 0 0 1 0 1 1

Household
11 12 13 14 15 16 17 18 19 20

Income
38,700 40,100 49,500 38,000 42,000 54,000 51,700 39,400 40,900 52,800

Home Ownership
Status
1 0 1 0 1 1 1 0 0 1

a. Fit a logistic regression model to the response variable y. Use a simple linear regression model as the structure for the linear predictor.
b. Does the model deviance indicate that the logistic regression model from part a is adequate?
c. Provide an interpretation of the parameter 1 in this model. d. Expand the linear predictor to include a quadratic term in income. Is
there any evidence that this quadratic term is required in the model?
13.3 The compressive strength of an alloy fastener used in aircraft construction is being studied. Ten loads were selected over the range 2500­4300 psi and a number of fasteners were tested at those loads. The numbers of fasteners failing at each load were recorded. The complete test data are shown below.

Load, x (psi)
2500 2700 2900 3100 3300 3500 3700 3900 4100 4300

Sample Size, n
50 70 100 60 40 85 90 50 80 65

Number Failing, r
10 17 30 21 18 43 54 33 60 51

464 GENERALIZED LINEAR MODELS
a. Fit a logistic regression model to the data. Use a simple linear regression model as the structure for the linear predictor.
b. Does the model deviance indicate that the logistic regression model from part a is adequate?
c. Expand the linear predictor to include a quadratic term. Is there any evidence that this quadratic term is required in the model?
d. For the quadratic model in part c, find Wald statistics for each individual model parameter.
e. Find approximate 95% confidence intervals on the model parameters for the quadratic model from part c.
13.4 The market research department of a soft drink manufacturer is investigating the effectiveness of a price discount coupon on the purchase of a twoliter beverage product. A sample of 5500 customers was given coupons for varying price discounts between 5 and 25 cents. The response variable was the number of coupons in each price discount category redeemed after one month. The data are shown below.

Discount, x
5 7 9 11 13 15 17 19 21 23 25

Sample Size, n
500 500 500 500 500 500 500 500 500 500 500

Number Redeemed, r
100 122 147 176 211 244 277 310 343 372 391

a. Fit a logistic regression model to the data. Use a simple linear regression model as the structure for the linear predictor.
b. Does the model deviance indicate that the logistic regression model from part a is adequate?
c. Draw a graph of the data and the fitted logistic regression model.
d. Expand the linear predictor to include a quadratic term. Is there any evidence that this quadratic term is required in the model?
e. Draw a graph of this new model on the same plot that you prepared in part c. Does the expanded model visually provide a better fit to the data than the original model from part a?
f. For the quadratic model in part d, find Wald statistics for each individual model parameter.
g. Find approximate 95% confidence intervals on the model parameters for the quadratic logistic regression model from part d.

PROBLEMS 465
13.5 A study was performed to investigate new automobile purchases. A sample of 20 families was selected. Each family was surveyed to determine the age of their oldest vehicle and their total family income. A follow-up survey was conducted 6 months later to determine if they had actually purchased a new vehicle during that time period (y = 1 indicates yes and y = 0 indicates no). The data from this study are shown in the following table.

Income, x1

Age, x2

y

45,000 40,000 60,000 50,000 55,000 50,000 35,000 65,000 53,000 48,000

2

0

4

0

3

1

2

1

2

0

5

1

7

1

2

1

2

0

1

0

Income, x1

Age, x2

y

37,000

5

1

31,000

7

1

40,000

4

1

75,000

2

0

43,000

9

1

49,000

2

0

37,500

4

1

71,000

1

0

34,000

5

0

27,000

6

0

a. Fit a logistic regression model to the data. b. Does the model deviance indicate that the logistic regression model from
part a is adequate? c. Interpret the model coefficients 1 and 2. d. What is the estimated probability that a family with an income of $45,000
and a car that is 5 years old will purchase a new vehicle in the next 6 months? e. Expand the linear predictor to include an interaction term. Is there any evidence that this term is required in the model? f. For the model in part a, find statistics for each individual model parameter. g. Find approximate 95% confidence intervals on the model parameters for the logistic regression model from part a.
13.6 A chemical manufacturer has maintained records on the number of failures of a particular type of valve used in its processing unit and the length of time (months) since the valve was installed. The data are shown below.

Valve
1 2 3 4 5 6 7 8

Number of Failures
5 3 0 1 4 0 0 1

Months
18 15 11 14 23 10 5 8

Valve
9 10 11 12 13 14 15

Number of Failures
0 0 0 1 0 7 0

Months
7 12 3 7 2 30 9

466 GENERALIZED LINEAR MODELS

a. Fit a Poisson regression model to the data.
b. Does the model deviance indicate that the Poisson regression model from part a is adequate?
c. Construct a graph of the fitted model versus months. Also plot the observed number of failures on this graph.
d. Expand the linear predictor to include a quadratic term. Is there any evidence that this term is required in the model?
e. For the model in part a, find Wald statistics for each individual model parameter.
f. Find approximate 95% confidence intervals on the model parameters for the Poisson regression model from part a.
13.7 Myers [1990] presents data on the number of fractures (y) that occur in the upper seams of coal mines in the Appalachian region of western Virginia. Four regressors were reported: x1 = inner burden thickness (feet), the shortest distance between seam floor and the lower seam; x2 = percent extraction of the lower previously mined seam; x3 = lower seam height (feet); and x4 = time (years) that the mine has been in operation. The data are shown below.

Observation

Number of Fractures per Subregion, y

x1

x2

x3

x4

1

2

50

70

52

1.0

2

1

230

65

42

6.9

3

0

125

70

45

1.0

4

4

75

65

68

0.5

5

1

70

65

53

0.5

6

2

65

70

46

3.0

7

0

65

60

62

1.0

8

0

350

60

54

0.5

9

4

350

90

54

0.5

10

4

160

80

38

0.0

11

1

145

65

38

10.0

12

4

145

85

38

0.0

13

1

180

70

42

2.0

14

5

43

80

40

0.0

15

2

42

85

51

12.0

16

5

42

85

51

0.0

17

5

45

85

42

0.0

18

5

83

85

48

10.0

19

0

300

65

68

10.0

20

5

190

90

84

6.0

21

1

145

90

54

12.0

22

1

510

80

57

10.0

23

3

65

75

68

5.0

24

3

470

90

90

9.0

25

2

300

80

165

9.0

26

2

275

90

40

4.0

27

0

420

50

44

17.0

28

1

65

80

48

15.0

Observation
29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44

Number of Fractures per Subregion, y
5 2 3 3 3 0 0 2 0 0 3 2 3 5 0 3

PROBLEMS 467

x1

x2

40

75

900

90

95

88

40

85

140

90

150

50

80

60

80

85

145

65

100

65

150

80

150

80

210

75

11

75

100

65

50

88

x3

x4

51

15.0

48

35.0

36

20.0

57

10.0

38

7.0

44

5.0

96

5.0

96

5.0

72

9.0

72

9.0

48

3.0

48

0.0

42

2.0

42

0.0

60

25.0

60

20.0

a. Fit a Poisson regression model to these data using the log link.
b. Does the model deviance indicate that the model from part a is satisfactory?
c. Perform a type 3 partial deviance analysis of the model parameters. Does this indicate that any regressors could be removed from the model?
d. Compute Wald statistics for testing the contribution of each regressor to the model. Interpret the results of these test statistics.
e. Find approximate 95% Wald confidence intervals on the model parameters.

13.8 Reconsider the mine fracture data from Problem 13.7. Remove any regressors from the original model that you think might be unimportant and rework parts b­e of Problem 13.7. Comment on your findings.

13.9 Reconsider the mine fracture data from Problems 13.7 and 13.8. Construct plots of the deviance residuals from the best model you found and comment on the plots. Does the model appear satisfactory from a residual analysis viewpoint?

13.10

Reconsider the model for the automobile purchase data from Problem 13.5, part a. Construct plots of the deviance residuals from the model and comment on these plots. Does the model appear satisfactory from a residual analysis viewpoint?

13.11

Reconsider the model for the soft drink coupon data from Problem 13.4, part a. Construct plots of the deviance residuals from the model and comment on these plots. Does the model appear satisfactory from a residual analysis viewpoint?

13.12 Reconsider the model for the aircraft fastener data from Problem 13.3, part a. Construct plots of the deviance residuals from the model and comment

468 GENERALIZED LINEAR MODELS

on these plots. Does the model appear satisfactory from a residual analysis viewpoint?
13.13 The gamma probability density function is

f

(

y,

r,



)

=

r
(r)

e-y yr-1

for y,   0

Show that the gamma is a member of the exponential family. 13.14 The exponential probability density function is

f ( y,  ) = e-y for y,   0

Show that the exponential distribution is a member of the exponential family.
13.15 The negative binomial probability mass function is

f

(

y,



,



)

=

 

y+ -

- 1

1 





(1

-



)y

for y = 0, 1, 2, ... ,  > 0 and 0    1

Show that the negative binomial is a member of the exponential family.

13.16

The data in the table below are from an experiment designed to study the advance rate y of a drill. The four design factors are x1 = load, x2 = flow, x3 = drilling speed, and x4 = type of drilling mud (the original experiment is described by Cuthbert Daniel in his 1976 book on industrial
experimentation).

Observation

x1

x2

x3

x4

Advance Rate, y

1

-

-

-

-

1.68

2

+

-

-

-

1.98

3

-

+

-

-

3.28

4

+

+

-

-

3.44

5

-

-

+

-

4.98

6

+

-

+

-

5.70

7

-

+

+

-

9.97

8

+

+

+

-

9.07

9

-

-

-

+

2.07

10

+

-

-

+

2.44

11

-

+

-

+

4.09

12

+

+

-

+

4.53

13

-

-

+

+

7.77

14

+

-

+

+

9.43

15

-

+

+

+

11.75

16

+

+

+

+

16.30

PROBLEMS 469

a. Fit a generalized linear model to the advance rate response. Use a gamma response distribution and a log link, and include all four regressors in the linear predictor.
b. Find the model deviance for the GLM from part a. Does this indicate that the model is satisfactory?
c. Perform a type 3 partial deviance analysis of the model parameters. Does this indicate that any regressors could be removed from the model?
d. Compute Wald statistics for testing the contribution of each regressor to the model. Interpret the results of these test statistics.
e. Find approximate 95% Wald confidence intervals on the model parameters.

13.17 Reconsider the drill data from Problem 13.16. Remove any regressors from the original model that you think might be unimportant and rework parts b­e of Problem 13.16. Comment on your findings.

13.18

Reconsider the drill data from Problem 13.16. Fit a GLM using the log link and the gamma distribution, but expand the linear predictor to include all six of the two-factor interactions involving the four original regressors. Compare the model deviance for this model to the model deviance for the "main effects only model" from Problem 13.16. Does adding the interaction terms seem useful?

13.19 Reconsider the model for the drill data from Problem 13.16. Construct plots of the deviance residuals from the model and comment on these plots. Does the model appear satisfactory from a residual analysis viewpoint?

13.20

The table below shows the predicted values and deviance residuals for the Poisson regression model using x2 = bomb load as the regressor fit to the aircraft damage data in Example 13.8. Plot the residuals and comment on model adequacy.

y

y^

xi b^

ei

rpi

0

0.4789

-0.7364

-0.4789

-0.9786

1

0.4789

-0.7364

0.5211

0.6561

0

0.4789

-0.7364

-0.4789

-0.9786

0

0.6016

-0.5083

-0.6016

-1.0969

0

0.6016

-0.5082

-0.6016

-1.0969

0

0.6016

-0.5082

-0.6016

-1.0969

1

0.7558

-0.2800

0.2442

0.2675

0

0.7558

-0.2800

-0.7558

-1.2295

0

0.7558

-0.2800

-0.7558

-1.2295

2

0.9495

-0.0518

1.0505

0.9374

0

0.9495

-0.0518

-0.9495

-1.3781

1

0.9495

-0.0518

0.0505

0.0513

1

1.1929

0.1764

-0.1929

-0.1818

(Continued)

470 GENERALIZED LINEAR MODELS

y

y^

1

1.1929

2

1.1929

3

0.9495

1

0.9495

1

0.9495

1

1.8829

2

1.8829

0

1.8829

1

2.9719

1

2.9719

2

2.9719

5

1.1929

1

1.1929

3

1.1929

5

4.6907

5

4.6907

7

4.6907

xi b^
0.1764 0.1764 -0.0518 -0.0518 -0.0518 0.6328 0.6328 0.6328 1.0892 1.0892 1.0892 0.1764 0.1764 0.1764 1.5456 1.5456 1.5456

ei
-0.1929 0.8071 2.0505 0.0505 0.0505
-0.8829 0.1171
-1.8829 -1.9719 -1.9719 -0.9719
3.8071 -0.1929
1.8071 0.3093 0.3093 2.3093

rpi
-0.1818 0.6729 1.6737 0.0513 0.0513
-0.7072 0.0845
-1.9406 -1.3287 -1.3287 -0.5996
2.5915 -0.1818
1.3853 0.1413 0.1413 0.9930

13.21

Consider a logistic regression model with a linear predictor that includes an interaction term, say x = 0 + 1x1 + 2x2 + 12x1x2. Derive an expression for the odds ratio for the regressor x1. Does this have the same interpretation as in the case where the linear predictor does not have the interaction term?

13.22

The theory of maximum-likelihood states that the estimated large-sample covariance for maximum-likelihood estimates is the inverse of the information matrix, where the elements of the information matrix are the negatives of the expected values of the second partial derivatives of the log-likelihood function evaluated at the maximum-likelihood estimates. Consider the linear regression model with normal errors. Find the information matrix and the covariance matrix of the maximum-likelihood estimates.

13.23 Consider the automobile purchase late in Problem 13.5. Fit models using both the probit and complementary log-log functions. Compare three models to the one obtained using the logit.

13.24 Reconsider the pneumoconiosis data in Table 13.1. Fit models using both the probit and complimentary log-log functions. Compare these models to the one obtained in Example 13.1 using the logit.

13.25

On 28 January 1986 the space shuttle Challenger was destroyed in an explosion shortly after launch from Cape Kennedy. The cause of the explosion was eventually identified as catastrophic failure of the O-rings on the solid rocket booster. The failure likely occurred because the O-ring material was subjected to a lower temperature at launch (31°F) than was appropriate. The material and the solid rocket joints had never been tested at temperatures this low. Some O-ring failures had occurred during other shuttle launches (or engine static tests). The failure data observed prior to the Challenger launch is shown in the following table.

PROBLEMS 471

Temperature at Launch
53 56 57 63 66 67 67 67 68 69 70 70

At Least One O-ring Failure
1 1 1 0 0 0 0 0 0 0 0 1

Temperature at Launch
70 70 72 73 75 75 76 76 78 79 80 81

At Least One O-ring Failure
1 1 0 0 0 1 0 0 0 0 0 0

a. Fit a logistic regression model to these data. Construct a graph of the data and display the fitted model. Discuss how well the model fits the data.
b. Calculate and interpret the odds ratio. c. What is the estimated failure probability at 50°F? d. What is the estimated failure probability at 75°F? e. What is the estimated failure probability at 31°F? Notice that there
is extrapolation involved in obtaining this estimate. What influence would that have on your recommendation about launching the space shuttle? f. Calculate and analyze the deviance residuals for this model. g. Add a quadratic term in temperature to the logistic regression model in part a. Is there any evidence that this term improves the model?

13.26 A student conducted a project looking at the impact of popping temperature, amount of oil, and the popping time on the number of inedible kernels of popcorn. The data follow. Analyze these data using Poisson regression.

Temperature

Oil

Time

y

7

4

90

24

5

3

105

28

7

3

105

40

7

2

90

42

6

4

105

11

6

3

90

16

5

3

75

126

6

2

105

34

5

4

90

32

6

2

75

32

5

2

90

34

7

3

75

17

6

3

90

30

6

3

90

17

6

4

75

50

472 GENERALIZED LINEAR MODELS

13.27

Bailer and Piegorsch [2000] report on an experiment that examines the effect of a herbicide, nitrofen, on the umber of offspring produced by a particular freshwater invertebrate zooplankton. The data follow. Perform an appropriate analysis of these data.

Dose

Number of Offspring

Control 27 32 34 33 36 34 33 30 24 31

80

33 33 35 33 36 26 27 31 32 29

160

29 29 23 27 30 31 30 26 29 29

235

23 21 7 12 27 16 13 15 21 17

310

6 6 7 0 15 5 6 4 6 5

13.28

Chapman [1997­98] conducted an experiment using accelerated life testing to determine the estimated shelf life of a photographic developer. The data follow. Lifetimes often follow an exponential distribution. This company has found that the maximum density is a good indicator of overall developer/ film performance; correspondingly using generalized linear models. Perform appropriate residual analysis of your final model.

t (h)

Dmax (72°C)

t (h)

Dmax (82°C)

t (h)

Dmax (92°C)

72

3.55

48

3.52

24

3.46

144

3.27

96

3.35

48

2.91

216

2.89

144

2.50

72

2.27

288

2.55

192

2.10

96

1.49

360

2.34

240

1.90

120

1.20

432

2.14

288

1.47

144

1.04

504

1.77

336

1.19

168

0.65

13.29

Gupta and Das [2000] performed an experiment to improve the resistivity of a urea formaldehyde resin. The factors were amount of sodium hydroxide, A, reflux time, B, solvent distillate, C, phthalic anhydride, D, water collection time, E, and solvent distillate collection time, F. The data follow, where y1 is the resistivity from the first replicate of the experiment and y2 is the resistivity from the second replicate. Assume a gamma distribution. Use both the canonical and the log link to analyze these data. Perform appropriate residual analysis of your final models.

A

B

C

D

E

F

y1

y2

-1

-1

-1

-1

-1

-1

60

135

-1

-1

-1

1

-1

-1

220

160

0

-1

-1

-1

1

1

85

180

0

-1

-1

1

1

1

330

110

0

1

1

1

-1

-1

95

130

0

1

1

-1

-1

-1

190

175

-1

1

1

1

1

1

145

200

-1

1

1

-1

1

1

300

210

1

-1

1

1

-1

1

110

100

PROBLEMS 473

A

B

C

D

E

F

y1

y2

1

-1

1

-1

-1

1

125

130

1

-1

1

1

1

-1

300

170

1

-1

1

-1

1

-1

65

160

1

1

-1

-1

-1

1

170

90

1

1

-1

1

-1

1

70

250

1

1

-1

-1

1

-1

380

80

1

1

-1

1

1

-1

105

200

CHAPTER 14
REGRESSION ANALYSIS OF TIME SERIES DATA
14.1 INTRODUCTION TO REGRESSION MODELS FOR TIME SERIES DATA
Many applications of regression involve both predictor and response variables that are time series, that is, the variables are time-oriented. Regression models using time series data occur relatively often in economics, business, and many fields of engineering. The assumption of uncorrelated or independent errors that is typically made for regression data that is not time-dependent is usually not appropriate for time series data. Usually the errors in time series data exhibit some type of autocorrelated structure. By autocorrelation we mean that the errors are correlated with themselves at different time periods. We will give a formal definition shortly.
There are several sources of autocorrelation in time series regression data. In many cases, the cause of autocorrelation is the failure of the analyst to include one or more important predictor variables in the model. For example, suppose that we wish to regress the annual sales of a product in a particular region of the country against the annual advertising expenditures for that product. Now the growth in the population in that region over the period of time used in the study will also influence the product sales. Failure to include the population size may cause the errors in the model to be positively autocorrelated, because if the per-capita demand for the product is either constant or increasing with time, population size is positively correlated with product sales.
The presence of autocorrelation in the errors has several effects on the ordinary least-squares regression procedure. These are summarized as follows:
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc. 474

DETECTING AUTOCORRELATION: THE DURBIN­WATSON TEST

475

1. The ordinary least squares (OLS) regression coefficients are still unbiased, but they are no longer minimum-variance estimates. We know this from our study of generalized least squares in Section 5.5.
2. When the errors are positively autocorrelated, the residual mean square may seriously underestimate the error variance 2. Consequently, the standard errors of the regression coefficients may be too small. As a result confidence and prediction intervals are shorter than they really should be, and tests of hypotheses on individual regression coefficients may be misleading in that they may indicate that one or more predictor variables contribute significantly to the model when they really do not. Generally, underestimating the error variance 2gives the analyst a false impression of precision of estimation and potential forecast accuracy.
3. The confidence intervals, prediction intervals, and tests of hypotheses based on the t and F distributions are, strictly speaking, no longer exact procedures.

There are three approaches to dealing with the problem of autocorrelation. If autocorrelation is present because of one or more omitted predictors and if those predictor variable(s) can be identified and included in the model, the observed autocorrelation should disappear. Alternatively, the weighted least squares or generalized least squares methods discussed in Section 5.5 could be used if there were sufficient knowledge of the autocorrelation structure. Finally, if these approaches cannot be used, then the analyst must turn to a model that specifically incorporates the autocorrelation structure. These models usually require special parameter estimation techniques. We will provide an introduction to these procedures in Section 14.3.

14.2 DETECTING AUTOCORRELATION: THE DURBIN­WATSON TEST

Residual plots can be useful for the detection of autocorrelation. The most useful display is the plot of residuals versus time. If there is positive autocorrelation, residuals of identical sign occur in clusters. That is, there are not enough changes of sign in the pattern of residuals. On the other hand, if there is negative autocorrelation, the residuals will alternate signs too rapidly.
Various statistical tests can be used to detect the presence of autocorrelation. The test developed by Durbin and Watson (1950, 1951, 1971) is a very widely used procedure. This test is based on the assumption that the errors in the regression model are generated by a first-order autoregressive process observed at equally spaced time periods, that is,

t = t-1 + at

(14.1)

where t is the error term in the model at time period t, at is an NID (0, a2) random variable,  is a parameter that defines the relationship between successive values of the model errors t and t-1, and the time index is t = 1, 2, . . . , T (T is the number of observations available, and it usually stands for the current time period). We will require that || < 1, so that the model error term in time period t is equal to a

476

REGRESSION ANALYSIS OF TIME SERIES DATA

fraction of the error experienced the immediately preceding period plus a normally and independently distributed random shock or disturbance that is unique to the current period. In time series regression models  is sometimes called the autocorrelation parameter. Thus, a simple linear regression model with first-order autoregressive errors would be

yt = 0 + 1xt + t , t = t-1 + at

(14.2)

where yt and xt are the observations on the response and predictor variables at time period t.
When the regression model errors are generated by the first-order autoregressive process in Eq. (14.1), there are several interesting properties of these errors. By successively substituting for t, t-1, . . . on the right-hand side of Eq. (14.1) we obtain

 t =  jat- j j=0

In other words, the error term in the regression model for period t is just a linear combination of all of the current and previous realizations of the NID (0,  2)
random variables at. Furthermore, we can show that

E(t ) = 0

V(t )

=

2

=



2 a

 

1 1 - 2

 

Cov(t

,

t±

j

)

=



j

2 a

 

1

1 -

2

 

(14.3)

That is, the errors have zero mean and constant variance but have a nonzero covariance structure unless  = 0.
The autocorrelation between two errors that are one period apart, or the lag one autocorrelation, is

1 =

Cov(t , t+1 ) V(t ) V(t )

=



2 a

 

1

1 -

2

 



2 a

 

1

1 -

2

 



2 a

 

1

1 -

2

 

=

The autocorrelation between two errors that are k periods apart is

k = k , i = 1, 2,...

This is called the autocorrelation function. Recall that we have required that || < 1. When  is positive, all error terms are positively correlated, but the

DETECTING AUTOCORRELATION: THE DURBIN­WATSON TEST

477

magnitude of the correlation decreases as the errors grow further apart. Only if  = 0 are the model errors uncorrelated.
Most time series regression problems involve data with positive autocorrelation. The Durbin­Watson test is a statistical test for the presence of positive autocorrelation in regression model errors. Specifically, the hypotheses considered in the Durbin­Watson test are

H0 :  = 0 H1 :  > 0

(14.4)

The Durbin­Watson test statistic is

T

T

T

T

    (et - et-1)2

et2 + et2-1 - 2 et et-1

d = t=2 T
 et2

= t=2

t=2

t=2

T
 et2

 2(1 - r1)

t=1

t=1

(14.5)

where the et, t = 1, 2, . . . , T are the residuals from an OLS regression of yt on xt. and r1 is the lag one sample autocorrelation coefficient defined as

T -1
 etet+1

r1 =

t=1 T

 et2

t=1

(14.6)

For uncorrelated errors r1 = 0 (at least approximately) so the value of the Durbin­ Watson statistic should be approximately 2. Statistical testing is necessary to determine just how far away from 2 the statistic must fall in order for us to conclude that the assumption of uncorrelated errors is violated. Unfortunately, the distribution of the Durbin­Watson test statistic d depends on the X matrix, and this makes critical values for a statistical test difficult to obtain. However, Durbin and Watson (1951) show that d lies between lower and upper bounds, say dL and dU , such that if d is outside these limits, a conclusion regarding the hypotheses in Eq. (14.4) can be reached. The decision procedure is as follows:

If d < dL reject H0 :  = 0
If d > dU do not reject H0 :  = 0
If dL  d  dU the test is inconclusive
Table A.6 gives the bounds dL and dU for a range of sample sizes, various numbers of predictors, and three type I error rates ( = 0.05,  = 0.025, and  = 0.01). It is clear that small values of the test statistic d imply that H0 :  = 0 should be rejected because positive autocorrelation indicates that successive error terms are of similar magnitude, and the differences in the residuals et - et-1 will be small. Durbin and

478

REGRESSION ANALYSIS OF TIME SERIES DATA

Watson suggest several procedures for resolving inconclusive results. A reasonable approach in many of these inconclusive situations is to analyze the data as if there were positive autocorrelation present to see if any major changes in the results occur.
Situations where negative autocorrelation occurs are not often encountered. However, if a test for negative autocorrelation is desired, one can use the statistic 4 - d, where d is defined in Eq. (14.4). Then the decision rules for testing the hypotheses H0 :  = 0 versus H1 :  < 0 are the same as those used in testing for positive autocorrelation. It is also possible to test a two-sided alternative hypothesis (H0 :  = 0 versus H1 :   0 ) by using both of the one-sided tests simultaneously. If this is done, the two-sided procedure has type I error 2, where  is the type I error used for each individual one-sided test.

Example 14.1

A company wants to use a regression model to relate annual regional advertising expenses to annual regional concentrate sales for a soft drink company. Table 14.1 presents 20 years of these data. We will initially assume that a straight-line relationship is appropriate and fit a simple linear regression model by ordinary least squares. The Minitab output for this model is shown in Table 14.2 and the residuals are shown in the last column of Table 14.1. Because these are time series data, there is a possibility that autocorrelation may be present. The plot of residuals versus time, shown in Figure 14.1, has a pattern indicative of potential autocorrelation; there is a definite upward trend in the plot, followed by a downward trend.

TABLE 14.1 Soft Drink Concentrate Sales Data

Year

Sales (Units)

Expenditures (1,000 of dollars)

Residuals

1

3083

2

3149

3

3218

4

3239

5

3295

6

3374

7

3475

8

3569

9

3597

10

3725

11

3794

12

3959

13

4043

14

4194

15

4318

16

4493

17

4683

18

4850

19

5005

20

5236

75

-32.3298

78

-26.6027

80

2.2154

82

-16.9665

84

-1.1484

88

-2.5123

93

-1.9671

97

11.6691

99

-0.5128

104

27.0324

109

-4.4224

115

40.0318

120

23.5770

127

33.9403

135

-2.7874

144

-8.6060

153

0.5753

161

6.8476

170

-18.9710

182

-29.0625

DETECTING AUTOCORRELATION: THE DURBIN­WATSON TEST

479

TABLE 14.2 Minitab Output for the Soft Drink Concentrate Sales Data

Regression Analysis: Sales versus Expenditures

The regression equation is

Sales = 1609 + 20.1 Expenditures

Predictor

Coef SE Coef

T

P

Constant

1608.51 17.02 94.49 0.000

Expenditures

20.0910 0.1428 140.71 0.00

S = 20.5316 R-Sq = 99.9%

R-Sq(adj) = 99.9%

Analysis of Variance

Source Regression Residual Error Total

DF

SS

MS

F

P

1 8346283 8346283 19799.11 0.000

18

7588

422

19 8353871

Unusual Observations

Obs Expenditures

Sales

Fit SE Fit Residual St Resid

12

115 3959.00 3918.97

4.59

40.03

2.00R

R denotes an observation with a large standardized residual.

Durbin­Watson statistic = 1.08005

Residuals

Time Series Plot of Residuals 40 30 20 10
0 ­10 ­20 ­30 ­40
2 4 6 8 10 12 14 16 18 20 Time (Years)
Figure 14.1 Plot of residuals versus time for the soft drink concentrate sales model.
We will use the Durbin­Watson test for
H0 :  = 0 H1 :  > 0
The test statistic is calculated as follows:

480

REGRESSION ANALYSIS OF TIME SERIES DATA

20
 (et - et-1)2
d = t=2 20
 et2
t=1
= [-26.6027 - (-32.3298)]2 + [2.2154 - (-26.6027)]2 + + [-29.0625 - (-18.9710)]2 (-32.3298)2 + (-26.6027)2 + + (-29.0625)2
= 1.08

Minitab will also calculate and display the Durbin­Watson statistic. Refer to the

Minitab output in Table 14.2. If we use a significance level of 0.05, Table A.6 gives

the critical values corresponding to one predictor variable and 20 observations as

dL = 1.20 and dU = 1.41. Since the calculated value of the Durbin­Watson statistic

d = 1.08 is less than dL = 1.20, we reject the null hypothesis and conclude that the

errors in the regression model are positively autocorrelated.



14.3 ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS
A significant value of the Durbin­Watson statistic or a suspicious residual plot indicates a potential problem with autocorelated model errors. This could be the result of an actual time dependence in the errors or an ``artificial'' time dependence caused by the omission of one or more important predictor variables. If the apparent autocorrelation results from missing predictors and if these missing predictors can be identified and incorporated into the model, the apparent autocorrelation problem may be eliminated. This is illustrated in the following example.

Example 14.2

Table 14.3 presents an expanded set of data for the soft drink concentrate sales

problem introduced in Example 14.1. Because it is reasonably likely that regional

population affects soft drink sales, we have provided data on regional population

for each of the study years. Table 14.4 is the Minitab output for a regression model

that includes both predictor variables, advertising expenditures and population.

Both of these predictor variables are highly significant. The last column of Table

14.3 shows the residuals from this model. Minitab calculates the Durbin­Watson

statistic for this model as d = 3.05932, and the 5% critical values are dL = 1.10 and dU = 1.54, and since d is greater than dU, we conclude that there is no evidence to reject the null hypothesis. That is, there is no indication of autocorrelation in the

errors.

Figure 14.2 is a plot of the residuals from this regression model in time order.

This plot shows considerable improvement when compared to the plot of residuals

from the model using only advertising expenditures as the predictor. Therefore, we

conclude that adding the new predictor population size to the original model has

eliminated an apparent problem with autocorrelation in the errors.



ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

481

TABLE 14.3 Expanded Soft Drink Concentrate Sales Data for Example 14.2

Year

Sales (Units)

Expenditures (1,000 of dollars)

Population

Residuals

1

3083

2

3149

3

3218

4

3239

5

3295

6

3374

7

3475

8

3569

9

3597

10

3725

11

3794

12

3959

13

4043

14

4194

15

4318

16

4493

17

4683

18

4850

19

5005

20

5236

75

825000

-4.8290

78

830445

-3.2721

80

838750

14.9179

82

842940

-7.9842

84

846315

5.4817

88

852240

0.7986

93

860760

-4.6749

97

865925

6.9178

99

871640

-11.5443

104

877745

14.0362

109

886520

-23.8654

115

894500

17.1334

120

900400

-0.9420

127

904005

14.9669

135

908525

-16.0945

144

912160

-13.1044

153

917630

1.8053

161

922220

13.6264

170

925910

-3.4759

182

929610

0.1025

TABLE 14.4 Minitab Output for the Soft Drink Concentrate Data in Example 14.2

Regression Analysis: Sales versus Expenditures, Population

The regression equation is Sales = 320 + 18.4 Expenditures + 0.00168 Population

Predictor Constant Expenditures Population S = 12.0557

Coef SE Coef

T

P

320.3

217.3 1.47 0.159

18.4342

0.2915 63.23 0.000

0.0016787 0.0002829 5.93 0.000 R-Sq = 100.0% R-Sq(adj) = 100.0%

Analysis of Variance

Source

DF

Regression

2

Residual Error

17

Total

19

SS 8351400
2471 8353871

MS 4175700
145

F

P

28730.40 0.000

Source

DF Seq SS

Expenditures

1 8346283

Population

1

5117

Unusual Observations

Obs

Expenditures Sales

Fit SE Fit Residual St Resid

11

109 3794.00 3817.87

4.27 -23.87

-2.12R

R denotes an observation with a large standardized residual. Durbin­Watson statistic = 3.05932

482

REGRESSION ANALYSIS OF TIME SERIES DATA

Time Series Plot of RESI1 20

10

Residuals

0

­10

­20

­30 2 4 6 8 10 12 14 16 18 20 Time (Years)
Figure 14.2 Plot of residuals versus time for the soft drink concentrate sales model in example 14.2.

The Cochrane­Orcutt Method When the observed autocorrelation in the model errors cannot be removed by adding one or more new predictor variables to the model, it is necessary to take explicit account of the autocorrelative structure in the model and use an appropriate parameter estimation method. A very good and widely used approach is the procedure devised by Cochrane and Orcutt (1949).
We now describe the Cochrane­Orcutt method for the simple linear regression model with first-order autocorrelated errors given in Eq. (14.2). The procedure is based on transforming the response variable so that yt = yt -  yt-1. Substituting for yt and yt-1, the model becomes

yt = yt -  yt-1 = 0 + 1xt + t - (0 + 1xt-1 + t-1 = 0 (1 - ) + 1(xt - xt-1 ) + t - t-1 = 0 + 1xt + t

(14.7)

where 0 = 0 (1 - ) and xt = xt - xt-1. Notice that the error terms at in the transformed or reparameterized model are independent random variables. Unfortunately, this new reparameterized model contains an unknown parameter  and it is also no longer linear in the unknown parameters because it involves products of , 0, and 1. However, the first-order autoregressive process t = t-1 + at can be viewed as a simple linear regression through the origin and the parameter  can be
estimated by obtaining the residuals of an OLS regression of yt on xt and then regressing et on et-1. The OLS regression of et on et-1 results in

T
 etet-1

^ =

t=2 T

 et2

t=1

(14.8)

ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

483

Using ^ as an estimate of , we can calculate the transformed response and predictor variables as
yt = yt - ^ yt-1 xt = xt - ^ xt-1
Now apply ordinary least squares to the transformed data. This will result in estimates of the transformed slope ^0, the intercept ^1, and a new set of residuals. The Durbin­Watson test can be applied to these new residuals from the reparametrized model. If this test indicates that the new residuals are uncorrelated, then no additional analysis is required. However, if positive autocorrelation is still indicated, then another iteration is necessary. In the second iteration  is estimated with new residuals that are obtained by using the regression coefficients from the reparametrized model with the original regressor and response variables. This iterative procedure may be continued as necessary until the residuals indicate that the error terms in the reparametrized model are uncorrelated. Usually only one or two iterations are sufficient to produce uncorrelated errors.

Example 14.3

Table 14.5 presents data on the market share of a particular brand of toothpaste for 30 time periods and the corresponding selling price per pound. A simple linear regression model is fit to these data, and the resulting Minitab output is in Table 14.6. The residuals from this model are shown in Table 14.5. The Durbin­Watson

TABLE 14.5 Toothpaste Market Share Data

Time

Market Share

Price

Residuals

1

3.63

0.97

0.281193

2

4.20

0.95

0.365398

3

3.33

0.99

0.466989

4

4.54

0.91

-0.266193

5

2.89

0.98

-0.215909

6

4.87

0.90

-0.179091

7

4.90

0.89

-0.391989

8

5.29

0.86

-0.730682

9

6.18

0.85

-0.083580

10

7.20

0.82

0.207727

11

7.25

0.79

-0.470966

12

6.09

0.83

-0.659375

13

6.80

0.81

-0.435170

14

8.65

0.77

0.443239

15

8.43

0.76

-0.019659

16

8.29

0.80

0.811932

17

7.18

0.83

0.430625

18

7.90

0.79

0.179034

19

8.45

0.76

0.000341

20

8.23

0.78

0.266136

yt
2.715 1.612 3.178 1.033 3.688 2.908 3.286 4.016 4.672 4.305 3.125 4.309 5.869 4.892 4.842 3.789 4.963 5.219 4.774

xt
0.533 0.601 0.505 0.608 0.499 0.522 0.496 0.498 0.472 0.455 0.507 0.471 0.439 0.445 0.489 0.503 0.451 0.437 0.469

Residuals
-0.189435 0.392201
-0.420108 -0.013381 -0.058753 -0.268949 -0.535075
0.244473 0.256348 -0.531811 -0.423560 -0.131426 0.635804 -0.192552 0.847507 0.141344 0.027093 -0.063744 0.284026

484

REGRESSION ANALYSIS OF TIME SERIES DATA

TABLE 14.6 Minitab Regression Results for the Toothpaste Market Share Data

Regression Analysis: Market Share versus Price

The regression equation is Market Share = 26.9 - 24.3 Price

Predictor

Coef SE Coef

Constant

26.910

1.110

Price S = 0.428710

-24.290

1.298

R-Sq = 95.1%

T

P

24.25 0.000

-18.72 0.000 R-Sq(adj) = 94.8%

Analysis of Variance

Source

DF

SS

Regression

1 64.380

Residual Error

18

3.308

Total

19 67.688

Durbin­Watson statistic = 1.13582

MS 64.380
0.184

F 350.29

P 0.000

TABLE 14.7 Minitab Regression Results for Fitting the Transformed Model to the Toothpaste Sales Data

Regression Analysis: y-prime versus x-prime

The regression equation is y-prime = 16.1 - 24.8 x-prime

Predictor Constant x-prime

Coef SE Coef

T

P

16.1090 0.9610 16.76 0.000

-24.774 1.934 -12.81 0.000

S = 0.390963 R-Sq = 90.6%

R-Sq(adj) = 90.1%

Analysis of Variance

Source

DF

SS

Regression

1 25.080

Residual Error 17 2.598

Total

18 27.679

MS 25.080
0.153

F 164.08

P 0.000

Unusual Observations

Obs

x-prime y-prime

Fit SE Fit Residual St Resid

2

0.601 1.6120 1.2198 0.2242 0.3922 1.22 X

4

0.608 1.0330 1.0464 0.2367 -0.0134 -0.04 X

15

0.489 4.8420 3.9945 0.0904 0.8475

2.23R

R denotes an observation with a large standardized residual.

X denotes an observation whose X value gives it large influence. Durbin­Watson statistic = 2.15671

statistic for the residuals from this model is d = 1.13582 (see the Minitab output), and the 5% critical values are dL = 1.20 and dU = 1.41, so there is evidence to support the conclusion that the residuals are positively autocorrelated.
We use the Cochrane­Orcutt method to estimate the model parameters. The autocorrelation coefficient can be estimated using the residuals in Table 14.7 and Eq. (14.8) as follows:

ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

485

T
 etet-1

^ =

t=2 T

 et2

y=1

= 1.3547 3.3083

= 0.409

The transformed variables are computed according to

yt = yt - 0.409yt-1 xt = xt - 0.409xt-1

for t = 2, 3, . . . , 20. These transformed variables are also shown in Table 14.5. The

Minitab results for fitting a regression model to the transformed data are summa-

rized in Table 14.7. The residuals from the transformed model are shown in the last

column of Table 14.5. The Durbin­Watson statistic for the transformed model is

d = 2.15671, and the 5% critical values from Table A.6 are dL = 1.18 and dU = 1.40,

so we conclude that there is no problem with autocorrelated errors in the trans-

formed model. The Cochrane­Orcutt method has been effective in removing the

autocorrelation.

The slope in the transformed model 1 is equal to the slope in the original model,

1. A comparison of the slopes in the two models in Tables 14.6 and 14.7 shows that

the two estimates are very similar. However, if the standard errors are compared,

the Cochrane­Orcutt method produces an estimate of the slope that has a larger

standard error than the standard error of the ordinary least squares estimate. This

reflects the fact that if the errors are autocorrelated and OLS is used, the standard

errors of the model coefficients are likely to be underestimated.



The Maximum Likelihood Approach There are other alternatives to the Cochrane­Orcutt method. A popular approach is to use the method of maximum likelihood to estimate the parameters in a time-series regression model. We will concentrate on the simple linear regression model with first-order autoregressive errors

yt = 0 + 1xt + t , t = t-1 + at

(14.9)

One reason that the method of maximum likelihood is so attractive is that, unlike

the Cochrane­Orcutt method, it can be used in situations where the autocorrelative

structure of the errors is more complicated than first-order autoregressive.

Recall that the a's in Eq. (14.9) are normally and independently distributed with

mean zero and variance



2 a

and  is the autocorrelation parameter. Write this equa-

tion for yt-1 and subtract  yt-1 from yt. This results in

yt -  yt-1 = (1 - )0 + 1(xt - xt-1 ) + at

486

REGRESSION ANALYSIS OF TIME SERIES DATA

or

yt =  yt-1 + (1 - )0 + 1(xt -  xt-1) + at = (zt ,q) + at

(14.10)

where zt = [yt-1, xt ] and q = [, 0, 1]. We can think of zt as a vector or predictor variables and  as the vector of regression model parameters. Since yt-1 appears on the right-hand side of the model in Eq. (14.10), the index of time must run from 2, 3, . . . , T. At time period t = 2, we treat y1 as an observed predictor.
Because the a's are normally and independently distributed, the joint probability
density of the a's is

 f (a2 , a3,..., aT ) =

T t=2

1 a 2

e-

1 2

 

at a

 

2

 =

 

a

1 2

T -1 

exp

 

-

1

2

2 a

T t=1

a t2

 

and the likelihood function is obtained from this joint distribution by substituting for the a's:

 l(yt

, , 0 , 1 )

=

 

a

1 2

 T -1 

 exp 

-

1 2 a2

T t=2

{yt

- [ yt-1

+

(1 - )0

+

1 ( xt

-



xt

-1

)]}2

 

The log-likelihood is

ln l ( yt ,, 0 , 1 ) =

 -

T

- 2

1

ln

( 2

)

-

(T

-

1)

ln

a

-

1

2

2 a

T
{yt - [ yt-1 + (1 -  ) 0 + 1 ( xt
t=2

-  xt-1 )]}2

This log-likelihood is maximized with respect to the parameters , 0, and 1 by minimizing the quantity

T
 SSE = {yt - [ yt-1 + (1 - )0 + 1(xt - xt-1)]}2 t=2

(14.11)

which is the error sum of squares for the model. Therefore, the maximum likelihood estimators of , 0, and 1 are also least squares estimators.
There are two important points about the maximum likelihood (or least squares) estimators. First, the sum of squares in Eq. (14.11) is conditional on the initial value of the time series, y1. Therefore, the maximum likelihood (or least squares) estimators found by minimizing this conditional sum of squares are conditional maximum likelihood (or conditional least squares) estimators. Second, because the model involves products of the parameters  and 0, the model is no longer linear in the unknown parameters. That is, it is not a linear regression model and consequently we cannot give an explicit closed-form solution for the parameter estimators.

ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

487

Iterative methods for fitting nonlinear regression models must be used. From

Chapter 12, we know that these procedures work by linearizing the model about a

set of initial guesses for the parameters, solving the linearized model to obtain

improved parameters estimates, then using the improved estimates to define a new

linearized model which leads to new parameter estimates, and so on.

Suppose that we have obtained a set of parameter estimates, say q^  = [^, ^0, ^1].

The

maximum

likelihood

estimate

of



2 a

is

computed

as

^

2 a

=

SSE (q^ ) n-1

(14.12)

( ) where SSE q^ is the error sum of squares in Eq. (14.11) evaluated at the conditional

maximum likelihood (or conditional least squares) parameters estimates q^  = [^, ^0, ^1]. Some authors (and computer programs) use an adjusted number

of degrees of freedom in the denominator to account for the number of parameters

that have been estimated. If there are k predictors, then the number of estimated

parameters

will

be

p

=

k

+

3,

and

the

formula

for

estimating



2 a

is

^

2 a

=

SSE (q^ ) n- p-1

=

SSE (q^ ) n-k-4

(14.13)

In order to test hypotheses about the model parameters and to find confidence intervals, standard errors of the model parameters are needed. The standard errors are usually found by expanding the nonlinear model in a first-order Taylor series around the final estimates of the parameters q^  = [^, ^0, ^1]. This results in

( ) ( ) yt  

zt ,q^

+

q - q^





( zt , q )
q

|q=q^

+at

The

column

vector

of

derivatives,



(zt , q

q)

,

is

found

by

differentiating

the

model

with respect to each parameter in the vector  = [, 0, 1]. This vector of deriva-

tives is



1-



(zt ,q) q

=

   yt-1

xt - - 0

xt-1 - 1xt-1

  

This vector is evaluated for each observation at the set of conditional maximum likelihood parameter estimates q^  = [^, ^0, ^1] and assembled into an X matrix. Then the covariance matrix of the parameter estimates is found from

( ) Cov

q^

=



2 a

(XX)-1

When



2 a

is

replaced

by

the

estimate

^

2 a

from

Eq.

(14.13)

an

estimate

of

the

covari-

ance matrix results, and the standard errors of the model parameters are the main

diagonals of the covariance matrix.

488

REGRESSION ANALYSIS OF TIME SERIES DATA

Example 14.4

We will fit the regression model with time series errors in Eq. (14.9) to the toothpaste market share data originally analyzed in Example 14.3. Minitab will not fit these types of regression models, so we will use another widely available software package, SAS (the Statistical Analysis System). The SAS procedure for fitting regression models with time series errors is SAS PROC AUTOREG. Table 14.8 contains the output from this software program for the toothpaste market share data. Notice that the autocorrelation parameter (or the lag one autocorrelation) is estimated to be 0.4094, which is very similar to the value obtained by the Cochrane­ Orcutt method. The overall R2 for this model is 0.9601, and we can show that the residuals exhibit no autocorrelative structure, so this is likely a reasonable model for the data.
There is, of course, some possibility that a more complex autocorrelation structure that first-order may exist. SAS PROC AUTOREG can fit more complex patterns. Since there is obviously first-order autocorrelation present, an obvious possibility is that the autocorrelation might be second-order autoregressive, as in
t = 1t-1 + 2t-2 + at

where the parameters 1 and 2 are autocorrelations at lags one and two, respectively.The output from SAS AUTOREG for this model is in Table 14.9.The t statistic

for the lag two autocorrelation is not significant so there is no reason to believe that

this more complex autocorrelative structure is necessary to adequately model the

data. The model with first-order autoregessive errors is satisfactory.



Prediction of New Observations and Prediction Intervals We now consider how to obtain predictions of new observations. These are actually forecasts of future values at some lead time. It is very tempting to ignore the autocorrelation in the data when making predictions of future values (forecasting), and simply substitute the conditional maximum likelihood estimates into the regression equation:

y^t = ^0 + ^1xt
Now. suppose that we are at the end of the current time period, T, and we wish to obtain a prediction or forecast for period T + 1. Using the above equation, this results in

y^T +1(T ) = ^0 + ^1xT +1
assuming that the value of the predictor variable in the next time period xT+1 is known. Unfortunately, this naive approach isn't correct. From Eq. (14.10), we know that the observation at time period t is

yt =  yt-1 + (1 - )0 + 1(xt - xt-1 ) + at

(14.14)

TABLE 14.8 SAS PROC AUTOREG Output for the Toothpaste Market Share Data, Assuming First-Order Autoregressive Errors

The SAS System The AUTOREG Procedure Dependent Variable y

Ordinary Least Squares Estimates

SSE

3.30825739 DFE

18

MSE

0.18379 Root MSE

0.42871

SBC

26.762792 AIC

24.7713275

Regress R-Square

0.9511 Total R-Square

0.9511

Durbin­Watson

1.1358 Pr < DW

0.0098

Pr > DW

0.9902

NOTE: Pr<DW is the p-value for testing positive autocorrelation,

and Pr>DW is the p-value for testing negative autocorrelation.

Standard Variable Intercept x

ApproxVariable DF EstimateError
1 26.9099 1.1099 1 -24.2898 1.2978

t Value Pr > |t| 24.25 <.0001
-18.72 <.0001

Variable Label
x

Estimates of Autocorrelations

Lag Covariance Correlation -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 67891

0

0.1654

1.000000 |

1

0.0677

0.409437 |

|********************|

|********

|

Preliminary MSE 0.1377 Estimates of Autoregressive Parameters

Standard

Lag

Coefficient

1

-0.409437

Algorithm converged.

Error 0.221275

t Value -1.85

The SAS System The AUTOREG Procedure Maximum Likelihood Estimates

SSE

2.69864377 DFE

17

MSE

0.15874 Root MSE

0.39843

SBC

25.8919447 AIC

22.9047479

Regress R-Square Durbin­Watson Pr > DW

0.9170 Total R-Square 1.8924 Pr < DW 0.6528

0.9601 0.3472

NOTE: Pr<DW is the p-value for testing positive autocorrelation, and Pr>DW is the p-value for testing negative autocorrelation.

Standard Variable
Intercept x AR1

Approx Variable DF Estimate Error t Value Pr > |t| Label

1 26.3322 1.4777 1 -23.5903 1.7222 1 -0.4323 0.2203

17.82 -13.70
-1.96

<.0001 <.0001 x
0.0663

Autoregressive parameters assumed given.

Standard Variable
Intercept x

Approx Variable

DF Estimate

Error t Value Pr > |t| Label

1 26.3322 1 -23.5903

1.4776 17.82 1.7218 -13.70

<.0001 <.0001 x

TABLE 14.9 SAS PROC AUTOREG Output for the Toothpaste Market Share Data, Assuming Second-Order Autoregressive Errors

The SAS System The AUTOREG Procedure Dependent Variable y

Ordinary Least Squares Estimates

SSE

3.30825739 DFE

18

MSE

0.18379 Root MSE

0.42871

SBC

26.762792 AIC

24.7713275

Regress R-Square Durbin-Watson Pr > DW

0.9511 Total R-Square 1.1358 Pr < DW 0.9902

0.9511 0.0098

NOTE: Pr<DW is the p-value for testing positive autocorrelation, and Pr>DW is the p-value for testing negative autocorrelation.

Standard Variable Intercept x

Approx Variable

DF Estimate

Error t Value Pr > |t|

1 26.9099 1.1099

24.25

<.0001

1 -24.2898 1.2978 -18.72

<.0001

Label x

Estimates of Autocorrelations

Lag Covariance Correlation -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6

7891

0

0.1654

1.000000 | |********************|

1

0.0677

0.409437 | |******** |

2

0.0223

0.134686 | |*** |

Preliminary MSE 0.1375

Estimates of Autoregressive Parameters

Standard Lag

Coefficient

Error t Value

1

-0.425646 0.249804

2

0.039590 0.249804

Algorithm converged.

-1.70 0.16

The SAS System The AUTOREG Procedure Maximum Likelihood Estimates

SSE

2.69583958 DFE

16

MSE

0.16849 Root MSE

0.41048

SBC

28.8691217 AIC

24.8861926

Regress R-Square

0.9191 Total R-Square

0.9602

Durbin-Watson

1.9168 Pr < DW

0.3732

Pr > DW

0.6268

NOTE: Pr<DW is the p-value for testing positive autocorrelation,

and Pr>DW is the p-value for testing negative autocorrelation.

Standard

Approx Variable

Variable DF Estimate

Error t Value Pr > |t|

Intercept x AR1 AR2

1 26.3406 1 -23.6025 1 -0.4456 1 0.0297

1.5493 1.8047 0.2562 0.2617

17.00 -13.08
-1.74 0.11

<.0001 <.0001 0.1012
0.9110

Label x

Autoregressive parameters assumed given.

Standard Variable
Intercept x

Approx Variable

DF Estimate

Error t Value Pr > |t|

1 26.3406 1 -23.6025

1.5016 17.54 1.7502 -13.49

<.0001 <.0001

Label x

ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

491

So at the end of the current time period T the next observation is

yT +1 =  yT + (1 - )0 + 1(xT +1 - xT ) + aT +1

Assume that the future value of the regressor variable xT+1 is known. Obviously, at the end of the current time period, both yT and xT are known. The random error at time T + 1 aT+1 hasn't been observed yet, and because we have assumed that the expected value of the errors is zero, the best estimate we can make of aT+1 is aT+1 = 0. This suggests that a reasonable forecast of the observation in time period T + 1 that
we can make the end of the current time period T is

y^T +1(T ) = ^ yT + (1 - ^)^0 + ^1(xT +1 - ^ xT )

(14.15)

Notice that this forecast is likely to be very different than the naïve forecast obtained by ignoring the autocorrelation.
To find a prediction interval on the forecast, we need to find the variance of the prediction error. The one-step-ahead forecast error is

yT +1 - y^T +1(T ) = aT +1
assuming that all of the parameters in the forecasting model are known. The variance of the one-step ahead forecast error is

V

(aT

+

1

)

=



2 a

Using the variance of the one-step-ahead forecast error, we can construct a 100(1)% prediction interval for the lead-one forecast from Eq. (14.15). The PI is

y^T +1(T ) ± z /2 a

where z /2 is the upper /2 percentage point of the standard normal distribution. To actually compute an interval, we must replace a by an estimate, resulting in

y^T +1(T ) ± z /2^ a

(14.16)

as the PI. Because a and the model parameters in the forecasting equation been replaced by estimates, the probability level on the PI in Eq. (14.16) is only approximate.
Now suppose that we want to forecast two periods ahead assuming that we are at the end of the current time period, T. Using Eq. (14.14), we can write the observation at time period T + 2 as

yT +2 =  yT +1 + (1 - )0 + 1(xT +2 - xT +1 ) + aT +2 = [ yT + (1 - )0 + 1(xT +1 - xT ) + aT +1 ] + (1 - )0 + 1(xT +2 - xT +1 ) + aT +2

Assume that the future value of the regressor variables xT+1 and xT+2 are known. At the end of the current time period, both yT and xT are known. The random errors

492

REGRESSION ANALYSIS OF TIME SERIES DATA

at time T + 1 and T + 2 haven't been observed yet, and because we have assumed that the expected value of the errors is zero, the best estimate we can make of both aT+1 and aT+2 is zero. This suggests that the forecast of the observation in time period T + 2 made at the end of the current time period T is

y^T +2 (T ) = ^[^ yT + (1 - ^)^0 + ^1(xT +1 - ^ xT )] + (1 - ^)0 + ^1(xT +2 - ^ xT +1)

= ^ y^T +1(T ) + (1 - ^)^0 + ^1(xT +2 - ^ xT +1 )

(14.17)

The two-step-ahead forecast error is

yT +2 - y^T +2 (T ) = aT +2 + aT +1

assuming that all estimated parameters are actually known. The variance of the two-step ahead forecast error is

V(aT +2

+

aT +1 )

=



2 a

+



2

2 a

=

(1

+



2

)

2 a

Using the variance of the two-step-ahead forecast error, we can construct a 100(1)% PI for the lead-one forecast from Eq. (14.15):

y^T +2 (T ) ± z /2[(1 + 2 )]1/2  a

To actually compute the PI, both a and  must be replaced by estimates, resulting in

y^T +2 (T ) ± z /2[(1 + ^ 2 )]1/2 ^ a

(14.18)

as the PI. Because a and  have been replaced by estimates, the probability level on the PI in Eq. (14.18) is only approximate.
In general, if we want to forecast  periods ahead, the forecasting equation is

y^T + (T ) = ^ y^T + -1(T ) + (1 - ^)^0 + ^1(xT + - ^ xT + -1 )

(14.19)

The -step-ahead forecast error is (assuming that the estimated model parameters are known)
yT + - y^T + (T ) = aT + + aT + -1 + ... +  -1aT +1
and the variance of the -step-ahead forecast error is

V(aT +

+ aT + -1

+ ... +  -1aT +1 )

=

(1 + 2

+

...

+



2(

-1)

)

2 a

=

1 - 2 1+2



2 a

ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

493

A 100(1-)% PI for the lead- forecast from Eq. (14.19) is

y^T +

(T )

±

z /2

 

1 - 2 1+2

 

1/2

a

Replacing a and  by estimates, the approximate 100(1-)% PI is actually computed from

y^T +

(T )

±

z /2

 

1 - ^ 2 1 + ^ 2

 

1/2

^ a

(14.20)

The Case Where the Predictor Variable Must Also Be Forecast In the preceding discussion, we assumed that in order to make forecasts, any necessary values of the predictor variable in future time periods T +  are known. This is often (probably usually) an unrealistic assumption. For example, if you are trying to forecast how many new vehicles will be registered in the state of Arizona in some future year T +  as a function of the state population in year T + , it's pretty unlikely that you will actually know the state population in that future year.
A straightforward solution to this problem is to replace the required future values of the predictor variable in future time periods T +  by forecasts of these values. For example, suppose that we are forecasting one period ahead. From Eq. (14.15) we know that the forecast for yT+1 is
y^T +1(T ) = ^ yT + (1 - ^)^0 + ^1(xT +1 - ^ xT )

But the future value of xT+1 isn't known. Let x^T +1(T ) be an unbiased forecast of xT+1, made at the end of the current time period T. Now the forecast for yT+1 is

y^T +1(T ) = ^ yT + (1 - ^)^0 + ^1[x^T +1(T ) - ^ xT ]

(14.21)

If we assume that the model parameters are known, the one-step-ahead forecast error is

yT +1 - y^T +1(T ) = aT +1 + 1[xT +1 - x^T +1(T )]

and the variance of this forecast error is

V(aT +1 )

=



2 a

+

12

2 x

(1)

(14.22)

where



2 x

(1)

is

the

variance

of

the

one-step-ahead

forecast

error

for

the

predictor

variable x and we have assumed that the random error aT+1 in period T + 1 is inde-

pendent of the error in forecasting the predictor variable. Using the variance of the

one-step-ahead forecast error we can construct a 100(1-)% prediction interval for

the lead-one forecast from Eq. (14.21). The PI is

y^T +1(T )

±

z

/

2

[

2 a

+

12

2 x

(1)]1/

2

where z /2 is the upper /2 percentage point of the standard normal distribution. To

actually

compute

an

interval,

we

must

replace

the

parameters

1,



2 a

and



2 x

(1)

by

estimates, resulting in

494

REGRESSION ANALYSIS OF TIME SERIES DATA

y^T +1(T )

±

z

/

2

[^

2 a

+

^ 12^

2 x

(1)]1/

2

(14.23)

as the PI. Because the parameters have been replaced by estimates, the probability level on the PI in Eq. (14.23) is only approximate.
In general, if we want to forecast  periods ahead, the forecasting equation is

y^T + (T ) = ^ y^T + -1(T ) + (1 - ^)^0 + ^1[x^T + (T ) - ^ x^T + -1(T )]

(14.24)

The -step-ahead forecast error is, assuming that the model parameters are known,

yT + - y^T + (T ) = aT + + aT + -1 + ... +  -1aT +1 + 1[xT + - x^T + (T )]

and the variance of the -step-ahead forecast error is

V(aT +

+ aT + -1

+ ... +  -1aT +1 )

=

(1 + 2

+

...

+



2(

-

1)

)

2 a

+

12

2 x

(

)

=

1 - 2 1+2



2 a

+

12

2 x

(

)

where



2 x

(

)

is

the

variance

of

the

-step-ahead

forecast

error

for

the

predictor

vari-

able x. A 100(1-)% PI for the lead- forecast from Eq. (14.24) is

y^T + (T ) ±

z /2

 

1 - 2 1+2



2 a

+

12

2 x

(

)

1/

2

Replacing all of the unknown parameters by estimates, the approximate 100(1-)% PI is actually computed from

y^T + (T ) ±

z /2

 

1 - ^ 2 1 + ^ 2

^

2 a

+

^ 12^

2 x

(

 )

1/

2

(14.25)

Alternate Forms of the Model The regression model with autocorrelated errors

yt =  yt-1 + (1 - )0 + 1(xt - xt-1 ) + at

is a very useful model for forecasting time-series regression data. However, when using this model there are two alternatives that should be considered. The first of these is

yt =  yt-1 + 0 + 1xt + 2 xt-1 + at

(14.26)

This model removes the requirement that the regression coefficient for the lagged predictor variable xt-1 be equal to -1. An advantage of this model is that it can be fit by ordinary least squares. Another alternative model to consider is to simply drop the lagged value of the predictor variable from Eq. (14.26), resulting in

yt =  yt-1 + 0 + 1xt + at

(14.27)

ESTIMATING THE PARAMETERS IN TIME SERIES REGRESSION MODELS

495

Often just including the lagged value of the response variable is sufficient and Eq. (14.27) will be satisfactory.
The choice between models should always be a data-driven decision. The different models can be fit to the available data, and model selection can be based in the criteria that we have discussed previously, such as model adequacy checking and residual analysis, and (if enough data are available to do so split the data into an estimation set to fit the model and then evaluate how the different models perform on the remaining test or evaluation data set) forecasting performance over a test or trial period of data. See Montgomery, Jennings, and Kulahci (2008) for more discussion.

Example 14.5
Reconsider the toothpaste market share data originally presented in Example 14.3 and modeling with a time series regression model with first-order autoregressive errors in Example 14.4. First we will try fitting the model in Eq. (14.26). This model simply relaxes the restriction that the regression coefficient for the lagged predictor variable xt-1 (price in this example) be equal to -1. Since this is just a linear regression model, we can fit it using Minitab. Table 14.10 contains the Minitab results.
This model is a good fit to the data. The Durbin­Watson Statistic is d = 2.04203, which indicates no problems with autocorrelations in the residuals. However, note that the t-statistic for the lagged predictor variable (price) is not significant (P = 0.217) indicating that this variable could be removed from the model. If xt-1 is removed then the model becomes the one in Eq. (14.27). The Minitab output for this model is in Table 14.11.

TABLE 14.10 Minitab Results for Fitting Model (14.26) to the Toothpaste Market Share Data

Regression Analysis: y versus y(t-1), x, x(t-1)

The regression equation is y = 16.1 + 0.425 y(t-1) - 22.2 x + 7.56 x(t-1)

Predictor Constant y(t-1) x x(t-1) S = 0.402205

Coef SE Coef

16.100

6.095

0.4253 0.2239

-22.250

2.488

7.562

5.872

R-Sq = 96.0%

T

P

2.64 0.019

1.90 0.077

-8.94 0.000

1.29 0.217 R-Sq(adj) = 95.2%

Analysis of Variance

Source

DF

Regression

3

Residual Error

15

Total

18

SS 58.225
2.427 60.651

MS 19.408
0.162

F 119.97

P 0.000

Source

DF

Seq SS

y(t-1)

1

44.768

x

1

13.188

x(t-1)

1

0.268

Durbin­Watson statistic = 2.04203

496

REGRESSION ANALYSIS OF TIME SERIES DATA

TABLE 14.11 Minitab Results for Fitting Model (14.27) to the Toothpaste Market Share Data

Regression Analysis: y versus y(t-1), x

The regression equation is

y = 23.3 + 0.162 y(t-1) - 21.2 x

Predictor

Coef SE Coef

Constant

23.279

2.515

y(t-1)

0.16172 0.09238

x

-21.181

2.394

S = 0.410394

R-Sq = 95.6%

T

P

9.26 0.000

1.75 0.099

-8.85 0.000 R-Sq(adj) = 95.0%

Analysis of Variance

Source

DF

Regression

2

Residual Error

16

Total

18

SS 57.956
2.695 60.651

MS 28.978
0.168

F 172.06

P 0.000

Source

DF

Seq SS

y(t-1)

1 44.768

x

1 13.188

Durbin­Watson statistic = 1.61416

This model is also a good fit to the data. Both predictors, the lagged variable yt-1

and xt, are significant. The Durbin­Watson statistic does not indicate any significant

problems with autocorrelation. It seems that either of these models would be rea-

sonable ones for the toothpaste market share data. The advantage of these models

relative to the time series regression model with autocorrelated errors is that they

can be fit by ordinary least squares. In this example, including a lagged response

variable and a lagged predictor variable has essentially eliminated any problems

with autocorrelated errors.



PROBLEMS
14.1 Table B.17 contains data on the global mean surface air temperature anomaly and the global CO2 concentration. Fit a regression model to these data, using the global CO2 concentration as the predictor. Analyze the residuals from this model. Is there evidence of autocorrelation in these data? If so, use one iteration of the Cochrane­Orcutt method to estimate the parameters.
14.2 Table B.18 contains hourly yield measurements from a chemical process and the process operating temperature. Fit a regression model to these data with the Cochrane­Orcutt method, using the temperature as the predictor. Analyze the residuals from this model. Is there evidence of autocorrelation in these data?
14.3 The data in the table below give the percentage share of market of a particular brand of canned peaches (yt) for the past 15 months and the relative selling price (xt).

PROBLEMS 497

a. Fit a simple linear regression model to these data. Plot the residuals versus time. Is there any indication of autocorrelation?
b. Use the Durbin­Watson test to determine if there is positive autocorrelation in the errors. What are your conclusions?
c. Use one iteration of the Cochrane­Orcutt procedure to estimate the regression coefficients. Find the standard errors of these regression coefficients.
d. Is there positive autocorrelation remaining after the first iteration? Would you conclude that the iterative parameter estimation technique has been successful?

Market Share and Price of Canned Peaches

t

xt

yt

t

1

100

15.93

9

2

98

16.26

10

3

100

15.94

11

4

89

16.81

12

5

95

15.67

13

6

87

16.47

14

7

93

15.66

15

8

82

16.94

xt

yt

85

16.60

83

17.16

81

17.77

79

18.05

90

16.78

77

18.17

78

17.25

14.4 The data in the following table gives the monthly sales for a cosmetics manufacturer (yt) and the corresponding monthly sales for the entire industry (xt). The units of both variables are millions of dollars.
a. Build a simple linear regression model relating company sales to industry sales. Plot the residuals against time. Is there any indication of autocorrelation?
b. Use the Durbin­Watson test to determine if there is positive autocorrelation in the errors. What are your conclusions?
c. Use one iteration of the Cochrane­Orcutt procedure to estimate the model parameters. Compare the standard error of these regression coefficients with the standard error of the least-squares estimates.
d. Test for positive autocorrelation following the first iteration. Has the procedure been successful?

Cosmetic Sales Data for Exercise 14.4

t

xt

yt

t

xt

yt

1

5.00

0.318

10

6.16

0.650

2

5.06

0.330

11

6.22

0.685

3

5.12

0.356

12

6.31

0.713

4

5.10

0.334

13

6.38

0.724

5

5.35

0.386

14

6.54

0.775

6

5.57

0.455

15

6.68

0.78

7

5.61

0.460

16

6.73

0.796

8

5.80

0.527

17

6.89

0.859

9

6.04

0.598

18

6.97

0.88

498

REGRESSION ANALYSIS OF TIME SERIES DATA

14.5 Reconsider the data in Exercise 14.4. Define a new set of transformed variables as the first difference of the original variables, yt = yt - yt-1 and xt = xt - xt-1. Regress yt on xt through the origin. Compare the estimate of the slope from this first-difference approach with the estimate obtained from the iterative method in Exercise 14.4.
14.6 Consider the simple linear regression model yt = 0 + 1x + t, where the error are generated by the second-order autoregressive process

t = 1t-1 + 2t-2 + at
Discuss how the Cochrane­Orcutt iterative procedure could be used in this situation. What transformations would be used on the variable yt and xt? How would you estimate the parameters 1 and 2?
14.7 Consider the weighted least squares normal equations for the case of simple linear regression where time is the predictor variable. Suppose that the variances of the errors are proportional to the index of time such that wt = 1/t. Simplify the normal equations for this situation. Solve for the estimates of the model parameters.
14.8 Consider a simple linear regression model where time is the predictor variable. Assume that the errors are uncorrelated and have constant variance 2. Show that the variances of the model parameter estimates are

( ) V

^0

=  2 2(2T + 1) T(T - 1)

and

( ) V

^1

=  2 12 T(T 2 - 1)

14.9 Consider the data in Exercise 14.3. Fit a time series regression model with autocorrected errors to these data. Compare this model with the results you obtained in Exercise 14.3 using the Cochrane­Orcutt procedure.

14.10

Consider the data in Exercise 14.3. Fit the lagged variables regression models shown in Eq. (14.26) and (14.27) to these data. Compare these models with the results you obtained in Exercise 14.3 using the Cochrane­Orcutt procedure, and with the time series regression model from Exercise 14.9.

14.11

Consider the cosmetic sakes data in Exercise 14.4. Fit a time series regression model with autocorrected errors to these data. Compare this model with the results you obtained in Exercise 14.4 using the Cochrane­Orcutt procedure.

14.12 Consider the cosmetic sales data in Exercise 14.4. Fit the lagged variables regression models shown in Eq. (14.26) and (14.27) to these data. Compare these models with the results you obtained in Exercise 14.4 using the

PROBLEMS 499

Cochrane­Orcutt procedure, and with the time series regression model from Exercise 14.11.

14.13

Consider the global surface air temperature anomaly data and the CO2 concentration data in Table B.17. Fit a time series regression model to these data, using global surface air temperature anomaly as the response variable. Is there any indication of autocorrelation in the residuals? What corrective action and modeling strategies would you recommend?

CHAPTER 15
OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS
This chapter surveys a variety of topics that arise in the use of regression analysis. In several cases only a brief glimpse of the subject is given along with references to more complete presentations.
15.1 ROBUST REGRESSION 15.1.1 Need for Robust Regression When the observations y in the linear regression model y = X +  are normally distributed, the method of least squares is a good parameter estimation procedure in the sense that it produces an estimator of the parameter vector  that has good statistical properties. However, there are many situations where we have evidence that the distribution of the response variable is (considerably) nonnormal and/or there are outliers that affect the regression model. A case of considerable practical interest is one in which the observations follow a distribution that has longer or heavier tails than the normal. These heavy-tailed distributions tend to generate outliers, and these outliers may have a strong influence on the method of least squares in the sense that they "pull" the regression equation too much in their direction.
For example, consider the 10 observations shown in Figure 15.1 The point labeled A in this figure is just at the right end of the x space, but it has a response value that is near the average of the other 9 responses. If all the observations are considered, the resulting regression model is y^ = 2.12 + 0.971x, and R2 = 0.526. However, if we
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc. 500

ROBUST REGRESSION

501

14

> >

12
y = 0.715 + 1.45x 10

y8

y = 2.12 + 0.971x

6
A 4

2

012345678 x
Figure 15.1 A scatter diagram of a sample containing an influential observation.

fit the linear regression model to all observations other than observation A, we obtain y^ = 0.715 + 1.45x, for which R2 = 0.894. Both lines are shown in Figure 15.1. Clearly, point A has had a dramatic effect on the regression model and the resulting value of R2.
One way to deal with this situation is to discard observation A. This will produce a line that passes nicely through the rest of the data and one that is more pleasing from a statistical standpoint. However, we are now discarding observations simply because it is expedient from a statistical modeling viewpoint, and generally, this is not a good practice. Data can sometimes be discarded (or modified) on the basis of subject-matter knowledge, but when we do this purely on a statistical basis, we are usually asking for trouble. We also note that in more complicated situations, involving more regressors and a larger sample, even detecting that the regression model has been distorted by observations such as A can be difficult.
A robust regression procedure is one that dampens the effect of observations that would be highly influential if least squares were used.That is, a robust procedure tends to leave the residuals associated with outliers large, thereby making the identification of influential points much easier. In addition to insensitivity to outliers, a robust estimation procedure should produce essentially the same results as least squares when the underlying distribution is normal and there are no outliers. Another desirable goal for robust regression is that the estimation procedures and reference procedures should be relatively easy to perform.
The motivation for much of the work in robust regression was the Princeton robustness study (see Andrews et al. [1972]). Subsequently, there have been several types of robust estimators proposed. Some important basic references include Andrews [1974], Carroll and Ruppert [1988], Hogg [1974, 1979a,b], Huber [1972, 1973, 1981], Krasker and Welsch [1982], Rousseeuw [1984, 1998], and Rousseeuw and Leroy [1987].
To motivate some of the following discussion and to further demonstrate why it may be desirable to use an alternative to least squares when the observations are nonnormal, consider the simple linear regression model

yi = 0 + 1xi + i, i = 1, 2, ... , n

(15.1)

502

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

1 2

0

i

Figure 15.2 The double-exponential distribution.

where the errors are independent random variables that follow the double exponential distribution

f

(i ) =

1 2

e- i

,

- < i < 

(15.2)

The double-exponential distribution is shown in Figure 15.2. The distribution is more "peaked" in the middle than the normal and tails off to zero as |i| goes to infinity. However, since the density function goes to zero as e- i goes to zero and the normal density function goes to zero as e-i2 goes to zero, we see that the double-
exponential distribution has heavier tails than the normal. We will use the method of maximum likelihood to estimate 0 and 1. The likeli-
hood function is

n 

  L(0, 1) =

n i=1

1 e- i 2



=

1
(2

)n

 exp 


-



i  i=1 
 

(15.3)

Therefore, maximizing the likelihood function would involve minimizing in=1 i , the sum of the absolute errors. Recall that the method of maximum likelihood applied to the regression model with normal errors leads to the least-squares criterion. Thus, the assumption of an error distribution with heavier tails than the normal implies that the method of least squares is no longer an optimal estimation technique. Note that the absolute error criterion would weight outliers far less severely than would least squares. Minimizing the sum of the absolute errors is often called the L1-norm regression problem (least squares is the L2-norm regression problem). This criterion was first suggested by F. Y. Edgeworth in 1887, who argued that least squares was overly influenced by large outliers. One way to solve the problem is through a linear programming approach. For more details on L1-norm regression, see Sielken and Hartley [1973], Book et al. [1980], Gentle, Kennedy, and Sposito [1977], Bloomfield and Steiger [1983], and Dodge [1987].

ROBUST REGRESSION

503

The L1-norm regression problem is a special case of Lp-norm regression, in which

the

model

parameters

are

chosen

to

minimize



n i=1

i

p

where

1  p  2.

When

1 < p < 2, the problem can be formulated and solved using nonlinear programming

techniques. Forsythe [1972] has studied this procedure extensively for the simple

linear regression model.

15.1.2 M-Estimators
The L1-norm regression problem arises naturally from the maximum-likelihood approach with double-exponential errors. In general, we may define a class of robust estimators that minimize a function  of the residuals, for example,

n

n

  Minimize  (ei ) = Minimize  (yi - xib )

b

b

i=1

i=1

(15.4)

where xi denotes the ith row of X. An estimator of this type is called an M-estimator, where M stands for maximum-likelihood. That is, the function  is related to the

likelihood function for an appropriate choice of the error distribution. For example,

if the method of least squares is used (implying that the error distribution is normal),

then

 (z)

=

1 2

z2,

-

<

z

<

.

The M-estimator is not necessarily scale invariant [i.e., if the errors yi - xib

were multiplied by a constant, the new solution to Eq. (15.4) might not be same

as the old one]. To obtain a scale-invariant version 0 this estimator, we usually

solve

  Minimize b

n i=1

 

ei s



=

Minimize b

n i=1

 

yi

-x s

i b



(15.5)

where s is a robust estimate of scale. A popular choice for s is the median absolute deviation

s = median ei - median(ei ) 0.6745

(15.6)

The tuning constant 0.6745 makes s an approximately unbiased estimator of  if n is large and the error distribution is normal
To minimize Eq. (15.5), equate the first partial derivatives of  with respect to j (j = 0,1, . . . , k) to zero, yielding a necessary condition for a minimum. This gives the system of p = k + 1 equations

n
i=1

xij



yi

-x s

i b



=

0,

j = 0, 1,... , k

(15.7)

where  =  and xij is the ith observation on the jth regressor and xi0 - 1. In general, the  function is nonlinear and Eq. (15.7) must be solved by iterative methods. While several nonlinear optimization techniques could be employed, iteratively reweighted least squares (IRLS) is most widely used. This approach is usually attributed to Beaton and Tukey [1974].

504

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

To use iteratively reweighted least squares, suppose that an initial estimate b^0 is available and that s is an estimate of scale. Then write the p = k + 1 equations in Eq. (15.7),

  n
i=1

xij  yi

-x s

i b



=

n i=1

xij { [( yi

- xib )

s]

( yi
s

- xib )

s} ( yi

- xib )

= 0,

j = 0, 1,... , k

(15.8)

as

where

n
 xijwi0 (yi - xib ) = 0, j = 0, 1,... , k
i=1

(( )) wi0

=

  

 yi - xib^0 yi - xib^0

s s





1

if yi  xib^0 if yi = xib^0

(15.9) (15.10)

In matrix notation, Eq. (15.9) becomes

XW0Xb = XW0y

(15.11)

where W0 is an n × n diagonal matrix of "weights" with diagonal elements w10, w20, . . . , wn0 given by Eq. (15.10). We recognize Eq. (15.11) as the usual weighted leastsquares normal equations. Consequently, the one-step estimator is

b^1 = (XW0X)-1 XW0y

(15.12)

At the next step we recompute the weights from Eq. (15.10) but using b^1 instead of b^0. Usually only a few iterations are required to achieve convergence. The iteratively reweighted least-squares procedure could be implemented using a standard weighted least-squares computer program.
A number of popular robust criterion functions are shown in Table 15.1 behavior of these  functions and their corresponding  functions are illustrated in Figures 15.3 and 15.4, respectively. Robust regression procedures can be classified by the behavior of their  function. The  function controls the weight given to each residual and (apart from a constant of proportionality) is sometimes called the influence function. For example, the  function for least squares is unbounded, and thus least squares tends to be nonrobust when used with data arising from a heavy-tailed distribution. The Huber t function (Huber [1964]) has a monotone  function and does not weight large residuals as heavily as least squares. The last three influence functions actually redescend as the residual becomes larger. Ramsay's Ea function (see Ramsay [1977]) is a soft redescender, that is, the  function is asymptotic to zero for large |z|. Andrew's wave

ROBUST REGRESSION

505

TABLE 15.1 Robust Criterion Functions

Criterion

p(z)

Least squares Huber's t function t=2

1 2

z2

1 2

z2

z

t

-

1 2

t2

Ramsay's Ea function a = 0.3
Andrews'; wave function a = 1.339
Hampel's 17A function a = 1.7 b = 3.4
c = 8.5

a-2[1 - exp(-a|z|) · (1 + a|z|)]

a[1 - cos (z/a)]

2a

1 2

z2

a

z

-

1 2

a2

a(c

z

-

1 2

z2 )

- (7

6)a2

c-b

a (b + c - a)

(z) z z t sign (z) z exp (-a|z|)
sin (z/a) 0 z
a sin (z)
asign(z)(c - z )
c-b 0

w(z)

Range

1.0
1.0 t z exp (-a|z|)

z <  |z|  t |z| > t
|z| < 

sin(z a)
za
0 1.0

|z|  a
|z| > a |z|  a

a/|z|
a(c - z ) z (c - b)
0

a < |z|  b b < |z|  c |z| > c



Legend:

30

LS = Least squares

H2 = Huber, t = 2

17A = Hampel's function

E0.3 = Ramsay's function, a = 0.3

20

W = Andrews' wave function

LS H2

17A 10
E0.3 W

0

1

2

3

4

5

6

7

8

9

ei /s

Figure 15.3 Robust criterion functions.

function and Hampel's 17A function (see Andrews et al. [1972] and Andrews [1974]) are hard redescenders, that is, the  function equals zero for sufficiently large |z|. We should note that the  functions associated with the redescending  functions are nonconvex, and this in theory can cause convergence problems in the iterative estimation procedure. However, this is not a common occurrence. Furthermore, each

506

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

(z)

(z)

t

0

z

-t

0t

z

-t

(z) 1.0
z 0

(a) (z)

(b)

(c)

(z)

1.0

a

-

0

a z

-c

-b -a 0 a b

cz

-1.0

-a

(d)

(e)

Figure 15.4 Robust influence functions: (a) least squares; (b) Huber's t functions; (c) Ramsay's Ea function; (d) Andrews'; wave function; (e) Hampel's 17A function.

of the robust criterion functions requires the analyst to specify certain "tuning constants" for the  functions. We have shown typical values of these tuning constants in Table 15.1.
The starting value b^0 used in robust estimation can be an important consideration. Using the least-squares solution can disguise the high leverage points. The L1-norm estimates would be a possible choice of starting values. Andrews [1974] and Dutter [1977] also suggest procedures for choosing the starting values.
It is important to know something about the error structure of the final robust regression estimates b^ . Determining the covariance matrix of b^ is important if we are to construct confidence intervals or make other model inferences. Huber [1973] has shown that asymptotically b^ has an approximate normal distribution with covariance matrix



2

E[ 2 (
{E { (

 )]
 )}}2

(XX)-1

Therefore, a reasonable approximation for the covariance matrix of b^ is

n

 ns2  n - p

 

 2 [(yi - xib ) s]
i=1
n  [(yi - xib ) s]2

(XX)-1

 i=1



The weighted least-squares computer program also produces an estimate of the covariance matrix

ROBUST REGRESSION

507

 ( ) n wi yi - xib^ 2

i=1
n- p

(XWX)-1

Other suggestions are in Welsch [1975] and Hill [1979]. There is no general agreement about which approximation to the covariance matrix of b^ is best. Both Welsch and Hill note that these covariance matrix estimates perform poorly for X matrices that have outliers. Ill-conditioning (multicollinearity) also distorts robust regression estimates. However, There are indications that in many cases we can make approximate inferences about b^ using procedures similar to the usual normal theory.

Example 15.1 The Stack Loss Data
Andrews [1974] uses the stack loss data analyzed by Daniel and Wood [1980] to illustrate robust regression. The data, which are taken from a plant oxidizing ammonia to nitric acid, are shown in Table 15.2. An ordinary least-squares (OLS) fit to these data gives
y^ = -39.9 + 0.72x1 + 1.30x2 - 0.15x3

TABLE 15.2 Stack Loss Data from Daniel and Wood [1980]

Observation Number
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

Stack Loss, y
42 37 37 28 18 18 19 20 15 14 14 13 11 12
8 7 8 8 9 15 15

Air Flow, x1
80 80 75 62 62 62 62 62 58 58 58 58 58 58 50 50 50 50 50 56 70

Cooling Water Inlet Temperature, x2
27 27 25 24 22 23 24 24 23 18 18 17 18 19 18 18 19 19 20 20 20

Acid Concentration, x3
89 88 90 87 87 87 93 93 87 80 89 88 82 93 89 86 72 79 80 82 91

508

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

TABLE 15.3 Residuals for Various Fits to the Stack Loss Dataa

Residuals

Least Squares

Andrews'; Robust Fit

(1)

(2)

(3)

(4)

Observation

All 21 Points

1, 3, 4, 21 Out

All 21 Points

1,3,4,21 Out

1

3.24

6.08b

6.11

6.11

2

-1.92

1.15

1.04

1.04

3

4.56

6.44

6.31

6.31

4

5.70

8.18

8.24

8.24

5

-1.71

-0.67

-1.24

-1.24

6

-3.01

-1.25

-0.71

-0.71

7

-2.39

-0.42

-0.33

-0.33

8

-1.39

0.58

0.67

0.67

9

-3.14

-1.06

-0.97

-0.97

10

1.27

0.35

0.14

0.14

11

2.64

0.96

0.79

0.79

12

2.78

0.47

0.24

0.24

13

-1.43

-2.51

-2.71

-2.71

14

-0.05

-1.34

-1.44

-1.44

15

2.36

1.34

1.33

1.33

16

0.91

0.14

0.11

0.11

17

-1.52

-0.37

-0.42

-0.42

18

-0.46

0.10

0.08

0.08

19

-0.60

0.59

0.63

0.63

20

1.41

1.93

1.87

1.87

21

-7.24

-8.63

-8.91

-8.91

aAdapted from Table 5 in Andrews [1974], with permission of the publisher. bUnderlined residuals correspond to points not included in the fit.

The residuals from this model are shown in column 1 of Table 15.3 and a normal probability plot is shown in Figure 15.5a Daniel and Wood note that the residual for point 21 is unusually large and has considerable influence on the regression coefficients. After an insightful analysis, they delete points 1, 3, 4, and 21 from the data, The OLS fit to the remaining data yields
y^ = -37.6 + 0.80x1 + 0.58x2 - 0.07x3
The residuals from this model are shown in column 2 of Table 15.3, and the corresponding normal probability plot is in Figure 15.5b. This plot does not indicate any unusual behavior in the residuals.
Andrews [1974] observes that most users of regression lack the skills of Daniel and Wood and employs robust regression methods to produce equivalent results, A robust fit to the stack loss data using the wave function with a = 1.5 yields
Daniel and Wood fit a model involving x1, x2, and x12. Andrews elected to work with all three original regressors. He notes that if x3 is deleted and x12 added, smaller residuals result but the general findings are the same.

ROBUST REGRESSION

509

8

8

6

6

4

4

Residual Residual

2

2

0

0

-2

-2

-4

-4

-6

-6

-8

-8

1 2 5 10 20 40 60 80 90 95 98 99 30 50 70

1 2 5 10 20 40 60 80 90 95 98 99 30 50 70

Probability x 100% (a)

Probability x 100% (b)

Figure 15.5 Normal probability plots from least-squares fits: (a) least squares with all 21 points; (b) least squares with 1, 3, 4, and 21 deleted. (From Andrews [1974], with permission of the publisher.)

y^ = -37.2 + 0.82x1 + 0.52x2 - 0.07x3

This is virtually the same equation found by Daniel and Wood using OLS after much

careful analysis, The residuals from this model are shown in column 3 of Table 15.3,

and the normal probability plot is in Figure 15.6a. The four suspicious points are

clearly identified in this plot Finally, Andrews obtains a robust fit to the data with

points 1, 3, 4, and 21 removed. The resulting equation is identical to the one found

using all 21 data points, The residuals from this fit and the corresponding normal

probability plot are shown in column 4 of Table 15.3 and Figure 15.6b, respectively.

This normal probability plot is virtually identical to the one obtained from the OLS

analysis with points 1, 3, 4, and 21 deleted (Figure 15.5b)

Once again we find that the routine application of robust regression has led to

the automatic identification of the suspicious points. It has also produced a fit that

does not depend on these points in any important way. Thus, robust regression

methods can be viewed as procedures for isolating unusually influential points, so

that these points may be given further study.



Computing M-Estimates Not many statistical software packages compute Mestimates. S-PLUS and STATA do have this capability. SAS recently added it. The SAS code to analyze the stack loss data is:

proc robustreg; model y = xl x2 x3 / diagnostics leverage;
run;

SAS's default procedure uses the bisquare weight function (see Problem 15.3) and the median method for estimating the scale parameter.
Robust regression methods have much to offer the data analyst. They can be extremely helpful in locating outliers and highly influential observations. Whenever

510

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

8

8

6

6

4

4

Residual Residual

2

2

0

0

-2

-2

-4

-4

-6

-6

-8

-8

1 2 5 10 20 40 60 80 90 95 98 99 30 50 70
Probability x 100% (a)

1 2 5 10 20 40 60 80 90 95 98 99 30 50 70
Probability x 100% (b)

Figure 15.6 Normal probability plots from robust fits: (a) robust fit with all 21 points; (b) robust fit with 1, 3, 4, and 21 deleted. (From Andrews [1974], with permission of the publisher.)

a least-squares analysis is performed, it would be useful to perform a robust fit also. If the results of the two procedures are in substantial agreement, then use the leastsquares results, because inferences based on least squares are at present better understood. However, if the results of the two analyses differ, then reasons for these differences should be identified. Observations that are downweighted in the robust fit should be carefully examined.
15.1.3 Properties of Robust Estimators
In this section we introduce two important properties of robust estimators: breakdown and efficiency. We will observe that the breakdown point of an estimator is a practical concern that should be taken into account when selecting a robust estimation procedure. Generally, M-estimates perform poorly with respect to breakdown point. This has spurred development of many other alternative procedures.
Breakdown Point The finite-sample breakdown point is the smallest fraction of anomalous data that can cause the estimator to be useless. The smallest possible breakdown point is 1/n, that is, a single observation can distort the estimator so badly that it is of no practical use to the regression model-builder. The breakdown point of OLS is 1/n.
M-estimates can be affected by x-space outliers in an identical manner to OLS. Consequently, the breakdown point of the class of M-estimators is 1/n. This has a potentially serious impact on their practical use, since it can be difficult to determine the extent to which the sample is contaminated with anomalous data. Most experienced data analysts believe that the fraction of data that are contaminated by erroneous data typically varies between 1 and 10%. Therefore, we would generally want the breakdown point of an estimator to exceed 10%. This has led to the development of high-breakdown-point estimators.

EFFECT OF MEASUREMENT ERRORS IN THE REGRESSORS

511

Efficiency Suppose that a data set has no gross errors, there are no influential observations, and the observations come from a normal distribution. If we use a robust estimator on such a data set, we would want the results to be virtually identical to OLS, since OLS is the appropriate technique for such data. The efficiency of a robust estimator can be thought of as the residual mean square obtained from OLS divided by the residual mean square from the robust procedure. Obviously, we want this efficiency measure to be close to unity.
There is a lot of emphasis in the robust regression literature on asymptotic efficiency, that is, the efficiency of an estimator as the sample size n becomes infinite. This is a useful concept in comparing robust estimators, but many practical regression problems involve small to moderate sample sizes (n < 50, for instance), and small-sample efficiencies are known to differ dramatically from their asymptotic values. Consequently, a model-builder should be interested in the asymptotic behavior of any estimator that might be used in a given situation but should not be unduly excited about it. What is more important from a practical viewpoint is the finitesample efficiency, or how well a particular estimator works with reference to OLS on "clean" data for sample sizes consistent with those of interest in the problem at hand. The finite-sample efficiency of a robust estimator is defined as the ratio of the OLS residual mean square to the robust estimator residual mean square, where OLS is applied only to the clean data. Monte Carlo simulation methods are often used to evaluate finite-sample efficiency.

15.2 EFFECT OF MEASUREMENT ERRORS IN THE REGRESSORS
In almost all regression models we assume that the response variable y is subject to the error term  and that the regressor variables x1, x2, . . . , xk are deterministic or mathematical variables, not affected by error. There are two variations of this situation. The first is the case where the response and the regressors are jointly distributed random variables This assumption gives rise to the correlation model discussed in Chapter 2 (refer to Section 2.12). The second is the situation where there are measurement errors in the response and the regressors. Now if measurement errors are present only in the response variable y, there are no new problems so long as these errors are uncorrelated and have no bias (zero expectation). However, a different situation occurs when there are measurement errors in the x's. We consider this problem in this section.

15.2.1 Simple Linear Regression
Suppose that we wish to fit the simple linear regression model, but the regressor is measured with error, so that the observed regressor is

Xi = xi + ai, i = 1, 2, ... , n

where xi is the true value of the regressor, Xi is the observed value, and ai is the
measurement error with E(ai) = 0 and Var (ai ) = a2. The response variable yi is
subject to the usual error i, i = 1, 2, . . . , n, so that the regression model is

yi = 0 + i xi + i

(15.13)

512

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

We assume that the errors i and ai are uncorrelated, that is, E(iai) = 0. This is sometimes called the errors-in-both-variables model. Since Xi is the observed value of the regressor, we may write

yi = 0 + 1(Xi - ai ) + i = 0 + 1Xi + (i - 1ai )

(15.14)

Initially Eq. (15.14) may look like an ordinary linear regression model with error term i = i - 1ai. However, the regressor variable Xi is a random variable and is correlated with the error term i = i - 1ai. The correlation between Xi and i is easily seen, since

Cov (Xi,  i ) = E {[Xi - E (Xi )][ i - E ( i )]}

= E[(Xi - xi ) i ] = E[(Xi - xi )(i - 1ai )]

( ) = E aii - 1ai2

=

-1

2 a

Thus, if 1  0, the observed regressor Xi and the error term i are correlated. The usual assumption when the regressor is a random variable is that the regres-
sor variable and the error component are independent. Violation of this assumption introduces several complexities into the problem. For example, if we apply standard least-squares methods to the data (i.e., ignoring the measurement error), the estimators of the model parameters are no longer unbiased. In fact, we can show that if Cov(Xi, i) = 0, then

( ) E

^1

= 1 1+

where

 

=



2 a



2 x

and



2 x

=

n i=1

( xi - x )2
n

That

is,

^1

is

always

a

biased

estimator

of

1

unless



2 a

=

0,

which

occurs

only

when

there are no measurement errors in the xi.

Since measurement error is present to some extent in almost all practical regres-

sion situations, some advice for dealing with this problem would be helpful. Note

that

if



2 a

is

small

relative

to



2 x

the

bias

in

^1

will

be

small.

This

implies

that

if

the

variability in the measurement errors is small relative to the variability of the x's,

then the measurement errors can be ignored and standard least-squares methods

applied.

Several alternative estimation methods have been proposed to deal with the

problem of measurement errors in the variables. Sometimes these techniques are

discussed under the topics structural or functional relationships in regression. Econ-

omists have used a technique called two-stage least squares in these cases. Often

these methods require more extensive assumptions or information about the param-

eters of the distribution of measurement errors. Presentations of these methods are

in Graybill [1961], Johnston [1972], Sprent [1969], and Wonnacott and Wonnacott

[1970]. Other useful references include Davies and Hutton [1975], Dolby [1976],

Halperin [1961], Hodges and Moore [1972], Lindley [1974], Mandansky [1959], and

INVERSE ESTIMATION--THE CALIBRATION PROBLEM

513

Sprent and Dolby [1980]. Excellent discussions of the subject are also in Draper and Smith [1998] and Seber [1977].

15.2.2 The Berkson Model
Berkson [1950] has investigated a case involving measurement errors in xi where the method of least squares can be directly applied. His approach consists of setting the observed value of the regressor Xi to a target value. This forces Xi to be treated as fixed, while the true value of the regressor xi = Xi - ai becomes a random variable. As an example of a situation where this approach could be used, suppose that the current flowing in an electrical circuit is used as a regressor variable. Current flow is measured with an ammeter, which is not completely accurate, so measurement error is experienced. However, by setting the observed current flow to target levels of 100, 125, 150, and 175 A (for example), the observed current flow can be considered as fixed, and actual current flow becomes a random variable. This type of problem is frequently encountered in engineering and physical science. The regressor is a variable such as temperature, pressure, or flow rate and there is error present in the measuring instrument used to observe the variable. This approach is also sometimes called the controlled-independent-variable model.
If Xi is regarded as fixed at a preassigned target value, then Eq. (15.14), found by using the relationship Xi = xi + ai, is still appropriate. However, the error term in this model, i = i - 1ai, is now independent of Xi because Xi is considered to be a fixed or nonstochastic variable. Thus, the errors are uncorrelated with the regressor, and the usual least-squares assumptions are satisfied. Consequently, a standard least-squares analysis is appropriate in this case.

15.3 INVERSE ESTIMATION--THE CALIBRATION PROBLEM

Most regression problems involving prediction or estimation require determining the value of y corresponding to a given x, such as x0 In this section we consider the inverse problem; that is, given that we have observed a value of y, such as y0, determine the x value corresponding to it. For example, suppose we wish to calibrate a thermocouple, and we know that the temperature reading given by the thermocouple is a linear function of the actual temperature, say
Observed temperature = 0 + 1(actual temperature) + 

or

y = 0 + 1x + 

(15.15)

Now suppose we measure an unknown temperature with the thermocouple and obtain a reading y0. We would like to estimate the actual temperature, that is, the temperature x0 corresponding to the observed temperature reading y0. This situation arises often in engineering and physical science and is sometimes called the calibration problem. It also occurs in bioassay where a standard curve is constructed against which all future assays or discriminations are to be run.

514

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

Suppose that the thermocouple has been subjected to a set of controlled and known temperatures x1, x2, . . . , xn and a set of corresponding temperature readings y1, y2, . . . , yn obtained. One method for estimating x given y would be to fit the model (15.15), giving

y^ = ^0 + ^1x

(15.16)

Now let y0 be the observed value of y. A natural point estimate of the corresponding value of x is

x^ 0

=

y0 - ^0 ^1

(15.17)

assuming that ^1  0. This approach is often called the classical estimator. Graybill [1976] and Seber [1977] outline a method for creating a 100 (1 - )
percent confidence region for x0. Previous editions of this book did recommend this approach. Parker, et al. [2010] show that this method really does not work well. The actual confidence level is much less than the advertised (1 - ) percent. They establish that the interval based on the delta method works quite well. Let n be the number of data points in the calibration data collection. This interval is

x^ 0

±

t1-

2,n-2

1 ^1

MSRes

 

1

+

1 n

+

x^0 - x Sxx

 

where MSRes, x, and Sxx are all calculated from the data collected from the calibration.

Example 15.2 Thermocouple Calibration
A mechanical engineer is calibrating a thermocouple. He has chosen 16 levels of temperature evenly spaced over the interval 100­400°C. The actual temperature x (measured by a thermometer of known accuracy) and the observed reading on the thermocouple y are shown in Table 15.4 and a scatter diagram is plotted in Figure 15.7. Inspection of the scatter diagram indicates that the observed temperature on the thermocouple is linearly related to the actual temperature. The straight-line model is
y^ = -6.67 + 0.953x
with 2 = MSRes = 5.86. The F statistic for this model exceeds 20,000, so we reject H0: 1 = 0 and conclude that the slope of the calibration line is not zero. Residual analysis does not reveal any unusual behavior so this model can be used to obtain point and interval estimates of actual temperature from temperature readings on the thermocouple.
Suppose that a new observation on temperature of y0 = 200°C is obtained using the thermocouple. A point estimate of the actual temperature, from the calibration line, is

INVERSE ESTIMATION--THE CALIBRATION PROBLEM

515

x^ 0

=

y0 - ^0 ^1

=

200 - (-6.67)
0.953

=

216.86°C

the 95% prediction interval based on (15.18) is 211.21  x0  222.5

(15.18)


TABLE 15.4 Actual and Observed Temperature

Observation, i
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Actual Temperature, xi (°C)
100 120 140 160 180 200 220 240 260 280 300 320 340 360 380 400

Observed Temperature, yi (°C)
88.8 108.7 129.8 146.2 161.6 179.9 202.4 224.5 245.1 257.7 277.0 298.1 318.8 334.6 355.2 377.0

400

Observed temperature yi (°C)

300

200

100

0

0

100

200

300

400

500

Actual temperature, xi (°C)

Figure 15.7 Scatterplot of observed and actual temperatures, Example 15.2.

516

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

Other Approaches Many people do not find the classical procedure outlined in Example 15.2 entirely satisfactory. Williams [1969] claims that the classical estimator has infinite variance based on the assumption that this estimator follows a Cauchylike distribution. A Cauchy random variable is the inverse of a standard normal random variable. This standard normal random variable has a mean of 0, which does create problems for the Cauchy distribution. The analyst always can rescale the calibration data such that the slope is one. Typically, the variances for calibration experiments are very small, on the order of  = 0.01. In such a case, the slope for the calibration data is approximately 100 standard deviations away from 0. Williams and similar arguments about infinite variance have no practical import.
The biggest practical complaint about the classical estimator is the difficulty in implementing the procedure. Many analysts, particularly outside the classical laboratory-calibration context, prefer inverse regression, where the analyst treats the xs in the calibration experiment as the response and the ys as the regressor. Of course, this reversal of roles is problematic in itself. Ordinary least squares regression assumes that the regressors are measured without error and that the response is random. Clearly, inverse regression violates this basic assumption.
Krutchkoff [1967, 1969] performed a series of simulations comparing the classical approach to inverse regression. He concluded that inverse regression was a better approach in terms of mean squared error of prediction. However, Berkson [1969], Halperin [1970], and Williams [1969] criticized Krutchkoff's results and conclusions.
Parker et al. [2010] perform a thorough comparison of the classical approach and inverse regression. They show that both approaches yield biased estimates. The bias for the classical estimator is

( x0 - x ) 2
12Sxx

The bias for inverse regression is approximately

-

1

x0 +1
2

Interestingly, inverse regression suffers from more bias than the classical approach. Parker et al. conclude that for quite accurate instruments (  0.01), the classical
approach and inverse regression yield virtually the same intervals. For borderline instruments (  0.1), inverse regression gives slightly smaller widths. Both procedures yield coverage probabilities as advertised.
A number of other estimators have been proposed. Graybill [1961, 1976] considers the case where we have repeated observations on y at the unknown value of x. He develops point and interval estimates for x using the classical approach. The probability of obtaining a finite confidence interval for the unknown x is greater when those are repeat observations on y. Hoadley [1970] gives a Bayesian treatment of the problem and derives an estimator that is a compromise between the classical and inverse approaches. He notes that the inverse estimator is the Bayes estimator for a particular choice of prior distribution. Other estimators have been proposed

BOOTSTRAPPING IN REGRESSION

517

by Kalotay [1971], Naszódi [1978], Perng and Tong [1974], and Tucker [1980]. The paper by Scheffé [1973] is also of interest. In genenal, Parker et al. [2010] show that these approaches are not satisfactory since the resulting intervals are very conservative with the actual coverage probability much greater than 100 (1 - ).
In many, if not most, calibration studies the analyst can design the data collection experiment. That is, he or she can specify what x values are to be observed. Ott and Myers [1968] have considered the choice of an appropriate design for the inverse estimation problem assuming that the unknown x is estimated by the classical approach. They develop designs that are optimal in the sense of minimizing the integrated mean square error. Figures are provided to assist the analyst in design selection.

15.4 BOOTSTRAPPING IN REGRESSION
For the standard linear regression model, when the assumptions are satisfied, there are procedures available for examining the precision of the estimated regression coefficients, as well as the precision of the estimate of the mean or the prediction of a future observation at any point of interest. These procedures are the familiar standard errors, confidence intervals, and prediction intervals that we have discussed in previous chapters. However, there are many regression model-fitting situations either where there is no standard procedure available or where the results available are only approximate techniques because they are based on large-sample or asymptotic theory. For example, for ridge regression and for many types of robust fitting procedures there is no theory available for construction of confidence intervals or statistical tests, while in both nonlinear regression and generalized linear models the only tests and intervals available are large-sample results.
Bootstrapping is a computer-intensive procedure that was developed to allow us to determine reliable estimates of the standard errors of regression estimates in situations such as we have just described. The bootstrap approach was originally developed by Efron [1979, 1982]. Other important and useful references are Davison and Hinkley [1997], Efron [1987], Efron and Tibshirani [1986, 1993], and Wu [1986]. We will explain and illustrate the bootstrap in the context of finding the standard error of an estimated regression coefficient. The same procedure would be applied to obtain standard errors for the estimate of the mean response or a future observation on the response at a particular point. Subsequently we will show how to obtain approximate confidence intervals through bootstrapping.
Suppose that we have fit a regression model, and our interest focuses on a particular regression coefficient, say ^ . We wish to estimate the precision of this estimate by the bootstrap method. Now this regression model was fit using a sample of n observations. The bootstrap method requires us to select a random sample of size n with replacement from this original sample. This is called the bootstrap sample. Since it is selected with replacement, the bootstrap sample will contain observations from the original sample, with some of them duplicated and some of them omitted. Then we fit the model to this bootstrap sample, using the same regression procedure as for the original sample. This produces the first bootstrap estimate, say ^1*. This process is repeated a large number of times. On each repetition, a bootstrap sample is selected, the model is fit, and an estimate ^i* is obtained for i = 1, 2, . . . , m

518

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

bootstrap samples. Because repeated samples are taken from the original sample,

bootstrapping is also called a resampling procedure. Denote the estimated standard

( ) deviation ( ) tion s ^*

of is

the m bootstrap estimates ^i* by s ^* . This an estimate of the standard deviation of the

bootstrap standard deviasampling distribution of ^

and, consequently, it is a measure of the precision of estimation for the regression

coefficient .

15.4.1 Bootstrap Sampling in Regression
We will describe how bootstrap sampling can be applied to a regression model. For convenience, we present the procedures in terms of a linear regression model, but they could be applied to a nonlinear regression model or a generalized linear model in essentially the same way.
There are two basic approaches for bootstrapping regression estimates. In the first approach, we fit the linear regression model y = X  +  and obtain the n residuals e = [e1, e2, . . . , en]. Choose a random sample of size n with replacement from these residuals and arrange them in a bootstrap residual vector e*. Attach the bootstrapped residuals to the predicted values y^ = Xb^ to form a bootstrap vector of responses y*. That is, calculate

y* = Xb^ + e*

(15.19)

These bootstrapped responses are now regressed on the original regressors by the regression procedure used to fit the original model. This produces the first bootstrap estimate of the vector of regression coefficients. We could now also obtain bootstrap estimates of any quantity of interest that is a function of the parameter estimates. This procedure is usually referred to as bootstrapping residuals.
Another bootstrap sampling procedure, usually called bootstrapping cases (or bootstrapping pairs), is often used in situations where there is some doubt about the adequacy of the regression function being considered or when the error variance is not constant and/or when the regressors are not fixed-type variables. In this variation of bootstrap sampling, it is the n sample pairs (xi, yi) that are considered to be the data that are to be resampled. That is, the n original sample pairs (xi, yi) are sampled with replacement n times, yielding a bootstrap sample, say (x*i , yi*) for i = 1, 2, . . . , n. Then we fit a regression model to this bootstrap sample, say

y* = Xb^ + e

(15.20)

resulting in the first bootstrap estimate of the vector of regression coefficients. These bootstrap sampling procedures would be repeated m times. Generally,
the choice of m depends on the application. Sometimes, reliable results can be obtained from the bootstrap with a fairly small number of bootstrap samples. Typically, however, 200­1000 bootstrap samples are employed. One way to select
( ) m is to observe the variability of the bootstrap standard deviation s ^* as m ( ) increases. When s ^* stabilizes, a bootstrap sample of adequate size has been
reached.

BOOTSTRAPPING IN REGRESSION

519

15.4.2 Bootstrap Confidence Intervals
We can use bootstrapping to obtain approximate confidence intervals for regression coefficients and other quantities of interest, such as the mean response at a particular point in x space, or an approximate prediction interval for a future observation on the response. As in the previous section, we will focus on regression coefficients, as the extension to other regression quantities is straightforward.
A simple procedure for obtaining an approximate 100(1 - ) percent confidence interval through bootstrapping is the reflection method (also known as the percentile method). This method usually works well when we are working with an unbiased estimator. The reflection confidence interval method uses the lower 100(/2) and upper 100(1 - /2) percentiles of the bootstrap distribution of ^i*. Let these percen
tiles be denoted by ^* ( 2) and ^* (1 -  2), respectively. Operationally, we would
obtain these percentiles from the sequence of bootstrap estimates that we have computed, ^i*, i = 1, 2, . . . , m. Define the distances of these percentiles from ^ , the estimate of the regression coefficient obtained for the original sample, as follows:

D1 = ^ - ^* ( 2) D2 = ^* (1 -  2) - ^

(15.21)

Then the approximate 100(1 - /2) percent bootstrap confidence interval for the regression coefficient  is given by

^ - D2    ^ + D1

(15.22)

Before presenting examples of this procedure, we note two important points:

1. When using the reflection method to construct bootstrap confidence intervals, it is generally a good idea to use a larger number of bootstrap samples than would ordinarily be used to obtain a bootstrap standard error. The reason is that small tail percentiles of the bootstrap distribution are required, and a larger sample will provide more reliable results. Using at least m = 500 bootstrap samples is recommended.
2. The confidence interval expression in Eq. (15.22) associates D2 with the lower confidence limit and D1 with the upper confidence limit, and at first glance this looks rather odd since D1 involves the lower percentile of the bootstrap distribution and D2 involves the upper percentile. To see why this is so, consider the usual sampling distribution of ^ for which the lower 100(/2) and upper
100(1 - /2) percentiles are denoted by ^ ( 2) and ^ (1 -  2), respectively.
Now we can state with probability 100(1 - /2) that ^ will fall in the interval

^ ( 2)  ^  ^ (1 -  2)

(15.23)

Expressing these percentiles in terms of the distances from the mean of the
( ) sampling distribution of ^ , that is, E ^ = , we obtain

520

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

d1 =  - ^ ( 2) and d2 = ^ (1 -  2) - 
Therefore,
^ ( 2) =  - d1 ^ (1 -  2) =  + d2
Substituting Eq. (15.24) into Eq. (15.23) produces  - d1  ^   + d2
which can be written as

(15.24)

 -  - ^ - d1  ^ -  - ^   -  - ^ + d2 -^ - d1  -  -^ + d2 ^ - d2    ^ + d1

this last equation is of the same form as the bootstrap confidence interval, Eq. (15.22), with D1 and D2 replacing d1, and d2 and using ^ as an estimate of the mean of the sampling distribution.

We now present two examples. In the first example, standard methods are available for constructing the confidence interval, and our objective is to show that similar results are obtained by bootstrapping. The second example involves nonlinear regression, and the only confidence interval results available are based on asymptotic theory. We show how the bootstrap can be used to check the adequacy of the asymptotic results.

Example 15.3 The Delivery Time Data
The multiple regression version of these data, first introduced in Example 3.1 has been used several times throughout the book to illustrate various regression techniques. We will show how to obtain a bootstrap confidence interval for the regression coefficient for the predictor cases, 1. From Example 3.1, the least-squares estimate of 1 is ^1 = 1.61591. In Example 3.8 we found that the standard error of ^1 is 0.17073, and the 95% confidence interval for 1 is 1.26181  1  1.97001.
Since the model seems to fit the data well, and there is not a problem with inequality of variance, we will bootstrap residuals to obtain an approximate 95% bootstrap confidence interval for 1. Table 3.3 shows the fitted values and residuals for all 25 observations based on the original least-squares fit. To construct the first bootstrap sample, consider the first observation. The fitted value for this observation is y^1 = 21.7081, from Table 3.3. Now select a residual at random from the last column of this table, say e5 = -0.4444. This becomes the first bootstrap residual e1* = -0.4444. Then the first bootstrap observation becomes y1* = y1 + e1* = 21.7081 - 0.4444 = 21.2637. Now we would repeat this process for each subsequent observation using the fitted values y^i and the bootstrapped residuals ei* for i = 2, 3, . . . , 25 to construct the

BOOTSTRAPPING IN REGRESSION

521

remaining observations in the bootstrap sample. Remember that the residuals are

sampled from the last column of Table 3.3 with replacement. After the bootstrap

sample is complete, fit a linear regression model to the observations (xi1, xi2, yi*), i = 2,

3, . . . , 25. The result from this yields the first bootstrap estimate of the regression

cbooeoftfisctrieanpte, s^t1i*,m1 =at1e.s64^21*,3u,1u. W=e1,re2p, .e.a.te, d10t0h0is.

process m Figure 15.8

= 1000 shows

times, producing 1000 the histogram of these

bootstrap estimates. Note that the shape of this histogram closely resembles the normal distribution. This is not unexpected, since the sampling distribution of ^1

should be a normal distribution. Furthermore, the standard deviation of the 1000

( ) ( ) bthoeootrsytr-abpaseesdtimstaantedsairsdser^r1*or=o0f.1^819, 9se4, To construct the approximate 95%

which is reasonably close to the usual normal-

^1 = 0.17073. bootstrap confidence

interval

for

^1,

we

need

the 2.5th and 97.5th percentiles of the bootstrap sampling distribution. These quanti-
ties are ^1*(0.025) = 1.24652 and ^1*(0.975) = 1.98970, respectively (refer to Figure

15.8). The distances D1 and D2 are computed from Eq. (15.21) as follows:

D1 = ^1 - ^1*(0.025) = 1.61591 - 1.24652 = 0.36939 D2 = ^1*(0.975) - ^1 = 1.98970 - 1.61591 = 0.37379

Finally, the approximate 95% bootstrap confidence interval is obtained from Eq. (15.22).

^1 - D2  1  ^1 + D1 1.61591 - 0.37379  1  1.61591 + 0.36939
1.24212  1  1.98530

This is very similar to the exact normal-theory confidence interval found in Example

3.8, 1.26181  1  1.97001. We would expect the two confidence intervals to

closely agree, since there is no serious problem here with the usual regression

assumptions.



frequency

120

100

80

60

40

20

0

1

1.3

1.6

1.9

2.2

2.5

> >
> >

i _ 1 = 1.63166, S(1) = 0.18994, 1 (0.025) = 1.24652, 1 (0.975) = 1.98970

Figure 15.8 Histogram of bootstrap ^1*, Example 15.3.

522

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

The most important applications of the bootstrap in regression are in situations either where there is no theory available on which to base statistical inference or where the procedures utilize large-sample or asymptotic results. For example, in nonlinear regression, all the statistical tests and confidence intervals are largesample procedures and can only be viewed as approximate procedures. In a specific problem the bootstrap could be used to examine the validity of using these asymptotic procedures.

Example 15.4 The Puromycin date
Examples 12.2 and 12.3 introduced the puromycin data, and we fit the Michaelis­ Menten model
y = 1x +  x +2
to the data in Table 12.1 which resulted in estimates of ^1 = 212.7 and ^2 = 0.0641, respectively. We also found the large-sample standard errors for these parameter
( ) ( ) estimates to be se ^1 = 6.95 and se ^2 = 8.28 × 10-3, and the approximate 95% con-
fidence intervals were computed in Example 12.6 as
197.2  1  228.2
and
0.0457  2  0.0825
Since the inference procedures used here are based on large-sample theory, and the sample size used to fit the model is relatively small (n = 12), it would be useful to check the validity of applying the asymptotic results by computing bootstrap standard deviations and bootstrap confidence intervals for 1 and 2. Since the Michaelis-Menten model seems to fit the data well, and there are no significant problems with inequality of variance, we used the approach of bootstrapping residuals to obtain 1000 bootstrap samples each of size n = 12. Histograms of the resulting bootstrap estimates of 1 and 2 are shown in Figures 15.9 and 15.10, respectively. The sample average, standard deviation, and 2.5th and 97.5th percentiles are also shown for each bootstrap distribution. Notice that the bootstrap averages and standard deviations are reasonably close to the values obtained from the original nonlinear least-squares fit. Furthermore, both histograms are reasonably normal in appearance, although the distribution for ^1* may be slightly skewed.
We can calculate the approximate 95% confidence intervals for 1 and 2. Consider first 1. From Eq. (15.21) and the information in Figure 15.9 we find
D1 = ^1 - ^1*(0.025) = 212.7 - 200.386 = 12.314 D2 = ^1*(0.975) - ^1 = 226.614 - 212.7 = 13.914
Therefore, the approximate 95% confidence interval is found from Eq. (15.22) as follows:

frequency > > >

BOOTSTRAPPING IN REGRESSION

523

120

100

80

60

40

20

0

180

190

200

210

220

230

240

_ 1 = 212.88599, S(1) = 6.87566, 1 (0.025) = 200.386, 1 (0.975) = 226.614

Figure 15.9 Histogram of bootstrap estimates ^1*, Example 15.4.

150

120

frequency > > >

90

60

30

0

29

49

69

89

109

(x 0.001) ­2 = 0.06388, S(2) = 0.00845, 2 (0.025) = 0.04757, 2 (0.975) = 0.08043

Figure 15.10 Histogram of bootstrap estimates ^2*, Example 15.4.

^1 - D2  1  ^1 + D1 212.7 - 13.914  1  212.7 + 12.314
198.786  1  225.014
This is very close to the asymptotic normal-theory interval calculated in the original problem. Following a similar procedure we obtain the approximate 95% bootstrap confidence interval for 2 as

524

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

0.04777  2  0.08063

Once again, this result is similar to the asymptotic normal-theory interval calculated

in the original problem. This gives us some assurance that the asymptotic results

apply, even though the sample size in this problem is only n = 12.



15.5 CLASSIFICATION AND REGRESSION TREES (CART)
The general classification problem can be stated as follows: given a response of interest and certain taxonomic data (measurement data or categorical descriptors) on a collection of units, use these data to predict the "class" into which each unit falls. The algorithm for accomplishing this task can then be used to make predictions about future units where the taxonomic data are known but the response is not. This is, of course, a very general problem, and many different statistical tools might be applied to it, including standard multiple regression, logistic regression or generalized linear models, cluster analysis, discriminant analysis, and so forth. In recent years, statisticians and computer scientists have developed tree-based algorithms for the classification problem. We give a brief introduction to these techniques in this section. For more details, see Breiman, Friedman, Olshen, and Stone [1984] and Gunter [1997a,b, 1998].
When the response variable is discrete, the procedure is usually called classification, and when it is continuous, the procedure leads to a regression tree. The usual acronym for the algorithms that perform these procedures is CART, which stands for classification and regression trees. A classification or regression tree is a hierarchical display of a series of questions about each unit in the sample. These questions relate to the values of the taxonomic data on each unit. When these questions are answered, we will know the "class" to which each unit most likely belongs. The usual display of this information is called a tree because it is logical to represent the questions as an upside-down tree with a root at the top, a series of branches connecting nodes, and leaves at the bottom. At each node, a question about one of the taxonomic variables is posed and the branch taken at the node depends on the answer. Determining the order in which the questions are asked is important, because it determines the structure of the tree. While there are many ways of doing this, the general principle is to ask the question that maximizes the gain in node purity at each node-splitting opportunity, where node purity is improved by minimizing the variability in the response data at the node. Thus, if the response is a discrete classification, higher purity would imply fewer classes or categories. A node containing a single class or category of the response would be completely pure. If the response is continuous, then a measure of variability such as a standard deviation, a mean square error, or a mean absolute deviation of the responses at a node should be made as small as possible to maximize node purity.
There are numerous specific algorithms for implementing these very general ideas, and many different computer software codes are available. CART techniques are often applied to very large or massive data sets, so they tend to be very computer intensive. There are many applications of CART techniques in situations ranging from interpretation of data from designed experiments to large-scale data exploration (often called data mining, or knowledge discovery in data bases).

CLASSIFICATION AND REGRESSION TREES (CART)

525

Example 15.5 The Gasoline Mileage Data

Table B.3 presents gasoline mileage performance data on 32 automobiles, along with 11 taxonomic variables. There are missing values in two of the observations, so we will confine our analysis to only the 30 vehicles for which complete samples are available. Figure 15.11 presents a regression tree produced by S-PLUS applied to this data set. The bottom portion of the figure shows the descriptive information (also in hierarchical format) produced by S-PLUS for each node in the tree. The measure of node purity or deviance at each node is just the corrected sum of squares of the observations at that node, yval is the average of these observations, and n refers to the number of observations at the node.
At the root node, we have all 30 cars, and the deviance there is just the corrected sum of squares of all 30 cars. The average mileage in the sample is 20.04 mpg. The first branch is on the variable CID, or cubic inches of engine displacement. There are four cars in node 2 that have a CID below 115.25, their deviance is 22.55, and the average mileage performance is 33.38 mpg. The deviance in node 3 from the right-hand branch of the root node is 295.6 and the sum of the deviances from nodes 2 and 3 is 318.15. There are no other splits possible at any level on any variable to classify the observations that will result in a lower sum of deviances than 318.15.

1 CID < 115.25

2 33.38

6 20.92

3 HP < 141.5
7 HP < 185
14 17.00

15 13.50

dc), split, n, deviance,yval * denotes terminal node
1) root 30 1139.0000 20.04 2) CID < 115.25 4 22.5500 33.38 * 3) CID > 115.25 295.600 17.99 6) HP < 141.5 11 32.4200 20.92 * 7) HP > 141.5 11 99.0400 15.83 14) HP < 185 10 50.1600 17.00 * 15) HP > 185.5 8.1400 13.50 *
Figure 15.11 CART analysis from S-PLUS for the gasoline mileage data from Table B.3.

526

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

Node 2 is a terminal node because the node deviance is a smaller percentage of the

root node deviance than the user specified allowance. Terminal nodes can also occur

if there are not enough observations (again, user specified) to split the node. So, at

this point, if one wishes to identify cars in the highest-mileage performance group,

all we need to look at is engine displacement.

Node 3 contains 26 cars, and it is subsequently split at the next node by horse-

power. Eleven cars with horsepower below 141.5 form one branch from this node,

while 15 cars with horsepower above 141.5 form the other branch. The left-hand

branch results in the terminal node 6. The right-hand branch enters another node

(7) which is branched again on horsepower. This illustrates an important feature of

regression trees; the same question can be asked more than once at different nodes

of the tree, reflecting the complexity of the interrelationships among the variables

in the problem. Nodes 14 and 15 are terminal nodes, and the cars in both terminal

nodes have similar mileage performance.

The tree indicates that we may be able to classify cars into higher-mileage,

medium-mileage, and lower-mileage classifications by examining CID and horse-

power--only 2 of the 11 taxonomic variables given in the original data set. For

purposes of comparison, forward variable selection using mpg as the response would

choose CID as the only important variable, and either stepwise regression or back-

ward elimination would select rear axle ratio, length, and weight. However, remem-

ber that the objectives of CART and multiple regression are somewhat different:

one is trying to find an optimal (or near-optimal) classification structure, while the

other seeks to develop a prediction equation.



15.6 NEURAL NETWORKS
Neural networks, or more accurately artificial neural networks, have been motivated by the recognition that the human brain processes information in a way that is fundamentally different from the typical digital computer. The neuron is the basic structural element and information-processing module of the brain.A typical human brain has an enormous number of them (approximately 10 billion neurons in the cortex and 60 trillion synapses or connections between them) arranged in a highly complex, nonlinear, and parallel structure. Consequently, the human brain is a very efficient structure for information processing, learning, and reasoning.
An artificial neural network is a structure that is designed to solve certain types of problems by attempting to emulate the way the human brain would solve the problem. The general form of a neural network is a "black-box" type of model that is often used to model high-dimensional, nonlinear data. Typically, most neural networks are used to solve prediction problems for some system, as opposed to formal model building or development of underlying knowledge of how the system works. For example, a computer company might want to develop a procedure for automatically reading handwriting and converting it to typescript. If the procedure can do this quickly and accurately, the company may have little interest in the specific model used to do it.
Multilayer feedforward artificial neural networks are multivariate statistical models used to relate p predictor variables x1, x2, . . . , xp to q response variables y1,

NEURAL NETWORKS 527
x1
x2
y1 x3
x4 y2
x5
x6 Figure 15.12 Artificial neural network with one hidden layer.
y2, . . . , yq. The model has several layers, each consisting of either the original or some constructed variables. The most common structure involves three layers: the inputs, which are the original predictors; the hidden layer, comprised of a set of constructed variables; and the output layer, made up of the responses. Each variable in a layer is called a node. Figure 15.12 shows a typical three-layer artificial neural network.
A node takes as its input a transformed linear combination of the outputs from the nodes in the layer below it. Then it sends as an output a transformation of itself that becomes one of the inputs, to one or more nodes on the next layer. The transformation functions are usually either sigmoidal (S shaped) or linear and are usually called activation functions or transfer functions. Let each of the k hidden layer nodes au be a linear combination of the input variables:
p
 au = w1juxj + u j =1
where the w1ju are unknown parameters that must be estimated (called weights) and u is a parameter that plays the role of an intercept in linear regression (this parameter is sometimes called the bias node).
Each node is transformed by the activation function g( ). Much of the neural networks literature refers to these activation functions notationally as ( ) because of their S shape (this is an unfortunate choice of notation so far as statisticians are concerned). Let the output of node au be denoted by Zu = g(au). Now we form a linear combination of these outputs, say b = uk=0 w2uzu, where z0 = 1. Finally, the
th response y is a transformation of the b , say y = g (b ), where g ( ) is the activa-
tion function for the response. This can all be combined to give

528

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

   k

p



y

=

g  u=1

w2u g 

j =1

w1 ju xj

+ 1j 

+2u  

(15.25)

The response y is a transformed linear combination of transformed linear combinations of the original predictors. For the hidden layer, the activation function is often chosen to be either the logistic function g(x) = 1/(1 + e-x) or the hyperbolic tangent function g(x) = tanh(x) = (ex - e-x)/(ex + e-x). The choice of activation function for the output layer depends on the nature of the response. If the response is bounded or dichotomous, the output activation function is usually taken to be sigmoidal, while if it is continuous, an identify function is often used.
The model in Eq. (15.25) is a very flexible form containing many parameters, and it is this feature that gives a neural network a nearly universal approximation property. That is, it will fit many naturally occurring functions. However, the parameters in Eq. (15.25) must be estimated, and there are a lot of them. The usual approach is to estimate the parameters by minimizing the overall residual sum of squares taken over all responses and all observations. This is a nonlinear least-squares problem, and a variety of algorithms can be used to solve it. Often a procedure called backpropagation (which is a variation of steepest descent) is used, although derivative-based gradient methods have also been employed. As in any nonlinear estimation procedure, starting values for the parameters must be specified in order to use these algorithms. It is customary to standardize all the input variables, so small essentially random values are chosen for the starting values.
With so many parameters involved in a complex nonlinear function, there is considerable danger of overfitting. That is, a neural network will provide a nearly perfect fit to a set of historical or "training" data, but it will often predict new data very poorly. Overfilling is a familiar problem to statisticians trained in empirical model building. The neural network community has developed various methods for dealing with this problem, such as reducing the number of unknown parameters (this is called "optimal brain surgery"), stopping the parameter estimation process before complete convergence and using cross-validation to determine the number of iterations to use, and adding a penalty function to the residual sum of squares that increases as a function of the sum of the squares of the parameter estimates. There are also many different strategies for choosing the number of layers and number of neurons and the form of the activation functions. This is usually referred to as choosing the network architecture. Cross-validation can be used to select the number of nodes in the hidden layer. Good references on artificial neural networks are Bishop [1995], Haykin [1994], and Ripley [1994].
Artificial neural networks are an active area of research and application, particularly for the analysis of large, complex, highly nonlinear problems. The overfilling issue is frequently overlooked by many users and advocates of neural networks, and because many members of the neural network community do not have sound training in empirical model building, they often do not appreciate the difficulties overfitting may cause. Furthermore, many computer programs for implementing neural networks do not handle the overfitting problem particularly well. Our view is that neural networks are a complement to the familiar statistical tools of regression analysis and designed experiments and not a replacement for them, because a neural

DESIGNED EXPERIMENTS FOR REGRESSION

529

network can only give a prediction model and not fundamental insight into the underlying process mechanism that produced the data.

15.7 DESIGNED EXPERIMENTS FOR REGRESSION
Many properties of the fitted regression model depend on the levels of the predictor variables. For example, the XX matrix determines the variances and covariances of the model regression coefficients. Consequently, in situations where the levels of the x's can be chosen it is natural to consider the problem of experimental design. That is, if we can choose the levels of each of the predictor variables (and even the number of observations to use), how should we go about this? We have already seen an example of this in Chapter 5 on fitting polynomials where a central composite design was used to fit a second-order polynomial in two variables. Because many problems in engineering, business, and the sciences use low-order polynomial models (typically first-order and second-order polynomials) in their solution there is an extensive literature on experimental designs for fitting these models. For example, see the book on experimental design by Montgomery (2009) and the book on response surface methodology by Myers, Montgomery, and Anderson-Cook (2009). This section gives an overview of designed experiments for regression models and some useful references.
Suppose that we want to fit a first-order polynomial in three variables, say,
y = 0 + 1x1 + 2 x2 + 3x3 + 
and we can specify the levels of the three regressor variables. Assume that the regressor variables are continuous and can be varied over the range from -1 to +1; that is, -1  xi  +1, i = 1,2,3. Factorial designs are very useful for fitting regression models. By a factorial design we mean that every possible level of a factor is run in combination with every possible level of all other factors. For example, suppose that we want to run each of the regressor variables at two levels, -1 and +1. Then the factorial design is called a 23 factorial design and it has n = 8 runs. The design matrix D is just an 8 × 3 matrix containing the levels of the regressors:

-1 -1 -1

 

1

-1

-1

-1 1 -1

D

=

 

1

-1

1 -1

-1 1

 1

-1

 1

-1 1 1

 1 1 1

530

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

The X matrix (or model matrix) is

and the XX matrix is

1 -1 -1 -1

1 1 -1 -1

1 -1 1 -1

X

=

1 1

1 -1

1 -1

-1 1

 1

1 -1

 1

1 -1 1 1

1 1 1 1

8 0 0 0 XX = 0 8 0 0
0 0 8 0 0 0 0 8

Notice that the XX matrix is diagonal, indicating that the 23 factorial design is orthogonal. The variance of any regression coefficient is

( ) Var ^ =  2 8
Furthermore, there is no other eight-run design on the design space bounded by ±1 that would make the variance of the model regression coefficients smaller.
For the 23 design, the determinant of the XX matrix is |XX| = 4096. This is the maximum possible value of the determinant for an eight-run design on the design space bounded by ±1. It turns out that the volume of the joint confidence region that contains all the model regression coefficients is inversely proportional to the square root of the determinant of XX. Therefore, to make this joint confidence region as small as possible, we would want to choose a design that makes the determinant of XX as large as possible. This is accomplished by choosing the 23 design.
These results generalize to the case of a first-order model in k variables, or a first-order model with interaction. A 2k factorial design (i.e., a factorial design with all k factors at two levels (±1)) will minimize the variance of the regression coefficients and minimize the volume of the joint confidence region on all of the model parameters. A design with this property is called a D-optimal design. Optimal designs resulted from the work of Kiefer (1959, 1961) and Kiefer and Wolfowitz (1959). Their work is couched in a measure theoretic framework in which an experimental design is viewed in terms of design measure. Design optimality moved into the practical arena in the 1970s and 1980s as designs were put forth as being efficient in terms of criteria inspired by Kiefer and his coworkers. Computer algorithms were developed that allowed "optimal" designs to be generated by a computer package

DESIGNED EXPERIMENTS FOR REGRESSION

531

based on the practitioner's choice of sample size, model, ranges on variables, and other constraints.
Now consider the variance of the predicted response for the first-order model in the 23design

( ) Var [y^ (x1, x2, x3 )] = Var ^0 + ^1x1 + ^2x2 + ^3x3

( ) =  2
8

1 + x12 + x22 + x33

The variance of the predicted response is a function of the point in the design
space where the prediction is made (x1, x2, and x3) and the variance of the model regression coefficients. The estimates of the regression coefficients are independent because the 23 design is orthogonal and the model parameters all have variance 2/8. Therefore, the maximum prediction variance occurs when x1 = x2 = x3 = ±1 and is equal to 2/2.
To determine how good this is, we need to know the best possible value of pre-
diction variance that can be attained. It turns out that the smallest possible value of the maximum prediction variance over the design space is p2/n, where p is the number of model parameters and n is the number of runs in the design.The 23 design has n = 8 runs and the model has p = 4 parameters, so the model that we fit to the data from this experiment minimizes the maximum prediction variance over the
design region. A design that has this property is called a G-optimal design. In general, 2k designs are G-optimal designs for fitting the first-order model or the
first-order model with interaction.
We can evaluate the prediction variance at any point of interest in the design space. For example, when we are at the center of the design where x1 = x2 = x3 = 0, the prediction variance is

( ) Var[y^ (x1 = 0, x2 = 0, x3 = 0)] = Var

^ 0

= 2 8

and when x1 = 1, x2 = x3 = 0, the prediction variance is

( ) Var[y^ (x1 = 1, x2 = 0, x3 = 0)] = Var

^0 + ^1

= 2 4

The average prediction variance at these two points is

12 2  8

+

2 4

 

=

3 2 16

.

A design that minimizes the average prediction variance over a selected set of points is called a V-optimal design.
An alternative to averaging the prediction variance over a specific set of points in the design space is to consider the average prediction variance over the entire

532

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

design space. One way to calculate this average prediction variance or the integrated variance is
 I = 1 Var[y^ (x)]dx AR

where A is the area or volume of the design space and R is the design region. To compute the average, we are integrating the variance function over the design space and dividing by the area or volume of the region. Now for a 23design, the volume of the design region is 8, and the integrated variance is

   ( ) I =  2 1 8

1 -1

1 -1

1
1 + x12 + x22 + x32
-1

dx1dx2dx3 = 0.25 2

It turns out that this is the smallest possible value of the average prediction variance that can be obtained from an eight-run design used to fit a first-order model on this design space. A design with this property is called an I-optimal design. In general, 2k designs are I-optimal designs for fitting the first-order model or the firstorder model with interaction.
Now consider designs for fitting second-order polynomials. As we noted in Chapter 7, second-order polynomial models are widely used in industry in the application of response surface methodology (RSM), a collection of experimental design, model fitting, and optimization techniques that are widely used in process improvement and optimization. The second-order polynomial in k factors is

k

k

k

   y = 0 + ixi + iixi2 +

ij xi xj + i.

i=1

i=1

i< j=2

This model has 1 + 2k + k(k - 1)/2 parameters, so the design must contain at least this many runs. In Section 7.4 we illustrated designing an experiment to fit a secondorder model in k = 2 factors and the associated model fitting and analysis typical of most RSM studies.
There are a number of standard designs for fitting second-order models. The two most widely used designs are the central composite design and the Box-Behnken design. The central composite design was used in Section 7.4. A central composite design consists of a 2k factorial design (or a fractional factorial that will allow estimation of all of the second-order model terms), 2k axial runs, defined as follows:

x1

x2

...

xk

-

0

...

0



0

...

0

0

-

...

0

0



...

0







0

0

...

-

0

0

...



DESIGNED EXPERIMENTS FOR REGRESSION

533

Figure 15.13 The central composite design for k = 2 and  = k = 2 .

Figure 15.14 The central composite design for k = 3 and  = k = 3.
and nC center runs at x1 = x2 = ··· = xk = 0. There is considerable flexibility in the use of the central composite design because the experimenter can choose both the axial distance  and the number of center runs. The choice of these two parameters can be very important. Figures 15.13 and 15.14 show the CCD for k = 2 and k = 3. The value of the axial distance generally varies from 1.0 to k, the former placing all of the axial points on the face of the cube or hypercube producing a design on a cuboidal region, the latter resulting in all points being equidistant from the design center producing a design on a spherical region. When  = 1 the central composite design is usually called a face-centered cube design. As we observed in Section 7.4, when the axial distance  = 4 F, where F is the number of factorial design points, the central composite design is rotatable; that is, the variance of the predicted response Var[y^ (x)] is constant for all points that are the same distance from the design center. Rotatability is a desirable property when the model fit to the data from the design is going to be used for optimization. It ensures that the variance of the predicted

534

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

Figure 15.15 The Box­Behnken design for k = 3 factors with one center point.
response depends only on the distance of the point of interest from the design center and not on the direction. Both the central composite design and the Box­Behnken design also perform reasonably well relative to the D-optimality and I-optimality criteria.
The Box­Behnken design is also a spherical design that is either rotatable or approximately rotatable. The Box­Behnken design for k = 3 factors is shown in Figure 15.15. All of the points in this design are on the surface of a sphere of radius 2. Refer to Montgomery (2009) or Myers, Montgomery, and AndersonCook (2009) for additional details of central composite and Box­Behnken designs as well as information on other standard designs for fitting the second-order polynomial model.
The JMP software will construct D-optimal and I-optimal designs. The approach used is based on a coordinate exchange algorithm developed by Meyer and Nachtsheim (1995). The experimenter specifies the number of factors, the model that is to be fit, the number of runs in the design, any constraints or restrictions on the design region, and the optimality criterion to be used (D or I). The coordinate exchange technique begins with a randomly chosen design and then systematically searches over each coordinate of each run to find a setting for that coordinate that produces the best value of the criterion. When the search is completed on the last run, it begins again with the first coordinate of the first run. This is continued until no further improvement in the criterion can be made. Now it is possible that the design found by this method is not optimal because ii may depend on the random starting design, so another random design is created and the coordinate exchange process repeated. After several random starts the best design found is declared optimal. This algorithm is extremely efficient and usually produces optimal or very near optimal designs.
To illustrate the construction of optimal designs suppose that we want to run an experiment to fit a second-order model in k = 4 factors. The region of interest is cuboidal and all four factors are defined to be in the interval from -1 to +1. This model has p = 15 parameters, so the design must have at least 15 runs. The central composite design in k = 4 factors has between 25 and 30 runs, depending on the number of center points. This is a relatively large design in comparison to the number of parameters that must be estimated. A fairly typical use of optimal designs

DESIGNED EXPERIMENTS FOR REGRESSION

535

TABLE 15.5 An 18 Run D-Optimal Design for a SecondOrder Model in k = 4 Factors

Run

X1

X2

X3

X4

1

0

0

1

0

2

1

1

1

-1

3

-1

-1

1

1

4

1

1

-1

1

5

-1

1

1

1

6

-1

-1

1

-1

7

1

-1

1

-1

8

1

-1

1

1

9

0

1

-1

-1

10

1

0

-1

-1

11

-1

0

-1

1

12

-1

1

1

-1

13

-1

-1

-1

-1

14

0

1

0

1

15

-1

0

0

-1

16

1

-1

0

0

17

-1

1

-1

0

18

0

-1

-1

1

is to create a custom design in situations where resources do not permit using the number of runs associated with a standard design. We will construct optimal designs with 18 runs. The 18-run D-optimal design constructed using JMP is shown in Table 15.5, and the I-optimal design is shown in Table 15.6. Both of these designs look somewhat similar. JMP reports the D-efficiency of the design in Table 15.5 as 44.98232% and the D-efficiency of the design in Table 15.6 as 39.91903%. Note that the D-optimal design algorithm did not produce a design with 100% D-efficiency, because the D-efficiency is computed relative to a "theoretical" orthogonal design that may not exist. The G-efficiency for the design in Table 15.5 is 75.38478% and for the design in Table 15.6 it is 73.57805%. The G-efficiency of a design is easy to calculate, because as we observed earlier the theoretical minimum value of the maximum value of the scaled prediction variance over the design space design space is p2/n, where p is the number of model parameters and n is the number of runs in the design, so all we have to do is find the actual maximum value of the prediction variance, and the G-efficiency can be calculated from

{ } GEfficiency = Max

p
nVar[y^ (x)]
2

Typically, efficiencies are reported on a percentage basis. Both designs have very similar G-efficiencies. JMP also reports the average (integrated) prediction variance over the design space as 0.652794 2 for the D-optimal design and 0.48553 2 for the I-optimal design. It is not surprising that the integrated variance is smaller for the I-optimal design as it was constructed to minimize this quantity.

536

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

TABLE 15.6 An 18 Run I-Optimal Design for a SecondOrder Model in k = 4 Factors

Run

X1

X2

X3

X4

1

1

0

1

1

2

-1

-1

1

1

3

-1

-1

0

-1

4

1

1

1

-1

5

0

-1

1

-1

6

-1

1

1

0

7

1

-1

-1

1

8

1

-1

-1

-1

9

0

1

0

1

10

-1

1

-1

-1

11

0

0

0

0

12

0

0

0

0

13

-1

0

-1

1

14

1

-1

0

0

15

-1

0

1

-1

16

0

-1

-1

0

17

1

1

-1

0

18

0

0

0

-1

Prediction Variance

1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Fraction of Space
Figure 15.16 Fraction of design space plot for the D-optimal and I-optimal designs in Tables 15.5 and 15.6.
To further compare these two designs, consider the graph in Figure 15.16. This is a fraction of design space (FDS) plot. For any value of prediction variance on the vertical scale the curve shows the fraction or proportion of the total design space in which the prediction variance is less than or equal to the vertical scale value. An "ideal" design would have a low, flat curve on the FDS plot. The lower curve in Figure 15.16 is the I-optimal design and the upper curve is for the D-optimal

PROBLEMS 537
design. Obviously, the I-optimal design outperforms the D-optimal design in terms of prediction variance over almost all of the design space. It does have a lower G-efficiency, indicating that there is a very small portion of the design space where the maximum prediction variance for the D-optimal design is less than the prediction variance for the I-optimal design. That point is at the extreme end of the region.

PROBLEMS

15.1 Explain why an estimator with a breakdown point of 50% may not give satisfactory results in fitting a regression model.
15.2 Consider the continuous probability distribution f(x). Suppose that  is an unknown location parameter and that the density may be written as f(x - ) for - <  < . Let x1, x2, . . . , xn be a random sample of size n from the density. a. Show that the maximum-likelihood estimator of  is the solution to

n
 (xi -  ) = 0
i=1

that maximizes the logarithm of the likelihood function In

L()

=



n i=1

ln

f

(

xi

-



),

where



(x)

=

(x)

and

(x)

=

-ln

f(x).

b. If f(x) is a nonmal distribution, find (x),  (x) and the corresponding

maximum-likelihood estimator of .

c. If f(x) = (2)-1e-|x|/ (the double-exponential distribution), find (x) and

 (x). Show that the maximum-likelihood estimator of  is the sample

median. Compare Ibis estimator with the estimator found in part b. Does

the sample median seem to be a reasonable estimator in this case?

d. If f(x) = [ (1 + x2)]-1 (the Cauchy distribution), find (x) and  (x). How

would

you

solve



n i=1



( xi

-



)

in

this

case?

15.3 Tukey's Biwelght. A popular  function for robust regression is Tukey's biweight, where



(z)

=

z 

1

-

(z

a

)2



2
,

z a

0,

z >a

with a = 5, 6 Sketch the  function for a = 5 and discuss its behavior. Do you think that Tukey's biweight would give results similar to Andrews' wave function?

15.4 The U.S. Air Force uses regression models for cost estimating, an application that almost always involves outliers. Simpson and Montgomery [1998a] present 19 observations on first-unit satellite cost data (y) and the weight of the electronics suite (x). The data are shown in the following table.

538

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

Observation
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Cost ($K)
2449 2248 3545 794 1619 2079
918 1231 3641 4314 2628 3989 2308 376 5428 2786 2497 5551 5208

Weight (Ib)
90.6 87.8 38.6 28.6 28.9 23.3 21.1 17.5 27.6 39.2 34.9 46.6 80.9 14.6 48.1 38.1 73.2 40.8 44.6

a. Draw a scatter diagram of the data. Discuss what types of outliers may be present.
b. Fit a straight line to these data with OLS. Does this fit seem satisfactory?
c. Fit a straight line to these data with an M-estimator of your choice. Is the fit satisfactory? Discuss why the M-estimator is a poor choice for this problem.
d. Discuss the types of estimators that you think might be appropriate for this data set.
15.5 Table B.14 presents data on the transient points of an electronic inverter. Fit a model to those data using an M-estimator. Is there an indication that observations might have been incorrectly recorded?
15.6 Consider the regression model in Problem 2.10 relating systolic blood pressure to weight. Suppose that we wish to predict an individual's weight given an observed value of systolic blood pressure. Can this be done using the procedure for predicting x given a value of y described in Section 15.3? In this particular application, how would you respond to the suggestion of building a regression model relating weight to systolic blood pressure?
15.7 Consider the regression model in Problem 2.4 relating gasoline mileage to engine displacement. a. If a particular car has an observed gasoline mileage of 17 miles per gallon, find a point estimate of the corresponding engine displacement. b. Find a 95% confidence interval on engine displacement.

PROBLEMS 539

15.8 Consider a regression model relating total heat flux to radial deflection for the solar energy data in Table B.2.
a. Suppose that the observed total heat flux is 250 kW. Find a point estimate of the corresponding radial deflection.
b. Construct a 90% confidence interval on radial deflection.

15.9 Consider the soft drink delivery time data in Example 3.1. Find an approximate 95% bootstrap confidence interval on the regression coefficient for distance using m = 1000 bootstrap samples. Compare this to the usual normal-theory confidence interval.

15.10

Consider the soft drink delivery time data in Example 3.1. Find the bootstrap estimate of the standard deviation of ^1 using the following numbers of bootstrap samples: m = 100, m = 200, m = 300, m = 400, and m = 500. Can you
draw any conclusions about how many bootstrap samples are necessary to obtain a reliable estimate of the precision of estimation for ^1?

15.11 Describe how you would find a bootstrap estimate of the standard deviation of the estimate of the mean response at a particular point, say x0.

15.12 Describe how you would find an approximate bootstrap confidence interval on the mean response at a particular point, say x0.

15.13

Consider the nonlinear regression model fit to the data in Problem 12.11. Find the bootstrap standard errors for the regression coefficients ^1, ^2, and ^3 using m = 1000 bootstrap samples. Based on the results you obtain, comment on how the asymptotic theory seems to apply to this problem.

15.14

Consider the nonlinear regression model fit to the data in Problem 12.11. Find approximate 95% bootstrap confidence intervals for the regression coefficients ^1, ^2, and ^3 using m = 1000 bootstrap samples. Compare these intervals to the ones based on the large-sample results. Based on the comparison of these intervals, comment on how the asymptotic theory seems to apply to this problem.

15.15 Consider the NFL team performance data in Table B.1. Construct a regression tree for this data set.

15.16

A Designed Experiment for Linear Regression. You wish to fit a simple

linear regression model over the region -1  x  1 using n = 10 observations.

Four experimental designs are under consideration: (i) 5 observations at

x = -1 and 5 observations at x = +1, (ii) 4 observations at x = -1, 2 observa-

tions

at

x

=

0,

and

4

observations

at

x

=

+1,

(iii)

2

observations

at

x

=

-1,

-

1 2

,

0,

+

1 2

,

and

+1,

and

(iv)

1

observation

at

x

=

-1,

-0.8,-0.6,

-0.4,

-0.2,

+0.2,

+0.4,

+0.6, +0.8, and +1. For each of these designs, find the number of degrees of

freedom available for evaluating pure error and testing lack of fit, the stan-

dard error of the slope (up to a constant ), and the value of the determinant

of XX. Based on these analyses, which design would you select?

15.17 An analyst is fitting a simple linear regression model with the objective of obtaining a minimum-variance estimate of the intercept 0. How should the data collection experiment be designed?

540

OTHER TOPICS IN THE USE OF REGRESSION ANALYSIS

15.18 15.19

Suppose that you are fitting a simple linear regression model that will be used to predict the mean response at a particular point such as x0. How should the data collection experiment be designed so that a minimumvariance estimate of the mean of y at x0 is obtained?
Consider the linear regression model y = 0 + 1x1 + 2x2 + , where the regressors have been coded so that

n

n

n

n

    xi1 = xi2 = 0 and

xi21 = xi22 = n

i=1

i=1

i=1

i=1

a. Show that an orthogonal design (XX diagonal) minimizes the variance of ^1 and ^2.
b. Show that any design for fitting this first-order model that is orthogonal is also rotatable.

APPENDIX A

STATISTICAL TABLES

Table A.1 Table A.2 Table A.3 Table A.4 Table A.5 Table A.6

Cumulative Standard Normal Distribution Percentage Points of the 2 Distribution Percentage Points of the t Distribution Percentage Points of the F Distribution Orthogonal Polynomials Critical Values of the Durbin­Watson Statistic

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
541

542 APPENDIX A

TABLE A.1 Cumulative Standard Normal Distribution

z
(z) =

1 e-u2 2du

- 2

z

.00

.01

.02

.03

.04

z

.0

.50000

.50399

.50798

.51197

.51595

.0

.1

53983

.54379

.54776

.55172

.55567

.1

.2

.57926

.58317

.58706

.59095

.59483

.2

.3

.61791

.62172

.62551

.62930

.63307

.3

.4

.65542

.65910

.66276

.66640

.67003

.4

.5

.69146

.69497

.69847

.70194

.70540

.5

.6

.72575

.72907

.73237

.73565

.73891

.6

.7

.75803

.76115

.76424

.76730

.77035

.7

.8

.78814

.79103

.79389

.79673

.79954

.8

.9

.81594

.81859

.82121

.82381

.82639

.9

1.0

.84134

.84375

.84613

.84849

.85083

1.0

1.1

.86433

.86650

.86864

.87076

.87285

1.1

1.2

.88493

.88686

.88877

.89065

.89251

1.2

1.3

.90320

.90490

.90658

.90824

.90988

1.3

1.4

.91924

.92073

.92219

.92364

.92506

1.4

1.5

.93319

.93448

.93574

.93699

.93822

1.5

1.6

.94520

.94630

.94738

.94845

.94950

1.6

1.7

.95543

.95637

.95728

.95818

.95907

1.7

1.8

.96407

.96485

.96562

.96637

.96711

1.8

1.9

.97128

.97193

.97257

.97320

.97381

1.9

2.0

.97725

.97778

.97831

.97882

.97932

2.0

2.1

.98214

.98257

.98300

.98341

.98382

2.1

2.2

.98610

.98645

.98679

.98713

.98745

2.2

2.3

.98928

.98956

.98983

.99010

.99036

2.3

2.4

.99180

.99202

.99224

.99245

.99266

2.4

2.5

.99379

.99396

.99413

.99430

.99446

2.5

2.6

.99534

.99547

.99560

.99573

.99585

2.6

2.7

.99653

.99664

.99674

.99683

.99693

2.7

2.8

.99744

.99752

.99760

.99767

.99774

2.8

2.9

.99813

.99819

.99825

.99831

.99836

2.9

3.0

.99865

.99869

.99874

.99878

.99882

3.0

3.1

.99903

.99906

.99910

.99913

.99916

3.1

3.2

.99931

.99934

.99936

.99938

.99940

3.2

3.3

.99952

.99953

.99955

.99957

.99958

3.3

3.4

.99966

.99968

.99969

.99970

.99971

3.4

3.5

.99977

.99978

.99978

.99979

.99980

3.5

3.6

.99984

.99985

.99985

.99986

.99986

3.6

3.7

.99989

.99990

.99990

.99990

.99991

3.7

3.8

.99993

.99993

.99993

.99994

.99994

3.8

3.9

.99995

.99995

.99996

.99996

.99996

3.9

STATISTICAL TABLES

543

TABLE A.1 (Continued)

z
(z) =

1 e-u2 2du

- 2

z

.05

.06

.07

.08

.09

z

.0

.51994

.52392

.52790

.53188

.53586

.0

.1

.55962

.56356

56749

.57142

57534

.1

.2

.59871

.60257

.60642

.61026

.61409

.2

.3

.63683

.64058

.64431

.64803

.65173

.3

.4

.67364

.67724

.68082

.68438

.68793

.4

.5

.70884

.71226

.71566

.71904

.72240

.5

.6

.74215

.74537

.74857

.75175

.75490

.6

.7

.77337

.77637

.77935

.78230

.78523

.7

.8

.80234

.80510

.80785

.81057

.81327

.8

.9

.82894

.83147

.83397

.83646

.83891

.9

1.0

.85314

.85543

.85769

.85993

.86214

1.0

1.1

.87493

.87697

.87900

.88100

.88297

1.1

1.2

.89435

.89616

.89796

.89973

.90147

1.2

1.3

.91149

.91308

.91465

.91621

.91773

1.3

1.4

.92647

.92785

.92922

.93056

.93189

1.4

1.5

.93943

.94062

.94179

.94295

.94408

1.5

1.6

.95053

.95154

.95254

.95352

.95448

1.6

1.7

.95994

.96080

.96164

.96246

.96327

1.7

1.8

.96784

.96856

.96926

.96995

.97062

1.8

1.9

.97441

.97500

.97558

.97615

.97660

1.9

2.0

.97982

.98030

.98077

.98124

.98169

2.0

2.1

.98422

.98461

.98500

.98537

.98574

2.1

2.2

.98778

.98809

.98840

.98870

.98899

2.2

2.3

.99061

.99086

.99111

.99134

.99158

2.3

2.4

.99286

.99305

.99324

.99343

.99361

2.4

2.5

.99461

.99477

.99492

.99506

.99520

2.5

2.6

.99598

.99609

.99621

.99632

.99643

2.6

2.7

.99702

.99711

.99720

.99728

.99736

2.7

2.8

.99781

.99788

.99795

.99801

.99807

2.8

2.9

.99841

.99846

.99851

.99856

.99861

2.9

3.0

.99886

.99889

.99893

.99897

.99900

3.0

3.1

.99918

.99921

.99924

.99926

.99929

3.1

3.2

.99942

.99944

.99946

.99948

.99950

3.2

3.3

.99960

.99961

.99962

.99964

.99965

3.3

3.4

.99972

.99973

.99974

.99975

.99976

3.4

3.5

.99981

.99981

.99982

.99983

.99983

3.5

3.6

.99987

.99987

.99988

.99988

.99989

3.6

3.7

.99991

.99992

.99992

.99992

.99992

3.7

3.8

.99994

.99994

.99995

.99995

.99995

3.8

3.9

.99996

.99996

.99996

.99997

.99997

3.9

Source: Reproduced with permission from Probability and Statistics in Engineering and Management Science, 3rd ed., 1990, by W. W. Hines and D. C. Montgomery, Wiley, New York.

544 APPENDIX A

TABLE A.2 Percentage Points of the 2 Distribntion



v

.995

.990

.975

.950 .900 .500 .100

.050

.025

.010

.005

1

.00+

.00+

.00+ .00+ .02

.45

2.71 3.84

5.02

6.63

7.88

2

.01

.02

.05

.10

.21 1.39

4.61 5.99

7.38

9.21 10.60

3

.07

.11

.22

.35

.58 2.37

6.25 7.81

9.35 11.34 12.84

4

.21

.30

.48

.71

1.06 3.36

7.48

9.49 11.14 13.28 14.86

5

.41

.55

.83

1.15

1.61 4.35

9.24 11.07 12.83 15.09 16.75

6

.68

.87

1.24 1.64 2.20 5.35 10.65 12.59 14.45 16.81 18.55

7

.99

1.24

1.69 2.17 2.83 6.35 12.02 14.07 16.01 18.48 20.28

8 1.34

1.65

2.18 2.73 3.49 7.34 13.36 15.51 17.53 20.09 21.96

9 1.73

2.09

2.70 3.33 4.17 8.34 14.68 16.92 19.02 21.67 23.59

10 2.16

2.56

3.25 2.94 4.87 9.34 15.99 18.31 20.48 23.21 25.19

11 2.60

3.05

3.82 4.57 5.58 10.34 17.28 19.68 21.92 24.72 26.76

12 3.07

3.57

4.40 5.23 6.30 11.34 18.55 21.03 23.34 26.22 28.30

13 3.57

4.11

5.01 5.89 7.04 12.34 19.81 22.36 24.74 27.69 29.82

14 4.07

4.66

5.63 6.57 7.79 13.34 21.06 23.68 26.12 29.14 31.32

15 4.60

5.23

6.27 7.26 8.55 14.34 22.31 25.00 27.49 30.58 32.80

16 5.14

5.81

6.91 7.96 9.31 15.34 23.54 26.30 28.85 32.00 34.27

17 5.70

6.41

7.56 8.67 10.09 16.34 24.77 27.59 30.19 33.41 35.72

18 6.26

7.01

8.23 9.39 10.87 17.34 25.99 28.87 31.53 34.81 37.16

19 6.84

7.63

8.91 10.12 11.65 18.34 27.20 30.14 32.85 36.19 38.58

20 7.43

8.26

9.59 10.85 12.44 19.34 28.41 31.41 34.17 37.57 40.00

21 8.03

8.90 10.28 11.59 13.24 20.34 29.62 32.67 35.48 38.93 41.40

22 8.64

9.54 10.98 12.34 14.04 21.34 30.81 33.92 36.78 40.29 42.80

23 9.26 10.20 11.69 13.09 14.85 22.34 32.01 35.17 38.08 41.64 44.18

24 9.89 10.86 12.40 13.85 15.66 23.34 33.20 36.42 39.36 42.98 45.45

25 10.52 11.52 13.12 14.61 16.47 24.34 34.28 37.65 40.65 44.31 46.93

26 11.16 12.20 13.84 15.38 17.29 25.34 35.56 38.89 41.92 45.64 48.29

27 11.81 12.88 14.57 16.15 18.11 26.34 36.74 40.11 43.19 46.96 49.65

28 12.46 13.57 15.31 16.93 18.94 27.34 37.92 41.34 44.46 48.28 50.99

29 13.12 14.26 16.05 17.71 19.77 28.34 39.09 42.56 45.72 49.59 52.34

30 13.79 14.95 16.79 18.49 20.60 29.34 40.26 43.77 46.98 50.89 53.67

40 20.71 22.16 24.43 26.51 29.05 39.34 51.81 55.76 59.34 63.69 66.77

50 27.99 29.71 32.36 34.76 37.69 49.33 63.17 67.50 71.42 76.15 79.49

60 35.53 37.48 40.48 43.19 46.46 59.33 74.40 79.08 83.30 88.38 91.95

70 43.28 45.44 48.76 51.74 55.33 69.33 85.53 90.53 95.02 100.42 104.22

80 51.17 53.54 57.15 60.39 64.28 79.33 96.58 101.88 106.63 112.33 116.32

90 59.20 61.75 65.65 69.13 73.29 89.33 107.57 113.14 118.14 124.12 128·30

100 67.33 70.06 74.22 77.93 82.36 99.33 118.50 124.34 139.56 135.81 140.17

v = degrees of freedom.
Source: Reproduced with permission from Probability and in Engineering and Management Science, 3rd ed., 1990, by W. W. Hines and D. C. Montgomery, Wiley, New York.

STATISTICAL TABLES

545

TABLE A.3 Percentage Points of the t Distribution



v

.40

.25

.10

.05

.025

.01

.005

.0025

.001

.0005

1 .325 1.000 3.078 6.314 12.706 31.821 63.657 127.32

318.31

636.62

2 .289 .816 1.886 2.920

4.303 6.965

9.925 14.089 23.326

31.598

3 .277 .765 1.638 2.353

3.182 4.541

5.841

7.453

10.213

12.924

4 .271 .741 1.533 2.132

2.776 3.747

4.604

5.598

7.173

8.610

5 .267 .727 1.476 2.015

2.571 3.365

4.032

4.773

5.893

6.869

6 .265 .718 1.440 1.943

2.447 3.143

3.707

4.317

5.208

5.959

7 .263 .711 1.415 1.895

2.365 2.998

3.499

4.029

4.785

5.408

8 .262 .706 1.397 1.860

2.306 2.896

3.355

2.833

4.504

5.041

9 .261 .703 1.383 1.833

2.262 2.821

3.250

3.690

4.297

4.781

10 .260 .700 1.372 1.812

2.228 2.764

3.169

3.581

4.144

4.587

11 .260 .697 1.363 1.796

2.201 2.718

3.106

3.497

4.025

4.437

12 .259 .695 1.356 1.782

2.179 2.681

3.055

3.428

3.930

4.318

13 .259 .694 1.350 1.771

2.160 2.650

3.012

3.372

3.852

4.221

14 .258 .692 1.345 1.761

2.145 2.624

2.977

3.326

3.787

4.140

15 .258 .691 1.341 1.753

2.131 2.602

2.947

3.286

3.733

4.073

16 .258 .690 1.337 1.746

2.120 2.583

2.921

3.252

3.686

4.015

17 .257 .689 1.333 1.740

2.110 2.567

2.898

3.222

3.646

3.965

18 .257 .688 1.330 1.734

2.101 2.552

2.878

3.197

3.610

3.922

19 .257 .688 1.328 1.729

2.093 2.539

2.861

3.174

3.579

3.883

20 .257 .687 1.325 1.725

2.086 2.528

2.845

3.153

3.552

3.850

21 .257 .686 1.323 1.721

2.080 2.518

2.831

3.135

3.527

3.819

22 .256 .686 1.321 1.717

2.074 2.508

2.819

3.119

3.505

3.792

23 .256 .685 1.319 1.714

2.069 2.500

2.807

3.104

3.485

2.767

24 .256 .685 1.318 1.711

2.064 2.492

2.797

3.091

3.467

3.745

25 .256 .684 1.316 1.708

2.060 2.485

2.787

8.078

3.450

3.725

26 .256 .684 1.315 1.706

2.056 2.479

2.779

3.067

3.435

3.707

27 .256 .684 1.314 1.703

2.052 2.473

2.771

3.057

3.421

3.690

28 .256 .683 1.313 1.701

2.048 2.467

2.763

3.047

3.408

2.674

29 .256 .683 1.311 1.699

2.045 2.462

2.756

3.308

3.396

3.659

30 .256 .683 1.310 1.697

2.042 2.457

2.750

3.030

3.385

3.646

40 .255 .681 1.303 1.648

2.021 2.423

2.704

2.971

3.307

3.551

60 .254 .679 1.296 1.671

2.000 2.390

2.660

2.915

3.232

3.460

120 .254 .677 1.289 1.658

1.980 2.358

2.617

2.860

3.160

3.373



.253 .674 1.282 1.645

1.960 2.326

2.576

2.807

3.090

3.291

Source: Adapted with pennission from Biometrika Tables for Statisticians, Vol. 1, 3rd ed., 1966, by E. S. Pearson and H. O. Hartley, Cambridge University Press, Cambridge.

TABLE A.4 Percentage Points of the F-Distribution

F.25,1, 2

546

Degrees of Freedom for the Denominator (v2)

Degrees of Freedom for the Numerator (v1)

v2

1

2

3

4

5

6

7

8

9 10 12 15 20 24 30 40 60 120 

1 5.83 7.50 8.20 8.58 8.82 8.98 9.10 9.19 9.26 9.32 9.41 9.49 9.58 9.63 9.67 9.71 9.76 9.80 9.85 2 2.57 3.00 3.15 3.23 3.28 3.31 3.34 3.35 3.37 3.38 3.39 3.41 3.43 3.43 3.44 3.45 3.46 3.47 3.48 3 2.02 2.28 2.36 2.39 2.41 2.42 2.43 2.44 2.44 2.44 2.45 2.46 2.46 2.46 2.47 2.47 2.47 2.47 2.47 4 1.81 2.00 2.05 2.06 2.07 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 5 1.69 1.85 1.88 1.89 1.89 1.89 1.89 1.89 1.89 1.89 1.89 1.89 1.88 1.88 1.88 1.88 1.87 1.87 1.87 6 1.62 1.76 1.78 1.79 1.79 1.78 1.78 1.78 1.77 1.77 1.77 1.76 1.76 1.75 1.75 1.75 1.74 1.74 1.74
7 1.57 1.70 1.72 1.72 1.71 1.71 1.70 1.70 1.70 1.69 1.68 1.68 1.67 1.67 1.66 1.66 1.65 1.65 1.65 8 1.54 1.66 1.67 1.66 1.66 1.65 1.64 1.64 1.63 1.63 1.62 1.62 1.61 1.60 1.60 1.59 1.59 1.58 1.58 9 1.51 1.62 1.63 1.63 1.62 1.61 1.60 1.60 1.59 1.59 1.58 1.57 1.56 1.56 1.55 1.54 1.54 1.53 1.53 10 1.49 1.60 1.60 1.59 1.59 1.58 1.57 1.56 1.56 1.55 1.54 1.53 1.52 1.52 1.51 1.51 1.50 1.49 1.48 11 1.47 1.58 1.58 1.57 1.56 1.55 1.54 1.53 1.53 1.52 1.51 1.50 1.49 1.49 1.48 1.47 1.47 1.46 1.45 12 1.46 1.56 1.56 1.55 1.54 1.53 1.52 1.51 1.51 1.50 1.49 1.48 1.47 1.46 1.45 1.45 1.44 1.43 1.42 13 1.45 1.55 1.55 1.53 1.52 1.51 1.50 1.49 1.49 1.48 1.47 1.46 1.45 1.44 1.43 1.42 1.42 1.41 1.40 14 1.44 1.53 1.53 1.52 1.51 1.50 1.49 1.48 1.47 1.46 1.45 1.44 1.43 1.42 1.41 1.41 1.40 1.39 1.38 15 1.43 1.52 1.52 1.51 1.49 1.48 1.47 1.46 1.46 1.45 1.44 1.43 1.41 1.41 1.40 1.39 1.38 1.37 1.36 16 1.42 1.51 1.51 1.50 1.48 1.47 1.46 1.45 1.44 1.44 1.43 1.41 1.40 1.39 1.38 1.37 1.36 1.35 1.34 17 1.42 1.51 1.50 1.49 1.47 1.46 1.45 1.44 1.43 1.43 1.41 1.40 1.39 1.38 1.37 1.36 1.35 1.34 1.33 18 1.41 1.50 1.49 1.48 1.46 1.45 1.44 1.43 1.42 1.42 1.40 1.39 1.38 1.37 1.36 1.35 1.34 1.33 1.32 19 1.41 1.49 1.49 1.47 1.46 1.44 1.43 1.42 1.41 1.41 1.40 1.38 1.37 1.36 1.35 1.34 1.33 1.32 1.30 20 1.40 1.49 1.48 1.47 1.45 1.44 1.43 1.42 1.41 1.40 1.39 1.37 1.36 1.35 1.34 1.33 1.32 1.31 1.29 21 1.40 1.48 1.48 1.46 1.44 1.43 1.42 1.41 1.40 1.39 1.38 1.37 1.35 1.34 1.33 1.32 1.31 1.30 1.28 22 1.40 1.48 1.47 1.45 1.44 1.42 1.41 1.40 1.39 1.39 1.37 1.36 1.34 1.33 1.32 1.31 1.30 1.29 1.28 23 1.39 1.47 1.47 1.45 1.43 1.42 1.41 1.40 1.39 1.38 1.37 1.35 1.34 1.33 1.32 1.31 1.30 1.28 1.27 24 1.39 1.47 1.46 1.44 1.43 1.41 1.40 1.39 1.38 1.38 1.36 1.35 1.33 1.32 1.31 1.30 1.29 1.28 1.26 25 1.39 1.47 1.46 1.44 1.42 1.41 1.40 1.39 1.38 1.37 1.36 1.34 1.33 1.32 1.31 1.29 1.28 1.27· 1.25 26 1.38 1.46 1.45 1.44 1.42 1.41 1.39 1.38 1.37 1.37 1.35 1.34 1.32 1.31 1.30 1.29 1.28 1.26 1.25 27 1.38 1.46 1.45 1.43 1.42 1.40 1.39 1.38 1.37 1.36 1.35 1.33 1.32 1.31 1.30 1.28 1.27 1.26 1.24 28 1.38 1.46 1.45 1.43 1.41 1.40 1.39 1.38 1.37 1.36 1.34 1.33 1.31 1.30 1.29 1.28 1.27 1.25 1.24 29 1.38 1.45 1.45 1.43 1.41 1.40 1.38 1.37 1.36 1.35 1.34 1.32 1.31 1.30 1.29 1.27 1.26 1.25 1.23 30 1.38 1.45 1.44 1.42 1.41 1.39 1.38 1.37 1.36 1.35 1.34 1.32 1.30 1.29 1.28 1.27 1.26 1.24 1.23 40 1.36 1.44 1.42 1.40 1.39 1.37 1.36 1.35 1.34 1.33 1.31 1.30 1.28 1.26 1.25 1.24 1.22 1.21 1.19 60 1.35 1.42 1.41 1.38 1.37 1.35 1.33 1.32 1.31 1.30 1.29 1.27 1.25 1.24 1.22 1.21 1.19 1.17 1.15 120 1.34 1.40 1.39 1.37 1.35 1.33 1.31 1.30 1.29 1.28 1.26 1.24 1.22 1.21 1.19 1.18 1.16 1.13 1.10  1.32 1.39 1.37 1.35 1.33 1.31 1.29 1.28 1.27 1.25 1.24 1.22 1.19 1.18 1.16 1.14 1.12 1.08 1.00

Degrees of Freedom for the Denominator (v2)

F.10,1, 2

Degrees of Freedom for the Numerator (v1)

v2

12

3

4

5

6

7

8

9 10 12 15 20 24 30 40 60 120 

1 39.86 49.50 53.59 55.83 57.24 58.20 58.91 59.44 59.86 60.19 60.71 61.22 61.74 62.00 62.26 62.53 62.79 63.06 63.33

2 8.53 9.00 9.16 9.24 9.29 9.33 9.35 9.37 9.38 9.39 9.41 9.42 9.44 9.45 9.46 9.47 9.47 9.48 9.49

3 5.54 5.46 5.39 5.34 5.31 5.28 5.27 5.25 5.24 5.23 5.22 5.20 5.18 5.18 5.17 5.16 5.15 5.14 5.13

4 4.54 4.32 4.19 4.11 4.05 4.01 3.98 3.95 3.94 3.92 3.90 3.87 3.84 3.83 3.82 3.80 3.79 3.78 2.76

5 4.06 3.78 3.62 3.52 3.45 3.40 3.37 3.34 3.32 3.30 3.27 3.24 3.21 3.19 3.17 3.16 3.14 3.12 3.10

6 3.78 3.46 3.29 3.18 3.11 3.05 3.01 2.98 2.96 2.94 2.90 2.87 2.84 2.82 2.80 2.78 2.76 2.74 2.72

7 3.59 3.26 3.07 2.96 2.88 2.83 2.78 2.75 2.72 2.70 2.67 2.63 2.59 2.58 2.56 2.54 2.51 2.49 2.47

8 3.46 3.11 2.92 2.81 2.73 2.67 2.62 2.59 2.56 2.54 2.50 2.46 2.42 2.40 2.38 2.36 2.34 2.32 2.29

9 3.36 3.01 2.81 2.69 2.61 2.55 2.51 2.47 2.44 2.42 2.38 2.34 2.30 2.28 2.25 2.23 2.21 2.18 2.16

10 3.29 2.92 2.73 2.61 2.52 2.46 2.41 2.38 2.35 2.32 2.28 2.24 2.20 2.18 2.16 2.13 2.11 2.08 2.06

11 3.23 2.86 2.66 2.54 2.45 2.39 2.34 2.30 2.27 2.25 2.21 2.17 2.12 2.10 2.08 2.05 2.03 2.00 1.97

12 3.18 2.81 2.61 2.48 2.39 2.33 2.28 2.24 2.21 2.19 2.15 2.10 2.06 2.04 2.01 1.99 1.96 1.93 1.90

13 3.14 2.76 2.56 2.43 2.35 2.28 2.23 2.20 2.16 2.14 2.10 2.05 2.01 1.98 1.96 1.93 1.90 1.88 1.85

14 3.10 2.73 2.52 2.39 2.31 2.24 2.19 2.15 2.12 2.10 2.05 2.01 1.96 1.94 1.91 1.89 1.86 1.83 1.80

15 3.07 2.70 2.49 2.36 2.27 2.21 2.16 2.12 2.09 2.06 2.02 1.97 1.92 1.90 1.87 1.85 1.82 1.79 1.76

16 3.05 2.67 2.46 2.33 2.24 2.18 2.13 2.09 2.06 2.03 1.99 1.94 1.89 1.87 1.84 1.81 1.78 1.75 1.72

17 3.03 2.64 2.44 2.31 2.22 2.15 2.10 2.06· 2.03 2.00 1.96 1.91 1.86 1.84 1.81 1.78 1.75 1.72 1.69

18 3.01 2.62 2.42 2.29 2.20 2.13 2.08 2.04 2.00 1.98 1.93 1.89 1.84 1.81 1.78 1.75 1.72 1.69 1.66

19 2.99 2.61 2.40 2.27 2.18 2.11 2.06 2.02 1.98 1.96 1.91 1.86 1.81 1.79 1.76 1.73 1.70 1.67 1.63

20 2.97 2.59 2.38 2.25 2.16 2.09 2.04 2.00 1.96 1.94 1.89 1.84 1.79 1.77 1.74 1.71 1.68 1.64 1.61

21 2.96 2.57 2.36 2.23 2.14 2.08 2.02 1.98 1.95 1.92 1.87 1.83 1.78 1.75 1.72 1.69 1.66 1.62 1.59

22 2.95 2.56 2.35 2.22 2.13 2.06 2.01 1.97 1.93 1.90 1.86 1.81 1.76 1.73 1.70 1.67 1.64 1.60 1.57

23 2.94 2.55 2.34 2.21 2.11 2.05 1.99 1.95 1.92 1.89 1.84 1.80 1.74 1.72 1.69 1.66 1.62 1.59 1.55

24 2.93 2.54 2.33 2.19 2.10 2.04 1.98 1.94 1.91 1.88 1.83 1.78 1.73 1.70 1.67 1.64 1.61 1.57 1.53

25 2.92 2.53 2.32 2.18 2.09 2.02 1.97 1.93 1.89 1.87 1.82 1.77 1.72 1.69 1.66 1.63 1.59 1.56 1.52

26 2.91 2.52 2.31 2.17 2.08 2.01 1.96 1.92 1.88 1.86 1.81 1.76 1.71 1.68 1.65 1.61 1.58 1.54 1.50

27 2.90 2.51 2.30 2.17 2.07 2.00 1.95 1.91 1.87 1.85 1.80 1.75 1.70 1.67 1.64 1.60 1.57 1.53 1.49

28 2.89 2.50 2.29 2.16 2.06 2.00 1.94 1.90 1.87 1.84 1.79 1.74 1.69 1.66 1.63 1.59 1.56 1.52 1.48

29 2.89 2.50 2.28 2.15 2.06 1.99 1.93 1.89 1.86 1.83 1.78 1.73 1.68 1.65 1.62 1.58 1.55 1.51 1.47

30 2.88 2.49 2.28 2.14 2.03 1.98 1.93 1.88 1.85 1.82 1.77 1.72 1.67 1.64 1.61 1.57 1.54 1.50 1.46

40 2.84 2.44 2.23 2.09 2.00 1.93 1.87 1.83 1.79 1.76 1.71 1.66 1.61 1.57 1.54 1.51 1.47 1.42 1.38

60 2.79 2.39 2.18 2.04 1.95 1.87 1.82 1.77 1.74 1.71 1.66 1.60 1.54 1.51 1.48 1.44 1.40 1.35 1.29

120 2.75 2.35 2.13 1.99 1.90 1.82 1.77 1.72 1.68 1.65 1.60 1.55 1.48 1.45 1.41 1.37 1.32 1.26 1.19



2.17 2.30 2.08 1.94 1.85 1.77 1.72 1.67 1.63 1.60 1.55 1.49 1.42 1.38 1.34 1.30 1.24 1.17 1.00

547

(Continued)

TABLE A.4 (Continued)

F.05,1, 2

Degrees of Freedom for the Denominator (v2)

v2

1

2

1 161.4 199.5

2 18.51 19.00

3 10.13 9.55

4 7.71 6.94

5 6.61 5.79

6 5.99 5.14

7 5.59 4.74

8 5.32 4.46

9 5.12 4.26

10 4.96 4.10

11 4.48 3.98

12 4.75 3.89

13 4.67 3.81

14 4.60 3.74

15 4.54 3.68

16 4.49 3.63

17 4.45 3.59

18 4.41 3.55

19 4.38 3.52

20 4.35 3.49

21 4.32 3.47

22 4.30 3.44

23 4.28 3.42

24 4.26 3.40

25 4.24 3.39

26 4.23 3.37

27 4.21 3.35

28 4.20 3.34

29 4.18 3.33

30 4.17 3.32

40 4.08 3.23

60 4.00 3.15

120 3.92 3.07



3.84 3.00

3

4

5

215.7 19.16 9.28 6.59 5.41 4.76 4.35 4.07 3.86 3.71 3.59 3.49 3.41 3.34 3.29 3.24 3.20 3.16 3.13 3.10 3.07 3.05 3.03 3.01 2.99 2.98 2.96 2.95 2.93 2.92 2.84 2.76 2.68 2.60

224.6 19.25 9.12 6.39 5.19 4.53 4.12 3.84 3.63 3.48 3.36 3.26 3.18 3.11 3.06 3.01 2.96 2.93 2.90 2.87 2.84 2.82 2.80 2.78 2.76 2.74 2.73 2.71 2.70 2.69 2.61 2.53 2.45 2.37

230.2 19.30 9.01 6.26 5.05 4.39 3.97 3.69 3.48 3.33 3.20 3.11 3.03 2.96 2.90 2.85 2.81 2.77 2.74 2.71 2.68 2.66 2.64 2.62 2.60 2.59 2.57 2.56 2.55 2.53 2.45 2.37 2.29 2.21

Degrees of Freedom for the Numerator (v1)

6

7

8

9

10

12

15

20

234.0 19.33 8.94 6.16 4.95 4.28 3.87 3.58 3.37 3.22 3.09 3.00 2.92 2.85 2.79 2.74 2.70 2.66 2.63 2.60 2.57 2.55 2.53 2.51 2.49 2.47 2.46 2.45 2.43 2.42 2.34 2.25 2.17 2.10

236.8 19.35 8.89 6.09 4.88 4.21 3.79 3.50 3.29 3.14 3.01 2.91 2.83 2.76 2.71 2.66 2.61 2.58 2.54 2.51 2.49 2.46 2.44 2.42 2.40 2.39 2.37 2.36 2.35 2.33 2.25 2.17 2.09 2.01

238.9 19.37 8.85 6.04 4.82 4.15 3.73 3.44 3.23 3.07 2.95 2.85 2.77 2.70 2.64 2.59 2.55 2.51 2.48 2.45 2.42 2.40 2.37 2.36 2.34 2.32 2.31 2.29 2.28 2.27 2.18 2.10 2.02 1.94

240.5 19.38 8.81 6.00 4.77 4.10 3.68 3.39 3.18 3.02 2.90 2.80 2.71 2.65 2.59 2.54 2.49 2.46 2.42 2.39 2.37 2.34 2.32 2.30 2.28 2.27 2.25 2.24 2.22 2.21 2.12 2.04 1.96 1.88

241.9 19.40 8.79 5.96 4.74 4.06 3.64 3.35 3.14 2.98 2.85 2.75 2.67 2.60 2.54 2.49 2.45 2.41 2.38 2.35 2.32 2.30 2.27 2.25 2.24 2.22 2.20 2.19 2.18 2.16 2.08 1.99 1.91 1.83

243.9 19.41 8.74 5.91 4.68 4.00 3.57 3.28 3.07 2.91 2.79 2.69 2.60 2.53 2.48 2.42 2.38 2.34 2.31 2.28 2.25 2.23 2.20 2.18 2.16 2.15 2.13 2.12 2.10 2.09 2.00 1.92 1.83 1.75

245.9 19.43 8.70 5.86 4.62 3.94 3.51 3.22 3.01 2.85 2.72 2.62 2.53 2.46 2.40 2.35 2.31 2.27 2.23 2.20 2.18 2.15 2.13 2.11 2.09 2.07 2.06 2.04 2.03 2.01 1.92 1.84 1.75 1.67

248.0 19.45 8.66 5.80 4.56 3.87 3.44 3.15 2.94 2.77 2.65 2.54 2.46 2.39 2.33 2.28 2.23 2.19 2.16 2.12 2.10 2.07 2.05 2.03 2.01 1.99 1.97 1.96 1.94 1.93 1.84 1.75 1.66 1.57

24

30

40

60

120



249.1 19.45 8.64 5.77 4.53 3.84 3.41 3.12 2.90 2.74 2.61 2.51 2.42 2.35 2.29 2.24 2.19 2.15 2.11 2.08 2.05 2.03 2.01 1.98 1.96 1.95 1.93 1.91 1.90 1.89 1.79 1.70 1.61 1.52

250.1 19.46 8.62 5.75 4.50 3.81 3.38 3.08 2.86 2.70 2.57 2.47 2.38 2.31 2.25 2.19 2.15 2.11 2.07 2.04 2.01 1.98 1.96 1.94 1.92 1.90 1.88 1.87 1.85 1.84 1.74 1.65 1.55 1.46

251.1 19.47
8.59 5.72 4.46 3.77 3.34 3.04 2.83 2.66 2.53 2.43 2.34 2.27 2.20 2.15 2.10 2.06 2.03 1.99 1.96 1.94 1.91 1.89 1.87 1.85 1.84 1.82 1.81 1.79 1.69 1.59 1.55 1.39

252.2 19.48
8.57 5.69 4.43 3.74 3.30 3.01 2.79 2.62 2.49 2.38 2.30 2.22 2.16 2.11 2.06 2.02 1.98 1.95 1.92 1.89 1.86 1.84 1.82 1.80 1.79 1.77 1.75 1.74 1.64 1.53 1.43 1.32

253.3 19.49
8.55 5.66 4.40 3.70 3.27 2.97 2.75 2.58 2.45 2.34 2.25 2.18 2.11 2.06 2.01 1.97 1.93 1.90 1.87 1.84 1.81 1.79 1.77 1.75 1.73 1.71 1.70 1.68 1.58 1.47 1.35 1.22

254.3 19.50
8.53 5.63 4.36 3.67 3.23 2.93 2.71 2.54 2.40 2.30 2.21 2.13 2.07 2.01 1.96 1.92 1.88 1.84 1.81 1.78 1.76 1.73 1.71 1.69 1.67 1.65 1.64 1.62 1.51 1.39 1.25 1.00

Degrees of Freedom for the Denominator (v2)

F.025,1, 2

Degrees of Freedom for the Numerator (v1)

v2

1

2

3

4

5

6

7

8

9

10

12

15

20

24

30

40

60

120



1 647.8 799.5 864.2 899.6 921.8 937.1 948.2 956.7 963.3 968.6 976.7 984.9 993.1 997.2 1001 1006 1010 1014 1018

2 38.51 39.00 39.17 39.25 39.30 39.33 39.36 39.37 39.39 39.40 39.41 39.43 39.45 39.46 39.46 39.47 29.48 39.49 39.50

3 17.44 16.04 15.44 15.10 14.88 14.73 14.62 14.54 14.47 14.42 14.34 14.25 14.17 14.12 14.08 14.04 13.99 13.95 13.90

4 12.22 10.65 9.98 9.60 9.36 9.20 9.07 8.98 8.90 8.84 8.75 8.66 8.56 8.51 8.46 8.41 8.36 8.31 8.26

5 10.01 8.43 7.76 7.39 7.15 6.98 6.85 6.76 6.68 6.62 6.52 6.43 6.33 6.28 6.23 6.18 6.12 6.07 6.02

6

8.81 7.26 6.60 6.23 5.99 5.82 5.70 5.60 5.52 5.46 5.37 5.27 5.17 5.12 5.07 5.01 4.96 4.90 4.85

7

8.07 6.54 5.89 5.52 5.29 5.12 4.99 4.90 4.82 4.76 4.67 4.57 4.47 4.42 4.36 4.31 4.25 4.20 4.14

8

7.57 6.06 5.42 5.05 4.82 4.65 4.53 4.43 4.36 4.30 4.20 4.10 4.00 3.95 3.89 3.84 3.78 3.73 3.67

9

7.21 5.71 5.08 4.72 4.48 4.32 4.20 4.10 4.03 3.96 3.87 3.77 3.67 3.61 3.56 3.51 3.45 3.39 3.33

10

6.94 5.46 4.83 4.47 4.24 4.07 3.95 3.85 3.78 3.72 3.62 3.52 3.42 3.37 3.31 3.26 3.20 3.14 3.08

11

6.72 5.26 4.63 4.28 4.04 3.88 3.76 3.66 3.59 3.53 3.43 3.33 3.23 3.17 3.12 3.06 3.00 2.94 2.88

12

6.55 5.10 4.47 4.12 3.89 3.73 3.61 3.51 3.44 3.37 3.28 3.18 3.07 3.02 2.96 2.91 2.85 2.79 2.72

13

6.41 4.97 4.35 4.00 3.77 3.60 3.48 3.39 3.31 3.25 3.15 3.05 2.95 2.89 2.84 2.78 2.72 2.66 2.60

14

6.30 4.86 4.24 3.89 3.66 3.50 3.38 3.29 3.21 3.15 3.05 2.95 2.84 2.79 2.73 2.67 2.61 2.55 2.49

15

6.20 4.77 4.15 3.80 3.58 3.41 3.29 3.20 3.12 3.06 2.96 2.86 2.76 2.70 2.64 2.59 2.52 2.46 2.40

16

6.12 4.69 4.08 3.73 3.50 3.34 3.22 3.12 3.05 2.99 2.89 2.79 2.68 2.63 2.57 2.51 2.45 2.38 2.32

17

6.04 4.62 4.01 3.66 3.44 3.28 3.16 3.06 2.98 2.92 2.82 2.72 2.62 2.56 2.50 2.44 2.38 2.32 2.25

18

5.98 4.56 3.95 3.61 3.38 3.22 3.10 3.01 2.93 2.87 2.77 2.67 2.56 2.50 2.44 2.38 2.32 2.26 2.19

19

5.92 4.51 3.90 3.56 3.33 3.17 3.05 2.96 2.88 2.82 2.72 2.62 2.51 2.45 2.39 2.33 2.27 2.20 2.13

20

5.87 4.46 3.86 3.51 3.29 3.13 3.01 2.91 2.84 2.77 2.68 2.57 2.46 2.41 2.35 2.29 2.22 2.16 2.09

21

5.83 4.42 3.82 3.48 3.25 3.09 2.97 2.87 2.80 2.73 2.64 2.53 2.42 2.37 2.31 2.25 2.18 2.11 2.04

22

5.79 4.38 3.78 3.44 3.22 3.05 2.93 2.84 2.76 2.70 2.60 2.50 2.39 2.33 2.27 2.21 2.14 2.08 2.00

23

5.75 4.35 3.75 3.41 3.18 3.02 2.90 2.81 2.73 2.67 2.57 2.47 2.36 2.30 2.24 2.18 2.11 2.04 1.97

24

5.72 4.32 3.72 3.38 3.15 2.99 2.87 2.78 2.70 2.64 2.54 2.44 2.33 2.27 2.21 2.15 2.08 2.01 1.94

25

5.69 4.29 3.69 3.35 3.13 2.97 2.85 2.75 2.68 2.61 2.51 2.41 2.30 2.24 2.18 2.12 2.05 1.98 1.91

26

5.66 4.27 3.67 3.33 3.10 2.94 2.82 2.73 2.65 2.59 2.49 2.39 2.28 2.22 2.16 2.09 2.03 1.95 1.88

27

5.63 4.24 3.65 3.31 3.08 2.92 2.80 2.71 2.63 2.57 2.47 2.36 2.25 2.19 2.13 2.07 2.00 1.93 1.85

28

5.61 4.22 3.63 3.29 3.06 2.90 2.78 2.69 2.61 2.55 2.45 2.34 2.23 2.17 2.11 2.05 1.98 1.91 1.83

29

5.59 4.20 3.61 3.27 3.04 2.88 2.76 2.67 2.59 2.53 2.43 2.32 2.21 2.15 2.09 2.03 1.96 1.89 1.81

30

5.57 4.18 3.59 3.25 3.03 2.87 2.75 2.65 2.57 2.51 2.41 2.31 2.20 2.14 2.07 2.01 1.94 1.87 1.79

40

5.42 4.05 3.46 3.13 2.90 2.74 2.62 2.53 2.45 2.39 2.29 2.18 2.07 2.01 1.94 1.88 1.80 1.72 1.64

60

5.29 3.93 3.34 3.01 2.79 2.63 2.51 2.41 2.33 2.27 2.17 2.06 1.94 1.88 1.82 1.74 1.67 1.58 1.48

120

5.15 3.80 3.23 2.89 2.67 2.52 2.39 2.30 2.22 2.16 2.05 1.94 1.82 1.76 1.69 1.61 1.53 1.43 1.31



5.02 3.69 3.12 2.79 2.57 2.41 2.29 2.19 2.11 2.05 1.94 1.83 1.71 1.64 1.57 1.48 1.39 1.27 1.00

(Continued)

TABLE A.4 (Continued)

F.01,1, 2

Degrees of Freedom for the Denominator (v2)

Degrees of Freedom for the Numerator (v1)

v2

1

2

3

4

5

6

7

8

9

10

12

15

20

24

30

40

60

120



1 4052 4999.5 5403 5625 5764 5859 5928 5982 6022 6056 6106 6157 6209 6235 6261 6287 6313 6339 6366

2

98.50 99.00 99.17 99.25 99.30 99.33 99.36 99.37 99.39 99.40 99.42 99.43 99.45 99.46 99.47 99.47 99.48 99.49 99.50

3

34.12 30.82 29.46 28.71 28.24 27.91 27.67 27.49 27.35 27.23 27.05 26.87 26.69 26.00 26.50 26.41 26.32 26.22 26.13

4

21.20 18.00 16.69 15.98 15.52 15.21 14.98 14.80 14.66 14.55 14.37 14.20 14.02 13.93 13.84 13.75 13.65 13.56 13.46

5

16.26 13.27 12.06 11.39 10.97 10.67 10.46 10.29 10.16 10.05

9.89

9.72

9.55

9.47

9.38

9.29

9.20

9.11

9.02

6

13.75 10.92 9.78 9.15

8.75

8.47

8.26

8.10

7.98

7.87

7.72

7.56

7.40

7.31

7.23

7.14

7.06

6.97

6.88

7

12.25

9.55 8.45 7.85

7.46

7.19

6.99

6.84

6.72

6.62

6.47

6.31

6.16

6.07

5.99

5.91

5.82

5.74

5.65

8

11.26

8.65 7.59 7.01

6.63

6.37

6.18

6.03

5.91

5.81

5.67

5.52

5.36

5.28

5.20

5.12

5.03

4.95

4.86

9

10.56

8.02 6.99 6.42

6.06

5.80

5.61

5.47

5.35

5.26

5.11

4.96

4.81

4.73

4.65

4.57

4.48

4.40

4.31

10

10.04

7.56 6.55 5.99

5.64

5.39

5.20

5.06

4.94

4.85

4.71

4.56

4.41

4.33

5.25

4.17

4.08

4.00

3.91

11

9.65

7.21 6.22 5.67

5.32

5.07

4.89

4.74

4.63

4.54

4.40

4.25

4.10

4.02

3.94

3.86

3.78

3.69

3.60

12

9.33

6.93 5.95 5.41

5.06

4.82

4.64

4.50

4.39

4.30

4.16

4.01

3.86

3.78

3.70

3.62

3.54

3.45

3.36

13

9.07

6.70 5.74 5.21

4.86

4.62

4.44

4.30

4.19

4.10

3.96

3.82

3.66

3.59

3.51

3.43

3.34

3.25

3.17

14

8.86

6.51 5.56 5.04

4.69

4.46

4.28

4.14

4.03

3.94

3.80

3.66

3.51

3.43

3.35

3.27

3.18

3.09

3.00

15

8.68

6.36 5.42 4.89

4.36

4.32

4.14

4.00

3.89

3.80

3.67

3.52

3.37

3.29

3.21

3.13

3.05

2.96

2.87

16

8.53

6.23 5.29 4.77

4.44

4.20

4.03

3.89

3.78

3.69

3.55

3.41

3.26

3.18

3.10

3.02

2.93

2.84

2.75

17

8.40

6.11 5.18 4.67

4.34

4.10

3.93

3.79

3.68

3.59

3.46

3.31

3.16

3.08

3.00

2.92

2.83

2.75

2.65

18

8.29

6.01 5.09 4.58

4.25

4.01

3.84

3.71

3.60

3.51

3.37

3.23

3.08

3.00

2.92

2.84

2.75

2.66

2.57

19

8.18

5.93 5.01 4.50

4.17

3.94

3.77

3.63

3.52

3.43

3.30

3.15

3.00

2.92

2.84

2.76

2.67

2.58

2.59

20

8.10

5.85 4.94 4.43

4.10

3.87

3.70

3.56

3.46

3.37

3.23

3.09

2.94

2.86

2.78

2.69

2.61

2.52

2.42

21

8.02

5.78 4.87 4.37

4.04

3.81

3.64

3.51

3.40

3.31

3.17

3.03

2.88

2.80

2.72

2.64

2.55

2.46

2.36

22

7.95

5.72 4.82 4.31

3.99

3.76

3.59

3.45

3.35

3.26

3.12

2.98

2.83

2.75

2.67

2.58

2.50

2.40

2.31

23

7.88

5.66 4.76 4.26

3.94

3.71

3.54

3.41

3.30

3.21

3.07

2.93

2.78

2.70

2.62

2.54

2.45

2.35

2.26

24

7.82

5.61 4.72 4.22

3.90

3.67

3.50

3.36

3.26

3.17

3.03

2.89

2.74

2.66

2.58

2.49

2.40

2.31

2.21

25

7.77

5.57 4.68 4.18

3.85

3.63

3.46

3.32

3.22

3.13

2.99

2.85

2.70

2.62

2.54

2.45

2.36

2.27

2.17

26

7.72

5.53 4.64 4.14

3.82

3.59

3.42

3.29

3.18

3.09

2.96

2.81

2.66

2.58

2.50

2.42

2.33

2.23

2.13

27

7.68

5.49 4.60 4.11

3.78

3.56

3.39

3.26

3.15

3.06

2.93

2.78

2.63

2.55

2.47

2.38

2.29

2.20

2.10

28

7.64

5.45 4.57 4.07

3.75

3.53

3.36

3.23

3.12

3.03

2.90

2.75

2.60

2.52

2.44

2.35

2.26

2.17

2.06

29

7.60

5.42 4.54 4.04

3.73

3.50

3.33

3.20

3.09

3.00

2.87

2.73

2.57

2.49

2.41

2.33

2.23

2.14

2.03

30

7.56

5.39 4.51 4.02

3.70

3.47

3.30

3.17

3.07

2.98

2.84

2.70

2.55

2.47

2.39

2.30

2.21

2.11

2.01

40

7.31

5.18 4.31 3.83

3.51

3.29

3.12

2.99

2.89

2.80

2.66

2.52

2.37

2.29

2.20

2.11

2.02

1.92

1.80

60

7.08

4.98 4.13 3.65

3.34

3.12

2.95

2.82

2.72

2.63

2.50

2.35

2.20

2.12

2.03

1.94

1.84

1.73

1.60

120

6.85

4.79 3.95 3.48

3.17

2.96

2.79

2.66

2.56

2.47

2.34

2.19

2.03

1.95

1.86

1.76

1.66

1.53

1.38



6.63

4.61 3.78 3.32

3.02

2.80

2.64

2.51

2.41

2.32

2.18

2.04

1.88

1.79

1.70

1.59

1.47

1.32

1.00

Source: Adapted with permission from Biometrika Tables for Statisticians, Vol. 1, 3rd ed., 1966, by E. S. Pearson and H. O. Hartley, Cambridge University Press. Cambridge.

TABLE A.5 Orthogonal Polynomials

n=3

n=4

n=5

n=6

n=7

Xj

P1 P2 P1 P2 P3 P1 P2 P3 P4 P1 P2

P3

P4

P5

P1 P2 P3

P4

P5

P6

1

-1 1 -3 1 -1 -2 2 -1 1 -5 5 -5 1 -1 -3 5 -1

3 -1

1

2

0 -2 -1 -1 3 -1 -1 2 -4 -3 -1

7 -3

5 -2 0 1 -7 4 -6

3

1 1 1 -1 -3 0 -2 0 6 -1 -4

4 2 -10 -1 -3 1

1 -5 15

4

3 1 1 1 -1 -2 -4 1 -4 -4 2 10 0 -4 0

6 0 -20

5

2 2 1 1 3 -1 -7 -3 -5 1 -3 -1

15

5

6

55

51

1 2 0 -1 -7 -4 -6

7

351

31

1

n

{Pi (X j )}2

2

6 20

4 20 10 14 10 70 70 84 180 28 252 28 84

6 154 84 924

j=1



1

3

2

1

10 3

1

1

5 6

35 12

2

3 2

5 3

7

21

12

10

1

1

1 6

7 12

7

77

20

60

n=8

n=9

n = 10

Xj

P1

P2

P3

P4

P5

P6 P1

P2

P3

P4

P5

P6

P1 P2

P3

P4

P5

P6

1

-7 7 -7 7

-7 1 -4

28 -14

14 -4

4 -9 6 -42

18 -6

3

2

-5 1 5 -13

23 -5 -3

7

7 -21 11 -17 -7 2

14 -22 14 -11

3

-3 -3 7 -3 -17 9 -2

-8 13 -11 -4

22 -5 -5

35 -17 -1 10

4

-1 -5

3 9 -15 -5 -1 -17

9

9 -9

1 -3 -3

31

3 -11 6

5

1 -5 -3 9

15 -5 0 -20

0

18 0 -20 -1 -4

12

18 -6 -8

6

3 -3 -7 -3

17 9 1 -17 -9

99

1 1 -4 -12

18

6 -8

7

5 1 -5 -13 -23 -5 2

-8 -13 -11

4

22 3 -3 -31

3 11 6

8

77 77

7 13

7 -7 -21 -11 -17 5 -1 -35 -17 1 10

9

4

28 14

14 4

4 7 2 -14 -22 -14 -11

10

96

42

18

6

3

n
{Pi (X j )}2 168 168 264 616 2,184 264 60 2,772 990 2,002 468 1,980 330 132 8,580 2,860 780 660
j=1



2

1

2 3

7

7

12

10

11 60

1

3

5 6

7 12

3

11

20

60

2

1 2

5 3

5

1

11

12

10

240

551

Source: Adapted with permission from Biometrika Tables for Statisticians, Vol. 1, 3rd ed., 1966, by E. S. Pearson and H. O. Hartley, Cambridge University Press. Cambridge.

552 APPENDIX A

TABLE A.6 Critical Values of the Durbin--Watson Statistic

Sample Size 15
20 25 30 40 50 60 80 100

Probability in Lower Tail (Significance Level = )
.01 .025 .05 .01 .025 .05 .01 .025 .05 .01 .025 .05 .01 .025 .05 .01 .025 .05 .01 .025 .05 .01 .025 .05 .01 .025 .05

k = Number of Regressors (Excluding the Intercept)

1

2

3

4

5

dL

dU

dL

dU

dL

dU

dL

dU

dL

dU

.81 1.07 .70 1.25 .59 1.46 .49 1.70 .39 1.96 .95 1.23 .83 1.40 .71 1.61 .59 1.84 .48 2.09 1.08 1.36 .95 1.54 .82 1.75 .69 1.97 .56 2.21 .95 1.15 .86 1.27 .77 1.41 .63 1.57 .60 1.74 1.08 1.28 .99 1.41 .89 1.55 .79 1.70 .70 1.87 1.20 1.41 1.10 1.54 1.00 1.68 .90 1.83 .79 1.99 1.05 1.21 .98 1.30 .90 1.41 .83 1.52 .75 1.65 1.13 1.34 1.10 1.43 1.02 1.54 .94 1.65 .86 1.77 1.20 1.45 1.21 1.55 1.12 1.66 1.04 1.77 .95 1.89 1.13 1.26 1.07 1.34 1.01 1.42 .94 1.51 .88 1.61 1.25 1.38 1.18 1.46 1.12 1.54 1.05 1.63 .98 1.73 1.35 1.49 1.28 1.57 1.21 1.65 1.14 1.74 1.07 1.83 1.25 1.34 1.20 1.40 1.15 1.46 1.10 1.52 1.05 1.58 1.35 1.45 1.30 1.51 1.25 1.57 1.20 1.63 1.15 1.69 1.44 1.54 1.39 1.60 1.34 1.66 1.29 1.72 1.23 1.79 1.32 1.40 1.28 1.45 1.24 1.49 1.20 1.54 1.16 1.59 1.42 1.50 1.38 1.54 1.34 1.59 1.30 1.64 1.26 1.69 1.50 1.59 1.46 1.63 1.42 1.67 1.38 1.72 1.34 1.77 1.38 1.45 1.35 1.48 1.32 1.52 1.28 1.56 1.25 1.60 1.47 1.54 1.44 1.57 1.40 1.61 1.37 1.65 1.33 1.69 1.55 1.62 1.51 1.65 1.48 1.69 1.44 1.73 1.41 1.77 1.47 1.52 1.44 1.54 1.42 1.57 1.39 1.60 1.36 1.62 1.54 1.59 1.52 1.62 1.49 1.65 1.47 1.67 1.44 1.70 1.61 1.66 1.59 1.69 1.56 1.72 1.53 1.74 1.51 1.77 1.52 1.56 1.50 1.58 1.48 1.60 1.45 1.63 1.44 1.65 1.59 1.63 1.57 1.65 1.55 1.67 1.53 1.70 1.51 1.72 1.65 1.69 1.63 1.72 1.61 1.74 1.59 1.76 1.57 1.78

Source: Adapted from "Testing for Serial Correlation in Least Squares Regression II," by J. Durbin and G. S. Watson, Biometrika, Vol. 38, 1951, with permission of the publisher.

APPENDIX B

DATA SETS FOR EXERCISES

Table B.1 Table B.2 Table B.3 Table B.4 Table B.5 Table B.6 Table B.7 Table B.8 Table B.9 Table B.10 Table B.11 Table B.12 Table B.13 Table B.14 Table B.15 Table B.16 Table B.17 Table B.18 Table B.19 Table B.20 Table B.21

National Football League 1976 Team Performance Solar Thermal Energy Test Data Gasoline Mileage Performance for 32 Automobiles Property Valuation Data Belle Ayr Liquefaction Runs Tube-Flow Reactor Data Oil Extraction from Peanuts Data Clathrate Formation Data Pressure Drop Data Kinematic Viscosity Data Wine Quality Data Heat Treating Data Jet Turbine Engine Thrust Data Electronic Inverter Data Air Pollution and Mortality Data Life Expectancy Data Patient Satisfaction Data Fuel Consumption Data Wine Quality of Young Red Wines Methanol Oxidation in Supercritical Water Hald Cement Data

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
553

554 APPENDIX B

TABLE B.1 National Football League 1976 Team Performance

Team

y

x1

Washington 10 2113

Minnesota 11 2003

New

11 2957

England

Oakland

13 2285

Pittsburgh 10 2971

Baltimore 11 2309

Los Angeles 10 2528

Dallas

11 2147

Atlanta

4 1689

Buffalo

2 2566

Chicago

7 2363

Cincinnati 10 2109

Cleveland

9 2295

Denver

9 1932

Detroit

6 2213

Green Bay

5 1722

Houston

5 1498

Kansas City 5 1873

Miami

6 2118

New

4 1775

Orleans

New York

3 1904

Giants

New York

3 1929

Jets

Philadelphia 4 2080

St. Louis

10 2301

San Diego

6 2040

San

8 2447

Francisco

Seattle

2 1416

Tampa Bay 0 1503

x2

x3

x4

x5

1985 38.9 64.7 +4 2855 38.8 61.3 +3 1737 40.1 60.0 +14

x6

x7

x8

x9

868 59.7 2205 1917 615 55.0 2096 1575 914 65.6 1847 2175

2905 41.6 45.3 -4 957 61.4 1903 2476 1666 39.2 53.8 +15 836 66.1 1457 1866 2927 39.7 74.1 +8 786 61.0 1848 2339 2341 38.1 65.4 +12 754 66.1 1564 2092 2737 37.0 78.3 -1 761 58.0 1821 1909 1414 42.1 47.6 -3 714 57.0 2577 2001 1838 42.3 54.2 -1 797 58.9 2476 2254 1480 37.3 48.0 +19 984 67.5 1984 2217 2191 39.5 51.9 +6 700 57.2 1917 1758 2229 37.4 53.6 -5 1037 58.8 1761 2032 2204 35.1 71.4 +3 986 58.6 1709 2025 2140 38.8 58.3 +6 819 59.2 1901 1686 1730 36.6 52.6 -19 791 54.4 2288 1835 2072 35.3 59.3 -5 776 49.6 2072 1914 2929 41.1 55.3 +10 789 54.3 2861 2496 2268 38.2 69.6 +6 582 58.7 2411 2670 1983 39.3 78.3 +7 901 51.7 2289 2202

1792 39.7 38.1 -9 734 61.9 2203 1988

1606 39.7 68.8 -21 627 52.7 2592 2324

1492 35.5 68.8 -8 722 57.8 2053 2550

2835 35.3 74.1 +2 683 59.7 1979 2110

2416 38.7 50.0

0 576 54.9 2048 2628

1638 39.9 57.1 -8 848 65.3 1786 1776

2649 37.4 56.3 -22 1503 39.3 47.0 -9

684 43.8 2876 2524 875 53.5 2560 2241

y: Games won (per 14-game season) x1: Rushing yards (season) x2: Passing yards (season) x3: Punting average (yards/punt) x4: Field goal percentage (FGs made/FGs attempted 2season) x5: Turnover differential (turnovers acquired­turnovers lost) x6: Penalty yards (season) x7: Percent rushing (rushing plays/total plays) x8: Opponents' rushing yards (season) x9: Opponents' passing yards (season)

TABLE B.2 Solar Thermal Energy Test Data

y
271.8 264.0 238.8 230.7 251.6 257.9 263.9 266.5 229.1 239.3 258.0 257.6 267.3 267.0 259.6 240.4 227.2 196.0 278.7 272.3 267.4 254.5 224.7 181.5 227.5 253.6 263.0 265.8 263.8

x1
783.35 748.45 684.45 827.80 860.45 875.15 909.45 905.55 756.00 769.35 793.50 801.65 819.65 808.55 774.95 711.85 694.85 638.10 774.55 757.90 753.35 704.70 666.80 568.55 653.10 704.05 709.60 726.90 697.15

x2
33.53 36.50 34.66 33.13 35.75 34.46 34.60 35.38 35.85 35.68 35.35 35.04 34.07 32.20 34.32 31.08 35.73 34.11 34.79 35.77 36.44 37.82 35.07 35.26 35.56 35.73 36.46 36.26 37.20

y: Total heat flux (kwatts) xl: Insolation (watts/m2) x2: Position of focal point in east direction (inches) x3: Position of focal point in south direction (inches) x4: Position of focal point in north direction (inches) x5: Time of day

DATA SETS FOR EXERCISES

555

x3
40.55 36.19 37.31 32.52 33.71 34.14 34.85 35.89 33.53 33.79 34.72 35.22 36.50 37.60 37.89 37.71 37.00 36.76 34.62 35.40 35.96 36.26 36.34 35.90 31.84 33.16 33.83 34.89 36.27

x4
16.66 16.46 17.66 17.50 16.40 16.28 16.06 15.93 16.60 16.41 16.17 15.92 16.04 16.19 16.62 17.37 18.12 18.53 15.54 15.70 16.45 17.62 18.12 19.05 16.51 16.02 15.89 15.83 16.71

x5
13.20 14.11 15.68 10.53 11.00 11.31 11.96 12.58 10.66 10.85 11.41 11.91 12.85 13.58 14.21 15.56 15.83 16.41 13.10 13.63 14.51 15.38 16.10 16.73 10.58 11.28 11.91 12.65 14.06

556 APPENDIX B

TABLE B.3 Gasoline Mileage Performance for 32 Antomobiles

Automobile y

x1

x2 x3

x4

x5

x6 x7 x8

x9

x10 x11

Apollo

18.90 350 165 260 8.0 :1 2.56 :1 4 3 200.3 69.9 3910 A

Omega

17.00 350 170 275 8.5 :1 2.56 :1 4 3 199.6 72.9 2860 A

Nova

20.00 250 105 185 8.25 :1 2.73 :1 1 3 196.7 72.2 3510 A

Monarch 18.25 351 143 255 8.0 :1 3.00 :1 2 3 199.9 74.0 3890 A

Duster

20.07 225 95 170 8.4 :1 2.76 :1 1 3 194.1 71.8 3365 M

Jenson

11.2 440 215 330 8.2 :1 2.88 :1 4 3 184.5 69 4215 A

Conv.

Skyhawk 22.12 231 110 175 8.0 :1 2.56 :1 2 3 179.3 65.4 3020 A

Monza

21.47 262 110 200 8.5 :1 2.56 :1 2 3 179.3 65.4 3180 A

Scirocco 34.70 89.7 70 81 8.2 :1 3.90 :1 2 4 155.7 64 1905 M

Corolla

30.40 96.9 75 83 9.0 :1 4.30 :1 2 5 165.2 65 2320 M

SR-5

Camaro

16.50 350 155 250 8.5 :1 3.08 :1 4 3 195.4 74.4 3885 A

Datsun

36.50 85.3 80 83 8.5 :1 3.89 :1 2 4 160.6 62.2 2009 M

B210

Capri II

21.50 171 109 146 8.2 :1 3.22 :1 2 4 170.4 66.9 2655 M

Pacer

19.70 258 110 195 8.0 :1 3.08 :1 1 3 171.5 77 3375 A

Babcat

20.30 140 83 109 8.4 :1 3.40 :1 2 4 168.8 69.4 2700 M

Granada 17.80 302 129 220 8.0 :1 3.0 :1 2 3 199.9 74 3890 A

Eldorado 14.39 500 190 360 8.5 :1 2.73 :1 4 3 224.1 79.8 5290 A

Imperial 14.89 440 215 330 8.2 :1 2.71 :1 4 3 231.0 79.7 5185 A

Nova LN 17.80 350 155 250 8.5 :1 3.08 :1 4 3 196.7 72.2 3910 A

Valiant

16.41 318 145 255 8.5 :1 2.45 :1 2 3 197.6 71 3660 A

Starfire

23.54 231 110 175 8.0 :1 2.56 :1 2 3 179.3 65.4 3050 A

Cordoba 21.47 360 180 290 8.4 :1 2.45 :1 2 3 214.2 76.3 4250 A

Trans AM 16.59 400 185 NA 7.6 :1 3.08 :1 4 3 196 73 3850 A

Corolla E-5 31.90 96.9 75 83 9.0 :1 4.30 :1 2 5 165.2 61.8 2275 M

Astre

29.40 140 86 NA 8.0 :1 2.92 :1 2 4 176.4 65.4 2150 M

Mark IV 13.27 460 223 366 8.0 :1 3.00 :1 4 3 228 79.8 5430 A

Celica GT 23.90 133.6 96 120 8.4 :1 3.91 :1 2 5 171.5 63.4 2535 M

Charger SE 19.73 318 140 255 8.5 :1 2.71 :1 2 3 215.3 76.3 4370 A

Cougar

13.90 351 148 243 8.0 :1 3.25 :1 2 3 215.5 78.5 4540 A

Elite

13.27 351 148 243 8.0 :1 3.26 :1 2 3 216.1 78.5 4715 A

Matador 13.77 360 195 295 8.25 :1 3.15 :1 4 3 209.3 77.4 4215 A

Corvette 16.50 350 165 255 8.5 :1 2.73 :1 4 3 185.2 69 3660 A

y: Miles/gallon x1: Displacement (cubic in.) x2: Horsepower (ft-lb) x3: Torqne (ft-lb) x4: Compression ratio x5: Rear axle ratio Source: Motor Trend, 1975.

x6: Carburetor (barrels) x7: No. of transmission speeds x8: Overall length (in.) x9: Width (in.) x10: Weight (lb) x11: Type of transmission (A automatic; M manual)

DATA SETS FOR EXERCISES

557

TABLE B.4 Property Valuation Data

y

x1

x2

x3

x4

x5

x6

x7

x8

x9

25.9 4.9176 1.0 3.4720 0.9980

1.0

7

29.5 5.0208 1.0 3.5310 1.5000

2.0

7

27.9 4.5429 1.0 2.2750 1.1750

1.0

6

25.9 4.5573 1.0 4.0500 1.2320

1.0

6

29.9 5.0597 1.0 4.4550 1.1210

1.0

6

29.9 3.8910 1.0 4.4550 0.9880

1.0

6

30.9 5.8980 1.0 5.8500 1.2400

1.0

7

28.9 5.6039 1.0 9.5200 1.5010

0.0

6

35.9 5.8282 1.0 6.4350 1.2250

2.0

6

31.5 5.3003 1.0 4.9883 1.5520

1.0

6

31.0 6.2712 1.0 5.5200 0.9750

1.0

5

30.9 5.9592 1.0 6.6660 1.1210

2.0

6

30.0 5.0500 1.0 5.0000 1.0200

0.0

5

36.9 8.2464 1.5 5.1500 1.6640

2.0

8

41.9 6.6969 1.5 6.9020 1.4880

1.5

7

40.5 7.7841 1.5 7.1020 1.3760

1.0

6

43.9 9.0384 1.0 7.8000 1.5000

15

7

37.5 5.9894 1.0 5.5200 1.2560

2.0

6

37.9 7.5422 1.5 5.0000 1.6900

1.0

6

44.5 8.7951 1.5 9.8900 1.8200

2.0

8

37.9 6.0831 1.5 6.7265 1.6520

1.0

6

38.9 8.3607 1.5 9.1500 1.7770

2.0

8

36.9 8.1400 1.0 8.0000 1.5040

2.0

7

45.8 9.1416 1.5 7.3262 1.8310

1.5

8

4

42

0

4

62

0

3

40

0

3

54

0

3

42

0

3

56

0

3

51

1

3

32

0

3

32

0

3

30

0

2

30

0

3

32

0

2

46

1

4

50

0

3

22

1

3

17

0

3

23

0

3

40

1

3

22

0

4

50

1

3

44

0

4

48

1

3

3

0

4

31

0

y: Sale price of the house/1000 x1: Taxes (local, school, county)/1000 x2: Number of baths x3: Lot size (sq ft × 1000) x4: Living space (sq ft × 1000) x5: Number of garage stalls x6: Number of rooms x7: Number of bedrooms x8: Age of the home (years) x9: Number of fireplaces
Source: "Prediction, Linear Regression and Minimum Sum of Relative Errors," by S. C. Narula and J. F. Wellington, Technometrics, 19, 1977. Also see "Letter to the Editor," Technometrics, 22, 1980.

558 APPENDIX B

TABLE B.5 Belle Ayr Liquefaction Runs

Run No.

y

x1

x2

x3

x4

x5

x6

x7

1

36.98 5.1

400 51.37 4.24

1484.83 2227.25 2.06

2

13.74 26.4 400 72.33 30.87

289.94

434.90 1.33

3

10.08 23.8 400 71.44 33.01

320.79

481.19 0.97

4

8.53 46.4 400 79.15 44.61

164.76

247.14 0.62

5

36.42

7.0 450 80.47 33.84 1097.26 1645.89 0.22

6

26.59 12.6 450 89.90 41.26

605.06

907.59 0.76

7

19.07 18.9 450 91.48 41.88

405.37

608.05 1.71

8

5.96 30.2 450 98.6

70.79

253.70

380.55 3.93

9

15.52 53.8 450 98.05 66.82

142.27

213.40 1.97

10

56.61

5.6 400 55.69

8.92 1362.24 2043.36 5.08

11

26.72 15.1 400 66.29 17.98

507.65

761.48 0.60

12

20.80 20.3 400 58.94 17.79

377.60

566.40 0.90

13

6.99 48.4 400 74.74 33.94

158.05

237.08 0.63

14

45.93

5.8 425 63.71 11.95

130.66 1961.49 2.04

15

43.09 11.2 425 67.14 14.73

682.59 1023.89 1.57

16

15.79 27.9 425 77.65 34.49

274.20

411.30 2.38

17

21.60

5.1 450 67.22 14.48 1496.51 2244.77 0.32

18

35.19 11.7 450 81.48 29.69

652.43

978.64 0.44

19

26.14 16.7 450 83.88 26.33

458.42

687.62 8.82

20

8.60 24.8 450 89.38 37.98

312.25

468.28 0.02

21

11.63 24.9 450 79.77 25.66

307.08

460.62 1.72

22

9.59 39.5 450 87.93 22.36

193.61

290.42 1.88

23

4.42 29.0 450 79.50 21.52

155.96

233.95 1.43

24

38.89

5.5 460 72.73 17.86 1392.08 2088.12 1.35

25

11.19 11.5 450 77.88 25.20

663.09

994.63 1.61

26

75.62

5.2 470 75.50

8.66 1464.11 2196.17 4.78

27

36.03 10.6 470 83.15 22.39

720.07 1080.11 5.88

y: CO2 x1: Space time, min. x2: Temperature, °C x3: Percent solvation x4: Oil yield (g/100 g MAF) x5: Coal total x6: Solvent total x7: Hydrogen consumption
Source: "Belle Ayr Liquefaction Runs with Solvent," Industrial Chemical Process Design Development. 17, No. 3, 1978.

DATA SETS FOR EXERCISES

559

TABLE B.6 Tube-Flow Reactor Data

Run No.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

y
0.000450 0.000450 0.000473 0.000507 0.000457 0.000452 0.000453 0.000426 0.001215 0.001256 0.001145 0.001085 0.001066 0.001111 0.001364 0.001254 0.001396 0.001575 0.001615 0.001733 0.002753 0.003186 0.003227 0.003469 0.001911 0.002588 0.002635 0.002725

x1
0.0105 0.0110 0.0106 0.0116 0.0121 0.0123 0.0122 0.0122 0.0123 0.0122 0.0094 0.0100 0.0101 0.0099 0.0110 0.0117 0.0110 0.0104 0.0067 0.0066 0.0044 0.0073 0.0078 0.0067 0.0091 0.0079 0.0068 0.0065

x2
90.9 84.6 88.9 488.7 454.4 439.2 447.1 451.6 487.8 467.6 95.4 87.1 82.7 87.0 516.4 488.0 534.5 542.3 98.8 84.8 69.6 436.9 406.3 447.9 58.5 394.3 461.0 469.2

x3
0.0164 0.0165 0.0164 0.0187 0.0187 0.0187 0.0186 0.0187 0.0192 0.0192 0.0163 0.0162 0.0162 0.0163 0.0190 0.0189 0.0189 0.0189 0.0163 0.0162 0.0163 0.0189 0.0192 0.0192 0.0164 0.0177 0.0174 0.0173

x4
0.0177 0.0172 0.0157 0.0082 0.0070 0.0065 0.0071 0.0062 0.0153 0.0129 0.0354 0.0342 0.0323 0.0337 0.0161 0.0149 0.0163 0.0164 0.0379 0.0360 0.0327 0.0263 0.0200 0.0197 0.0331 0.0674 0.0770 0.0780

y: NbOCl3 concentration (g-mol/l) x1: COCl2 concentration (g-mol/l) x2: Space time (sec) x3: Molar density (g-mol/l) x4: Mole fraction CO2
Source: "Kinetics of Chlorination of Niobium Oxychloride by Phosgene in a Tube-Flow Reactor," Industrial and Engineering Chemistry, Process Design Development, 11, No. 2, 1972.

560 APPENDIX B

TABLE B.7 Oil Extraction from Peanuts Data

Pressure (bars)

Temp. (°C)

Moisture (% by weight)

Flow Rate (L/min)

Particle Size (mm)

Yield

415

25

5

550

25

5

415

95

5

550

95

5

415

25

15

550

25

15

415

95

15

550

95

15

415

25

5

550

25

5

415

95

5

550

95

5

415

25

15

550

25

15

415

95

15

550

95

15

40

1.28

63

40

4.05

21

40

4.05

36

40

1.28

99

40

4.05

24

40

1.28

66

40

1.28

71

40

4.05

54

60

4.05

23

60

1.28

74

60

1.28

80

60

4.05

33

60

1.28

63

60

4.05

21

60

4.05

44

60

1.28

96

Source: "An Application of Fractional Experimental Designs," by M. B. Kilgo, Quality Engineering, 1, pp. 19­23.

TABLE B.8 Clathrate Formation Data

x1

x2

y

0

10

7.5

0

50

15

0

85

22

0

110

28.6

0

140

31.6

0

170

34

0

200

35

0

230

35.5

0

260

36.5

0

290

385

0

10

12.3

0

30

18

0

62

20.8

0

90

25.7

0

150

32.5

0

210

34

0

270

35

0.02

10

14.4

x1

x2

y

0.02

30

19

0.02

60

26.4

0.02

90

28.5

0.02

120

29

0.02

210

35

0.02

30

15.1

0.02

60

26.4

0.02

120

27

0.02

150

29

0.05

20

21

0.05

40

27.3

0.05

130

48.5

0.05

190

50.4

0.05

250

52.5

0.05

60

34.4

0.05

90

46.5

0.05

120

50

0.05

150

51.9

y: Clathrate formation (mass %) x1: Amount of surfactant (mass %) x2: Time (minutes)
Source: "Study on a Cool Storage System Using HCFC (Hydro-chloro-fluoro-carbon)-14 lb (1,1-dichloro1-fluoro-ethane) Clathrate," by T. Tanii, M. Minemoto, K. Nakazawa, and Y. Ando, Canadian Journal of Chemical Engineering, 75, 353­360.

TABLE B.9 Pressure Drop Data

x1

x2

x3

2.14

10

034

4.14

10

0.34

8.15

10

0.34

2.14

10

0.34

4.14

10

0.34

8.15

10

0.34

2.14

10

0.34

4.14

10

0.34

8.15

10

0.34

2.14

10

0.34

4.14

10

0.34

8.15

10

0.34

2.14

2.63

0.34

4.14

2.63

0.34

8.15

2.63

0.34

2.14

2.63

0.34

4.14

2.63

0.34

8.15

2.63

0.34

2.14

2.63

0.34

4.14

2.63

0.34

8.15

2.63

0.34

2.14

2.63

0.34

4.14

2.63

0.34

8.15

2.63

0.34

5.6

1.25

0.34

5.6

1.25

0.34

5.6

1.25

0.34

5.6

1.25

0.34

4.3

2.63

0.34

4.3

2.63

0.34

4.3

2.63

0.34

4.3

2.63

0.34

4.3

2.63

0.34

5.6

10.1

0.25

5.6

10.1

0.25

5.6

10.1

0.25

5.6

10.1

0.25

5.6

10.1

0.34

5.6

10.1

0.34

5.6

10.1

0.34

5.6

10.1

0.34

4.3

10.1

0.34

4.3

10.1

0.34

4.3

10.1

0.34

4.3

10.1

0.34

2.4

10.1

0.34

2.4

10.1

0.34

2.4

10.1

0.34

2.4

10.1

0.34

DATA SETS FOR EXERCISES

561

x4
1 1 1 0.246 0.379 0.474 0.141 0.234 0.311 0.076 0.132 0.184 0.679 0.804 0.89 0.514 0.672 0.801 0.346 0.506 0.669 1 1 1 0.848 0.737 0.651 0.554 0.748 0.682 0.524 0.472 0.398 0.789 0.677 0.59 0.523 0.789 0.677 0.59 0.523 0.741 0.617 0.524 0.457 0.615 0.473 0.381 0.32

y
28.9 31 26.4 27.2 26.1 23.2 19.7 22.1 22.8 29.2 23.6 23.6 24.2 22.1 20.9 17.6 15.7 15.8 14 17.1 18.3 33.8 31.7 28.1 18.1 16.5 15.4 15 19.1 16.2 16.3 15.8 15.4 19.2 8.4 15 12 21.9 21.3 21.6 19.8 21.6 17.3 20 18.6 22.1 14.7 15.8 13.2
(Continued)

562 APPENDIX B

TABLE B.9 (Continued)

x1

x2

x3

x4

y

5.6

10.1

0.55

0.789

30.8

5.6

10.1

0.55

0.677

27.5

5.6

10.1

0.55

0.59

25.2

5.6

10.1

0.55

0.523

22.8

2.14

112

0.34

0.68

41.7

4.14

112

0.34

0.803

33.7

8.15

112

0.34

0.889

29.7

2.14

112

0.34

0.514

41.8

4.14

112

0.34

0.672

37.1

8.15

112

0.34

0.801

40.1

2.14

112

0.34

0.306

42.7

4.14

112

0.34

0.506

48.6

8.15

112

0.34

0.668

42.4

y: Dimensionless factor for the pressure drop through a bubble cap x1: Superficial fluid velocity of the gas (cm/s) x2: Kinematic viscosity x3: Mesh opening (cm) x4: Dimensionless number relating the superficial fluid velocity of the gas to the superficial fluid velocity of the liquid
Source: "A Correlation of Two-Phase Pressure Drops in Screen-plate Bubble Column," by C. H. Liu, M. Kan, and B. H. Chen, Canadian Journal of Chemical Engineering, 71, 460­463.

DATA SETS FOR EXERCISES

563

TABLE B.10 Kinematic Viscosity Data

x1

x2

0.9189

-10

0.9189

0

0.9189

10

0.9189

20

0.9189

30

0.9189

40

0.9189

50

0.9189

60

0.9189

70

0.9189

80

0.7547

-10

0.7547

0

0.7547

10

0.7547

20

0.7547

30

0.7547

40

0.7547

50

0.7547

60

0.7547

70

0.7547

80

0.5685

-10

0.5685

0

0.5685

10

0.5685

20

0.5685

30

0.5685

40

0.5685

50

0.5685

60

0.5685

70

0.5685

80

0.361

-10

0.361

0

0.361

10

0.361

20

0.361

30

0.361

40

0.361

50

0.361

60

0.361

70

0.361

80

y
3.128 2.427 1.94 1.586 1.325 1.126 0.9694 0.8473 0.7481 0.6671 2.27 1.819 1.489 1.246 1.062 0.916 0.8005 0.7091 0.6345 0.5715 1.593 1.324 1.118 0.9576 0.8302 0.7282 0.647 0.5784 0.5219 0.4735 1.161 0.9925 0.8601 0.7523 0.6663 0.594 0.5338 0.4804 0.4361 0.4016

y: Kinematic viscosity (10-6 m2/s). xl: Ratio of 2-methoxyethanol to 1,2-dimethoxyethane (dimensionless). x2: Temperature (°C).
Source: "Viscosimetric Studies on 2-Methoxyethanol + 1, 2-Dimethoxyethane Binary Mixtures from -10 to 80°C," Canadian Journal of Chemical Engineering, 75, 494­501.

564 APPENDIX B

TABLE B.11 Wine Quality Data (Found in Minitab)

Clarity, x1
1 1 1 1 1 1 1 1 1 1 1 0.5 0.8 0.7 1 0.9 1 1 1 0.9 0.9 1 0.7 0.7 1 1 1 1 1 1 1 0.8 1 1 0.8 0.8 0.8 0.8

Aroma, x2
3.3 4.4 3.9 3.9 5.6 4.6 4.8 5.3 4.3 4.3 5.1 3.3 5.9 7.7 7.1 5.5 6.3 5 4.6 3.4 6.4 5.5 4.7 4.1 6 4.3 3.9 5.1 3.9 4.5 5.2 4.2 3.3 6.8 5 3.5 4.3 5.2

Body, x3
2.8 4.9 5.3 2.6 5.1 4.7 4.8 4.5 4.3 3.9 4.3 5.4 5.7 6.6 4.4 5.6 5.4 5.5 4.1 5 5.4 5.3 4.1 4 5.4 4.6 4 4.9 4.4 3.7 4.3 3.8 3.5 5 5.7 4.7 5.5 4.8

Flavor, x4
3.1 3.5 4.8 3.1 5.5 5 4.8 4.3 3.9 4.7 4.5 4.3 7 6.7 5.8 5.6 4.8 5.5 4.3 3.4 6.6 5.3 5 4.1 5.7 4.7 5.1 5 5 2.9 5 3 4.3 6 5.5 4.2 3.5 5.7

Oakiness, x5
4.1 3.9 4.7 3.6 5.1 4.1 3.3 5.2 2.9 3.9 3.6 3.6 4.1 3.7 4.1 4.4 4.6 4.1 3.1 3.4 4.8 3.8 3.7 4 4.7 4.9 5.1 5.1 4.4 3.9 6 4.7 4.5 5.2 4.8 3.3 5.8 3.5

Quality, y
9.8 12.6 11.9 11.1 13.3 12.8 12.8 12 13.6 13.9 14.4 12.3 16.1 16.1 15.5 15.5 13.8 13.8 11.3 7.9 15.1 13.5 10.8 9.5 12.7 11.6 11.7 11.9 10.8 8.5 10.7 9.1 12.1 14.9 13.5 12.2 10.3 13.2

The wine type here is Pinot Noir. Region refers to distinct geographic regions.

Region
1 1 1 1 1 1 1 1 3 1 3 2 3 3 3 3 3 3 1 2 3 3 2 2 3 2 1 2 2 2 2 1 1 3 1 1 1 1

DATA SETS FOR EXERCISES

565

TABLE B.12 Heat Treating Data

Temp

Soaktime

Soakpct

1650

0.58

1.10

1650

0.66

1.10

1650

0.66

1.10

1650

0.66

1.10

1600

0.66

1.15

1600

0.66

1.15

1650

1.00

1.10

1650

1.17

1.10

1650

1.17

1.10

1650

1.17

1.10

1650

1.17

1.10

1650

1.17

1.10

1650

1.17

1.15

1650

1.20

1.15

1650

2.00

1.15

1650

2.00

1.10

1650

2.20

1.10

1650

2.20

1.10

1650

2.20

1.15

1650

2.20

1.10

1650

2.20

1.10

1650

2.20

1.10

1650

3.00

1.15

1650

3.00

1.10

1650

3.00

1.10

1650

3.00

1.15

1650

3.33

1.10

1700

4.00

1.10

1650

4.00

1.10

1650

4.00

1.15

1700

12.50

1.00

1700

18.50

1.00

y = PITCH: Results of the pitch carbon analysis test TEMP: Furnace temperature SOAKTIME: Duration of the carburizing cycle SOAKPCT: Carbon concentration DIFFTIME: Duration of the diffuse cycle DIFFPCT: Carbon concentration of the diffuse cycle

Difftime
0.25 0.33 0.33 0.33 0.33 0.33 0.50 0.58 0.58 0.58 0.58 0.58 0.58 1.10 1.00 1.10 1.10 1.10 1.10 1.10 1.10 1.50 1.50 1.50 1.50 1.66 1.50 1.50 1.50 1.50 1.50 1.50

Diffpct
0.90 0.90 0.90 0.95 1.00 1.00 0.80 0.80 0.80 0.80 0.90 0.90 0.90 0.80 0.80 0.80 0.80 0.80 0.80 0.90 0.90 0.90 0.80 0.70 0.75 0.85 0.80 0.70 0.70 0.85 0.70 0.70

Pitch
0.013 0.016 0.015 0.016 0.015 0.016 0.014 0.021 0.018 0.019 0.021 0.019 0.021 0.025 0.025 0.026 0.024 0.025 0.024 0.025 0.027 0.026 0.029 0.030 0.028 0.032 0.033 0.039 0.040 0.035 0.056 0.068

566 APPENDIX B

TABLE B.13 Jet Turbine Engine Thrust Data

Observation Number

y

x1

x2

x3

x4

x5

x6

1

4540 2140 20640 30250 205 1732

99

2

4315 2016 20280 30010 195 1697 100

3

4095 1905 19860 29780 184 1662

97

4

3650 1675 18980 29330 164 1598

97

5

3200 1474 18100 28960 144 1541

97

6

4833 2239 20740 30083 216 1709

87

7

4617 2120 20305 29831 206 1669

87

8

4340 1990 19961 29604 196 1640

87

9

3820 1702 18916 29088 171 1572

85

10

3368 1487 18012 28675 149 1522

85

11

4445 2107 20520 30120 195 1740 101

12

4188 1973 20130 29920 190 1711 100

13

3981 1864 19780 29720 180 1682 100

14

3622 1674 19020 29370 161 1630 100

15

3125 1440 18030 28940 139 1572 101

16

4560 2165 20680 30160 208 1704

98

17

4340 2048 20340 29960 199 1679

96

18

4115 1916 19860 29710 187 1642

94

19

3630 1658 18950 29250 164 1576

94

20

3210 1489 18700 28890 145 1528

94

21

4330 2062 20500 30190 193 1748 101

22

4119 1929 20050 29960 183 1713 100

23

3891 1815 19680 29770 173 1684 100

24

3467 1595 18890 29360 153 1624

99

25

3045 1400 17870 28960 134 1569 100

26

4411 2047 20540 30160 193 1746

99

27

4203 1935 20160 29940 184 1714

99

28

3968 1807 19750 29760 173 1679

99

29

3531 1591 18890 29350 153 1621

99

30

3074 1388 17870 28910 133 1561

99

31

4350 2071 20460 30180 198 1729 102

32

4128 1944 20010 29940 186 1692 101

33

3940 1831 19640 29750 178 1667 101

34

3480 1612 18710 29360 156 1609 101

35

3064 1410 17780 28900 136 1552 101

36

4402 2066 20520 30170 197 1758 100

37

4180 1954 20150 29950 188 1729

99

38

3973 1835 19750 29740 178 1690

99

39

3530 1616 18850 29320 156 1616

99

40

3080 1407 17910 28910 137 1569 100

y: Thrust x1: Primary speed of rotation x2: Secondary speed of rotation x3: Fuel flow rate x4: Pressure x5: Exhaust temperature x6: Ambient temperature at time of test

TABLE B.14 Electronic Inverter Data

Observation Number

x1

x2

1

3

3

2

8

30

3

3

6

4

4

4

5

8

7

6

10

20

7

8

6

8

6

24

9

4

10

10

16

12

11

3

10

12

8

3

13

3

6

14

3

8

15

4

8

16

5

2

17

2

2

18

10

15

19

15

6

20

15

6

21

10

4

22

3

8

23

6

6

24

2

3

25

3

3

y: Transient point (volts) of PMOS-NMOS inverters x1: Width of the NMOS device x2: Length of the NMOS device x3: Width of the PMOS device x4: Length of the PMOS device

DATA SETS FOR EXERCISES

567

x3

x4

x5

y

3

3

0

0.787

8

8

0

0.293

6

6

0

1.710

4

12

0

0.203

6

5

0

0.806

5

5

0

4.713

3

3

25

0.607

4

4

25

9.107

12

4

25

9.210

8

4

25

1.365

8

8

25

4.554

3

3

25

0.293

3

3

50

2.252

8

3

50

9.167

4

8

50

0.694

2

2

50

0.379

2

3

50

0.485

3

3

50

3.345

2

3

50

0.208

2

3

75

0.201

3

3

75

0.329

2

2

75

4.966

6

4

75

1.362

8

6

75

1.515

8

8

75

0.751

TABLE B.15 Air Pollution and Mortality Data

City

Mort

Precip Educ

San Jose, CA Wichita, KS San Diego, CA Lancaster, PA Minneapolis, MN Dallas, TX Miami, FL Los Angeles, CA Grand Rapids, MI Denver, CO Rochester, NY Hartford, CT Fort Worth, TX

790.73 823.76 839.71 844.05 857.62 860.10 861.44 861.83 871.34 871.77 874.28 887.47 891.71

13.00 28.00 10.00 43.00 25.00 35.00 60.00 11.00 31.00 15.00 32.00 43.00 31.00

12.20 12.10 12.10 9.50 12.10 11.80 11.50 12.10 10.90 12.20 11.10 11.50 11.40

Nonwhite
3.00 7.50 5.90 2.90 3.00 14.80 11.50 7.80 5.10 4.70 5.00 7.20 11.50

Nox

SO2

32.00 2.00 66.00 7.00 11.00 1.00 1.00 319.00 3.00 8.00 4.00 3.00 1.00

3.00 1.00 20.00 32.00 26.00 1.00 1.00 130.00 10.00 28.00 18.00 10.00 1.00

(Continued)

568 APPENDIX B

TABLE B.15 (Continued)

City

Mort

Portland, OR Worcester, MA Seattle, WA Bridgeport, CT Springfield, MA San Francisco, CA York, PA Utica, NY Canton, OH Kansas City, MO Akron, OH New Haven, CT Milwasukee, WI Boston, MA Dayton, OH Providence, RI Flint, MI Reading, PA Syracuse, NY Houston, TX Saint Louis, MO Youngstown, OH Columbus, OH Detroit, MI Nashville, TN Allentown, PA Washington, DC Indianapolis, IN Cincinnati, OH Greensboro, NC Toledo, OH Atlanta, GA Cleveland, OH Louisville, KY Pittsburgh, PA New York, NY Albany, NY Buffalo, NY Wilmington, DE Memphis, TE Philadelphia, PA Chattanooga, TN Chicago, IL Richmond, VA Birmingham, AL Baltimore, MD New Orleans, LA

893.99 895.70 899.26 899.53 904.16 911.70 911.82 912.20 912.35 919.73 921.87 923.23 929.15 934.70 936.23 938.50 941.18 946.18 950.67 952.53 953.56 954.44 958.84 959.22 961.01 962.35 967.80 968.66 970.47 971.12 972.46 982.29 985.95 989.27 991.29 994.65 997.88 1001.90 1003.50 1006.49 1015.02 1017.61 1024.89 1025.50 1030.38 1071.29 1113.06

Precip
37.00 45.00 35.00 45.00 45.00 18.00 42.00 40.00 36.00 35.00 36.00 46.00 30.00 43.00 36.00 42.00 30.00 41.00 38.00 46.00 34.00 38.00 37.00 31.00 45.00 44.00 41.00 39.00 40.00 42.00 31.00 47.00 35.00 30.00 36.00 42.00 35.00 36.00 45.00 50.00 42.00 52.00 33.00 44.00 53.00 43.00 54.00

Educ
12.00 11.10 12.20 10.60 11.10 12.20 9.00 10.30 10.70 12.00 11.40 11.30 11.10 12.10 11.40 10.10 10.80 9.60 11.40 11.40 9.70 10.70 11.90 10.80 10.10 9.80 12.30 11.40 10.20 10.40 10.70 11.10 11.10 9.90 10.60 10.70 11.00 10.50 11.30 10.40 10.50 9.60 10.90 11.00 10.20 9.60 9.70

Nonwhite
3.60 1.00 5.70 5.30 3.40 13.70 4.80 2.50 6.70 12.60 8.80 8.80 5.80 3.50 12.40 2.20 13.10 2.70 3.80 21.00 17.20 11.70 13.10 15.80 21.00 0.80 25.90 15.60 13.00 22.70 9.50 27.10 14.70 13.10 8.10 11.30 3.50 8.10 12.10 36.70 17.50 22.20 16.30 28.60 38.50 24.40 31.40

Nox
21.00 3.00 7.00 4.00 4.00
171.00 8.00 2.00 7.00 4.00 15.00 3.00 23.00 32.00 4.00 4.00 4.00 11.00 5.00 5.00 15.00 13.00 9.00 35.00 14.00 6.00 28.00 7.00 26.00 3.00 7.00 8.00 21.00 37.00 59.00 26.00 10.00 12.00 11.00 18.00 32.00 8.00 63.00 9.00
32.00 38.00 17.00

SO2
44.00 8.00
20.00 4.00 20.00 86.00 49.00 11.00 20.00 4.00 59.00 8.00 125.00 62.00 16.00 18.00 11.00 89.00 25.00 1.00 68.00 39.00 15.00 124.00 78.00 33.00 102.00 33.00 146.00 5.00 25.00 24.00 64.00 193.00 263.00 108.00 39.00 37.00 42.00 34.00 161.00 27.00 278.00 48.00 72.00 206.00 1.00

DATA SETS FOR EXERCISES

569

TABLE B.16 Life Expectancy Data

Country

PeopleLifeExp per-TV

Argentina

70.5

Bangladesh

53.5

Brazil

65

Canada

76.5

China

70

Colombia

71

Egypt

60.5

Ethiopia

51.5

France

78

Germany

76

India

57.5

Indonesia

61

Iran

64.5

Italy

78.5

Japan

79

Kenya

61

Korea, North

70

Korea, South

70

Mexico

72

Morocco

64.5

Burma

54.5

Pakistan

56.5

Peru

64.5

Philippines

64.5

Poland

73

Romania

72

Russia

69

South Africa

64

Spain

78.5

Sudan

53

Taiwan

75

Thailand

68.5

Turkey

70

Ukraine

70.5

United Kingdom 76

United States

75.5

Venezuela

74.5

Vietnam

65

4 315
4 1.7 8 5.6 15 503 2.6 2.6 44 24 23 3.8 1.8 96 90 4.9 6.6 21 592 73 14 8.8 3.9 6 3.2 11 2.6 23 3.2 11 5 3 3 1.3 5.6 29

People-per-Dr
370 6,166
684 449 643 1,551 616 36,660 403 346 2,471 7,427 2,992 233 609 7,615 370 1,066 600 4,873 3,485 2,364 1,016 1,062 480 559 259 1,340 275 12,550 965 4,883 1,189 226 611 404 576 3,096

LifeExpMale
74 53 68 80 72 74 61 53 82 79 58 63 65 82 82 63 73 73 76 66 56 57 67 67 77 75 74 67 82 54 78 73 72 75 79 79 78 67

LifeExpFemale
67 54 62 73 68 68 60 50 74 73 57 59 64 75 76 59 67 67 68 63 53 56 62 62 69 69 64 61 75 52 72 66 68 66 73 72 71 63

570 APPENDIX B

Table B.17 Patient Satisfaction Data

Satisfaction

Age

Severity

68

55

50

77

46

24

96

30

46

80

35

48

43

59

58

44

61

60

26

74

65

88

38

42

75

27

42

57

51

50

56

53

38

88

41

30

88

37

31

102

24

34

88

42

30

70

50

48

82

58

61

43

60

71

46

62

62

56

68

38

59

70

41

26

79

66

52

63

31

83

39

42

75

49

40

Surgical-Medical
0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1

Anxiety
2.1 2.8 3.3 4.5 2 5.1 5.5 3.2 3.1 2.4 2.2 2.1 1.9 3.1 3 4.2 4.6 5.3 7.2 7.8 7 6.2 4.1 3.5 2.1

DATA SETS FOR EXERCISES

571

TABLE B.18 Fuel Consumption Data

y

x1

x2

x3

x4

x5

x6

x7

x8

343

0

356

1

344

0

356

1

352

0

361

1

372

0

355

1

375

0

359

1

364

0

357

1

368

0

360

1

372

0

352

1

52.8

811.7

2.11

220

261

87

1.8

52.8

811.7

2.11

220

261

87

1.8

50.0

821.3

2.11

223

260

87

16.6

50.0

821.3

2.11

223

260

87

16.6

47.2

832.0

2.09

221

261

92

23.0

47.2

832.0

2.09

221

261

92

23.0

47.0

831.3

2.26

190

323

75

25.1

47.0

831.3

2.26

190

323

75

25.1

48.3

836.8

2.47

180

364

71

26.1

48.3

836.8

2.47

180

364

71

26.1

44.7

808.3

1.41

180

300

64

20.0

44.7

808.3

1.41

180

300

64

20.0

55.7

808.7

1.44

176

299

64

20.5

55.7

808.7

1.44

176

299

64

20.5

52.8

813.2

1.96

175

301

75

17.3

52.8

813.2

1.96

175

301

75

17.3

y: fuel consumption (g/km) x1: vehicle (0--bus, 1--truck) x2: cetane number x3: density (g/L, 15°C) x4: viscosity (KV, 40°C) x5: initial boiling point (degrees C) x6: final boiling point (degrees C) x7: flash point (degrees C) x8: total aromatics (percent)
Source: "A Multivariate Statistical Analysis of Fuel-Related Polycyclic Aromatic Hydrocarbon Emissions from Heavy-Duty Diesel Vehicles," by R. Westerholm and H. Li, Environmental Science and Technology, 28, 965­972.

572 APPENDIX B

TABLE B.19 Wine Quality of Young Red Wines

y

x1

x2

x3

x4

x5

x6

x7

x8

x9

x10

19.2 0 18.3 0 17.1 0 17.3 0 16.8 0 16.5 0 15.8 0 15.2 0 15.2 0 14.0 0 14.0 0 13.8 0 13.6 0 12.8 0 18.5 1 17.3 1 16.3 1 16.3 1 16.0 1 16.0 1 15.7 1 15.5 1 15.3 1 15.3 1 14.8 1 14.3 1 14.3 1 14.2 1 14.0 1 13.8 1 12.5 1 11.5 1

3.85

66

9.35 5.65 2.40 3.25 0.33 19 0.065

3.73

79 11.15 6.95 3.15 3.80 0.36 21 0.076

3.88

73

9.40 5.75 2.10 3.65 0.40 18 0.073

3.86

99 12.85 7.70 3.90 3.80 0.35 22 0.076

3.98

75

8.55 5.05 2.05 3.00 0.49 12 0.060

3.85

61 10.30 6.20 2.50 3.70 0.38 20 0.074

3.93

66

4.90 2.75 1.20 1.55 0.29 11 0.031

3.66

86

6.40 4.00 1.50 2.50 0.27 19 0.050

3.91

78

5.80 3.30 1.40 1.90 0.40

9 0.038

3.47 178

3.60 2.25 0.75 1.50 0.37

8 0.030

3.91

81

3.90 2.15 1.00 1.15 0.32

7 0.023

3.75 108

5.80 3.20 1.60 1.60 0.38

8 0.032

3.90

92

5.40 2.85 1.55 1.30 0.44

6 0.026

3.92

96

5.00 2.70 1.40 1.30 0.35

7 0.026

3.87

89

9.15 5.60 1.95 3.65 0.46 16 0.073

3.97

59 10.25 6.10 2.40 3.70 0.40 19 0.074

3.76

22

8.20 5.00 1.85 3.15 0.25 25 0.063

3.76

77

8.35 5.05 1.90 3.15 0.37 17 0.063

3.98

58 10.15 6.00 2.60 3.40 0.38 18 0.068

3.88

85

6.85 4.10 1.50 2.60 0.33 16 0.052

3.75 120

8.80 5.50 1.85 3.65 0.39 19 0.073

3.98

94

5.45 3.05 1.50 1.55 0.41

8 0.031

3.69 122

8.00 5.05 1.90 3.15 0.27 23 0.063

3.77 144

5.60 3.35 1.10 2.25 0.36 12 0.045

3.74

10

7.90 4.75 1.95 2.80 0.25 23 0.056

3.76 100

5.55 3.25 1.15 2.10 0.34 12 0.042

3.91

73

4.65 2.70 0.95 1.75 0.36 10 0.035

3.60 301

4.25 2.40 1.25 1.15 0.42

6 0.023

3.76 104

8.70 5.10 2.25 2.85 0.34 17 0.057

3.90

67

7.40 4.40 1.60 2.80 0.45 13 0.056

3.80

89

5.35 3.15 1.20 1.95 0.32 12 0.039

3.65 192

6.35 3.90 1.25 2.65 0.63

8 0.053

y: quality rating (20 maximum) x1: wine varietal (0--Cabernet Sauvignon, 1--Shiraz) x2: pH x3: Total SO2 (ppm) x4: color density x5: wine color x6: polymeric pigment color x7: anthocyanin color x8: total anthocyanins (g/L) x9: degree of ionization of anthocyanins (percent) x10: ionized anthocyanins (percent)
Source: "Wine Quality: Correlations with Colour Density and Anthocyanin Equilibria in a Group of Young Red Wines," by T. C. Somers and M. E. Evans, Journal of the Science of Food and Agriculture, 25, 1369­1379.

DATA SETS FOR EXERCISES

573

TABLE B.20 Methanol Oxidation in Supercritical Water

x1

x2

x3

x4

x5

y

0

454

8.8

3.90

1.30

1.1

0

474

8.2

3.68

1.16

4.2

0

524

7.0

2.78

1.25

94.2

0

503

7.4

2.27

1.57

20.7

0

493

7.6

2.40

1.55

15.7

0

493

7.6

1.28

2.71

15.9

0

493

7.5

5.68

0.54

14.7

0

493

7.6

4.65

0.74

10.8

0

493

7.4

3.30

1.01

9.6

0

493

7.4

2.52

1.12

12.7

0

493

7.5

2.44

0.86

7.1

0

493

7.5

2.47

0.45

9.0

1

530

6.7

1.97

1.74

96.0

1

522

6.9

2.03

0.94

78.4

1

522

6.9

2.05

0.93

78.3

1

503

7.3

2.16

0.94

71.4

1

453

8.7

2.76

0.90

0.5

1

483

7.7

2.42

0.91

3.1

x1: reactor system x2: temperature (degrees C) x3: reactor residence time (seconds) x4: inlet concentration of methanol x5: ratio of inlet oxygen to inlet methanol y: percent conversion
Source: "Revised Global Kinetic Measurements of Methanol Oxidation in Supercritical Water," by J. W. Tester, P. A. Webley, and H. R. Holgate, Industrial and Engineering Chemical Research, 32, 236­239.

TABLE B.21 Hald Cement Data

Observation

i

yi

xi1

xi2

xi3

xi4

1

78.5

7

26

6

60

2

74.3

1

29

15

52

3

104.3

11

56

8

20

4

87.6

11

31

8

47

5

95.9

7

52

6

33

6

109.2

11

55

9

22

7

102.7

3

71

17

6

8

72.5

1

31

22

44

9

93.1

2

54

18

22

10

115.9

21

47

4

26

11

83.8

1

40

23

34

12

113.3

11

66

9

12

13

109.4

10

68

8

12

Source: Hald, A. [1952], Statistical Theory with Engineering Applications, Wiley, New York.

APPENDIX C
SUPPLEMENTAL TECHNICAL MATERIAL
C.1 Background on Basic Test Statistics C.2 Background from the Theory of linear Models C.3 Important Results on SSR and SSRes C.4 Gauss-Markov Theorem, Var() =  2I C.5 Computational Aspects of Multiple Regression C.6 Result on the Inverse of a Matrix C.7 Development of the PRESS Statistic C.8 Development of S(2i) C.9 Outlier Test Based on R-Student C.10 Independence of Residuals and Fitted Values C.11 Gauss-Markov Theorem, Var() = V C.12 Bias in MSRes When the Model Is Underspecified C.13 Computation of Influence Diagnostics C.14 Generalized Linear Models
C.1 BACKGROUND ON BASIC TEST STATISTICS We indicate that Y is a random variable that follows a normal distribution with mean  and variance  2 by
Y  N (,  2 )
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc. 574

BACKGROUND ON BASIC TEST STATISTICS

575

C.1.1 Central Distributions
1. Let Y1, Y2, . . . , Yn be independent normally distributed random variables with
E(Yi) = i and Var (Yi ) = i2. Let a1, a2, . . . , an be known constants. If we define
the linear combination of the Yi's by

n
 U = aiYi i=1

then

  U

  N 

n i=1

ai i,

n i=1

ai2

2 i

 

The key point is that linear combinations of normally distributed random variables also follow normal distributions. 2. If Y  N(,2), then

Z = Y -   N (0, 1)


Z is called the standard normal random variable. 3. Let Z = (Y - )/. If Y  N(, 2), then Z2 follows a  2 distribution, which we
denote by

Z 2  12

The key point is that the square of a standard normal random variable is a 2 random variable with one degree of freedom.
4. Let Y1, Y2, . . . , Yn be independent normally distributed random variables with
E(Yi) = i and Var (Yi ) = i2, and let

Zi

=

Yi - i i

.

Then

n

 Zi2





2 n

i=1

The key points are (1) the sum of n squared standard normal random variables follows a 2 distribution with n degrees of freedom and (2) the sum of 2 random variables also follows a 2 distribution.
5. The Central Limit Theorem If Y1, Y2, . . . , Yn are independent and identically distributed random variables with E(Yi) =  and Var(Yi) = 2 < , then

Y - n

576 APPENDIX C

converges in distribution to a standard normal distnbution as n  . The key point is that if n is sufficiently large, then Y approximately follows a normal distribution. What constitutes sufficiently large depends on the underlying distribution of the Yi's. 6. If Z  N(0, 1), V  2 , and Z and V are independent, then

Z V

 t

where tv is the t distribution with v degrees of freedom. 7. Let V  2 , and let W  2 . If V and W are independent, then

V W

 



F ,

.

where F, is the F distribution with  and  degrees of freedom. The key point is that the ratio of two independent 2 random variables, each divided by their
respective degrees of freedom, follows an F distribution.

C.1.2 Noncentral Distributions 1. Let X  N(, 1), and let V  2 . If X and V are independent, then

X V

 t,

where t, is the noncentral t distribution with  degrees of freedom and noncentrality parameter .
2. If X  N(, 1), then

X2





2
1,

2

where 12,2 is noncentrality

the noncentral parameter 2.

2

distribution

with

one

degree

of

freedom

and

3. If X1, X2, . . . , Xn, are independent normally distributed random variables with E(Xi) = i and Var(Xi) = 1, then

n
 Xi2  n2,
i=1
where the noncentrality parameter, , is.

n

  =



2 i

i=1

BACKGROUND FROM THE THEORY OF LINEAR MODELS

577

4. Let V  2,, and let W  2 . If V and W are independent, then

V W

 



F,, 

where F,, is a noncentral F distnbution with  and  degrees of freedom and noncentrality parameter .

C.2 BACKGROUND FROM THE THEORY OF LINEAR MODELS

C.2.1 Basic Definitions
1. Rank of a Matrix The rank of a matrix, A, is the number of linearly independent columns. Equivalently, it is the number of linearly independent rows.
2. Identity Matrix The identity matrix of order k, denoted by I or Ik, is a k × k square matrix whose diagonal elements are 1's and whose nondiagonal elements are 0's; thus,

1 0 0

0

I = 0 1 0

0





0 0 0

1 

3. Inverse of a Matrix Let A be a k × k matrix. The inverse of A, denoted by A-1, is another k × k matrix such that

AA-1 = A-1A = I
If the inverse exists, it is unique. 4. Transpose of a Matrix Let A be an n × k matrix. The transpose of A,
denoted by A or AT, is a k × n matrix whose columns are the rows of A; thus, if

a11 a12 A = a21 a22
 an1 an2

a1k 

a11 a21

a2k

 

,

then

A

=

 

a12

a22





ank 

a1k a2k

an1 

an2

 



ank 

Note: If A is an n × m matrix and B is an m × p matrix, then

(AB) = BA

5. Symmetric Matrix Let A be a k × k matrix. A is said to be symmetric if A = A.
6. Idempotent Matrix Let A be a k × k matrix. A is called idempotent if

A = AA

578 APPENDIX C

If A is also symmetric, then A is called symmetric idempotent. If A is symmetric idempotent, then I - A is also symmetric idempotent.
7. Orthonormal Matrix Let A be a k × k matrix. If A is an orthonormal matrix, then AA = I. As a consequence, if A is an orthonormal matrix, then A-1 = A.
8. Quadratic Form Let y be a k × 1 vector, and let A be a k × k matrix. The function

kk

  yAy =

aij yi yj

i=1 j=1

is called a quadratic form. A is called the matrix of the quadratic form. 9. Positive Definite and Positive Semidefinite Matrices Let A be a k × k
matrix. A is said to be positive definite if the following condition holds

(a) A = A (A is symmetric) (b) yAy > 0y  k, y  0

A is said to be positive semidefinite if the following condition holds:

(c) yAy = 0 for some y  0 10. Trace of a Matrix Let A be a k × k matrix. The trace of A, denoted by
trace(A) or tr(A), is the sum of the diagonal elements of A; thus,
k
 trace(A) = aii i=1
Note: (a) If A is an m × n matrix and B is an n × m matrix, then
trace(AB) = trace(BA)
(b) If the matrices are appropriately conformable, then
trace(ABC) = trace(CAB)
(c) If A and B are k × k matrices and a and b are scalars, then
trace(aA + bB) = a trace(A) + b trace(B)
11. Rank of an Idempotent Matrix Let A be an idempotent matrix. The rank of A is its trace.
12. An Important Identity for a Partitioned Matrix Let X be an n × p matrix partitioned such that
X = [X1X2 ]

BACKGROUND FROM THE THEORY OF LINEAR MODELS

579

We note that

X (XX)-1 XX = X

X (XX)-1 X [X1X2 ] = X X (XX)-1 X [X1X2 ] = [X1X2 ]

Consequently,

X (XX)-1 XX1 = X1 and X (XX)-1 XX2 = X2

Similarly,

X1X (XX)-1 X = X1 and X2 X (XX)-1 X = X2

13. Inverse of a Partitioned Matrix Consider a matrix of the form

X

X

=

 X 1 X 1 X2 X1

X1X2  X2 X2 

It can be shown that the inverse of this matrix is

(X X )-1

=

(X1X1 )-1
 

+

(X1X1 )-1 X1X2GX2 X1 (X1X1 )-1 -GX2 X1 (X1X1 )-1

- (X1X1 )-1 X1X2G



G



where H1 = X1 (X1X1 )-1 X1 and G = [X2 (I - H1 ) X2 ]-1 .

C.2.2 Matrix Derivatives
Let A be a k × k matrix of constants, a be a k × 1 vector of constants, and y be a k × 1 vector of variables.

1. If z = ay, then

z = ay = a y y

2. If z = yy, then

z = yy = 2y y y

3. If z = aAy, then

z = aAy = Aa y y

580 APPENDIX C

4. If z = yAy, then

z = yAy = Ay + Ay y y

If A is symmetric, then

yAy = 2Ay y

C.2.3 Expectations
Let A be a k × k matrix of constants, a be a k × 1 vector of constants, and y be a k × 1 random vector with mean µ and nonsingular variance­covariance matrix V.
1. E(ay) = aµ. 2. E(Ay) = Aµ. 3. Var(ay) = aVa. 4. Var(Ay) = AVA. Note: If V = 2I, then Var(Ay) = 2AA. 5. E(yAy) = trace(AV) + µAµ. Note: If V = 2I, then E(yAy) = 2 trace(A) + µAµ.

C.2.4 Distribution Theory Let A be a k × k matrix of constants and y be a k × 1 multivariate normal random vector with mean  and nonsingular variance­covariance matrix V; thus,
y  N (m, V)
Let U be the quadratic form defined by U = yAy.
1. If AV or VA is an idempotent matrix of rank p, then

U   2p,

where  = µAµ. 2. Let V = 2I, which is a typical assumption. If A is idempotent with rank p, then

U 2



 2p,

where  = µAµ/2.
3. Let B be a q × k matrix, and let W be the linear form given by W = By. The quadratic form U = yAy and W are independent if

IMPORTANT RESULTS ON SSR AND SSRES

581

BVA = 0
Note: If V = 2I, then U and W are independent if BA = 0. 4. Let B be a k × k matrix. Let V = yBy. The two quadratic forms, U and V, are
independent if

AVB = 0 Note: If V = 2I, then U and V are independent if AB = 0.

C.3 IMPORTANT RESULTS ON SSR AND SSRES
C.3.1 SSR By definition,

n
 SSR = (y^i - y)2 i=1
We note that y^ = X(XX)-1Xy and that

 y

=

1 n

n i=1

yi

=

1 1y n

where 1 is an n × 1 vector all of whose elements are l's. Further, n = 11; thus,
y = (11)-1 1y. Consequently, we can write SSR as

n
 SSR = (y^i - y)2 i=1 = [y^ - 1y][y^ - 1y]
= X (XX)-1 Xy - 1 (11)-1 1y  X (XX)-1 Xy - 1 (11)-1 1y
= y X (XX)-1 X - 1 (11)-1 1  X (XX)-1 X - 1 (11)-1 1 y

Please note that X = [1 XR], where XR is the matrix formed by the actual values for the regressors. Consequently, SSR involves a special case of a partitioned matrix. We thus may use the special identity for partitioned matrices to show that

X (XX)-1 X1 = 1 and 1X (XX)-1 X = 1

Consequently, we can show that [X(XX)-1X - 1(11)-11] is idempotent. Under the assumption that Var() = 2I,

SSR 2

=

1 2

y X (XX)-1 X - 1 (11)-1 1 y

582 APPENDIX C

follows a noncentral 2 distribution with noncentrality parameter  and degrees of freedom equal to the rank of [X(XX)-1X - 1(11)-11]. Since this matrix is idempotent, its rank is its trace. We note that
trace X (XX)-1 X - 1 (11)-1 1 = trace X (XX)-1 X - trace 1 (11)-1 1 = trace XX (XX)-1  - trace 11 (11)-1 
= trace(Ip ) - trace(1)
= p-1=k

Under the assumption that the model is correct,

E (y) = Xb = [1

XR

]

 0 bR

 

=

0

1

+

XR

bR

Thus, the noncentrality parameter is



=

1 2

E (y) 

X (XX)-1

X

-

1 (11)-1 1

E

(y)

=

1 2

b X

X (XX)-1 X

-

1 (11)-1 1 Xb

=

1 2

[0

bR

]

 

1 XR

 



X

(X

X

)-1

X



-

1

(1

1

)-1

1





[1

X

R

]

 

0 bR

 

=

1 2

[0

bR

]

  

1X XR X

(X (X

X)-1 X)-1

X X

 

- -

11 XR

(11)-1 1 1 (11)-1

 1



  

[1

XR

]

 

0 bR

 

=

1 2

[0

bR

]

  

XR

-

0
XR 1 (11)-1

1   [1

XR

]

 

0 bR

 

=

1 2

[0

bR

]

0 0

0

  0 

XR XR

-

XR

1

(11)-1

1XR

 

bR



=

1 2

bR

XR XR

-

XR 1 (11)-1 1XR

 bR

If we define the matrix of centered regressors values, XC, by

 x11 - x1

XC

=

 

x21



- x1

xn1 - x1

x12 - x2 x22 - x2
xn2 - x2

x1k - xk 

x2k

-

xk

 



xnk - xk 

where x1 is the average value for the first regressor, x2 is the average value for the second regressor, and so forth, then it is easily established that we can rewrite the noncentrality parameter as



=

1 2

bR [XC XC

] bR

IMPORTANT RESULTS ON SSR AND SSRES

583

The expected value for SSR is
( ) E (SSR ) = E y X (XX)-1 X - 1 (11)-1 1 y ( ) = trace X (XX)-1 X - 1 (11)-1 1 2I
+ E (y) X (XX)-1 X - 1 (11)-1 1 E (y)
= k 2 + bR XC XC bR
As a result,

E (MSR

)

=

E 

SSR k



=

2

+

bR XC XC bR k

C.3.2 SSRes By definition,
n
 SSRes = (yi - y^i )2 i=1
We note that we can rewrite SSRes as

SSRes = (y - y^ ) (y - y^ ) = y - X (XX)-1 Xy  y - X (XX)-1 Xy = y I - X (XX)-1 X y

It is trivial to show that [I - X(XX)-l X] is symmetric idempotent. Consequently,

SSRes 2

=

1 2

y I - X (XX)-1 X y

follows a 2 distribution. The degrees of freedom come from the rank of [I - X(XX)-1X], which is the trace. It is straightforward to show that the trace is
n - p. Under the assumption that the model is correct,

E (y) = Xb

Thus, the noncentrality parameter is

1 2

E (y) 

I

-

X (XX)-1 X E (y)

=

1 2

b X

I

-

X (XX)-1 X Xb

=

1 2

b

 X  X

-

XX (XX)-1 XX

b

=

0

As a result,

584 APPENDIX C

SSRes 2



n2- p

The expected value for SSRes is
( ) E (SSRes ) = E y I - X (XX)-1 X y ( ) = trace I - X (XX)-1 X 2I + E (y) I - X (XX)-1 X E (y)
= (n - p) 2

As a result,

E (MSRes )

=

E

 

SSRes  n - p 

=

2

C.3.3 Global or Overall F Test
An F statistic is the ratio of two independent 2 random variables, each divided by its respective degrees of freedom. We have shown that both SSR/2 and SSRes/2 follow 2 distributions. The key point now is to show that they are independent. From basic linear models theory, SSR and SSRes are independent under the assumption that Var() = 2I if

X (XX)-1 X - 1 (11)-1 1 2I I - X (XX)-1 X = 0

We note that
X (XX)-1 X - 1 (11)-1 1 2I I - X (XX)-1 X =  2 X (XX)-1 X - 1 (11)-1 1 I - X (XX)-1 X =  2 X (XX)-1 X - 1 (11)-1 1 - X (XX)-1 XX (XX)-1 X + 1 (11)-1 1X (XX)-1 X = X (XX)-1 X - X (XX)-1 X - 1 (11)-1 1 + 1 (11)-1 1 = 0

Thus, SSR and SSRes are independent. We next note that

SSR = MSR k 2  2

and

SSRes
(n - p) 2

=

MSRes 2

are 2 random variables, each divided by their respective degrees of freedom. As a result,

where

MSR MSRes

 Fk,n- p,



=

1 2

bR XC XC bR

IMPORTANT RESULTS ON SSR AND SSRES

585

In the special case of simple linear regression, we have only a single regressor; thus, R = 1, and
n
 XC XC = (xi - x )2 i=1
As a result, for simple linear regression,

where  = 12 in=1 (xi - x )2 .

MSR MSRes

 F1,n-2,

C.3.4 Extra-Sum-of-Squares Principle SSR is a special case of the extra-sum-of-squares principle. Consider the model
y = Xb + e = X1b1 + X2b2 + e

where X1 is the p1 × 1 model matrix associated with 1, X2 is the p2 × 1 model matrix associated with 2, and p1 + p2 = p. A common measure of the contribution of 2 given the presence of 1 in the model is
R (b2 b1 ) = y X (XX)-1 X - X1 (X1X1 )-1 X1  y

In the case of SSR, X1 = 1 and X2 = XR. In some sense, SSR = R(R|0). It can be
shown that X (XX)-1 X - X1 (X1X1 )-1 X1  is symmetric idempotent. The keys to this derivation are that X(XX)-1XX1 = X1 and that X1X (XX)-1 X = X1 . It is then
straightforward to show that

R(b2
2

b1 )





2
p2,

where  = (1 )  2 b2X2 I - X1 (X1X1 )-1 X1  X2b2 . It then follows that

( ) R b2 b1
p2 MSRes

 Fp2,n- p,

C.3.5 Relationship of the t Test for an Individual Coefficient and the Extra-Sum-of-Squares Principle
Squaring the t test for an individual coefficient is exactly equivalent to the F test using the extra-sum-of-squares principle, where X2 is simply the column vector of the model matrix X associated with the specific coefficient, j. Once again, consider the model

y = Xb + e = X1b1 + X2b2 + e

(C.3.1)

To show that the t test is exactly equivalent to the F test based on the extra-sumof-squares principle, we need to establish that

586 APPENDIX C

( ) ^

2 j

Var ^ j

=

1 2

y

X

(X  X )-1

X

-

X1 (X1X1 )-1

X1 

y

( ) We first need to express

^

2 j

Var ^ j

( ) Var ^ j are both scalars. As a result,

in matrix form. We first note that ^ j and

^

2 j

( ) ( ) Var ^ j

= ^ j Var ^ j -1 ^ j

We now need to express ^ j in matrix form. Let H1 = X1 (X1X1 )-1 X1 . Premultiplying
both sides of (C.3.1) by I - H1 yields
(I - H1 )y = (I - H1 )X11 + (I - H1 )X22 + (I - H1 )e

However,

(I - H1 ) X1 = X1 - H1X1 = X1 - X1 (X1X1 )-1 X1X1 = X1 - X1 = 0

Thus,

(I - H1 )y = (I - H1 )X22 + (I - H1 )e

Let y* = (I - H)y, X*2 = (I - H1 ) X2 , and * = (I - H1). We observe that

Var (e*) = Var[(I - H1 )e ] =  2 [I - H1 ]

For this particular case, the ordinary least-squares estimate of 2 is the same as the generalized least squares estimate. We leave the proof to the reader. The appropriate estimate of 2 is

^ 2 = (X*2 X*2 )-1 X*2 y*
= [X2 (I - H1 ) (I - H1 ) X2 ]-1 X2 (I - H1 ) (I - H1 ) y
However, I - H1 is symmetric and idempotent. As a result,

^2 = [X2 (I - H1 ) X2 ]-1 X2 (I - H1 ) y

It can be shown that

( ) Var ^2 =  2 [X2 (I - H1 ) X2 ]-1

As a result,

^

2 j

( ) ( ) Var ^ j

= ^2 Var

^ 2

-1 ^ 2

=1 2

y (I - H1 ) X2 [X2 (I - H1 ) X2 ]-1 X2 (I - H1 ) y

(C.3.2)

Recall from Section C.2.1 (13),

GAUSS­MARKOV THEOREM, VAR() = 2I

587

(X  X )-1

=

(X1X1 )-1
 

+

(X1X1 )-1 X1X2GX2 X1 (X1X1 )-1 -GX2 X1 (X1X1 )-1

- (X1X1 )-1 X1X2G



G



where H1 = X1 (X1X1 )-1 X1 and G = [X2 (I - H1 ) X2 ]-1. As a result,

X

(X  X )-1

X

=

[X1X2

]

(X1X1
 

)-1

+

(X1X1 )-1 X1X2GX2 X1 -GX2 X1 (X1X1 )-1

( X 1 X 1

)-1

- (X1X1 )-1 X1X2G X1 

G

 

X

2



= H1 + H1X2GX2 H1 - X2GX2 H1 - H1X2GX2 + X2GX2

= H1 - (I - H1 ) X2GX2 (I - H1 )

As a result,

X (XX)-1 X - X1 (X1X1 )-1 X1 = (I - H1 ) X2GX2 (I - H1 )

Consequently,

1 2

y

X (XX)-1 X

-

X1 (X1X1 )-1 X1  y

=

1 2

y (I

-

H1 ) X2GX2 (I

-

H1 )y

=

1 2

y (I

-

H1 ) X2 [X2 (I

-

H1 ) X2 ]-1 (I

-

H1 )y

which is exactly (C.3.2). Thus, the square of the t test for an individual coefficient is exactly the same F statistic based on the extra-sum-of-squares principle.

C.4 GAUSS­MARKOV THEOREM, VAR() =  2I
The Gauss­Markov Theorem establishes that the ordinary least-squares (OLS)
estimator of , b^ = (XX)-1 Xy, is BLUE (best linear unbiased estimator). By best,
we mean that b^ has the smallest variance, in some meaningful sense, among the class of all unbiased estimators that are linear combinations of the data. One problem is that b^ is a vector; hence, its variance is actually a matrix. Consequently, we seek to show that b^ minimizes the variance for any linear combination of the estimated coefficients, b^ . We note that
( ) ( ) Var b^ = Var b^
=   2 (XX)-1  =  2  (XX)-1
which is a scalar. Let b be another unbiased estimator of  that is a linear
( ) combination of the data. Our goal, then, is to show that Var b   2  (XX)-1 ( ) with at least one such that Var b   2  (XX)-1 .

588 APPENDIX C
We first note that we can write any other estimator of  that is a linear combination of the data as
b = (XX)-1 X + B y + b0
where B is a p × n matrix and b0 is a p × 1 vector of constants that appropriately adjusts the OLS estimator to form the alternative estimate. We next note that if the model is correct, then
( ) ( ) E b = E (XX)-1 X + B y + b0
= (XX)-1 X + B (y) + b0 = (XX)-1 X + B Xb + b0 = (XX)-1 XXb + BXb + b0
= b + BXb + b0
Consequently, b is unbiased if and only if both b0 = 0 and BX = 0. The variance of b is
( ) ( ) Var b = Var (XX)-1 X + B y
= (XX)-1 X + B Var (y) (XX)-1 X + B  = (XX)-1 X + B 2I (XX)-1 X + B  =  2 (XX)-1 X + B X (XX)-1 + B =  2 (XX)-1 X + BB
because BX = 0, which in turn implies that (BX) = XB = 0. As a result,
Var( b) = Var(b) ( ) =   2 (XX)-1 X + BB
=  2  (XX)-1 +  2 BB
( ) = Var b +  2 BB

We first note that BB is at least a positive semidefinite matrix; hence, 2 BB  0. Next note that we can define * = B . As a result,

p

 BB = * * =

*2 i

i=1

which must be strictly greater than 0 for some  0 unless B = 0. Thus, the OLS estimate of  is the best linear unbiased estimator.

COMPUTATIONAL ASPECTS OF MULTIPLE REGRESSION

589

C.5 COMPUTATIONAL ASPECTS OF MULTIPLE REGRESSION

In this section we briefly outline an important computational procedure for solving the least-squares regression problem. The least-squares criterion is
min S (b ) = (y - Xb ) (y - Xb ) b

and recall from Section 3.2.2 that the least-squares solution vector is normal to the p-dimensional estimation space. Since the Euclidean norm is invariant under an orthogonal transformation, an equivalent formulation of the least-squares problem is

min S (b ) = (Qy - QXb ) (Qy - QXb ) b

(C.5.1)

where Q is an n × n orthogonal matrix. Now Q may be chosen so that

QX

=

R  0 

where R is a p × p upper triangular matrix (i.e., a matrix with zeros below the main diagonal). If we let

Qy

=

q1 q2

 

=

Q1y  Q2 y

where Q1 is a p × n matrix consisting of the first p rows of Q, Q2 is an (n - p) × n matrix consisting of the last n - p rows of Q, and q1 is a p × 1 column vector, then the solution to (C.5.l) satisfies

Rb^ = q1

(C.5.2)

or

b^ = R-1q1 = R-1Q1y

(C.5.3)

One advantage of this approach is that we can obtain a numerically stable inverse of R by the method of back substitution. To illustrate, suppose that R and q1 = Q1y are, for p = 3,

3 1 2

 3

R = 0 3 1 , q1 = Q1y = -1

0 0 2

 4

The equations (C.5.2) are

3 0 0

1 3 0

2 1 2

^^01 ^ 2

    

=

 3 -1  4

590 APPENDIX C

and the system of equations that must actually be solved is

3^0 + 1^1 + 2^2 = 3
3^1 + 1^2 = -1
2^2 = 4
From the bottom equation, we have 2^2 = 4 or ^2 = 2. Substituting in the
equation directly, it yields 3^1 + 1(2) = -1 or ^1 = -1. Finally, the first equation gives
3^0 + 1^1 + 22 = 3 or ^0 = 0 . Algorithms for computing the QR decomposition are described by Golub [1969],
Lawson and Hanson [1974], and Seber [1977]. The (XX)-1 matrix can be found directly from the QR factorization. Since

QX

=

R  0 

then

X

=

Q

R  0 

=

Q1 R

Consequently, since Q1Q1 = I,

(XX)-1 = (RQ1Q1R)-1 = (RR)-1 = R-1 (R)-1

(C.5.4)

This decomposition also leads to efficient computation of the elements of the hat matrix, which we have seen to be useful in several respects. Note that

H = X (XX)-1 X = Q1RR-1 (R)-1 RQ1 = Q1Q1

(C.5.5)

Therefore, the main diagonal elements of the hat matrix may be formed as the sums of squares of the rows of Q1. Thus, we may easily compute many important regression diagnostic statistics, such as the studentized residuals and Cook's distance measure. Belsley, Kuh, and Welsch [1980] show how a number of regression diagnostics may be computed using these ideas.

C.6 RESULT ON THE INVERSE OF A MATRIX

The result given in this section is the Sherman­Morrison­Woodbury theorem (or the Woodbury matrix identity). It is used in obtaining the computational form of the PRESS statistic and several influence diagnostics. Consider the p × p matrix XX and let x be the ith row of X. Note that XX - xx is the XX matrix with the ith row removed. The result is

(XX

-

xx  )-1

=

(X  X )-1

+

(XX)-1 xx (XX)-1 1 - x (XX)-1 x

(C.6.1)

DEVELOPMENT OF THE PRESS STATISTIC

591

This result can be proved by multiplying the right-hand side by XX - xx to give an identity matrix as follows:

(X  X )-1


+

(XX)-1 xx 1- x(X

(X  X )-1 X)-1 x

  

(X

X

-

xx)

=

I

+

1

(XX)-1 xx - x (XX)-1

x

-

(X  X )-1

xx

-

1

(X
- x

X)-1 xx (X  X )-1

x

(X

X)-1

xx

( ) (XX)-1 xx - (XX)-1 xx
= I+

1 - x (XX)-1 x - (XX)-1 x x (XX)-1 x x 1 - x (XX)-1 x

=

I

-

(X

X)-1

xx

-

(X  X )-1

xx



+

(X

X)-1 xx x (XX)-1 1 - x (XX)-1 x

x

-

(X

X)-1

xx

x

(X  X )-1

x

=I

Note that we can write the result (C.6.1) as

[ ] X(i)X(i)

-1

= (XX)-1 + (XX)-1 xixi(XX)-1
1 - hii

(C.6.2)

since hii = xi(XX)-1 xi and X(i) represents the original X matrix with the ith row xi
withheld.

C.7 DEVELOPMENT OF THE PRESS STATISTIC

We have used the PRESS statistic as a measure of regression model validity and potential performance in prediction. Recall that e(i) = yi - y^(i) is the PRESS residual, where y^(i) is the predicted value obtained from a model fit with the ith observation
withheld. Then

n

n

 [ ] PRESS = e(2i) =

yi - y^(i) 2

i=1

i=1

(C.7.1)

It would initially seem that calculating PRESS requires fitting n different regres-
sions. However, it is possible to calculate PRESS from the results of a single leastsquares fit to all n observations. To see how this is accomplished, let b^(i) be the vector of regression coefficients obtained by withholding the ith observation. Then

[ ] b^(i) = X(i)X(i) -1 X(i)y(i)

(C.7.2)

where X(i) and y(i) are the X and y vectors with the ith observation withheld. Thus, the ith PRESS residual may be written as

e(i) = yi - y^(i) = yi - xib^(i)
( ) = yi - xi X(i)X(i) -1 X(i)y(i)

592 APPENDIX C

There is a close connection between the (XX)-1 and [X(i)X(i)]-1 matrices; specifically, from Eq. (C.6.2),

[ ] X(i)X(i)

-1

= (XX)-1 + (XX)-1 xixi(XX)-1
1 - hii

where hii = xi(XX)-1 xi. Using Eq. (C.7.3), we may write

e(i)

=

yi

-

xi (XX)-1


+

(XX)-1 xixi(XX)-1
1 - hii

  X(i)y(i) 

=

yi

-

xi(XX)-1 X(i)y(i)

-

xi(XX)-1 xi xi(XX)-1 X(i)y(i)
1 - hii

= (1 - hii ) yi - (1 - hii ) xi(XX)-1 X(i)y(i) - hii xi(XX)-1 X(i)y(i)
1 - hii

= (1 - hii ) yi - xi(XX)-1 X(i)y(i)
1 - hii

Since Xy = X(i)y(i) + xi yi, this last equation becomes

(C.7.3)

e(i)

=

(1 -

hii ) yi

-

xi(XX)-1 (Xy
1 - hii

-

xi yi

)

= (1 - hii ) yi - xi(XX)-1 Xy + xi(XX)-1 xi yi
1 - hii

= (1 - hii ) yi - xib^ + hii yi
1 - hii

= yi - xib^ 1 - hii

(C.7.4)

Now the numerator of Eq. (C.7.4) is the ordinary residual ei from a least-squares fit to all n observations, so the ith PRESS residual is

e(i)

=

ei 1 - hii

(C.7.5)

Thus, since PRESS is just the sum of the squares of the PRESS residuals, a simple computing formula is

 PRESS =

n i=1

 

1

ei - hii

 

2

(C.7.6)

In this form, it is easy to see that PRESS is just a weighted sum of squares of the residuals, where the weights are related to the leverage of the observations. PRESS weights the residuals corresponding to high-leverage observations more severely than the residuals from less influential points.

DEVELOPMENT OF S(2i)

593

C.8 DEVELOPMENT OF S(2i )

In Chapter 4 we presented an expression for the residual mean square in a regres-
sion model with the ith observation withheld. The resulting quantity, S(2i), is used in computing R-student. This computing formula for S(2i) may be derived by starting with the Sherman­Morrison­Woodbury identity from Section C.6:

[ ] X(i)X(i)

-1

= (XX)-1 + (XX)-1 xixi(XX)-1
1 - hii

If we postmultiply both sides by Xy - xiyi, we obtain

b^(i)

=

b^

- (XX)-1 xi yi

+

(XX)-1 xixi(XX)-1 (Xy
1 - hii

-

xi yi

)

which reduces to

Now

b^

-

b^(i)

=

(XX)-1 xiei
1 - hii

(C.8.l)

( ) (n - p - 1)S(2i) =

yj - xj b^(i) 2

ji

and after using Eq. (C.8.l), this becomes

(C.8.2)

( )  ji

yj

- xj b^(i)

2

=

n j=1

 

yi

-

xj b

+

xj (XX)-1
1 - hii

x i ei

 

2

-

 

yi

-

xib^

+

hii ei 1 - hii

 

2

 =

n

 

ej

j=1

+

hii ei 1 - hii

 

2

-

(1

ei2 - hii

)2

(C.8.3)

When we expand the first term on the right-hand side of Eq. (C.8.3), we obtain

    n
j=1

 

e

j

+

hii ei 1 - hii

2 

=

n j=1

e

2 j

+

2ei 1 - hii

n j=1

ej hij

-

ei2
(1 - hii )2

n
hi2j
j=1

However,

since

Hy = Hy^,



n j=

1

ej

hij

= 0.

Also,

since

H

is

idempotent,



n j=

1

hi2j

= hii.

Therefore, Eq. (C.8.2) can be written as

 (n - p - 1)S(2i)

=

n j=1

e

2 j

+

hii ei2
(1 - hii

)2

-

(1

ei2 - hii

)2

 =

n j=1

e

2 j

-

ei2 1 - hii

=

(n

-

p) MSRes

-

ei2 1 - hii

594 APPENDIX C

Finally, we obtain the result in Eq. (4.12), that is,

S(2i)

= (n - p) MSRes - ei2
n- p-1

(1 - hii )

C.9 OUTLIER TEST BASED ON R-STUDENT

A common way to model an outlier is the mean shift outlier model. Suppose we fit the model y = X +  when the true model is
y = Xb + d + e

where  is an n × 1 vector of zeros except for the uth observation, which has a value of u. Thus,

0





0

d

=

u

 

0





 0 

For both the model we fit and the mean shift outlier model, assume that E()  N(0,  2I). Our goal is to find an appropriate test statistic for the hypotheses
H0: u = 0, H0: u  0
This procedure assumes that we are specifically interested in the uth observation, that is, that we have a priori information that the uth observation may be an outlier.
The first step is to find an appropriate estimate of u. A logical candidate is the uth residual. Let e = [I - X(XX)-lX]y be the n × 1 vector of residuals. The expected value of e is
( ) E (e) = E I - X (XX)-1 X y
= I - X (XX)-1 X E (y) = I - X (XX)-1 X[Xb + d ] = I - X (XX)-1 X Xb + I - X (XX)-1 X d = [X - X] b + I - X (XX)-1 X d = I - X (XX)-1 X d
Thus,

OUTLIER TEST BASED ON R-STUDENT

595

E (eu ) = (-huu )u

where h uu is the uth hat diagonal or the uth diagonal element of X(XX)-lX. Consequently, an unbiased estimator of u is

^u

=

eu 1 - huu

In Chapter 4, we showed that ^u is simply the uth PRESS residual. The next step is to determine the variance of our estimator. We note that

( ) Var (e) = Var I - X (XX)-1 X y
= I - X (XX)-1 X 2I I - X (XX)-1 X  =  2 I - X (XX)-1 X I - X (XX)-1 X =  2 I - X (XX)-1 X

Thus, Var(eu) = (1 - huu)2. The variance of ^u then is

Var

 

eu 1 - huu

 

=

1
(1 - huu )2

Var (eu )

=

(1 - huu ) 2 (1 - huu )2

= 2 1 - huu

We next note that e is a linear combination of y. Thus, e is a linear combination of normally distributed random variables. As a result, e follows a normal distribution, as does ^u. Consequently, under H0: u = 0,

( ) eu (1 - huu ) =

eu

 1 - huu  1 - huu

follows a standard normal distribution. We note that this quantity is simply an example of a studentized residual, as we saw in Chapter 4. In general, 2 is unknown. We have seen that MSRes is an unbiased estimator of 2. Further, we have seen that
MSRes 2
is a 2 random variable divided by its degrees of freedom. As a result, a candidate test statistic is

eu
MSRes (1 - huu )
which follows a t distribution if e = [I - X(XX)-1X]y and SSRes = y[I - X(XX)-1X]y are independent. We can show that e and SSRes are independent if

596 APPENDIX C

I - X (XX)-1 X 2I I - X (XX)-1 X = 0
Unfortunately,
I - X (XX)-1 X 2I I - X (XX)-1 X =  2 I - X (XX)-1 X  0
The problem is that

n
 SSRes = ee = ei2 i=1
which means that SSRes is correlated with each individual residual because the square of each individual residual is a component of SSRes. In Section C.8 we developed an estimate of 2 with the uth observation deleted. This estimate is independent of eu by the basic independence assumption on our random errors. As a result, an appropriate test statistic for the mean shift outlier model is

eu S(u) 1 - huu
which is the externally studentized residual or R-student. Under H0: u = 0, this statistic follows the central tn-p-1 distribution, and under H0: u  0, this statistic follows the tn- p-1, distribution, where

( )  = 

u

= i 1 - huu

1 - huu



It is important to note that the power of this test depends on huu. Recall that if we fit an intercept to our model, then 1/n huu  1. Maximum power occurs when huu = 1/n, which is at the center of the data cloud in terms of the X's. As huu  1, the power goes to 0. In other words, this test has less ability to detect outliers at the
high-leverage data points.

C.10 INDEPENDENCE OF RESIDUALS AND FITTED VALUES

We know that y^ = Xb^ = X (XX)-1 Xy = Hy and e = y - y^ = (I - H) y . Furthermore,
we assume that y  N(X, 2I). To prove that the residuals and fitted values are independent, from the new vector

y^  e

=

 I

H - H

y

=

My

Because y is multivariate normal, the new vector My is also multivariate normal. The expected value of My is

GAUSS-MARKOV THEOREM, VAR() = V

597

E

(My)

=

ME

(y)

=

 I

H - H

Xb

=

Xb

 

0

  

The covariance matrix of My is

Var (My) = M Var (y)M

=



2

 I

H -H

  

[H

I - H]

=



2


(I

HH
- H)

H

H(I - H)  (I - H)(I - H)

=



2

H

 

0

0 I - H

Because all of the covariances between y^ and e are zero and the random variables y^ and e are jointly normally distributed, the fitted values and the residuals are
independent.

C.11 GAUSS-MARKOV THEOREM, VAR() = V
The Gauss-Markov theorem establishes that the generalized least-squares (GLS)
estimator of , b^ ( ) = XV-1X -1 XV-1y, is BLUE (best linear unbiased estimator).
Once again, by best, we mean b^ minimizes the variance for any linear combination of the estimated coefficients, b^ . We note that if the model is correct,
E (X ) V-1X -1 XV-1y = ( ) XV-1X -1 XV-1E (y) ( ) = XV-1X -1 XV-1Xb = b
Thus, (XV-1X)-lXV-ly is an unbiased estimator of . The variance of this estimator is
Var (XV ) -1X -1 XV-1y = (XV ) -1X -1 XV-1  Var (y) (XV-1X)-1 XV-1   ( ) ( ) =  XV-1X -1 XV-1  V  XV-1X -1 XV-1   ( ) ( ) =  XV-1X -1 XV-1  V V-1X XV-1X -1  = (XV ) -1X -1

Thus,

( ) ( ) Var b^ =  Var b^ =  (XV ) -1X -1 

598 APPENDIX C
Let b be another unbiased estimator of  that is a linear combination of the data.
( ) Our goal, then, is to show that Var b   (XV-1 )X -1 with at least one such ( ) that Var b >  (XV-1 )X -1 .
We first note that we can write any other estimator of  that is a linear combination of the data as
b = (XV ) -1X -1 XV-1 + B y + b0
where B is an p × n matrix and b0 is a p × 1 vector of constants that appropriately adjusts the GLS estimator to form the alternative estimate. We next note that if the model is correct, then
( ) ( ) E b = E ( )  XV-1X -1 XV-1 + B y + b0
= (X ) V-1X -1 XV-1 + B E (y) + b0 = (X ) V-1X -1 XV-1 + B Xb + b0 ( ) = XV-1X -1 XV-1Xb + BXb + b0
= b + BXb + b0
Consequently, b is unbiased if and only if both b0 = 0 and BX = 0. The variance of b is
( ) ( ) Var b = Var (X ) V-1X -1 XV-1 + B y
= (X ) V-1X -1 XV-1 + B Var (y) (X ) V-1X -1 XV-1 + B  = (X ) V-1X -1 XV-1 + B V (X ) V-1X -1 XV-1 + B  = (X ) V-1X -1 XV-1 + B V V-1X ( ) XV-1X -1 + B = (X ) V-1X -1 + BVB
because BX = 0, which in turn implies that (BX) = XB = 0. As a result,
Var( b) = Var(b)
( ) =  (X ) V-1X -1 + BVB
=  (XV-1X)-1 + BVB
( ) = Var b + BVB
We note that V is a positive definite matrix. Consequently, there exists some nonsingular matrix  such that V = . As a result, BVB = BB is at least a positive semidefinite matrix; hence, BVB  0. Next note that we can define * = B . As a result,

BIAS IN MSRES WHEN THE MODEL IS UNDERSPECIFIED

599

p

 BVB = * * =

*2 i

i=1

which must be strictly greater than zero for some  0 unless B = 0. Thus, the GLS estimate of  is the best linear unbiased estimator.

C.12 BIAS IN MSRES WHEN THE MODEL IS UNDERSPECIFIED

In Section C.3.2, we have shown that if the model is correctly specified, then E(MSRes) = 2, and thus, MSRes is an unbiased estimator of 2. Now, suppose we fit the model y = Xpp + , where Xp is the n × p model matrix associated with p, the vector of parameters we fit. Suppose further that the actual model is

y = Xpbp + Xrbr + e + [Xp

Xr

]

 

bp br

 

+

e

where Xr is the model matrix associated with r, the vector of important terms we
did not fit, for whatever reason. For both models we assume that E() = 0 and
Var() =  2I. In Section 10.1.2, we showed for this situation that b^ p = (XpX p )-1 Xpy
is a biased estimator of p. Consider the expected value of SSRes, which is

( ) E (SSRes ) = E y I - X p (XpX p )-1 Xp  y ( ) = trace I - X p (XpX p )-1 Xp  2I

+

[

b

p

br

]

 

X p Xr

 



I

-

X

p

(

X

p

X

p

)-1

X

p



[X

p

X

r

]

 

bp br

 

=  2 trace I - X p (XpX p )-1 Xp 

+

[

b

p

br

]

  

Xp - XpX p XrXr - XrX

(Xp p(X

Xp p X

)-1 Xp )p -1 X

p

  

[X

p

Xr

]

 

bp br

 

=

(n

-

p)

2

+

[

b

p

b

r

]

  

Xr

X

r

-

0
XrX p (XpX

p

)-1

X p

 

[X



p

X

r

]

 

bp br

 

=

(n

-

p)

2

+

[

b

p

b

r

]

0 

0

0

 bp 

X r X r

-

XrX p (XpX p )-1 XpXr

 

 br



= (n - p) 2 + br XrXr - XrX p (XpX p )-1 XpXr  br

The expected value for MSRes in this situation is

E (MSRes )

=

E

 

SSRes  n - p 

= 2

+

br XrXr

- XrX p (XpX p )-1 XpXr  br
n- p

As a result, MSRes is not an unbiased estimator of 2 when the model is underspecified. The bias is

600 APPENDIX C

br XrXr - XrX p (XpX p )-1 Xp  br
n- p

C.13 COMPUTATION OF INFLUENCE DIAGNOSTICS
In this section we will develop the very useful computational forms of the influence diagnostics DFFITS, DFBETAS, and Cook's D given initially in Chapter 6.

C.13.1 DFFITSi Recall from Eq. (6.9) that

DFFITSi

=

y^i - y^(i) S(2i ) hii

,

i = 1, 2, ... , n

Also, from Section C.8, we have

b^i

-

b^(i)

=

(XX)-1 xiei
1 - hii

Multiplying both sides of Eq. (C.13.2) by xi produces

y^ i

-

y^ (i )

=

hii ei 1 - hii

Dividing both sides of Eq. (C.13.3) by S(2i)hii will produce DFFITSi:

DFFITSi

=

y^i - y^(i) S(2i ) hii

=

hii ei 1 - hii

 1 1 2

 

S(2i ) hii

 

=

S(2i)

ei
(1

-

hii

)

 

1

hii - hii

 

1

2

=

ti

 

1

hii - hii

 

1

2

where ti is R-student.

(C.13.1) (C.13.2) (C.13.3)
(C.13.4)

C.13.2 Cook's Di

We may use Eq. (C.13.2) to develop a computational form for Cook's Di. Recall that Cook's Di statistic is

( ) ( ) b^i - b^(i) XX b^i - b^(i)

Di =

pMSRes

, i = 1, 2, ... , n

(C.13.5)

Using Eq. (C.13.2) in Eq. (C.13.5), we obtain

GENERALIZED LINEAR MODELS

601

Di

=

xi(XX)-1 (XX)(XX)-1 xiei2 (1 - hii )2 pMSRes

=

 

ei 1 - hii

2 

 

hii pMSRes

 

=

ri2 p

 

hii 1 - hii

 

where ri is the studentized residual.

C.13.3 DFBETASj,i The DFBETASj,i statistic is defined in Eq. (6.7) as

DFBETASj,i

=

 j -  j(i) S(2i)C jj

Thus, DFBETASj,i is just the jth element of b^ - b^(i) in Eq. (C.13.2) divided by a standardization factor. Now

^ j

- ^ j(i)

=

rj,i ei 1 - hii

and recall that R = (XX)-1X, so that

(C.13.6)

(RR) = (XX)-1 XX (XX)-1   = (XX)-1 = C = RR
Therefore, Cjj = rjrj , so we may write the standardization factor

S(2i)C jj = S(2i)rjrj

Finally, the computation form of DFBETASj,i is

DFBETASj,i

=

^ j - ^ j(i) S(2i)C jj

=

 rj,iei  1 - hii

 

1= S(2i ) rjrj

rj,i rjrj

ti 1 - hii

where ti is R-student.

C.14 GENERALIZED LINEAR MODELS

C.14.1 Parameter Estimation in Logistic Regression The log-likelihood for a logistic regression model was given in Eq. (14.8) as

n

n

  ln L(y, b ) = yixib - ln[1 + exp(xib )]

i=1

i=1

602 APPENDIX C

In many applications of logistic regression models we have repeated observations or trials at each level of the x variables. Let yi represent the number of 1's observed for the ith observation and ni be the number of trials at each observation. Then the log-likelihood becomes

n

n

n

   ln L(y, b ) = yii - ni ln (1 - i ) - yi ln (1 - i )

i=1

i=1

i=1

The maximum-likelihood estimates (MLEs) may be computed using an iteratively reweighted least-squares (IRLS) algorithm. To see this recall that the MLEs are the solutions to

L = 0 b

which can be expressed as

Note that and

L i = 0 i b

   L = n ni - n

ni

n
+

yi

i i=1 i i=1 1 - i i=1 1 - i

 i b

=

  

1

exp (xib ) + exp (xib

)

-

 

1

exp (xib ) + exp (xib

)

2 

  

x

i

Putting this all together gives

   L
b

=

  

n i=1

ni i

-

n i=1

ni 1 - i

+

n i=1

1

yi -

i

  



i

(1

-



i

)

xi

 =

n i=1

 yi   i

- ni 1-i

+

1

yi -

i

 



i

(1

-



i

)

xi

n
 = (yi - nii ) xi

i=1

Therefore, the maximum-likelihood estimator solves

X(y - m) = 0

where y = [y1, y2, . . . , yn] and  = [n11, n22, . . . , nnn]. This set of equations is often called the maximum-likelihood score equations. They are actually the same form of

GENERALIZED LINEAR MODELS

603

the normal equations that we have seen previously for linear least squares, because in the linear regression model, E(y) = X =  and the normal equations are
XXb^ = Xy

which can be written as

X (y - Xb ) = 0 X(y - m) = 0

The Newton-Raphson method is actually used to solve the score equations for the logistic regression model. This procedure observes that in the neighborhood of the solution, we can use a first-order Taylor series expansion to form the approximation

pi

- i



 

 i b

 



(b*

-

b)

(C.14.1)

where

pi

=

yi ni

and * is the value of  that solves the score equations. Now i = xib, and

i b

= xi

We note that By the chain rule

i

=

exp (i ) 1 + exp (i )

 i b

=

 i i

i b

=

 i i

xi

Therefore, we can rewrite Eq. (C.14.1) as

pi

- i



 

 i i

 

xi(b* -

b)

pi

-i



 

 i i

 

(xib* - xib )

pi

- i



 

 i i

 

(i*

- i )

(C.14.2)

604 APPENDIX C

where i* is the value of i evaluated at *. We note that
(yi - nii ) = (ni pi - nii ) = ni ( pi - i )

and since

i

=

exp (i ) 1 + exp (i )

we can write

 i b

=

1

exp (i ) + exp (i

)

-

 

1

exp (i ) + exp (i

)

2 

Consequently,

yi - nii  [nii (1 - i )] = (i* - i )

Now the variance of the linear predictor i* = xib* is, to a first approximation,

Var (i* )



1
nii (1 - i )

Thus,

yi

-

ni i



  

1
Var (i*

)

  

(i*

- i )

=

0

and we may rewrite the score equations as

n
i=1

1
 Var (i

)

 

(i*

-

i

)

=

0

or, in matrix notation,

XV-1 (h* - h) = 0

where V is a diagonal matrix of the weights formed from the variances of the i. Because  = X we may write the score equations as
XV-1 (h* - Xb ) = 0
and the maximum-likelihood estimate of  is
( ) b^ = XV-1X -1 XV-1h*

GENERALIZED LINEAR MODELS

605

However, there is a problem because we do not know *. Our solution to this problem uses Eq. (C.14.2):

pi

- i



 

 i i

 

(i*

- i )

which we can solve for i*,

i*



i

+

( pi

- i

)

i  i

Let zi = i + (pi - i)(i) (i) and z = [z1, z2, . . . , zn]. Then the Newton-Raphson estimate of  is

( ) b^ = XV-1X -1 XV-1z

Note that the random portion of zi is

( pi

-

i

)

i  i

Thus,

Var

( pi

-

i

)

i  i

 

=

i 

(1 -
ni

i

)
 

i  i

2 

=

i 

(1 -
ni



i

)

 

 



i

1
(1 -

i


)

2

=

ni i

1
(1 - i

)

So V is the diagonal matrix of weights formed from the variances of the random part of z. Thus, the IRLS algorithm based on the Newton­Raphson method can be described as follows:
1. Use ordinary least squares to obtain an initial estimate of , say b^0. 2. Use b^0 to estimate V and . 3. Let h0 = Xb^0. 4. Base z1 on 0. 5. Obtain a new estimate b^1, and iterate until some suitable convergence criterion
is satisfied.

C.14.2 Exponential Family
It is easy to show that the normal, binomial, and Poisson distributions are members of the exponential family. Recall that the exponential fantily of distributions is defined by Eq. (13.48), repeated below for convenience:

606 APPENDIX C
f (yi, i, ) = exp {[yii - b(i )] a() + h (yi, )}

1. The Normal Distribution

f

( yi ,

i,

)

=

1 2 2

exp 

-

1 2 2

(y

-

)2 

=

exp - ln(2 2 ) -

y2 2 2

+

y 2

-

2 2 2

 

=

exp

 

1 2

 

-

y2 2

+

y

-

2 2

 

-

1 2

ln(2 2 )

=

exp

 

1 2

 

y

-

2 2

 

-

y2 2 2

-

1 2

ln(2 2 )

Thus, for the normal distribution, we have

i = ,

b ( i

)

=

2 2

,

a() =  2

h (yi,

)

=

-

y2 2 2

-

1 ln(2 2 )
2

E (y) = db(i ) = ,
di

Var

(y)

=

d 2 b ( i

d

2 i

)

a ( )

=

2

2. The Binomial Distribution

f

(

yi,

i,



)

=

 

n m



y

(1

-



)n-

y

=

exp

ln 

 

n y

+

y

ln 

+

(n

-

y)

ln

(1

-



)


=

exp

ln 

 

n y

+

y

ln



+

n

ln

(1

-



)

-

y

ln

(1

-



)


=

exp

 y 

ln



1

 -





+

n

ln

(1

-



)

+

ln

 

n y 

  

Therefore, for the binomial distribution,

i

=

ln

 

1

 -



 

,



=

exp (i ) 1 + exp (i

)

GENERALIZED LINEAR MODELS

607

b(i ) = n ln (1 -  ),

a() = 1,

h

(

yi,



)

=

ln

 

n y

E (y) = db(i ) = db(i ) d

di

d di

We note that

d di

=

1

exp (i ) + exp (i

)

-

 

1

exp (i ) + exp (i

)

2 

=  (1 -  )

Therefore,

E

(y)

=



1

n -





(1

-



)

=

n

We recognize this as the mean of the binomial distribution. Also,

Var (y) = dE (y) = dE (y) d = n (1 -  )

di

d di

This last expression is just the variance of the binomial distribution. 3. The Poisson Distribution

f (yi, i, ) =

 ye- y!

= exp[y ln  -  ln (y!)]

Therefore, for the Poisson distribution, we have
i = ln () and  = exp(i ) b(i ) =  a() = 1
h(yi, ) = - ln (y!)
Now

E (y) = db(i ) = db(i ) d

di

d di

However, since

d di

= exp (i ) = 

608 APPENDIX C the mean of the Poisson distribution is
E (y) = 1 = 

The variance of the Poisson distribution is

Var (y) = dE (y) = 
di

C.14.3 Parameter Estimation in the Generalized Linear Model
Consider the method of maximum likelihood applied to the GLM, and suppose we use the canonical link. The log-likelihood function is

n
[yii - b(i )]

(y, b ) = i=1 a()

+ h (yi, )

For the canonical link, we have i = g [E (yi )] = g (i ) = xib ; therefore,

  
b

=

 i

i b

=

1
a ( )

n i=1

 

yi

-

db ( i
di

)

 

xi

=

1
a ( )

n i=1

( yi

- i )xi

Consequently, we can find the maximum-likelihood estimates of the parameters by solving the system of equations

 1
a ( )

n i=1

( yi

- i )xi

=

0

In most cases, a() is a constant, so these equations become

n
 (yi - i ) xi = 0
i=1
This is actually a system of p = k + 1 equations, one for each model parameter. In matrix form, these equations are
X(y - m) = 0

where  = [1, 2, . . . , p]. These are called the maximum-likelihood score equations, and they are just the same equations that we saw previously in the case of logistic regression, where  = [n11, n22, . . . , nnn].
To solve the score equations, we can use IRLS, just as we did in the case of logistic regression. We start by finding a first-order Taylor series approximation in the neighbothood of the solution

GENERALIZED LINEAR MODELS

609

( ) yi

- i



di di

i* - i

Now for a canonical link i = i, and

( ) yi

- i



di di

i* - i

Therefore, we have

(C.14.3)

i*

- i



( yi

-

i

)

di di

This expression provides a basis for approximating the variance of ^ i. In maximum-likelihood estimation, we replace i by its estimate, ^ i. Then we have

Var (i*

-

i

)



Var

( yi

-

i

)

di di

 

Since i* and i are constants,

Var

(^ i

)



 

di di

2 

Var

(

yi

)

But

di di

=

1
Var (i )

where Var(yi) = Var(i)a(). Consequently,

Var

(^ i

)



 

1
Var (i

2
) 

Var

(i

) a ( )



1
Var (i

)

a ( )

For convenience, define Var(i) = [Var(i)]-1, so we have

Var (^i )  Var (^i )a()

Substituting this into Eq. (C.14.3) results in

yi

-

i



1
Var (i

)

(i*

- )

(C.14.4)

If we let V be an n × n diagonal matrix whose diagonal elements are the Var(i), then in matrix form, Eq. (C.14.4) becomes

610 APPENDIX C

y - m  V-1 (h* - h)
We may then rewrite the score equations as follows:
X(y - m) = 0 XV-1 (h* - h) = 0 XV-1 (h* - Xb ) = 0
Thus, the maximum-likelihood estimate of  is
( ) b^ = XV-1X -1 XV-1h*

Now just as we saw in the logistic regression situation, we do not know *, so we pursue an iterative scheme based on

zi

=

^ i

+ (yi

-

^ i

)

di d i

Using iteratively reweighted least squares with the Newton­Raphson method, the solution is found from
( ) b^ = XV-1X -1 XV-1z

Asymptotically, the random component of z comes from the observations yi. The diagonal elements of the matrix V are the variances of the zi's, apart from a().
As an example, consider the logistic regression case:

i

=

ln

 

1

 -

i


i

 

di = di = d ln[i (1 - i )]

di di

d i

=

1 - i i

 i

 

1

-



i

+

i
(1 - i )2

  

=

1-i
i (1 - i

)

1

+

1

i - i

 

=

1 i

1-i +i  1 - i

 

=

1
i (1 - i )

Thus, for logistic regression, the diagonal elements of the matrix V are

 

di di

2 

Var (yi

)

=

1
i (1 - i

2
) 

i

(1 - i
ni

)

=

1
nii (1 - i

)

GENERALIZED LINEAR MODELS

611

which is exactly what we obtained previously. Therefore, IRLS based on the Newton­Raphson method can be described as
follows:
1. Use ordinary least squares to obtain an initial estimate of , say b^0. 2. Use b^0 to estimate V and . 3. Let h0 = Xb^0. 4. Base z1 on 0. 5. Obtain a new estimate b^1, and iterate until some suitable convergence criterion
is satisfied.

If we do not use the canonical link, then i  i, and the appropriate derivative of the log-likelihood is

 = d di di di b di di di b

Note that:

1.

d di

=

1
a ( )

 

yi

-

db ( i
di

)


=

1
a ( )

( yi

-

i

)

2.

di di

=

1
var (i

)

3.

di b

= xi

Putting this all together yields

 b

=

yi - i
a ( )

1 di
Var (i ) di

xi

Once again, we can use a Taylor series expansion to obtain

( ) yi

- i



di di

i* - i

Following an argument similar to that employed before,

Var

(^ i

)



 

di di

2 

Var

(

yi

)

and eventually we can show that

612 APPENDIX C

 
b

=

n i=1

a

i* - i
() Var (i

)

x

i

Equating this last expression to zero and writing it in matrix form, we obtain

XV-1 (h* - h) = 0

or, since  = X,

XV-1 (h* - Xb ) = 0

The Newton­Raphson solution is based on
( ) b^ = XV-1X -1 XV-1z

where

zi

=

^ i

+ (yi

-

^ i

)

di d i

Just as in the case of the canonical link, the matrix V is a diagonal matrix formed from the variances of the estimated linear predictors, apart from a().

APPENDIX D
INTRODUCTION TO SAS
D.l Basic Data Entry D.2 Creating Permanent SAS Data Sets D.3 Importing Data from an EXCEL File D.4 Output Command D.5 Log File D.6 Adding Variables to an Existing SAS Data Set One of the hardest parts about learning SAS is creating data sets. For the most part, this appendix deals with data set creation. It is vital to note that the default data set used by SAS at any given time is the data set most recently created. We can specify the data set for any SAS procedure (PROC). Suppose we wish to do multiple regression analysis on a data set named delivery. The appropriate PROC REG statement is proc reg data=delivery; We now consider in more detail how to create SAS data sets.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
613

614 APPENDIX D
D.1 BASIC DATA ENTRY
A. Using the SAS Editor Window The easiest way to enter data into SAS is to use the SAS Editor. We will use the delivery time data, given in Table 3.2 as the example throughout this appendix.
Step 1: Open the SAS Editor Window The SAS Editor window opens automatically upon starting the Windows or UNIX versions of SAS.
Step 2: The Data Command Each SAS data set requires a name, which the data statement provides. This appendix uses a convention whereby all capital letters within a SAS command indicates a name the user must provide. The simplest form of the data statement is
data NAME;
The most painful lesson learning SAS is the use of the semicolon (;). Each SAS command must end in a semicolon. It seems like 95% of the mistakes made by SAS novices is to forget the semicolon. SAS is merciless about the use of the semicolon! For the delivery time data, an appropriate data command is
data delivery;
Later, we will discuss appropriate options for the data command.
Step 3: The Input Command The input command tells SAS the name of each variable in the data set. SAS assumes that each variable is numeric. The general form of the input command is
input VARl VAR2 ... ;
We first consider the command when all of the variables are numeric, as in the delivery data from Chapter 2:
input time cases distance;
We designate a variable as alphanumeric (contains some characters other than numbers) by placing a $ after the variable name. For example, suppose we know the delivery person's name for each delivery. We could modify these names through the following input command:
input time cases distance person $;
Step 4: Give the Actual Data We alert SAS to the actual data by either the cards (which is fairly archaic), or the lines commands. The simplest way to enter the data is in space-delimited form. Each line represents a row from Table 3.2. Do not place a semicolon (;) at the end of the data rows. Many SAS users do place a semicolon

BASIC DATA ENTRY 615

on a row unto itself after the data to indicate the end of the data set. This semicolon is not required, but many people consider it good practice. For the delivery data, the actual data portion of the SAS code follows:

cards; 16.68 11.50 12.03 14.88 13.75 18.11
8.00 17.83 79.24 21.50 40.33 21.00 13.50 19.75 24.00 29.00 15.35 19.00
9.50 35.10 17.90 52.32 18.75 19.83 10.75 ;

7

560

3

220

3

340

4

80

6

150

7

330

2

110

7

210

30

1460

5

605

16

688

10

215

4

255

6

462

9

448

10

776

6

200

7

132

3

36

17

770

10

140

26

810

9

450

8

635

4

150

Step 5: Using PROC PRINT to Check Data Entry It is very easy to make mistakes in entering data. If the data set is sufficiently small, it is always wise to print it. The simplest statement to print a data set in SAS is

proc print;

which prints the most recently created data set. This statement prints the entire data set. If we wish to print a subset of the data, we can print specific variables:

proc print; var VAR1 VAR2 ... ;

Many SAS users believe that it is good practice to specify the desired data set. In this manner, we guarantee that we print the data set we want. The modified command is

616 APPENDIX D

proc print data=NAME;

The following command prints the entire delivery data set:

proc print data=delivery;

The following commands print only the times from the delivery data set:

proc print data=delivery; var time;

The run command submits the code. When submitted, SAS produces two files: the output file and the log file. The output file for the delivery data PROC PRINT command follows:

The SAS System

Obs

time

cases

distance

1

16.68

7

2

11.50

3

3

12.03

3

4

14.88

4

5

13.75

6

6

18.11

7

7

8.00

2

8

17.83

7

9

79.24

30

10

21.50

5

11

40.33

16

12

21.00

10

13

13.50

4

14

19.75

6

15

24.00

9

16

29.00

10

17

15.35

6

18

19.00

7

19

9.50

3

20

35.10

17

21

17.90

10

22

52.32

26

23

18.75

9

24

19.83

8

25

10.75

4

560 220 340
80 150 330 110 210 1460 605 688 215 255 462 448 776 200 132
36 770 140 810 450 635 150

The resulting log file follows:

NOTE: Copyright (c) 2002­2003 by SAS Institute Inc., Cary, NC, USA.
NOTE: SAS (r) 9.1 (TS1M2)

BASIC DATA ENTRY 617

Licensed to VA POLYTECHNIC INST &

STATE UNIV-CAMPUSWIDE-IN, Site 0001798011.

NOTE: This session is executing on the WIN_PRO platform

NOTE: SAS initialization used:

real time

19.30 seconds

cpu time

1.56 seconds

1

data delivery;

2

input time cases distance;

3

cards;

NOTE: The data set WORK.DELIVERY has 25 observations and 3

variables.

NOTE: DATA statement used (Total process time):

real time

1.22 seconds

CPU time

0.23 seconds

29 proc print data-delivery;

30 run;

NOTE: There were 25 observations read from the data set

WORK.DELIVERY.

NOTE: PROCEDURE PRINT used (Total process time):

real time

0.55 seconds

cpu time

0.17 seconds

The log file provides a brief summary of the SAS session. It tells the analyst how many observations are in the data set, how many observations have missing data (in this case, there are no missing data), the commands executed, and any errors. The log file is almost essential for debugging SAS code. Section D.5 provides more details about this file.

B. Entering Data from a Text File We can use the infile statement to read data from a text file. The form of this statement is
infile `FULL FILE NAME';
The infile statement requires the full file name, including all path information (all the directories). The full file name must be enclosed by single quotes. Of course, the statement must end in a semicolon (;). The following example has the data in a text file named delivery.txt that is located in the directory
C:\My Stuff\Disk-Books\Regression 5th Ed
of my Windows laptop. UNIX follows a slightly different path convention. The following example illustrates how to use the infile statement for the delivery data:

618 APPENDIX D
data delivery; infile `C:\My Stuff\Disk-Books\Regression 5th Ed\
delivery.txt; input time cases distance;
run;
D.2 CREATING PERMANENT SAS DATA SETS
There are many occasions where we expect to use a single data set many times. For example, many regression courses require projects that involve analyzing a single data set several times over the semester as the students learn more analytical techniques. In such a situation, it is nice to read the data only once and then create a permanent data set that is available for future use.
Step 1: Specify the Directory for the Permanent Data Set We specify the directory for our permanent data set through the libname statement, which has the form
libname NAME1 `FULL DIRECTORY NAME';
NAME-l is the name for the directory that we use purely within the SAS code. FULL DIRECTORY NAME is the actual name of the directory, including the full path information.
Step 2: Use the Data Statement to Create the Data Set The key point is to use the appropriate permanent name for the data set in the data statement. Specifically, suppose that we wish to create a data set named setname and that we named the directory namel. The appropriate name for the permanent SAS data set is namel. setname. The following example creates a SAS data set named book.delivery in the directory; C:\ My Stuff \ Disk-Books \ Regression 5th Ed.
libname book 'c:\My Stuff\Disk-Books\Regression 5th Ed'; data book.delivery;
infile 'C:\My Stuff\Disk-Books\Regression 5th Ed\ delivery.txt';
input time cases distance; run;
The following code illustrates how to use the permanent data set. The libname statement must appear somewhere in the SAS code prior to the data set's use by a procedure:
libname book `c:\My Stuff\Disk-Books\Regression 5th Ed; proc reg data=book.delivery;
model time=cases distance; run;

IMPORTING DATA FROM AN EXCEL FILE

619

The output from this code follows:

The REG Procedure Model: MODELl
Dependent Variable: time

Number of Observations Read

25

Number of Observations Used

25

Analysis of Variance

Sum of

Mean

Source

DF Squares

Square F Value

Model

2 5550.81092 2775.40546 261.24

Error

22 233.73168 10.62417

Corrected Total 24 5784.54260

Pr > F <.0001

Root MSE Dependent Mean Coeff Var

3.25947 R- square 22.38400 Adj R- Sq 14.56162

0.9596 0.9559

Parameter Estimates

Variable Intercept cases distance

Parameter

DF Estimate

1

2.34123

1

1.61591

1

0.01438

Standard Error 1.09673 0.17073 0.00361

t Value 2.13 9.46 3.98

Pr > |t| 0.0442 <.0001 0.0006

D.3 IMPORTING DATA FROM AN EXCEL FILE
The PC version of SAS has a nice wizard for importing an EXCEL spreadsheet as a SAS data set. The user has the option to bring the data in as a permanent data set or a temporary data set. A temporary data set exists purely for the duration of the SAS session. To bring the EXCEL spreadsheet as a permanent data set, we need to run an appropriate libname statement prior to using the wizard.
The first row of the EXCEL spreadsheet needs to provide the variable names associated with each column. The names provided in the first row will become the variables in the SAS data set.
It is not as easy to import an EXCEL spreadsheet into the UNIX version of SAS. The steps required follow.
Step 1: Export the EXCEL Spreadsheet We will need the EXCEL spreadsheet in dbf format (DBF III, IV, or V), which is easily done by the Save As button in EXCEL.
Step 2: Get the dbf File into UNIX Format If the dbf file was created on a Windows computer, we need to change its format for UNIX. Save the file in a UNIX directory and then execute the following UNIX command:
dos2unix-ascii data>newdata

620 APPENDIX D
Step 3: Import the File into SAS Let NAME.dbf be the name of the dbf file. The following command creates a temporary work file named NAME:
proc import dbms=dbf out=work.NAME datafile="NAME.dbf";
Step 4: When in Doubt, Contact Your System's Administrator! Things often seem to go wrong when crossing platforms, such as from Windows to UNIX. What works for one set of systems may not work perfectly for another.
D.4 OUTPUT COMMAND
The output command allows the user to append a previously created data set with information generated by a SAS procedure. Many SAS procedures support the output command. Its general form is
output out=SAS-NAME (output list) ;
In this case SAS-NAME is the name of the data set created by the output command. The resulting data set is the data set used by the procedure plus the variables added through (output list). Suppose we wish to add the predicted values and the raw residuals to the delivery time data set. Let delivery2 be the new data set. Suppose that we call the predicted delivery times ptime and that we call the raw residuals res. The appropriate output command is
output out=delivery2 p=ptime r=res;
The p is SAS's designation for the predicted values generated by PROC REG, and r is the designation for the raw residuals. In the output list, the SAS designation always is on the left-hand side of the = sign. The variable name within the new data set is always on the right-hand side. To create a data set with
It is very important to remember that the default data set used by a SAS procedure is the one most recently created. One of the saving graces of the output command is that it includes the data set used by the procedure to create the output data set.
D.5 LOG FILE
Every SAS session generates a "log" file that provides a brief summary. New SAS users find out very quickly (and very painfully) that SAS source code is a computer program that must be compiled. As such, the code must follow certain syntax rules. It is important to note that SAS can produce an incorrect, even nonsensical, analysis even if SAS does not reject the syntax. The log file is almost essential for debugging SAS code.
The log file provides a brief summary of the SAS session. It tells the analyst how many observations are in the data set, how many observations have missing data

LOG FILE 621

(in this case, there are no missing data), the commands executed, and any errors. Below is a simple example from a correct analysis:

NOTE: Copyright (c) 2002-2003 by SAS Institute Inc., Cary, NC, USA
NOTE: SAS (r) 9.1 (TSIM2) Licensed to VA POLYTECHNIC INST & STATE UNIVCAMPUSWIDE-IN, Site 0001798011.
NOTE: This session is executing on the WIN_PRO platform.

NOTE: SAS initialization used:

real time

19.30 seconds

cpu time

1.56 seconds

1

data delivery; 2 input time cases distance;

3

cards;

NOTE: The data set WORK.DELIVERY has 25 observations and 3

variables.

NOTE: DATA statement used (Total process time):

real time

1.22 seconds

cpu time

0.23 seconds

29 proc print data=delivery; 30 run;

NOTE: There were 25 observations read from the data set

WORK.DELIVERY

NOTE: PROCEDURE PRINT used (Total process time):

real time

0.55 seconds

cpu time

0.17 seconds

Below is an example where we give the command

print data=deli very

instead of the proper syntax

proc print data=delivery;

NOTE: Copyright (c) 2002-2003 by SAS Institute Inc., Cary,

NC, USA.

NOTE: SAS (r) 9.1 (TS1M2)

Licensed to VA POLYTECHNIC INST & STATE UNIV-

CAMPUSWIDE-IN, Site 0001798011.

NOTE: This session is executing on the WIN_PRO

platform.

NOTE: SAS initialization used:

real time

5.03 seconds

cpu time

1.73 seconds

622 APPENDIX D

1

libname book `c:\My Stuff\Disk-Books\Regression 5th

Ed ';

NOTE: Libref BOOK was successfully assigned as follows:

Engine 2q V9

Physical Name: c:\My Stuff\Disk-Books\Regression 5th

Ed

2

print data=book.delivery;

--­

180

ERROR 180- 322: Statement is not valid or it is

used out of

proper order.

3

run;

One of the most frustrating errors in SAS occurs when we forget a semicolon. SAS rarely, if ever, flags a missing semicolon directly as an error! It flags a syntax problem later in the source code that is the consequence of the missing semicolon.
Finally, for large sets it is not practical to print the entire data set. Many people use SAS to create massive data sets through "merges," among other techniques. In these circumstances, the log file gives the first information, usually through the number of observations in the data set, of problems. As such, the log file is essential to good SAS programming.

D.6 ADDING VARIABLES TO AN EXISTING SAS DATA SET
We can add variables to a previously created SAS data set. For example, suppose that we would like to use cases2 = cases2 as a regressor for the delivery data, and suppose that the delivery data are in a SAS data set named delivery. We shall call the new data set delivery2. The appropriate SAS commands are
data delivery2; set delivery; cases2=cases*cases;
run;
Suppose we wish to create a new permanent SAS data set where we add cases2 to the permanent SAS data set book.delivery Suppose further that our code already includes the appropriate libname statement. The appropriate SAS commands are
data book.delivery2; set book.delivery; cases2=cases*cases;
run;

APPENDIX E
INTRODUCTION TO R TO PERFORM LINEAR REGRESSION ANALYSIS
R is a popular statistical software package, primarily because it is freely available at www.r-project.org. As a result, many instructors as well as many of the more sophisticated statistical practitioners are switching to it. We have found that using R makes sense with graduate students who are already familiar with statistical methodology, especially those students with some experience using more sophisticated statistical software packages such as SAS. We personally recommend using less sophisticated and fully supported statistical software packages such as Minitab and SAS-JMP for undergraduates and those new to formal statistical analysis. However, we realize that some instructors prefer to use R even for these less sophisticated students. As a result, we created this appendix to introduce some of the basics of R.
E.1 BASIC BACKGROUND ON R According to the project's webpage:
The R Foundation is a not-for-profit organization working in the public interest. It has been founded by the members of the R Development Core Team in order to
· Provide support for the R project and other innovations in statistical computing. We believe that R has become a mature and valuable tool and we would like to ensure its continued development and the development of future innovations in software for statistical and computational research.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
623

624 APPENDIX E
· Provide a reference point for individuals, instititutions or commercial enterprises that want to support or interact with the R development community.
· Hold and administer the copyright of R software and documentation.
R is an official part of the Free Software Foundation's GNU project, and the R Foundation has similar goals to other open source software foundations like the Apache Foundation or the GNOME Foundation.
Among the goals of the R Foundation are the support of continued development of R, the exploration of new methodology, teaching and training of statistical computing and the organization of meetings and conferences with a statistical computing orientation. We hope to attract sufficient funding to make these goals realities.
R is a very sophisticated statistical software environment, even though it is freely available. The contributors include many of the top researchers in statistical computing. In many ways, it reflects the very latest statistical methodologies. On the other hand, the contributors truly form a community that is quite fluid. It can take quite a bit of work to keep current with the latest features of R. The help documentation with the basic releases is really of limited value. Of course, it many ways, you get what you pay for!
R itself is a high-level programming language. Most of its commands are prewritten functions. It does have the ability to run loops and call other routines, for example, in C. Since it is primarily a programming language, it often presents challenges to novice users.
E.2 BASIC DATA ENTRY
The best way to understand R is through examples. We present here some of the R code illustrated through the text. We can illustrate many of the basic features of basic data entry and data manipulation with the vapor pressure data set in Exercise 5.2. The data are:

Temp
273 283 293 303 313 323 333 343 353 363 373

vp
4.6 9.2 17.5 31.8 55.3 92.5 149.4 233.7 355.1 525.8 760.0

The brute force way to enter the data uses the c() function:

BASIC DATA ENTRY 625
temp <- c(273, 283, 293, 303, 313, 323, 333, 343, 353, 363, 373) vp <- c(4.6, 9.2, 17.5, 31.8, 55.3, 92.5, 149.4, 233.7, 355.1, 525.8, 760.0)
To check your data entry, you can use the print() function. In our case,
print(temp) print(vp)
The resulting output is:
> print(temp) [1] 273 283 293 303 313 323 333 343 353 363 373 > print(vp) [1] 4.6 9.2 17.5 31.8 55.3 92.5 149.4 233.7 355.1 525.8 760.0
For small data sets, the brute force approach works well. For larger data sets, we recommend using the read.table() function. You can create a text file with the data in columns. Generally, the first row is a "header" giving the variable names. The read. table() function works well for this type of file. Let vapor.txt be such a file for the vapor pressure data. The first step is to change the working directory for R to the directory that contains the data file. You can do this under the File box. The following command reads the data file and places the data into the object vapor.
vapor <- read.table("vapor.txt", header=TRUE, sep="")
To check the contents of vapor, we can use the print() function. The resulting output is:

> print(vapor)

temp

vp

1 273

4.6

2 283

9.2

3 293 17.5

4 303 31.8

5 313 55.3

6 323 92.5

7 333 149.4

8 343 233.7

9 353 355.1

10 363 525.8

11 373 760.0

626 APPENDIX E
If we read the data from a file, then we cannot refer to the temperatures as temp even though temp was the name of the column in the original data file; rather, we must also specify the object that contains it. The following command prints the temp column of the vapor object.
> print(vapor$temp) [1] 273 283 293 303 313 323 333 343 353 363 373
Basic physical chemistry suggests modeling the natural log of the vapor pressure as a linear function of the inverse of the temperature. The following commands create the inverse of the temperatures and then prints them.
> inv_temp <- 1/vapor$temp > print(inv_temp) [1] 0.003663004 0.003533569 0.003412969 0.003300330 0.003194888 0.003095975 [7] 0.003003003 0.002915452 0.002832861 0.002754821 0.002680965
The log() function genrates the natural log. The following commands create the natural log of the vapor pressures and then prints them.
> log\_vp <- log(vapor$vp) > print(log_vp) [1] 1.526056 2.219203 2.862201 3.459466 4.012773 4.527209 5.006627 5.454038 [9] 5.872399 6.264921 6.633318
Another useful command for regression analysis is the sqrt() function, which works exactly like the log() function.
R does generate plots, but it takes a great deal of work to make good looking plots. The basic plot function is plot(y,x) where y is the object on the y-axis and x is the object on the y-axis. The following command generates the scatter plot for the vapor pressure data.
> plot(vapor$vp,vapor$temp)
The write.table() function generates an output data file that is useful for using other plotting software. The following code appends the inverse temperatures and the natural logs of the vapor pressures to the original data to form a new object vapor2 and then creates the output data file vapor_output.txt.
> vapor2 <- cbind(vapor,inv_temp,log\_vp) > write.table(vapor2,"vapor\_output.txt")
E.3 BRIEF COMMENTS ON OTHER FUNCTIONALITY IN R
R does a very nice job manipulating matrices. This textbook, however, uses statistical software to perform the matrix calculations "under the hood," so to speak. The text

R COMMANDER 627
does show the matrix formulations of the procedures we discuss. However, we do not expect students to perform these calculations directly. As a result, we consider an introduction to the matrix manipulations within R beyond our scope. As appropriate, the text does give the basic R code to perform analyses. We leave it to the course instructor to present the details of the matrix manipulations within R.
E.4 R COMMANDER
R Commander is an add-on package to R. It also is freely available. It provides an easy-to-use user interface, much like Minitab and JMP, to the parent R product. R Commander makes it much more convenient to use R; however, it does not provide much flexibility in its analysis. For example, R Commander does not allow the user to use the externally studentized residual for the residual plots. R Commander is a good way for users to get familiar with R. Ultimately, however, we recommend the use of the parent R product.

REFERENCES
Adichie, J. N. [1967], "Estimates of regression parameters based on rank tests," Ann. Math. Stat., 38, 894­904.
Aitkin, M. A. [1974], "Simultaneous inference and the choice of variable subsets," Technometrics 16, 221­227.
Akaike, H. [1973], "Information theory and an extension of the maximum likelihood principle," in B. N. Petrov and F. Csaki (editors), Second International Symposium on Information Theory. Budapest: Academiai Kiado.
Allen, D. M. [1971], "Mean square error of prediction as a criterion for selecting variables," Technometrics, 13, 469­475.
Allen, D. M. [1974], "The relationship between variable selection and data augmentation and a method for prediction," Technometrics, 16, 125­127.
Andrews, D. F. [1971], "Significance tests based on residuals," Biometrika, 58, 139­148. Andrews, D. F. [1974], "A robust method for multiple linear regression," Technometrics, 16,
523­531. Andrews, D. F. [1979], "The robustness of residual displays," in R. L. Launer and G. N. Wilkin-
son (Eds.), Robustness in Statistics, Academic Press, New York, pp. 19­32. Andrews, D. F., P. J. Bickel, F. R. Hampel, P. J. Huber, W. H. Rogers, and J. W. Tukey [1972],
Robust Estimates of Location, Princeton University Press, Princeton, N.J. Anscombe, F. J. [1961], "Examination of residuals," in Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Vol. 1, University of California, Berkeley, pp. 1­36. Anscombe, F. J. [1967], "Topics in the investigation of linear relations fitted by the method of least squares," J. R. Stat. Soc. Ser. B, 29, 1­52. Anscombe, F. J. [1973], "Graphs in statistical analysis," Am. Stat., 27(1), 17­21.
Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
628

REFERENCES 629
Anscombe, F. J. and J. W. Tukey [1963], "The examination and analysis of residuals," Technometrics, 5, 141­160.
Askin, R. G. and D. C. Montgomery [1980], "Augmented robust estimators," Technometrics, 22, 333­341.
Askin, R. G. and D. C. Montgomery [1984], "An analysis of constrained robust regression estimators," Nav. Res. Logistics Q, 31, 283­296.
Atkinson, A. C. [1983], "Diagnostic regression for shifted power transformations," Technometrics, 25, 23­33.
Atkinson, A. C. [1985], Plots, Transformations, and Regression, Clarendon Press, Oxford. Atkinson, A. C. [1994], "Fast very robust methods for the detection of multiple outliers,"
J. Am. Stat. Assoc., 89, 1329­1339. Bailer, A. J. and Piegorsch, W. W. [2000], "From quanal counts to mechanisms and systems:
The past present, and future of biometrics in environmental toxicology," Biometrics, 56, 327­336. Barnett, V. and T. Lewis [1994], Outliers in Statistical Data, 3rd ed., Wiley, New York. Bates, D. M. and D. G. Watts [1988], Nonlinear Regression Analysis and Its Applications, Wiley, New York. Beaton, A. E. [1964], The Use of Special Matrix Operators in Statistical Calculus, Research Bulletin RB-64-51, Educational Testing Service, Princeton, N.J. Beaton, A. E. and J. W. Tukey [1974], "The fitting of power series, meaning polynomials, illustrated on band spectroscopic data," Technometrics, 16, 147­185. Belsley, D. A., E. Kuh, and R. E. Welsch [1980], Regression Diagnostics: Identifying Influential Data and Sources of Collinearity, Wiley, New York. Bendel, R. B. and A. A. Afifi [1974], "Comparison of stopping rules in forward stepwise regressions," presented at the Joint Statistical Meeting, St. Louis, Mo. Berk, K. N. [1978], "Comparing subset regression procedures," Technometrics, 20, 1­6. Berkson, J. [1950], "Are there two regressions?" J. Am. Stat. Assoc., 45, 164­180. Berkson, J. [1969], "Estimation of a linear function for a calibration line; consideration of a recent proposal," Technometrics, 11, 649­660. Bishop, C. M. [1995], Neural Networks for Pattern Recognition, Clarendon Press, Oxford. Bloomfield, P. and W. L. Steiger [1983], Least Absolute Deviations: Theory, Applications, and Algorithms, Birkhuser Verlag, Boston. Book, D., J. Booker, H. O. Hartley, and R. I. Sielken, Jr. [1980], "Unbiased L1 estimators and their covariances," ONR THEMIS Technical Report No. 64, Institute of Statistics, Texas A & M University. Box, G. E. P. [1966], "Use and abuse of regression," Technometrics, 8, 625­629. Box, G. E. P. and D. W. Behnken [1960], "Some new three level designs for the study of quantitative variables," Technometrics, 2, 455­475. Box, G. E. P. and D. R. Cox [1964], "An analysis of transformations," J. R. Stat. Soc. Ser. B, 26, 211­243. Box, G. E. P. and N. R. Draper [1959], "A basis for the selection of a response surface design," J. Am. Stat. Assoc., 54, 622­654. Box, G. E. P. and N. R. Draper [1963], "The choice of a second-order rotatable design," Biometrika, 50, 335­352. Box, G. E. P. and N. R. Draper [1987], Empirical Model Building and Response Surfaces, Wiley, New York. Box, G. E. P. and J. S. Hunter [1957], "Multifactor experimental designs for exploring response surfaces," Ann. Math. Stat., 28, 195­242.

630 REFERENCES
Box, G. E. P., W. G. Hunter, and J. S. Hunter [1978], Statistics for Experimenters, Wiley, New York.
Box, G. E. P., G. M. Jenkins, and G. C. Reinsel [1994], Time Series Analysis, Forecasting, and Control, 3rd ed., Prentice-Hall, Englewood Cliffs, N.J.
Box, G. E. P. and P. W. Tidwell [1962], "Transformation of the independent variables," Technometrics, 4, 531­550.
Box, G. E. P. and J. M. Wetz [1973], "Criterion for judging the adequacy of estimation by an approximating response polynomial," Technical Report No. 9, Department of Statistics, University of Wisconsin, Madison.
Bradley, R. A. and S. S. Srivastava [1979], "Correlation and polynomial regression," Am. Stat., 33, 11­14.
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone [1984], Classification and Regression Trees, Wadsworth, Belmont, Calif.
Brown, P. J. [1977], "Centering and scaling in ridge regression," Technometrics, 19, 35­36. Brown, R. L., J. Durbin, and J. M. Evans [1975], "Techniques for testing the constancy of
regression relationships over time (with discussion)," J. R. Stat. Soc. Ser. B, 37, 149­192. Buse, A. and L. Lim [1977], "Cubic splines as a special case of restricted least squares," J. Am.
Stat. Assoc., 72, 64­68. Cady, F. B. and D. M. Allen [1972], "Combining experiments to predict future yield data,"
Agron. J., 64, 211­214. Carroll, R. J. and D. Ruppert [1985], "Transformation in regression: A robust analysis," Tech-
nometrics, 27, 1­12. Carroll, R. J. and D. Ruppert [1988], Transformation and Weighting in Regression, Chapman
& Hall, London. Chapman, R. E. [1997­98], "Degradation study of a photographic developer to determine
shelf life," Quality Engineering, 10, 137­140. Chatterjee, S. and B. Price [1977], Regression Analysis by Example, Wiley, New York. Coakley, C. W. and T. P. Hettmansperger [1993], "A bounded influence, high breakdown,
efficient regression estimator," J. Am. Stat. Assoc., 88, 872­880. Cochrane, D. and G. H. Orcutt [1949], "Application of least squares regression to relationships
containing autocorrelated error terms," J. Am. Stat. Assoc., 44, 32­61. Conniffe, D. and J. Stone [1973], "A critical view of ridge regression," The Statistician, 22,
181­187. Conniffe, D. and J. Stone [1975], "A reply to Smith and Goldstein," The Statistician, 24,
67­68. Cook, R. D. [1977], "Detection of influential observation in linear regression," Technometrics,
19, 15­18. Cook, R. D. [1979], "Influential observations in linear regression," J. Am. Stat. Assoc., 74,
169­174. Cook, R. D. [1993], "Exploring partial residual plots," Technometrics, 35, 351­362. Cook, R. D. and P. Prescott [1981], "On the accuracy of Bonferroni significance levels for
detecting outliers in linear models," Technometrics, 22, 59­63. Cook, R. D. and S. Weisberg [1983], "Diagnostics for heteroscedasticity in regression,"
Biometrika, 70, 1­10. Cook, R. D. and S. Weisberg [1994]. An Introduction to Regression Graphics, Wiley, New York. Cox, D. R. and E. J. Snell [1974], "The choice of variables in observational studies," Appl.
Stat., 23, 51­59. Curry, H. B. and I. J. Schoenberg [1966], "On Polya frequency functions IV: The fundamental
spline functions and their limits," J. Anal. Math., 17, 71­107.

REFERENCES 631
Daniel, C. [1976], Applications of Statistics to Industrial Experimentation, Wiley, New York. Daniel, C. and F. S. Wood [1980], Fitting Equations to Data, 2nd ed., Wiley, New York. Davies, R. B. and B. Hutton [1975], "The effects of errors in the independent variables in
linear regression," Biometrika, 62, 383­391. Davison, A. C. and D. V. Hinkley [1997], Bootstrap Methods and Their Application, Cambridge
University Press, London. De Jong, P. J., T. De Wet, and A. H. Welsh [1988], "Mallows-type bounded-influence-regression
trimmed means," J. Am. Stat. Assoc., 83, 805­810. DeLury, D. B. [1960], Values and Integrals of the Orthogonal Polynomials up to N = 26, Uni-
versity of Toronto Press, Toronto. Dempster, A. P., M. Schatzoff, and N. Wermuth [1977], "A simulation study of alternatives to
ordinary least squares," J. Am. Stat. Assoc., 72, 77­90. Denby, L. and W. A. Larson [1977], "Robust regression estimators compared via Monte
Carlo," Commun. Stat., A6, 335­362. Dodge, Y. [1987], Statistical Data Analysis Based on the L1-Norm and Related Methods, North-
Holland, Amsterdam. Dolby, G. R. [1976], "The ultrastructural relation: A synthesis of the functional and structural
relations," Biometrika, 63, 39­50. Dolby, J. L. [1963], "A quick method for choosing a transformation," Technometrics, 5,
317­325. Draper, N. R., J. Guttman, and H. Kanemasa [1971], "The distribution of certain regression
statistics," Biometrika, 58, 295­298. Draper, N. R. and H. Smith [1998], Applied Regression Analysis, 3rd ed., Wiley, New York. Draper, N. R. and R. C. Van Nostrand [1977a], "Shrinkage estimators: Review and comments,"
Technical Report No. 500, Department of Statistics, University of Wisconsin, Madison. Draper, N. R. and R. C. Van Nostrand [1977b], "Ridge regression: Is it worthwhile?" Technical
Report No. 501, Department of Statistics, University of Wisconsin, Madison. Draper, N. R. and R. C. Van Nostrand [1979], "Ridge regression and James­Stein estimators:
Review and comments," Technometrics, 21, 451­466. Durbin, J. [1970], "Testing for serial correlation in least squares regression when some of the
regressors are lagged dependent variables," Econometrica, 38, 410­421. Durbin, J. and G. S. Watson [1950], "Testing for serial correlation in least squares regression
I," Biometrika, 37, 409­438. Durbin, J. and G. S. Watson [1951], "Testing for serial correlation in least squares regression
II," Biometrika, 38, 159­178. Durbin, J. and G. S. Watson [1971], "Testing for serial correlation in least squares regression
III," Biometrika, 58, 1­19. Dutter, R. [1977], "Numerical solution of robust regression problems: Computational aspects,
a comparison," J. Stat. Comput. Simul., 5, 207­238. Dutter, R. and P. J. Hober [1981], "Numerical methods for the robust nonlinear regression
problem," J. Stat. Comput. Simul., 13, 79­114. Dykstra, O., Jr. [1971], "The augmentation of experimental data to maximize (XX)," Tech-
nometrics, 13, 682­688. Edwards, J. B. [1969], "The relation between the F-test and R2," Am. Stat., 23, 28. Efron, B. [1979], "Bootstrap methods: Another look at the jackknife," Ann. Stat., 7, 1­26. Efron, B. [1982], The Jackknife, the Bootstrap and Other Resampling Plans, Society for Indus-
trial and Applied Mathematics, Philadelphia.

632 REFERENCES
Efron, B. [1987], "Better bootstrap confidence intervals (with discussion)," J. Am. Stat. Assoc., 82, 172­200.
Efron, B. and R. Tibshirani [1986], "Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy," Stat. Sci., 1, 54­77.
Efron, B. and R. Tibshirani [1993], An Introduction to the Bootstrap, Chapman & Hall, London.
Efroymson, M. A. [1960], "Multiple regression analysis," in A. Ralston and H. S. Wilf (Eds.), Mathematical Methods for Digital Computers, Wiley, New York.
Ellerton, R. R. W. [1978], "Is the regression equation adequate--A generalization," Technometrics, 20, 313­316.
Eubank, R. L. [1988], Spline Smoothing and Nonparametric Regression, Dekker, New York. Eubank, R. L. and P. Speckman [1990], "Curve fitting by polynomial­trigonometric regres-
sion," Biometrika, 77, 1­9. Everitt, B. S. [1993], Cluster Analysis, 3rd ed., Halsted Press, New York. Farrar, D. E. and R. R. Glauber [1967], "Multicollinearity in regression analysis: The problem
revisited," Rev. Econ. Stat., 49, 92­107. Feder, P. I. [1974], "Graphical techniques in statistical data analysis--Tools for extracting
information from data," Technometrics, 16, 287­299. Forsythe, A. B. [1972], "Robust estimation of straight-line regression coefficients by minimiz-
ing pth power deviations," Technometrics, 14, 159­166. Forsythe, G. E. [1957], "Generation and use of orthogonal polynomials for data-fitting with
a digital computer," J. Soc. Ind. Appl. Math., 5, 74­87. Fuller, W. A. [1976], Introduction to Statistical Time Series, Wiley, New York. Furnival, G. M. [1971], "All possible regressions with less computation," Technometrics, 13,
403­408. Furnival, G. M. and R. W. M. Wilson, Jr. [1974], "Regression by leaps and bounds," Techno-
metrics, 16, 499­511. Gallant, A. R. and W. A. Fuller [1973], "Fitting segmented polynomial regression models
whose join points have to be estimated," J. Am. Stat. Assoc., 63, 144­147. Garside, M. J. [1965], "The best subset in multiple regression analysis," Appl. Stat., 14,
196­200. Gartside, P. S. [1972],"A study of methods for comparing several variances," J.Am. Stat.Assoc.,
67, 342­346. Gaylor, D. W. and J. A. Merrill [1968], "Augmenting existing data in multiple regression,"
Technometrics, 10, 73­81. Geisser, S. [1975], "The predictive sample reuse method with applications," J. Am. Stat. Assoc.,
70, 320­328. Gentle, J. M., W. J. Kennedy, and V. A. Sposito [1977], "On least absolute deviations estima-
tors," Commun. Stat., A6, 839­845. Gibbons, D. G. [1979], A Simulation Study of Some Ridge Estimators, General Motors
Research Laboratories, Mathematics Department, GMR-2659 (rev. ed.), Warren, Mich. Gnanadesikan, R. [1977], Methods for Statistical Analysis of Multivariate Data, Wiley, New
York. Goldberger, A. S. [1964], Econometric Theory, Wiley, New York. Goldstein, M. and A. F. M. Smith [1974], "Ridge-type estimators for regression analysis,"
J. R. Stat. Soc. Ser. B, 36, 284­291. Golub, G. H. [1969], "Matrix decompositions and statistical calculations," in R. C. Milton and
J. A. Welder (Eds.), Statistical Computation, Academic, New York.

REFERENCES 633
Gorman, J. W. and R. J. Toman [1966], "Selection of variables for fitting equations to data." Technometrics, 8, 27­51.
Graybill, F.A. [1961], An Introduction to Linear Statistical Models,Vol. 1, McGraw-Hill, New York. Graybill, F. A. [1976], Theory and Application of the Linear Model, Duxbury, North Scituate,
Mass. Guilkey, D. K. and J. L. Murphy [1975], "Directed ridge regression techniques in cases of
multicollinearity," J. Am. Stat. Assoc., 70, 769­775. Gupta, A. and A. K. Das [2000], "Improving resistivity of UF resin through setting of process
parameters," Quality Engineering, 12, 611­618. Gunst, R. F. [1979], "Similarities among least squares, principal component, and latent root
regression estimators," presented at the Washington, D.C., Joint Statistical Meetings. Gunst, R. F. and R. L. Mason [1977], "Biased estimation in regression: An evaluation using
mean squared error," J. Am. Stat. Assoc., 72, 616­628. Gunst, R. F. and R. L. Mason [1979], "Some considerations in the evaluation of alternative
prediction equations," Technometrics, 21, 55­63. Gunst, R. F., J. T. Webster, and R. L. Mason [1976], "A comparison of least squares and latent
root regression estimators," Technometrics, 18, 75­83. Gunter, B. [1997a], "Tree-based classification and regression. Part I: Background and funda-
mentals," Qual. Prog., 28, August, 159­163. Gunter, B. [1997b], "Tree-based classification and regression. Part II: Assessing classification
performance," Qual. Prog., 28, December, 83­84. Gunter, B. [1998], "Tree-based classification and regression. Part III: Tree-based procedures,"
Qual. Prog., 31, February, 121. Hadi, A S. and J. S. Simonoff [1993], "Procedures for the identification of multiple outliers in
linear models," J. Am. Stat. Assoc., 88, 1264­1272. Hahn, G. J. [1972], "Simultaneous prediction intervals for a regression model," Technometrics,
14, 203­214. Hahn, G. J. [1973], "The coefficient of determination exposed!" Chem. Technol., 3, 609­614. Hahn, G. J. [1979], "Fitting regression models with no intercept term," J. Qual. Technol., 9(2),
56­61. Hahn, G. J. and R.W. Hendrickson [1971],"A table of percentage points of the largest absolute
value of k student t variates and its applications," Biometrika, 58, 323­332. Haitovski, Y. [1969], "A note on the maximization of R2," Am. Stat., 23(1), 20­21. Hald, A. [1952], Statistical Theory with Engineering Applications, Wiley, New York. Halperin, M. [1961], "Fitting of straight lines and prediction when both variables are subject
to error," J. Am. Stat. Assoc., 56, 657­669. Halperin, M. [1970], "On inverse estimation in linear regression," Technometrics, 12,
727­736. Hawkins, D. M. [1973], "On the investigation of alternative regressions by principal compo-
nents analysis," Appl. Stat., 22, 275­286. Hawkins, D. M. [1994], "The feasible solution algorithm for least trimmed squares regression,"
Comput. Stat. Data Anal., 17, 185­196. Hawkins, D. M., D. Bradu, and G. V. Kass [1984], "Location of several outliers in multiple
regression using elemental sets," Technometrics, 26, 197­208. Hayes, J. G. (Ed.) [1970], Numerical Approximations to Functions and Data, Athlone Press,
London. Hayes, J. G. [1974], "Numerical methods for curve and surface fitting," J. Inst. Math. Appl., 10,
144­152.

634 REFERENCES
Haykin, S. [1994], Neural Networks: A Comprehensive Foundation, Macmillan Co., New York. Hemmerle, W. J. and T. F. Brantle [1978], "Explicit and constrained generalized ridge regres-
sion," Technometrics, 20, 109­120. Hettmansperger, T. P. and J. W. Mckean [1998], Robust Nonparametric Statistical Methods,
Vol. 5 of Kendall's Library of Statistics, Arnold, London. Hill, R. C., G. G. Judge, and T. B. Fomby [1978], "On testing the adequacy of a regression
model," Technometrics, 20, 491­494. Hill, R. W. [1979], "On estimating the covariance matrix of robust regression M-estimates,"
Commun. Stat., A8, 1183­1196. Himmelblau, D. M. [1970], Process Analysis by Statistical Methods, Wiley, New York. Hoadley, B. [1970], "A Bayesian look at inverse linear regression," J. Am. Stat. Assoc., 65,
356­369. Hoaglin, D. C. and R. E. Welsch [1978], "The hat matrix in regression and ANOVA," Am.
Stat., 32(1), 17­22. Hocking, R. R. [1972], "Criteria for selection of a subset regression: Which one should be
used," Technometrics, 14, 967­970. Hocking, R. R. [1974], "Misspecification in regression," Am. Stat., 28, 39­40. Hocking, R. R. [1976], "The analysis and selection of variables in linear regression," Biomet-
rics, 32, 1­49. Hocking, R. R. and L. R. LaMotte [1973], "Using the SELECT program for choosing subset
regressions," in W. O. Thompson and F. B. Cady (Eds.), Proceedings of the University of Kentucky Conference on Regression with a Large Number of Predictor Variables, Department of Statistics, University of Kentucky, Lexington. Hocking, R. R., F. M. Speed, and M. J. Lynn [1976], "A class of biased estimators in linear regression," Technometrics, 18, 425­437. Hodges, S. D. and P. G. Moore [1972], "Data uncertainties and least squares regression," Appl. Stat., 21, 185­195. Hoerl, A. E. [1959], "Optimum solution of many variable equations," Chem. Eng. Prog., 55, 69. Hoerl, A. E. and R. W. Kennard [1970a], "Ridge regression: Biased estimation for nonorthogonal problems," Technometrics, 12, 55­67. Hoerl, A. E. and R. W. Kennard [1970b], "Ridge regression: Applications to nonorthogonal problems," Technometrics, 12, 69­82. Hoerl, A. E. and R. W. Kennard [1976], "Ridge regression: Iterative estimation of the biasing parameter," Commun. Stat., A5, 77­88. Hoerl, A. E., R. W. Kennard, and K. F. Baldwin [1975], "Ridge regression: Some simulations," Commun. Stat., 4, 105­123. Hogg, R. V. [1974], "Adaptive robust procedures: A partial review and some suggestions for future applications and theory," J. Am. Stat. Assoc., 69, 909­925. Hogg, R. V. [1979a], "Statistical robustuess: One view of its use in applications today," Am. Stat., 33(3), 108­115. Hogg, R. V. [1979b], "An introduction to robust estimation," in R. L. Launer and G. N. Wilkinson (Eds.), Robustness in Statistics, Academic, New York, pp. 1­18. Hogg, R. V. and R. H. Randles [1975], "Adaptive distribution-free regression methods and their applications," Technometrics, 17, 399­407. Holland, P. W. and R. E. Welsch [1977], "Robust regression using iteratively reweighted least squares," Commun. Stat., A6, 813­828. Huber, P. J. [1964], "Robust estimation of a location parameter," Ann. Math. Stat., 35, 73­101.

REFERENCES 635
Huber, P. J. [1972], "Robust statistics: A review," Ann. Math. Stat., 43, 1041­1067. Huber, P. J. [1973], "Robust regression: Asymptotics, conjectures, and Monte Carlo," Ann.
Stat., 1, 799­821. Huber, P. J. [1981], Robust Statistics, Wiley, New York. Jaeckel, L. A. [1972], "Estimating regression coefficients by minimizing the dispersion of the
residuals," Ann. Math. Stat., 43, 1449­1458. Joglekar, G., J. H. Schuenemeyer, and V. LaRiccia [1989], "Lack-of-fit testing when replicates
are not available," Am. Stat., 43, 135­143. Johnson, R. A. and D. W. Wichern [1992], Applied Multivariate Statistical Analysis, Prentice-
Hall, Englewood Cliffs, N.J. Johnston, J. [1972], Econometric Methods, McGraw-Hill, New York. Jurecková, J. [1977], "Asymptotic relations of M-estimates and R-estimates in linear regres-
sion models," Ann. Stat., 5, 464­472. Kalotay, A. J. [1971], "Structural solution to the linear calibration problem," Technometrics,
13, 761­769. Kendall, M. G. and G. U. Yule [1950], An Introduction to the Theory of Statistics, Charles
Griffin, London. Kennard, R. W. and L. Stone [1969], "Computer aided design of experiments," Technometrics,
11, 137­148. Kennedy, W. J. and T. A. Bancroft [1971], "Model-building for prediction in regression using
repeated significance tests," Ann. Math. Stat., 42, 1273­1284. Khuri, A. H. and J. A. Cornell [1996], Response Surfaces: Designs and Analyses, 2nd ed.,
Dekker, New York. Kiefer, J. [1959], "Optimum experimental designs," Journal of the Royal Statistical Society,
Series B, 21, 272­304. Kiefer, J. [1961], "Optimum designs in regression problems. II," Annals of Mathematical Sta-
tistics, 32, 298­325. Kiefer, J. and J. Wolfowitz [1959], "Optimum designs in regression problems," Annals of
Mathematical Statistics, 30, 271­294. Krasker, W. S. and R. E. Welsch [1982], "Efficient bounded-influence regression estimation,"
J. Am. Stat. Assoc., 77, 595­604. Krutchkoff, R. G. [1967], "Classical and inverse regression methods of calibration," Techno-
metrics, 9, 425­439. Krutchkoff, R. G. [1969], "Classical and inverse regression methods of calibration in extrapo-
lation," Technometrics, 11, 605­608. Kunugi, T., T. Tamura, and T. Naito [1961], "New acetylene process uses hydrogen dilution,"
Chem. Eng. Prog., 57, 43­49. Land, C. E. [1974], "Confidence interval estimation for means after data transformation to
normality," J. Am. Stat. Assoc., 69, 795­802 (Correction, ibid., 71, 255). Larsen, W. A. and S. J. McCleary [1972], "The use of partial residual plots in regression analy-
sis," Technometrics, 14, 781­790. Lawless, J. F. [1978],"Ridge and related estimation procedures:Theory and practice," Commun.
Stat., A7, 139­164. Lawless, J. F. and P. Wang [1976], "A simulation of ridge and other regression estimators,"
Commun. Stat., A5, 307­323. Lawrence, K. D. and J. L. Arthur [1990], "Robust nonlinear regression," in K. D. Lawrence
and J. L. Arthur (Eds.), Robust Regression: Analysis and Applications, Dekker, New York, pp. 59­86.

636 REFERENCES
Lawson, C. R. and R. J. Hanson [1974], Solving Least Squares Problems, Prentice-Hall, Englewood Cliffs, N.J.
Leamer, E. E. [1973], "Multicollinearity: A Bayesian interpretation," Rev. Econ. Stat., 55, 371­380.
Leamer, E. E. [1978], Specification Searches: Ad Hoc Inference with Nonexperimental Data, Wiley, New York.
Levine, H. [1960], "Robust tests for equality of variances," in I. Olkin (Ed.), Contributions to Probability and Statistics, Stanford University Press, Palo Alto, Calif., pp. 278­292.
Lieberman, G. J., R. G. Miller, Jr., and M. A. Hamilton [1967], "Unlimited simultaneous discrimination intervals in regression," Biometrika, 54, 133­145.
Lindley, D. V. [1974], "Regression lines and the linear functional relationship," J. R. Stat. Soc. Suppl., 9, 218­244.
Lindley, D. V. and A. F. M. Smith [1972], "Bayes estimates for the linear model (with discussion)," J. R. Stat. Soc. Ser. B, 34, 1­41.
Looney, S. W. and T. R. Gulledge, Jr. [1985], "Use of the correlation coefficient with normal probability plots," Am. Stat., 35, 75­79.
Lowerre, J. M. [1974], "On the mean square error of parameter estimates for some biased estimators," Technometrics, 16, 461­464.
McCarthy, P. J. [1976], "The use of balanced half-sample replication in cross-validation studies," J. Am. Stat. Assoc., 71, 596­604.
McCullagh, P. and J. A. Nelder [1989], Generalized Linear Models, 2nd ed., Chapman & Hall, London.
McDonald, G. C. and J. A. Ayers [1978], "Some applications of `Chernuff faces': A technique for graphically representing multivariate data," in Graphical Representation of Multivariate Data, Academic Press, New York.
McDonald, G. C. and D. I. Galarneau [1975], "A Monte Carlo evaluation of some ridge-type estimators," J. Am. Stat. Assoc., 70, 407­416.
Mallows, C. L. [1964], "Choosing variables in a linear regression: A graphical aid," presented at the Central Regional Meeting of the Institute of Mathematical Statistics, Manhattan, Kans.
Mallows, C. L. [1966], "Choosing a subset regression," presented at the Joint Statistical Meetings, Los Angeles.
Mallows, C. L. [1973], "Some comments on Cp," Technometrics, 15, 661­675. Mallows, C. L. [1986], "Augmented partial residuals," Technometrics, 28, 313­319. Mallows, C. L. [1995], "More comments on Cp," Technometrics, 37, 362­372. (Also see [1997],
39, 115­116.) Mandansky, A. [1959], "The fitting of straight lines when both variables are subject to error,"
J. Am. Stat. Assoc., 54, 173­205. Mansfield, E. R. and M. D. Conerly [1987], "Diagnostic value of residual and partial residual
plots," Am. Stat., 41, 107­116. Mansfield, E. R., J. T. Webster, and R. F. Gunst [1977], "An analytic variable selection proce-
dure for principal component regression," Appl. Stat., 26, 34­40. Mantel, N. [1970], "Why stepdown procedures in variable selection," Technometrics, 12,
621­625. Marazzi, A. [1993], Algorithms, Routines and S Functions for Robust Statistics, Wadsworth
and Brooks/Cole, Pacific Grove, Calif. Maronna, R. A. [1976], "Robust M-estimators of multivariate location and scatter," Ann. Stat.,
4, 51­67.

REFERENCES 637
Marquardt, D. W. [1963], "An algorithm for least squares estimation of nonlinear parameters," J. Soc. Ind. Appl. Math., 2, 431­441.
Marquardt, D. W. [1970], "Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation," Technometrics, 12, 591­612.
Marquardt, D. W. and R. D. Snee [1975], "Ridge regression in practice," Am. Stat., 29(1), 3­20.
Mason, R. L., R. F. Gunst, and J. T. Webster [1975], "Regression analysis and problems of multicollinearity," Commun. Stat., 4(3), 277­292.
Mayer, L. S. and T. A. Willke [1973], "On biased estimation in linear models," Technometrics, 16, 494­508.
Meyer, R. K. and C. J. Nachtsheim [1995], "The coordinate exchange algorithm for constructing exact optimal designs," Technometrics, 37, 60­69.
Miller, D. M. [1984], "Reducing transformation bias in curve fitting," Am. Stat., 38, 124­ 126.
Miller, R. G., Jr. [1966], Simultaneous Statistical Inference, McGraw-Hill, New York. Montgomery, D. C. [2009], Design and Analysis of Experiments, 7th ed., Wiley, New York. Montgomery, D. C., L. A. Johnson, and J. S. Gardiner [1990], Forecasting and Time Series
Analysis, 2nd ed., McGraw-Hill, New York. Montgomery, D. C., C. L. Jennings, and M. Kulahci [2008], Introduction to Time Series Analysis
and Forecasting, Wiley, Hoboken, N.J. Montgomery, D. C., E. W. Martin, and E. A. Peck [1980], "Interior analysis of the observations
in multiple linear regression," J. Qual. Technol., 12(3), 165­173. Morgan, J. A. and J. F. Tatar [1972], "Calculation of the residual sum of squares for all possible
regressions," Technometrics, 14, 317­325. Mosteller, F. and J. W. Tukey [1968], "Data analysis including statistics," in G. Lindzey and
E. Aronson (Eds.), Handbook of Social Psychology, Vol. 2, Addison-Wesley, Reading, Mass. Mosteller, F. and J. W. Tukey [1977], Data Analysis and Regression: A Second Course in Statistics, Addison-Wesley, Reading, Mass. Moussa-Hamouda, E. and F. C. Leone [1974], "The 0-blue estimators for complete and censored samples in linear regression," Technometrics, 16, 441­446. Moussa-Hamouda, E. and F. C. Leone [1977a], "The robustness of efficiency of adjusted trimmed estimators in linear regression," Technometrics, 19, 19­34. Moussa-Hamouda, E. and F. C. Leone [1977b], "Efficiency of ordinary least squares from trimmed and Winsorized samples in linear regression," Technometrics, 19, 265­273. Mullet, G. M. [1976], "Why regression coefficients have the wrong sign," J. Qual. Technol., 8, 121­126. Myers, R. H. [1990], Classical and Modern Regression with Applications, 2nd ed., PWS-Kent Publishers, Boston. Myers, R. H. and D. C. Montgomery [1997], "A tutorial on generalized linear models," Journal of Quality Technology, 29, 274­291. Myers, R. H., D. C. Montgomery, and C. M. Anderson-Cook [2009], Response Surface Methodology: Process and Product Optimization Using Designed Experiments, 3rd ed., Wiley, New York. Myers, R. H., D. C. Montgomery, G. G. Vining, and T. J. Robinson [2010], Generalized Linear Models with Applications in Engineering and the Sciences, Wiley, Hoboken, NJ. Narula, S. and J. S. Ramberg [1972], Letter to the Editor, Am. Stat., 26, 42.
Naszódi, L. J. [1978], "Elimination of the bias in the course of calibration," Technometrics, 20, 201­205.

638 REFERENCES
Nelder, J. A. and R. W. M. Wedderburn [1972], "Generalized linear models," J. R. Stat. Soc. Ser. A, 153, 370­384;
Neter, J., M. H. Kuther, C. J. Nachtsheim, and W. Wasserman [1996], Applied Linear Statistical Models, 4th ed., Richard D. Irwin, Homewood, Ill.
Neyman, J. and E. L. Scott [1960], "Correction for bias introduced by a transformation of variables," Ann. Math. Stat., 31, 643­655.
Obenchain, R. L. [1975], "Ridge analysis following a preliminary test of the shrunken hypothesis," Technometrics, 17, 431­441.
Obenchain, R. L. [1977], "Classical F-tests and confidence intervals for ridge regression," Technometrics, 19, 429­439.
Ott, R. L. and R. H. Myers [1968], "Optimal experimental designs for estimating the independent variable in regression," Technometrics, 10, 811­823.
Parker, P. A., G. G. Vining, S. A. Wilson, J. L. Szarka, III, and N. G. Johnson [2010], "Prediction properties of classical and inverse regression for the simple linear calibration problem," J. Qual. Technol., 42, 332­347.
Pearson, E. S. and H. O. Hartley [1966], Biometrika Tables for Statisticians, Vol. 1, 3rd ed., Cambridge University Press, London.
Peixoto, J. L. [1987], "Hierarchical variable selection in polynomial regression models," Am. Stat., 41, 311­313.
Peixoto, J. L. [1990], "A property of well-formulated polynomial regression models," Am. Stat., 44, 26­30. (Also see [1991], 45, 82.)
Pena, D. and V. J. Yohai [1995], "The detection of influential subsets in linear regression by using an influence matrix," J. R. Stat. Soc. Ser. B, 57, 145­156.
Perng, S. K. and Y. L. Tong [1974], "A sequential solution to the inverse linear regression problem," Ann. Stat., 2, 535­539.
Pesaran, M. H. and L. J. Slater [1980], Dynamic Regression: Theory Algorithms, Halsted Press, New York.
Pfaffenberger, R. C. and T. E. Dielman [1985], "A comparison of robust ridge estimators," in Business and Economics Section Proceedings of the American Statistical Association, pp. 631­635.
Poirier, D. J. [1973], "Piecewise regression using cubic splines," J. Am. Stat. Assoc., 68, 515­524.
Poirier, D. J. [1975], "On the use of bilinear splines in economics," J. Econ., 3, 23­24. Pope, P. T. and J. T. Webster [1972], "The use of an F-statistic in stepwise regression proce-
dures," Technometrics, 14, 327­340. Pukelsheim, F. [1995], Optimum Design of Experiments, Chapman & Hall, London. Ramsay, J. O. [1977], "A comparative study of several robust estimates of slope, intercept, and
scale in linear regression," J. Am. Stat. Assoc., 72, 608­615. Rao, P. [1971], "Some notes on misspecification in regression," Am. Stat., 25, 37­39. Ripley, B. D. [1994], "Statistical ideas for selecting network architectures," in B. Kappen and
S. Grielen (Eds.), Neural Networks: Artificial Intelligence Industrial Applications, SpringerVerlag, Berlin, pp. 183­190. Rocke, D. M. and D. L. Woodruff [1996], "Identification of outliers in multivariate data," J. Am. Stat. Assoc., 91, 1047­1061. Rosenberg, S. H. and P. S. Levy [1972], "A characterization on misspecification in the general linear regression model," Biometrics, 28, 1129­1132. Rossman, A. J. [1994]. "Televisions, physicians and life expectancy," J. Stat. Educ., 2.
Rousseeuw, P. J. [1984], "Least median of squares regression," J. Am. Stat. Assoc., 79, 871­880.

REFERENCES 639
Rousseeuw, P. J. [1998], "Robust estimation and identifying outliers," in H. M. Wadsworth (Ed.), Handbook of Statistical Methods for Engineers and Scientists, McGraw­Hill, New York, Chapter 17.
Rousseeuw, P. J. and A. M. Leroy [1987], Robust Regression and Outlier Detection, Wiley, New York.
Rousseeuw, P. J. and B. L. van Zomeren [1990],"Unmasking multivariate outliers and leverage points," J. Am. Stat. Assoc., 85, 633­651.
Rousseeuw, P. J., and V. Yohai [1984], "Robust regression by means of S-estimators," in J. Franke, W. Härdle, and R. D. Martin (Eds.), Robust Nonlinear Time Series Analysis: Lecture Notes in Statistics, Vol. 26, Springer, Berlin, pp. 256­272.
Ryan, T. P. [1997], Modern Regression Methods, Wiley, New York. SAS Institute [1987], SAS Views: SAS Principles of Regression Analysis, SAS Institute, Cary,
N.C. Sawa,T. [1978],"Information criteria for discriminating among alternative regression models,"
Econometrica, 46, 1273­1282. Schatzoff, M., R. Tsao, and S. Fienberg [1968], "Efficient calculation of all possible regres-
sions," Technometrics, 10, 769­779. Scheffé, H. [1953], "A method for judging all contrasts in the analysis of variance," Ann. Math.
Stat., 40, 87­104. Scheffé, H. [1959], The Analysis of Variance, Wiley, New York. Scheffé, H. [1973], "A statistical theory of calibration," Ann. Stat., 1, 1­37. Schilling, E. G. [1974a], "The relationship of analysis of variance to regression. Part I. Bal-
anced designs," J. Qual. Technol., 6, 74­83. Schilling, E. G. [1974b], "The relationship of analysis of variance to regression. Part II. Unbal-
anced designs," J. Qual. Technol., 6, 146­153. Sclove, S. L. [1968], "Improved estimators for coefficients in linear regression," J. Am. Stat.
Assoc., 63, 596­606. Searle, S. R. [1971], Linear Models, Wiley, New York. Searle, S. R. and J. G. Udell [1970], "The use of regression on dummy variables in market
research," Manage. Sci. B, 16, 397­409. Seber, G. A. F. [1977], Linear Regression Analysis, Wiley, New York. Sebert, D. M., D. C. Montgomery, and D. A. Rollier [1998], "A clustering algorithm for
identifying multiple outliers in linear regression," Comput. Stat. Data Anal., 27, 461­ 484. Sielken, R. L., Jr. and H. O. Hartley [1973], "Two linear programming algorithms for unbiased estimation of linear models," J. Am. Stat. Assoc., 68, 639­641. Silvey, S. D. [1969], "Multicollinearity and imprecise estimation," J. R. Stat. Soc. Ser. B, 31, 539­552. Simpson, D. G., D. Ruppert, and R. J. Carroll [1992], "On one-step GM-estimates and stability of inference in linear regression," J. Am. Stat. Assoc., 87, 439­450. Simpson, J. R. and D. C. Montgomery [1996], "A biased-robust regression technique for the combined-outlier multicollinearity problem," J. Stat. Comput. Simul., 56, 1­22. Simpson, J. R. and D. C. Montgomery [1998a], "A robust regression technique using compound estimation," Nov. Res. Logistics, 45, 125­139. Simpson, J. R. and D. C. Montgomery [1998b], "The development and evaluation of alternative generalized M-estimation techniques," Commun. Stat. Simul. Comput., 27, 999­1018. Simpson, J. R. and D. C. Montgomery [1998c], "A performance-based assessment of robust regression methods," Commun. Stat. Simul. Comput., 27, 1031­1099.

640 REFERENCES
Smith, A. F. M. and M. Goldstein [1975], "Ridge regression: Some comments on a paper of Conniffe and Stone," The Statistician, 24, 61­66.
Smith, B. T., J. M. Boyle, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler [1974], Matrix Eigensystem Routines, Springer-Verlag, Berlin.
Smith, G. and F. Campbell [1980], "A critique of some ridge regression methods (with discussion)," J. Am. Stat. Assoc., 75, 74­103.
Smith, J. H. [1972], "Families of transformations for use in regression analysis," Am. Stat., 26(3), 59­61.
Smith, P. L. [1979], "Splines as a useful and convenient statistical tool," Am. Stat., 33(2), 57­62.
Smith, R. C. et al. [1992], "Ozone depletion: Ultraviolet radiation and phytoplankton biology in Antartic waters," Science, 255, 952­957.
Snee, R. D. [1973], "Some aspects of nonorthogonal data analysis, Part I. Developing prediction equations," J. Qual. Technol., 5, 67­79.
Snee, R. D. [1977], "Validation of regression models: Methods and examples," Technometrics, 19, 415­428.
Sprent, P. [1969], Models in Regression and Related Topics, Methuen, London. Sprent, P. and G. R. Dolby [1980], "The geometric mean functional relationship," Biometrics,
36, 547­550. Staudte, R. G. and S. J. Sheather [1990], Robust Estimation and Testing, Wiley, New York. Stefanski, W. [1991], "A note on high-breakdown estimators," Stat. Probab. Lett., 11,
353­358. Stefansky, W. [1971], "Rejecting outliers by maximum normed residual," Ann. Math. Stat., 42,
35­45. Stefansky, W. [1972], "Rejecting outliers in factorial designs," Technometrics, 14, 469­479. Stein, C. [1960], "Multiple regression," in I. Olkin (Ed.), Contributions to Probability and
Statistics: Essays in Honor of Harold Hotelling, Stanford Press, Stanford, Calif. Stewart, G. W. [1973], Introduction to Matrix Computations, Academic, New York. Stone, M. [1974], "Cross-validating choice and assessment of statistical predictions (with
discussion)," J. R. Stat. Soc. Ser. B, 36, 111­147. Stromberg, A. J. [1993], "Computation of high breakdown nonlinear regression parameters,"
J. Am. Stat. Assoc., 88, 237­244. Stromberg, A. J. and D. Ruppert [1989], "Breakdown in nonlinear regression," J. Am. Stat.
Assoc., 83, 991­997. Suich, R. and G. C. Derringer [1977], "Is the regression equation adequate--one criterion,"
Technometrics, 19, 213­216. Schwartz, G. [1978], "Estimating the dimension of a model," Ann. Stat., 6, 461­464. Theobald, C. M. [1974], "Generalizations of mean square error applied to ridge regression,"
J. R. Stat. Soc. Ser. B, 36, 103­106. Thompson, M. L. [1978a], "Selection of variables in multiple regression: Part I. A review and
evaluation," Int. Stat. Rev., 46, 1­19. Thompson, M. L. [1978b], "Selection of variables in multiple regression: Part II. Chosen
procedures, computations and examples," Int. Stat. Rev., 46, 129­146. Tucker, W. T. [1980], "The linear calibration problem revisited," presented at the ASQC Fall
Technical Conference, Cincinnati, Ohio. Tufte, E. R. [1974], Data Analysis for Politics and Policy, Prentice-Hall, Englewood Cliffs, N.J. Tukey, J. W. [1957], "On the comparative anatomy of transformations," Ann. Math. Stat., 28,
602­632.

REFERENCES 641
Wagner, H. M. [1959], "Linear programming techniques for regression analysis," J. Am. Stat. Assoc., 54, 206­212.
Wahba, G., G. H. Golub, and C. G. Health [1979], "Generalized cross-validation as a method for choosing a good ridge parameter," Technometrics, 21, 215­223.
Walker, E. [1984], Influence, Collinearity, and Robust Estimation in Regression, Ph.D. dissertation, Department of Statistics, Virginia Tech, Blacksburg.
Walker, E. and J. B. Birch [1988], "Influence measures in ridge regression," Technometrics, 30, 221­227.
Walls, R. E. and D. L. Weeks [1969], "A note on the variance of a predicted response in regression," Am. Stat., 23, 24­26.
Webster, J. T., R. F. Gunst, and R. L. Mason [1974], "Latent root regression analysis," Technometrics, 16, 513­522.
Weisberg, S. [1985], Applied Linear Regression, 2nd ed., Wiley, New York. Welsch, R. E. [1975], "Confidence regions for robust regression," in Statistical Computing
Section Proceedings of the American Statistical Association, Washington, D.C. Welsch, R. E. and S. C. Peters [1978],"Finding influential subsets of data in regression models,"
in A. R. Gallant and T. M. Gerig (Eds.), Proceedings of the Eleventh Interface Symposium on Computer Science and Statistics, Institute of Statistics, North Carolina State University, pp. 240­244. White, J. W. and R. F. Gunst [1979], "Latent root regression: Large sample analysis," Technometrics, 21, 481­488. Wichern, D.W., and G.A. Churchill [1978],"A comparison of ridge estimators," Technometrics, 20, 301­311. Wilcox, R. R. [1997], Introduction to Robust Estimation and Hypothesis Testing, Academic, New York. Wilde, D. J. and C. S. Beightler [1967], Foundations of Optimization, Prentice-Hall, Englewood Cliffs, N.J. Wilkinson, J. W. [1965], The Algebraic Eigenvalue Problem, Oxford University Press, London. Willan, A. R. and D. G. Watts [1978], "Meaningful multicollinearity measures," Technometrics, 20, 407­412. Williams, D. A. [1973], Letter to the Editor, Appl. Stat., 22, 407­408. Williams, E. J. [1969], "A note on regression methods in calibration," Technometrics, 11, 189­192. Wold, S. [1974], "Spline functions in data analysis," Technometrics, 16, 1­11. Wonnacott, R. J. and T. H. Wonnacott [1970], Econometrics, Wiley, New York. Wood, F. S. [1973], "The use of individual effects and residuals in fitting equations to data," Technometrics, 15, 677­695. Working, H. and H. Hotelling [1929], "Application of the theory of error to the interpretation of trends," J. Am. Stat. Assoc. Suppl. (Proc.), 24, 73­85. Wu, C. F. J. [1986], "Jackknife, bootstrap, and other resampling methods in regression analysis (with discussion)," Ann. Stat., 14, 1261­1350. Yale, C. and A. B. Forsythe [1976], "Winsorized regression," Technometrics, 18, 291­300. Yohai, V. J. [1987], "High breakdown-point and high efficiency robust estimates for regression," Ann. Stat., 15, 642­656. Younger, M. S. [1979], A Handbook for Linear Regression, Duxbury Press, North Scituate, Mass. Zellner, A. [1971], An Introduction to Bayesian Inference in Econometrics, Wiley, New York.

INDEX

Activation function in a neural network, 527
Adjusted R2, 87, 333 Akaike information criterion (AIC), 336 All possible regressions, 336, 342 Allocated codes and indicator variables, 273 Analysis of covariance, 272 Analysis of variance (ANOVA) in
regression, 25, 84 Analysis of variance identity for regression,
26 Analysis of variance model as a regression
model, 275 Artificial neural networks, 526 Assumptions in regression, 129 Asymptotic covariance matrix, 406 Asymptotic efficiency, 511 Asymptotic inference in nonlinear
regression, 409, 411 Asymptotically unbiased estimators, 52 Autocorrelation function, 475 Autocorrelation in regression data, 474 Average prediction variance, 531
Backpropagation, 528 Backward elimination, 346 Bayesian estimation, 312 Bernoulli random variable, 432

Best linear unbiased estimators, 19, 587 Biasing parameter in ridge regression,
306 BIC, 336 Binary response variable, 422. See also
Logistic regression Binomial distribution, 422, 608 Bivariate normal distribution, 53 Bonferroni method for confidence intervals,
102 Bootstrap confidence intervals, 519 Bootstrapping, 411, 517
cases, 518 residuals, 518 Box­Behnken design, 532 Box­Cox method, 182 Breakdown point, 510
Calibration problem, 513, 516 Candidate regressors, 328 Canonical link, 451 Categorical variables in regression, 260 Central composite design, 243, 532 Central limit theorem, 575 Chi-square distribution, definition, 575 Classical calibration estimator, 514 Classification and regression trees (CART),
524

Introduction to Linear Regression Analysis, Fifth Edition. Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining. © 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
642

INDEX 643

Cluster analysis to identify jointly influential points, 220
Cochrane­Orcutt method, 482 Coefficient of determination, 35.
See also R2 Collinearity, 7. See also Multicollinearity Comparing regression models, 272 Complimentary log-log link, 442,
452 Condition indices of X X, 298 Condition number of X X, 298 Confidence interval
in logistic regression, 437 on the mean response, 30, 46, 99 on model parameters, 29, 46, 98 Consistent estimators, 52 Cook's distance measure, 216, 600 Correlation coefficicent, 53 Correlation matrix of the parameter estimates, 80 of the regressors, 114, 292
as a multicollinearity diagnostic, 292, 294
COVRATIOi, 219 Cubic splines, 230, 231 Cuboidal design region, 533
Degrees of freedom in ANOVA, 26 Deleted residuals, 134 Deletion diagnostic, 217 Dependent variable, 2 Designed experiment, 5, 8, 71, 243, 275, 529,
530 Detecting multicollinearity, 292 Determinant of X X, 302 Deviance, 431, 525
residuals, 440, 458 DFBETASij, 217, 601 DFFITSi, 217, 600 D-optimal design, 530 Double exponential distribution, 502 Durbin­Watson test for autocorrelation,
475, 477
Efficiency of an estimator, 511 Eigenvalues of X X, 299 Empirical model, 3, 68 Empirical selection of a transformation, 172 Errors in the regressor variables, 511 Estimation of 2, 21 Exponential family, 421, 450, 605 Externally studentized residuals, 135

Extra sum of squares method, 89, 585 Extrapolation in regression, 33, 42, 107, 225 Extreme values, 130
Factorial design, 529 F-distribution (definition), 576 Finite-sample efficiency, 511 First-order autoregressive process, 475 Forward selection, 345 Fraction of design space plot, 536 Fractional increments, 408
Gauss­Markov theorem, 19, 587, 597 Generalized least squares, 188, 189 Generalized linear model, 421, 450 Generalized ridge regression, 313 Gompertz growth model, 412 Goodness of fit tests in logistic regression,
431, 432 G-optimal design, 531 Growth models, 412
Hat matrix in regression, 73, 110, 131, 212 Hessian matrix, 437 Hidden extrapolation in regression, 107 Hierarchy, 226, 395, 396 High-breakdown-point estimators, 510 Hosmer­Lemeshow goodness-of-fit test, 433
Idempotent matrix (definition), 577 Identity link, 445 Ill-conditioning, 226. See also
Multicollinearity Independent variable, 2 Indicator variables, 260, 268, 270, 173, 274,
275 Influence function, 504 Influential point, 132, 134, 211, 220 Integrated variance, 531 Interaction, 69 Intrinsically linear model, 176, 178, 398 Inverse estimation, 513, 516 I-optimal design, 532 Iteratively reweighted least squares (IRLS),
503, 605, 611
Joint confidence region on the model parameters, 101
Jointly influential points, 220
Lack of fit test in regression, 156, 159, 162 Lag one autocorrelation, 476

644 INDEX

Least absolute deviation (L1 norm) regression, 502
Least squares estimators, 14, 72, 73 normal equations, 14, 71, 72
Leverage point, 132, 211, 212, 213 Likelihood function, 51 Likelihood ratio tests in logistic regression,
430 Linear dependence of regressors, 117 Linear predictor, 423, 451 Linear regression model, 2, 389 Linearization of a nonlinear model, 400 Linearly independent regressors, 73 Link function, 445 Locally weighted regression (Loess), 237,
239 Log link, 445 Logistic growth model, 412 Logistic regression, 421, 423, 428, 430, 432,
433, 437, 440, 442, 444 Logistic response function, 423
Mallow's Cp, 334 Marquardt's compromise, 408 Matrix of scatter plots, 82 Maximum likelihood estimation
in the generalized linear model, 452, 454, 608
in logistic regression, 424, 601 in time series regression models, 485 Maximum likelihood estimators, 51, 83 Maximum likelihood score equations, 602 Mean shift outlier model, 594 Mechanistic model, 3, 411 M-estimators, 503, 509 Method of least squares, 13, 70, 395 Method of maximum likelihood, 51 Method of steepest decent, 407 Minimum variance estimators, 52 Mixed model, 195, 202 Model adequacy checking, 4, 15, 129, 130,
136, 139, 141, 142, 143, 149, 475 Model building, 91, 225, 304, 327, 338, 345,
346, 348, 351 Model independent estimate of error, 157,
160 Model misspecification, 329 Model-dependent estimate of error, 21, 81 Multicollinearity, 7, 117, 285, 292, 396
effects, 288 Multiple linear regression model, 4, 67

Near neighbors, 160 Neural networks, 526 Newton­Raphson method, 603 No-intercept regression model, 45 Nominal logistic regression, 442 Noncentral chi-square distribution
(definition), 576 Noncentral F distribution (definition), 576 Noncentral t distribution (definition), 575 Nonlinear least squares, 396 Nonlinear regression model, 389 Nonparametric regression, 236, 237 Nonsense relationships in regression, 44 Normal distribution, 22, 129, 136, 574, 606 Normal probability plot of residuals, 136
Observational study, 5, 8, 71 Odds ratio, 428, 429 Ordinal logistic regression, 444 Orthogonal design, 530 Orthogonal polynomials, 248 Orthogonal regressors, 93, 118 Orthonormal marix (definition), 578 Outliers, 7, 43, 139, 59 Overdispersion, 461 Overfitting, 88, 528
Partial deviance, 434 Partial F-test, 90 Partial regression
coefficients, 68, 115 plots, 143, 144 Partial residual plots, 143, 146 Pearson chi-square goodness-of-fit test, 432 Pearson residuals, 440 Piecewise polynomial models, 229, 234 Poisson distribution, 444, 607 Poisson regression, 444 Polynomial regression models, 223, 242 Polynomial and trigonometric terms, 235 Population regression model, 13 Positive definite and positive semidefinite
matrix, definition, 578 Power family link, 452 Power family transformations, 182 Prediction, 9, 33, 45, 104, 488
in time series regression models, 488 Prediction interval on a new observation,
33, 46, 104, 488, 491 Predictor variable, 2. See also Regressor
variable PRESS residuals, 134

INDEX 645

PRESS statistic, 151, 152, 337, 591 Principal component regression, 313, 315 Principal components, 314 Probit analysis, 442 Probit link, 452 Properties of ridge regression, 309 Pure error, 157
Quasi-likelihood, 462
R2, 35, 36, 46, 87, 332 for prediction, 151
Random regressor variable, 52 Rank of a matrix, definition, 577 Regression analysis, 1 Regression coefficients, 13 Regression models
with concurrent lines, 27 with parallel lines, 272 with random effects, 194 Regression or model sum of squares, 26, 84,
86 Regressor variable, 2, 52, 68, 73, 93, 118, 184,
260, 273, 274, 275, 285, 292, 328, 511 REML, 196, 200, 201 Residual analysis, 22, 130, 328 Residual mean square, 21, 80, 333 Residual plotting, 130, 136, 139, 141, 142,
143, 149, 475 Residual sum of squares, 20, 84, 86 Residuals, 73, 130 Response surface, 242
methodology, 242, 532 Response variable, 2, 68, 422, 444 Retrospective study, 5, 6 Ridge regression, 304, 307, 309, 312, 319 Ridge trace, 307 Robust estimation of parameters, 221. See
also Robust regression Robust regression, 500, 502, 503, 504, 510 Rotatability, 248, 533 R-student, 135
Sample correlation coefficient, 53 Sample regression model, 13 Scaled residuals, 130 Scatter diagram, 1, 82 Second-order model, 69, 242 Sherman­Morrison­Woodbury theorem, 590 Shrinkage estimators, 310 Significance of regression, 24, 25, 28, 84 Simple linear regression model, 2, 12

Simultaneous inference, 33, 100, 102, 103 Singular value decomposition, 299 Sources of multicollinearity in regression,
286 Spherical design region, 533 Splines, 229 Standard error of parameter estimates, 23 Standard error of regression, 21 Standardized Pearson residuals, 437 Standardized regression coefficients, 111,
115 Standardized residuals, 130 Starting values in nonlinear regression, 408 Statistical tests on residuals, 150 Stepwise regression, 345, 348, 350 Strength of a transformation, 172 Studentized residuals, 131 Sufficient statistics, 52
t-distribution (definition), 576 Testing the equality of regression
coefficients, 96 Testing the general linear hypothesis, 95 Tests based on deviance, 433 Time series data, 474 Trace of a matrix (definition), 578 Transformation of the regressor variables,
184, 187 Transformations, 139, 171, 176, 182, 184, 224,
397 to linearize a model, 397 t-tests on model parameters, 22, 28, 55, 88,
585
Unbiased estimators, 18, 52, 79, 81 Unit length scaling, 114, 117 Unit normal scaling, 113
Variable selection in regression, 88, 327, 338, 342, 346, 348, 351
Variance decomposition proportions, 300 Variance inflation factors, 118, 296 Variance stabilizing transformations, 172 Vector of increments, 401 V-optimal design, 531
Wald inference, 436 Weibull growth model, 412 Weighted least squares, 188, 190
estimator, 190, 191 Woodbury matrix identity, 590 Wrong signs for regression coefficients, 119

WILEY SERIES IN PROBABILITY AND STATISTICS established by Walter A. Shewhart and Samuel S. Wilks
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice, Harvey Goldstein, Iain M. Johnstone, Geert Molenberghs, David W. Scott, Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg Editors Emeriti: Vic Barnett, J. Stuart Hunter, Joseph B. Kadane, Jozef L. Teugels
The Wiley Series in Probability and Statistics is well established and authoritative. It covers many topics of current research interest in both pure and applied statistics and probability theory. Written by leading statisticians and institutions, the titles span both state-of-the-art developments in the field and classical methods.
Reflecting the wide range of current research in statistics, the series encompasses applied, methodological and theoretical statistics, ranging from applications and new techniques made possible by advances in computerized practice to rigorous treatment of theoretical approaches.
This series provides essential and invaluable reading for all statisticians, whether in aca- demia, industry, government, or research.
 ABRAHAM and LEDOLTER · Statistical Methods for Forecasting AGRESTI · Analysis of Ordinal Categorical Data, Second Edition AGRESTI · An Introduction to Categorical Data Analysis, Second Edition AGRESTI · Categorical Data Analysis, Second Edition ALTMAN, GILL, and McDONALD · Numerical Issues in Statistical Computing
for the Social Scientist AMARATUNGA and CABRERA · Exploration and Analysis of DNA
Microarray and Protein Array Data ANDL · Mathematics of Chance ANDERSON · An Introduction to Multivariate Statistical Analysis, Third
Edition * ANDERSON · The Statistical Analysis of Time Series ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG ·
Statistical Methods for Comparative Studies ANDERSON and LOYNES · The Teaching of Practical Statistics ARMITAGE and DAVID (editors) · Advances in Biometry ARNOLD, BALAKRISHNAN, and NAGARAJA · Records * ARTHANARI and DODGE · Mathematical Programming in Statistics * BAILEY · The Elements of Stochastic Processes with Applications to the
Natural Sciences BAJORSKI · Statistics for Imaging, Optics, and Photonics BALAKRISHNAN and KOUTRAS · Runs and Scans with Applications
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

BALAKRISHNAN and NG · Precedence-Type Tests and Applications BARNETT · Comparative Statistical Inference, Third Edition BARNETT · Environmental Statistics BARNETT and LEWIS · Outliers in Statistical Data, Third Edition BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ · Probability and Statistical
Inference BASILEVSKY · Statistical Factor Analysis and Related Methods: Theory and
Applications BASU and RIGDON · Statistical Methods for the Reliability of Repairable
Systems BATES and WATTS · Nonlinear Regression Analysis and Its Applications BECHHOFER, SANTNER, and GOLDSMAN · Design and Analysis of
Experiments for Statistical Selection, Screening, and Multiple Comparisons BEIRLANT, GOEGEBEUR, SEGERS, TEUGELS, and DE WAAL · Statistics
of Extremes: Theory and Applications BELSLEY · Conditioning Diagnostics: Collinearity and Weak Data in
Regression  BELSLEY, KUH, and WELSCH · Regression Diagnostics: Identifying
Influential Data and Sources of Collinearity BENDAT and PIERSOL · Random Data: Analysis and Measurement
Procedures, Fourth Edition BERNARDO and SMITH · Bayesian Theory BERRY, CHALONER, and GEWEKE · Bayesian Analysis in Statistics and
Econometrics: Essays in Honor of Arnold Zellner BHAT and MILLER · Elements of Applied Stochastic Processes, Third Edition BHATTACHARYA and WAYMIRE · Stochastic Processes with Applications BIEMER, GROVES, LYBERG, MATHIOWETZ, and SUDMAN ·
Measurement Errors in Surveys BILLINGSLEY · Convergence of Probability Measures, Second Edition BILLINGSLEY · Probability and Measure, Anniversary Edition BIRKES and DODGE · Alternative Methods of Regression BISGAARD and KULAHCI · Time Series Analysis and Forecasting by
Example BISWAS, DATTA, FINE, and SEGAL · Statistical Advances in the Biomedical
Sciences: Clinical Trials, Epidemiology, Survival Analysis, and Bioinformatics BLISCHKE AND MURTHY (editors) · Case Studies in Reliability and
Maintenance BLISCHKE AND MURTHY · Reliability: Modeling, Prediction, and
Optimization BLOOMFIELD · Fourier Analysis of Time Series: An Introduction, Second
Edition BOLLEN · Structural Equations with Latent Variables
 Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

BOLLEN and CURRAN · Latent Curve Models: A Structural Equation Perspective
BOROVKOV · Ergodicity and Stability of Stochastic Processes BOSQ and BLANKE · Inference and Prediction in Large Dimensions BOULEAU · Numerical Methods for Stochastic Processes BOX · Bayesian Inference in Statistical Analysis BOX · Improving Almost Anything, Revised Edition BOX · R. A. Fisher, the Life of a Scientist BOX and DRAPER · Empirical Model-Building and Response Surfaces * BOX and DRAPER · Evolutionary Operation: A Statistical Method for
Process Improvement BOX and DRAPER · Response Surfaces, Mixtures, and Ridge Analyses, Second
Edition BOX, HUNTER, and HUNTER · Statistics for Experimenters: Design,
Innovation, and Discovery, Second Editon BOX, JENKINS, and REINSEL · Time Series Analysis: Forcasting and Control,
Fourth Edition BOX, LUCEÑO, and PANIAGUA-QUIÑONES · Statistical Control by
Monitoring and Adjustment, Second Edition BRANDIMARTE · Numerical Methods in Finance: A MATLAB-Based
Introduction  BROWN and HOLLANDER · Statistics: A Biomedical Introduction BRUNNER, DOMHOF, and LANGER · Nonparametric Analysis of
Longitudinal Data in Factorial Experiments BUCKLEW · Large Deviation Techniques in Decision, Simulation, and
Estimation CAIROLI and DALANG · Sequential Stochastic Optimization CASTILLO, HADI, BALAKRISHNAN, and SARABIA · Extreme Value and
Related Models with Applications in Engineering and Science CHAN · Time Series: Applications to Finance with R and S-Plus®, Second Edition CHARALAMBIDES · Combinatorial Methods in Discrete Distributions CHATTERJEE and HADI · Regression Analysis by Example, Fourth Edition CHATTERJEE and HADI · Sensitivity Analysis in Linear Regression CHERNICK · Bootstrap Methods: A Guide for Practitioners and Researchers,
Second Edition CHERNICK and FRIIS · Introductory Biostatistics for the Health Sciences CHILÈS and DELFINER · Geostatistics: Modeling Spatial Uncertainty, Second
Edition CHOW and LIU · Design and Analysis of Clinical Trials: Concepts and
Methodologies, Second Edition CLARKE · Linear Models: The Theory and Application of Analysis of Variance
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

CLARKE and DISNEY · Probability and Random Processes: A First Course with Applications, Second Edition
* COCHRAN and COX · Experimental Designs, Second Edition COLLINS and LANZA · Latent Class and Latent Transition Analysis: With
Applications in the Social, Behavioral, and Health Sciences CONGDON · Applied Bayesian Modelling CONGDON · Bayesian Models for Categorical Data CONGDON · Bayesian Statistical Modelling, Second Edition CONOVER · Practical Nonparametric Statistics, Third Edition COOK · Regression Graphics COOK and WEISBERG · An Introduction to Regression Graphics COOK and WEISBERG · Applied Regression Including Computing and
Graphics CORNELL · A Primer on Experiments with Mixtures CORNELL · Experiments with Mixtures, Designs, Models, and the Analysis of
Mixture Data, Third Edition COVER and THOMAS · Elements of Information Theory COX · A Handbook of Introductory Statistical Methods * COX · Planning of Experiments CRESSIE · Statistics for Spatial Data, Revised Edition CRESSIE and WIKLE · Statistics for Spatio-Temporal Data CSÖRG and HORVÁTH · Limit Theorems in Change Point Analysis DANIEL · Applications of Statistics to Industrial Experimentation DANIEL · Biostatistics: A Foundation for Analysis in the Health Sciences,
Eighth Edition * DANIEL · Fitting Equations to Data: Computer Analysis of Multifactor Data,
Second Edition DASU and JOHNSON · Exploratory Data Mining and Data Cleaning DAVID and NAGARAJA · Order Statistics, Third Edition * DEGROOT, FIENBERG, and KADANE · Statistics and the Law DEL CASTILLO · Statistical Process Adjustment for Quality Control DEMARIS · Regression with Social Data: Modeling Continuous and Limited
Response Variables DEMIDENKO · Mixed Models: Theory and Applications DENISON, HOLMES, MALLICK and SMITH · Bayesian Methods for
Nonlinear Classification and Regression DETTE and STUDDEN · The Theory of Canonical Moments with Applications
in Statistics, Probability, and Analysis DEY and MUKERJEE · Fractional Factorial Plans DILLON and GOLDSTEIN · Multivariate Analysis: Methods and Applications DODGE · Alternative Methods of Regression
* Now available in a lower priced paperback edition in the Wiley Classics Library.

* DODGE and ROMIG · Sampling Inspection Tables, Second Edition * DOOB · Stochastic Processes DOWDY, WEARDEN, and CHILKO · Statistics for Research, Third Edition DRAPER and SMITH · Applied Regression Analysis, Third Edition DRYDEN and MARDIA · Statistical Shape Analysis DUDEWICZ and MISHRA · Modern Mathematical Statistics DUNN and CLARK · Basic Statistics: A Primer for the Biomedical Sciences,
Third Edition DUPUIS and ELLIS · A Weak Convergence Approach to the Theory of Large
Deviations EDLER and KITSOS · Recent Advances in Quantitative Methods in Cancer
and Human Health Risk Assessment * ELANDT-JOHNSON and JOHNSON · Survival Models and Data Analysis ENDERS · Applied Econometric Time Series  ETHIER and KURTZ · Markov Processes: Characterization and Convergence EVANS, HASTINGS, and PEACOCK · Statistical Distributions, Third Edition EVERITT · Cluster Analysis, Fifth Edition FELLER · An Introduction to Probability Theory and Its Applications, Volume
I, Third Edition, Revised; Volume II, Second Edition FISHER and VAN BELLE · Biostatistics: A Methodology for the Health
Sciences FITZMAURICE, LAIRD, and WARE · Applied Longitudinal Analysis, Second
Edition * FLEISS · The Design and Analysis of Clinical Experiments FLEISS · Statistical Methods for Rates and Proportions, Third Edition  FLEMING and HARRINGTON · Counting Processes and Survival Analysis FUJIKOSHI, ULYANOV, and SHIMIZU · Multivariate Statistics: High-
Dimensional and Large-Sample Approximations FULLER · Introduction to Statistical Time Series, Second Edition  FULLER · Measurement Error Models GALLANT · Nonlinear Statistical Models GEISSER · Modes of Parametric Statistical Inference GELMAN and MENG · Applied Bayesian Modeling and Causal Inference from
Incomplete-Data Perspectives GEWEKE · Contemporary Bayesian Econometrics and Statistics GHOSH, MUKHOPADHYAY, and SEN · Sequential Estimation GIESBRECHT and GUMPERTZ · Planning, Construction, and Statistical
Analysis of Comparative Experiments GIFI · Nonlinear Multivariate Analysis GIVENS and HOETING · Computational Statistics
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

GLASSERMAN and YAO · Monotone Structure in Discrete-Event Systems GNANADESIKAN · Methods for Statistical Data Analysis of Multivariate
Observations, Second Edition GOLDSTEIN · Multilevel Statistical Models, Fourth Edition GOLDSTEIN and LEWIS · Assessment: Problems, Development, and Statistical
Issues GOLDSTEIN and WOOFF · Bayes Linear Statistics GREENWOOD and NIKULIN · A Guide to Chi-Squared Testing GROSS, SHORTLE, THOMPSON, and HARRIS · Fundamentals of Queueing
Theory, Fourth Edition GROSS, SHORTLE, THOMPSON, and HARRIS · Solutions Manual to
Accompany Fundamentals of Queueing Theory, Fourth Edition * HAHN and SHAPIRO · Statistical Models in Engineering HAHN and MEEKER · Statistical Intervals: A Guide for Practitioners HALD · A History of Probability and Statistics and their Applications Before
1750 HALD · A History of Mathematical Statistics from 1750 to 1930  HAMPEL · Robust Statistics: The Approach Based on Influence Functions HANNAN and DEISTLER · The Statistical Theory of Linear Systems HARMAN and KULKARNI · An Elementary Introduction to Statistical
Learning Theory HARTUNG, KNAPP, and SINHA · Statistical Meta-Analysis with Applications HEIBERGER · Computation for the Analysis of Designed Experiments HEDAYAT and SINHA · Design and Inference in Finite Population Sampling HEDEKER and GIBBONS · Longitudinal Data Analysis HELLER · MACSYMA for Statisticians HERITIER, CANTONI, COPT, and VICTORIA-FESER · Robust Methods in
Biostatistics HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments,
Volume 1: Introduction to Experimental Design, Second Edition HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments,
Volume 2: Advanced Experimental Design HINKELMANN (editor) · Design and Analysis of Experiments, Volume 3:
Special Designs and Applications HOAGLIN, MOSTELLER, and TUKEY · Fundamentals of Exploratory
Analysis of Variance * HOAGLIN, MOSTELLER, and TUKEY · Exploring Data Tables, Trends and
Shapes * HOAGLIN, MOSTELLER, and TUKEY · Understanding Robust and
Exploratory Data Analysis
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

HOCHBERG and TAMHANE · Multiple Comparison Procedures HOCKING · Methods and Applications of Linear Models: Regression and the
Analysis of Variance, Second Edition HOEL · Introduction to Mathematical Statistics, Fifth Edition HOGG and KLUGMAN · Loss Distributions HOLLANDER and WOLFE · Nonparametric Statistical Methods, Second
Edition HOSMER and LEMESHOW · Applied Logistic Regression, Second Edition HOSMER, LEMESHOW, and MAY · Applied Survival Analysis: Regression
Modeling of Time-to-Event Data, Second Edition HUBER · Data Analysis: What Can Be Learned From the Past 50 Years HUBER · Robust Statistics  HUBER and RONCHETTI · Robust Statistics, Second Edition HUBERTY · Applied Discriminant Analysis, Second Edition HUBERTY and OLEJNIK · Applied MANOVA and Discriminant Analysis,
Second Edition HUITEMA · The Analysis of Covariance and Alternatives: Statistical Methods
for Experiments, Quasi-Experiments, and Single-Case Studies, Second Edition HUNT and KENNEDY · Financial Derivatives in Theory and Practice, Revised
Edition HURD and MIAMEE · Periodically Correlated Random Sequences: Spectral
Theory and Practice HUSKOVA, BERAN, and DUPAC · Collected Works of Jaroslav Hajek--with
Commentary HUZURBAZAR · Flowgraph Models for Multistate Time-to-Event Data IMAN and CONOVER · A Modern Approach to Statistics JACKMAN · Bayesian Analysis for the Social Sciences  JACKSON · A User's Guide to Principle Components JOHN · Statistical Methods in Engineering and Quality Assurance JOHNSON · Multivariate Statistical Simulation JOHNSON and BALAKRISHNAN · Advances in the Theory and Practice of
Statistics: A Volume in Honor of Samuel Kotz JOHNSON and BHATTACHARYYA · Statistics: Principles and Methods, Fifth
Edition JOHNSON, KEMP, and KOTZ · Univariate Discrete Distributions, Third Edition JOHNSON and KOTZ · Distributions in Statistics JOHNSON and KOTZ (editors) · Leading Personalities in Statistical Sciences:
From the Seventeenth Century to the Present JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate
Distributions, Volume 1, Second Edition
 Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions, Volume 2, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Discrete Multivariate Distributions
JUDGE, GRIFFITHS, HILL, LÜTKEPOHL, and LEE · The Theory and Practice of Econometrics, Second Edition
JUREC KOVÁ and SEN · Robust Statistical Procedures: Aymptotics and Interrelations
JUREK and MASON · Operator-Limit Distributions in Probability Theory KADANE · Bayesian Methods and Ethics in a Clinical Trial Design KADANE AND SCHUM · A Probabilistic Analysis of the Sacco and Vanzetti
Evidence KALBFLEISCH and PRENTICE · The Statistical Analysis of Failure Time
Data, Second Edition KARIYA and KURATA · Generalized Least Squares KASS and VOS · Geometrical Foundations of Asymptotic Inference  KAUFMAN and ROUSSEEUW · Finding Groups in Data: An Introduction to
Cluster Analysis KEDEM and FOKIANOS · Regression Models for Time Series Analysis KENDALL, BARDEN, CARNE, and LE · Shape and Shape Theory KHURI · Advanced Calculus with Applications in Statistics, Second Edition KHURI, MATHEW, and SINHA · Statistical Tests for Mixed Linear Models * KISH · Statistical Design for Research KLEIBER and KOTZ · Statistical Size Distributions in Economics and Actuarial
Sciences KLEMELÄ · Smoothing of Multivariate Data: Density Estimation and
Visualization KLUGMAN, PANJER, and WILLMOT · Loss Models: From Data to Decisions,
Third Edition KLUGMAN, PANJER, and WILLMOT · Solutions Manual to Accompany Loss
Models: From Data to Decisions, Third Edition KOSKI and NOBLE · Bayesian Networks: An Introduction KOTZ, BALAKRISHNAN, and JOHNSON · Continuous Multivariate
Distributions, Volume 1, Second Edition KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Volumes
1 to 9 with Index KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences:
Supplement Volume KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences:
Update Volume 1
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume 2
KOVALENKO, KUZNETZOV, and PEGG · Mathematical Theory of Reliability of Time-Dependent Systems with Practical Applications
KOWALSKI and TU · Modern Applied U-Statistics KRISHNAMOORTHY and MATHEW · Statistical Tolerance Regions: Theory,
Applications, and Computation KROESE, TAIMRE, and BOTEV · Handbook of Monte Carlo Methods KROONENBERG · Applied Multiway Data Analysis KULINSKAYA, MORGENTHALER, and STAUDTE · Meta Analysis: A
Guide to Calibrating and Combining Statistical Evidence KUROWICKA and COOKE · Uncertainty Analysis with High Dimensional
Dependence Modelling KVAM and VIDAKOVIC · Nonparametric Statistics with Applications to
Science and Engineering LACHIN · Biostatistical Methods: The Assessment of Relative Risks, Second
Edition LAD · Operational Subjective Statistical Methods: A Mathematical,
Philosophical, and Historical Introduction LAMPERTI · Probability: A Survey of the Mathematical Theory, Second Edition LANGE, RYAN, BILLARD, BRILLINGER, CONQUEST, and
GREENHOUSE · Case Studies in Biometry LARSON · Introduction to Probability Theory and Statistical Inference, Third
Edition LAWLESS · Statistical Models and Methods for Lifetime Data, Second Edition LAWSON · Statistical Methods in Spatial Epidemiology, Second Edition LE · Applied Categorical Data Analysis LE · Applied Survival Analysis LEE · Structural Equation Modeling: A Bayesian Approach LEE and WANG · Statistical Methods for Survival Data Analysis, Third Edition LEPAGE and BILLARD · Exploring the Limits of Bootstrap LEYLAND and GOLDSTEIN (editors) · Multilevel Modelling of Health
Statistics LIAO · Statistical Group Comparison LINDVALL · Lectures on the Coupling Method LIN · Introductory Stochastic Analysis for Finance and Insurance LINHART and ZUCCHINI · Model Selection LITTLE and RUBIN · Statistical Analysis with Missing Data, Second Edition LLOYD · The Statistical Analysis of Categorical Data LOWEN and TEICH · Fractal-Based Point Processes MAGNUS and NEUDECKER · Matrix Differential Calculus with Applications
in Statistics and Econometrics, Revised Edition MALLER and ZHOU · Survival Analysis with Long Term Survivors

MALLOWS · Design, Data, and Analysis by Some Friends of Cuthbert Daniel MANN, SCHAFER, and SINGPURWALLA · Methods for Statistical Analysis
of Reliability and Life Data MANTON, WOODBURY, and TOLLEY · Statistical Applications Using Fuzzy
Sets MARCHETTE · Random Graphs for Statistical Pattern Recognition MARDIA and JUPP · Directional Statistics MARKOVICH · Nonparametric Analysis of Univariate Heavy-Tailed Data:
Research and Practice MARONNA, MARTIN and YOHAI · Robust Statistics: Theory and Methods MASON, GUNST, and HESS · Statistical Design and Analysis of Experiments
with Applications to Engineering and Science, Second Edition McCULLOCH, SEARLE, and NEUHAUS · Generalized, Linear, and Mixed
Models, Second Edition McFADDEN · Management of Data in Clinical Trials, Second Edition * McLACHLAN · Discriminant Analysis and Statistical Pattern Recognition McLACHLAN, DO, and AMBROISE · Analyzing Microarray Gene Expression
Data McLACHLAN and KRISHNAN · The EM Algorithm and Extensions, Second
Edition McLACHLAN and PEEL · Finite Mixture Models McNEIL · Epidemiological Research Methods MEEKER and ESCOBAR · Statistical Methods for Reliability Data MEERSCHAERT and SCHEFFLER · Limit Distributions for Sums of
Independent Random Vectors: Heavy Tails in Theory and Practice MENGERSEN, ROBERT, and TITTERINGTON · Mixtures: Estimation and
Applications MICKEY, DUNN, and CLARK · Applied Statistics: Analysis of Variance and
Regression, Third Edition * MILLER · Survival Analysis, Second Edition MONTGOMERY, JENNINGS, and KULAHCI · Introduction to Time Series
Analysis and Forecasting MONTGOMERY, PECK, and VINING · Introduction to Linear Regression
Analysis, Fifth Edition MORGENTHALER and TUKEY · Configural Polysampling: A Route to
Practical Robustness MUIRHEAD · Aspects of Multivariate Statistical Theory MULLER and STOYAN · Comparison Methods for Stochastic Models and
Risks
* Now available in a lower priced paperback edition in the Wiley Classics Library.

MURRAY · X-STAT 2.0 Statistical Experimentation, Design Data Analysis, and Nonlinear Optimization
MURTHY, XIE, and JIANG · Weibull Models MYERS, MONTGOMERY, and ANDERSON-COOK · Response Surface
Methodology: Process and Product Optimization Using Designed Experiments, Third Edition MYERS, MONTGOMERY, VINING, and ROBINSON · Generalized Linear Models. With Applications in Engineering and the Sciences, Second Edition  NELSON · Accelerated Testing, Statistical Models, Test Plans, and Data Analyses  NELSON · Applied Life Data Analysis NEWMAN · Biostatistical Methods in Epidemiology OCHI · Applied Probability and Stochastic Processes in Engineering and Physical Sciences OKABE, BOOTS, SUGIHARA, and CHIU · Spatial Tesselations: Concepts and Applications of Voronoi Diagrams, Second Edition OLIVER and SMITH · Influence Diagrams, Belief Nets and Decision Analysis PALTA · Quantitative Methods in Population Health: Extensions of Ordinary Regressions PANJER · Operational Risk: Modeling and Analytics PANKRATZ · Forecasting with Dynamic Regression Models PANKRATZ · Forecasting with Univariate Box-Jenkins Models: Concepts and Cases PARDOUX · Markov Processes and Applications: Algorithms, Networks, Genome and Finance * PARZEN · Modern Probability Theory and Its Applications PEÑA, TIAO, and TSAY · A Course in Time Series Analysis PIANTADOSI · Clinical Trials: A Methodologic Perspective PORT · Theoretical Probability for Applications POURAHMADI · Foundations of Time Series Analysis and Prediction Theory POWELL · Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition POWELL and RYZHOV · Optimal Learning PRESS · Bayesian Statistics: Principles, Models, and Applications PRESS · Subjective and Objective Bayesian Statistics, Second Edition PRESS and TANUR · The Subjectivity of Scientists and the Bayesian Approach PUKELSHEIM · Optimal Experimental Design PURI, VILAPLANA, and WERTZ · New Perspectives in Theoretical and Applied Statistics  PUTERMAN · Markov Decision Processes: Discrete Stochastic Dynamic Programming
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

QIU · Image Processing and Jump Regression Analysis * RAO · Linear Statistical Inference and Its Applications, Second Edition RAO · Statistical Inference for Fractional Diffusion Processes RAUSAND and HØYLAND · System Reliability Theory: Models, Statistical
Methods, and Applications, Second Edition RAYNER · Smooth Tests of Goodnes of Fit: Using R, Second Edition RENCHER · Linear Models in Statistics RENCHER · Methods of Multivariate Analysis, Second Edition RENCHER · Multivariate Statistical Inference with Applications * RIPLEY · Spatial Statistics * RIPLEY · Stochastic Simulation ROBINSON · Practical Strategies for Experimenting ROHATGI and SALEH · An Introduction to Probability and Statistics, Second
Edition ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS · Stochastic Processes for
Insurance and Finance ROSENBERGER and LACHIN · Randomization in Clinical Trials: Theory and
Practice ROSS · Introduction to Probability and Statistics for Engineers and Scientists ROSSI, ALLENBY, and MCCULLOCH · Bayesian Statistics and Marketing  ROUSSEEUW and LEROY · Robust Regression and Outlier Detection ROYSTON and SAUERBREI · Multivariate Model Building: A Pragmatic
Approach to Regression Analysis Based on Fractional Polynomials for Modeling Continuous Variables * RUBIN · Multiple Imputation for Nonresponse in Surveys RUBINSTEIN and KROESE · Simulation and the Monte Carlo Method, Second Edition RUBINSTEIN and MELAMED · Modern Simulation and Modeling RYAN · Modern Engineering Statistics RYAN · Modern Experimental Design RYAN · Modern Regression Methods, Second Edition RYAN · Statistical Methods for Quality Improvement, Third Edition SALEH · Theory of Preliminary Test and Stein-Type Estimation with Applications SALTELLI, CHAN, and SCOTT (editors) · Sensitivity Analysis * SCHEFFE · The Analysis of Variance SCHIMEK · Smoothing and Regression: Approaches, Computation, and Application SCHOTT · Matrix Analysis for Statistics, Second Edition
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

SCHOUTENS · Levy Processes in Finance: Pricing Financial Derivatives SCHUSS · Theory and Applications of Stochastic Differential Equations SCOTT · Multivariate Density Estimation: Theory, Practice, and Visualization * SEARLE · Linear Models  SEARLE · Linear Models for Unbalanced Data  SEARLE · Matrix Algebra Useful for Statistics  SEARLE, CASELLA, and McCULLOCH · Variance Components SEARLE and WILLETT · Matrix Algebra for Applied Economics SEBER · A Matrix Handbook For Statisticians  SEBER · Multivariate Observations SEBER and LEE · Linear Regression Analysis, Second Edition  SEBER and WILD · Nonlinear Regression SENNOTT · Stochastic Dynamic Programming and the Control of Queueing
Systems * SERFLING · Approximation Theorems of Mathematical Statistics SHAFER and VOVK · Probability and Finance: It's Only a Game! SHERMAN · Spatial Statistics and Spatio-Temporal Data: Covariance Functions
and Directional Properties SILVAPULLE and SEN · Constrained Statistical Inference: Inequality, Order,
and Shape Restrictions SINGPURWALLA · Reliability and Risk: A Bayesian Perspective SMALL and MCLEISH · Hilbert Space Methods in Probability and Statistical
Inference SRIVASTAVA · Methods of Multivariate Statistics STAPLETON · Linear Statistical Models, Second Edition STAPLETON · Models for Probability and Statistical Inference: Theory and
Applications STAUDTE and SHEATHER · Robust Estimation and Testing STOYAN, KENDALL, and MECKE · Stochastic Geometry and Its Applications,
Second Edition STOYAN and STOYAN · Fractals, Random Shapes and Point Fields: Methods of
Geometrical Statistics STREET and BURGESS · The Construction of Optimal Stated Choice
Experiments: Theory and Methods STYAN · The Collected Papers of T. W. Anderson: 1943­1985 SUTTON, ABRAMS, JONES, SHELDON, and SONG · Methods for Meta-
Analysis in Medical Research TAKEZAWA · Introduction to Nonparametric Regression TAMHANE · Statistical Analysis of Designed Experiments: Theory and
Applications
* Now available in a lower priced paperback edition in the Wiley Classics Library. Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

TANAKA · Time Series Analysis: Nonstationary and Noninvertible Distribution Theory
THOMPSON · Empirical Model Building: Data, Models, and Reality, Second Edition
THOMPSON · Sampling, Third Edition THOMPSON · Simulation: A Modeler's Approach THOMPSON and SEBER · Adaptive Sampling THOMPSON, WILLIAMS, and FINDLAY · Models for Investors in Real World
Markets TIAO, BISGAARD, HILL, PEÑA, and STIGLER (editors) · Box on Quality
and Discovery: with Design, Control, and Robustness TIERNEY · LISP-STAT: An Object-Oriented Environment for Statistical
Computing and Dynamic Graphics TSAY · Analysis of Financial Time Series, Third Edition UPTON and FINGLETON · Spatial Data Analysis by Example, Volume II:
Categorical and Directional Data  VAN BELLE · Statistical Rules of Thumb, Second Edition VAN BELLE, FISHER, HEAGERTY, and LUMLEY · Biostatistics: A
Methodology for the Health Sciences, Second Edition VESTRUP · The Theory of Measures and Integration VIDAKOVIC · Statistical Modeling by Wavelets VINOD and REAGLE · Preparing for the Worst: Incorporating Downside Risk
in Stock Market Investments WALLER and GOTWAY · Applied Spatial Statistics for Public Health Data WEERAHANDI · Generalized Inference in Repeated Measures: Exact Methods
in MANOVA and Mixed Models WEISBERG · Applied Linear Regression, Third Edition WEISBERG · Bias and Causation: Models and Judgment for Valid Comparisons WELSH · Aspects of Statistical Inference WESTFALL and YOUNG · Resampling-Based Multiple Testing: Examples and
Methods for p-Value Adjustment * WHITTAKER · Graphical Models in Applied Multivariate Statistics WINKER · Optimization Heuristics in Economics: Applications of Threshold
Accepting WONNACOTT and WONNACOTT · Econometrics, Second Edition WOODING · Planning Pharmaceutical Clinical Trials: Basic Statistical Principles WOODWORTH · Biostatistics: A Bayesian Introduction WOOLSON and CLARKE · Statistical Methods for the Analysis of Biomedical
Data, Second Edition
* Now available in a lower priced paperback edition in the Wiley Classics Library.  Now available in a lower priced paperback edition in the Wiley­Interscience Paperback Series.

WU and HAMADA · Experiments: Planning, Analysis, and Parameter Design Optimization, Second Edition
WU and ZHANG · Nonparametric Regression Methods for Longitudinal Data Analysis
YANG · The Construction Theory of Denumerable Markov Processes YIN · Clinical Trial Design: Bayesian and Frequentist Adaptive Methods YOUNG, VALERO-MORA, and FRIENDLY · Visual Statistics: Seeing Data
with Dynamic Interactive Graphics ZACKS · Stage-Wise Adaptive Designs * ZELLNER · An Introduction to Bayesian Inference in Econometrics ZELTERMAN · Discrete Distributions--Applications in the Health Sciences ZHOU, OBUCHOWSKI, and MCCLISH · Statistical Methods in Diagnostic Medicine,
Second Edition
* Now available in a lower priced paperback edition in the Wiley Classics Library.

